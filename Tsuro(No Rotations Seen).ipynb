{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3137c4e",
   "metadata": {},
   "source": [
    "The Tile images used in the project were created by Matthias Felleisen and found on his website\n",
    "https://felleisen.org/matthias/4500-f19/tiles.html\n",
    "\n",
    "1. Filename: Tsuro(No Rotations Seen)\n",
    "2. Version number: 1\n",
    "3. Creation date: 4th October 2022\n",
    "4. Last modification date: 27th April 2023\n",
    "5. Authorâ€™s name: Joseph Henry\n",
    "6. Purpose of the program: Define a reinforcement learning environment based on the board game Tsuro along with an agent to train using the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bcaa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (2.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_agents==0.15.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (9.3.0)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.2.1)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (4.4.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.14.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.3.0)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.5.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym<=0.23.0,>=0.17.0->tf_agents==0.15.0) (0.0.8)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.1.8)\n",
      "Requirement already satisfied: decorator in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\horridjoe\\opencv\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\horridjoe\\opencv\\lib\\site-packages (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install tf_agents==0.15.0\n",
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4929ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# player piece colors [player1, player2]\n",
    "colors = ['#FF0000', '#0000FF']\n",
    "\n",
    "# paths for each tile 1-35\n",
    "node_combinations = [ \n",
    "    [(0,3), (1,5), (2,6), (4,7)], [(0,4), (1,5), (2,6), (3,7)], [(0,3), (1,6), (2,5), (4,7)], [(0,6), (1,5), (2,4), (3,7)],\n",
    "    [(0,1), (2,3), (4,5), (6,7)], [(0,4), (1,5), (2,3), (6,7)], [(0,6), (1,5), (2,3), (4,7)], [(0,5), (1,4), (2,7), (3,6)],\n",
    "    [(0,5), (1,4), (2,6), (3,7)], [(0,3), (1,4), (2,5), (6,7)], [(0,6), (1,4), (2,5), (3,7)], [(0,5), (1,4), (2,3), (6,7)],\n",
    "    [(0,2), (1,3), (4,6), (5,7)], [(0,2), (1,3), (4,5), (6,7)], [(0,5), (1,3), (2,7), (4,6)], [(0,6), (1,3), (2,7), (4,5)],\n",
    "    [(0,4), (1,3), (2,6), (5,7)], [(0,5), (1,3), (2,6), (4,7)], [(0,4), (1,3), (2,5), (6,7)], [(0,6), (1,3), (2,5), (4,7)],\n",
    "    [(0,5), (1,3), (2,4), (6,7)], [(0,6), (1,3), (2,4), (5,7)], [(0,3), (1,2), (4,7), (5,6)], [(0,3), (1,2), (4,6), (5,7)],\n",
    "    [(0,3), (1,2), (4,5), (6,7)], [(0,4), (1,2), (3,7), (5,6)], [(0,5), (1,2), (3,7), (4,6)], [(0,6), (1,2), (3,7), (4,5)],\n",
    "    [(0,4), (1,2), (3,6), (5,7)], [(0,5), (1,2), (3,6), (4,7)], [(0,4), (1,2), (3,5), (6,7)], [(0,6), (1,2), (3,5), (4,7)],\n",
    "    [(0,5), (1,2), (3,4), (6,7)], [(0,6), (1,2), (3,4), (5,7)], [(0,7), (1,2), (3,4), (5,6)]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7550d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class modelling the tile cards used within Tsuro\n",
    "class Tile():\n",
    "    def __init__(self, tile_num, tile_connections):\n",
    "        self.tile_num = tile_num\n",
    "        self.image = pygame.image.load(\"TsuroImages/\" + str(tile_num) + \".png\")\n",
    "        self.image = pygame.transform.scale(self.image, (100, 100))\n",
    "        self.tile_connections = tile_connections\n",
    "        self.rotation = 1\n",
    "    # decide where the player should move to\n",
    "    def move(self, current_node):\n",
    "        next_node = 0\n",
    "        next_player_tile = 0\n",
    "        for connection in self.tile_connections:\n",
    "            if current_node in connection:\n",
    "                n1, n2 = connection\n",
    "                if n1 == current_node:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n2)\n",
    "                else:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n1)\n",
    "                return next_node, next_player_tile, next_x, next_y\n",
    "        raise Exception(\"Issue in moving players\")\n",
    "    \n",
    "    # update number of times rotation should be applied to connections and image\n",
    "    def rotate_tile(self, rotate):\n",
    "        self.image = pygame.transform.rotate(self.image, rotate * -90)\n",
    "        self.tile_connections = [tuple((element + (2 * rotate)) % 8 for element in couple ) for couple in self.tile_connections]\n",
    "        self.rotation = 1 if (self.rotation + 1 % 4 == 0) else self.rotation + 1\n",
    "    \n",
    "    # get the current rotation value of the tile\n",
    "    def get_rotation(self):\n",
    "        return self.rotation\n",
    "    \n",
    "    # decide which tile and node the player should move to from its current position on this tile\n",
    "    def new_tile_node(self, current_node):\n",
    "        next_node = 0\n",
    "        next_x = 0\n",
    "        next_y = 0\n",
    "        next_player_tile = 0\n",
    "        match current_node:\n",
    "            case 0:\n",
    "                next_node = 3\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case 1:\n",
    "                next_node = 6\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 2:\n",
    "                next_node = 5\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 3:\n",
    "                next_node = 0\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 4:\n",
    "                next_node = 7\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 5:\n",
    "                next_node = 2\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 6:\n",
    "                next_node = 1\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 7:\n",
    "                next_node = 4\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case _:\n",
    "                raise Exception(\"Issue in tile board\")\n",
    "                \n",
    "        return next_node, next_player_tile, next_x, next_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f8391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to model the board game Tsuro as a reinforcement learning environment.\n",
    "class TsuroEnv(gym.Env):\n",
    "    # initialise the environment\n",
    "    def __init__(self):\n",
    "        self.current_player = 1\n",
    "        self.num_tiles = 35\n",
    "        self.tile_board_size = (6, 6)\n",
    "        self.player_board_size = (36,8)\n",
    "        self.num_players = 2\n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "\n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "        \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "\n",
    "        self.action_space = spaces.Discrete(11)\n",
    "        self.observation_space = spaces.Box(low=-1, high=35, shape=(327,))\n",
    "        \n",
    "    # Resets the environment to default state\n",
    "    def reset(self): \n",
    "        self.current_player = 1\n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "        \n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "            \n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "            \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        #########################################\n",
    "        #TODO: TESTING STUFF TO BE REMOVED LATER#\n",
    "        #########################################\n",
    "        for i in range(self.num_players):\n",
    "            self.player_board[random.randint(0,5)][i+1] = i+1\n",
    "            \n",
    "        initial_obs = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.tile_board.flatten()))\n",
    "\n",
    "        return initial_obs\n",
    "    \n",
    "    # Makes a move in the game based on inputs from player or AI\n",
    "    def step(self, action = -2, move = -1):\n",
    "        if move == -1:\n",
    "            card, rotate = self.get_card(action)\n",
    "        else:\n",
    "            card = move\n",
    "            rotate = 0\n",
    "        \n",
    "        action = self.player_tiles[self.current_player - 1][card]\n",
    "        \n",
    "        if action == -1:\n",
    "            observation = np.hstack((self.player_tiles[self.current_player - 1], \n",
    "                                     self.player_board.flatten(), self.tile_board.flatten()))\n",
    "            reward = -1\n",
    "            done = 0\n",
    "            return observation, reward, done, {}\n",
    "            \n",
    "        # Removes used tile and adds new tile from deck to hand\n",
    "        self.player_tiles[self.current_player-1].remove(action)\n",
    "        if len(self.remaining_tiles) > 0:\n",
    "            self.player_tiles[self.current_player-1].append(self.remaining_tiles.pop())\n",
    "        else:\n",
    "            self.player_tiles[self.current_player-1].append(-1)\n",
    "        \n",
    "        # Rotates tile (Only used by AI)\n",
    "        self.tiles[action].rotate_tile(rotate)\n",
    "        \n",
    "        reward = 0\n",
    "        self.place_tile(action+1)\n",
    "        self.move_players()\n",
    "        reward = self.reward_function()\n",
    "        if self.game_is_over():\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        self.current_player = self.next_player()\n",
    "        observation = np.hstack((self.player_tiles[self.current_player - 1],\n",
    "                                 self.player_board.flatten(), self.tile_board.flatten()))\n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    # Decides if the game is over\n",
    "    def game_is_over(self):\n",
    "        if len(self.remaining_players) <= 1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Decides the reward (Only used for AI)\n",
    "    def reward_function(self):\n",
    "        if not self.game_is_over():\n",
    "            return 1\n",
    "        if self.game_is_over() and self.current_player in self.remaining_players:\n",
    "            return 2\n",
    "        return -1\n",
    "    \n",
    "    # Places tile in self.tile_board\n",
    "    def place_tile(self, tile):\n",
    "        tile_number, node_number = np.where(self.player_board == self.current_player)\n",
    "        x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "        self.tile_board[x][y] += tile\n",
    "    \n",
    "    # Moves player piece in self.player_board\n",
    "    def move_players(self):\n",
    "        for player in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == player)\n",
    "            x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "            x = x[0]\n",
    "            y = y[0]\n",
    "            while self.tile_board[x][y] != 0:\n",
    "                tile = self.tiles[(self.tile_board[x][y])-1]\n",
    "                next_node, next_player_tile, next_x, next_y = tile.move(node_number)\n",
    "                self.player_board[tile_number[0]][node_number[0]] = 0\n",
    "                if ((tile_number[0] % 6 == 0) and ((tile_number[0] + next_player_tile) % 6 == 5)) or (tile_number[0] + next_player_tile < 0) or (tile_number[0] + next_player_tile > 35) or ((tile_number[0] % 6 == 5) and ((tile_number[0] + next_player_tile) % 6 == 0)):\n",
    "                    self.remaining_players.remove(player)\n",
    "                    break\n",
    "                else:\n",
    "                    self.player_board[tile_number[0] + next_player_tile][next_node] = player\n",
    "                    x += next_x\n",
    "                    y += next_y\n",
    "                    tile_number, node_number = np.where(self.player_board == player)\n",
    "    \n",
    "    # Quotient and Remainder\n",
    "    def euclidean_division(self, x, y = 6):\n",
    "        return x % y, x // y\n",
    "    \n",
    "    # Action (card, rotation) from input\n",
    "    def get_card(self, x, y = 4):\n",
    "        return  x // y, x % y\n",
    "    \n",
    "    # Gets the current state of the environment\n",
    "    def get_state(self):\n",
    "        observation = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.tile_board.flatten()))\n",
    "        return observation\n",
    "\n",
    "    # Decide whos turn it is\n",
    "    def next_player(self):\n",
    "        if len(self.remaining_players) == 0:\n",
    "            return -1\n",
    "        if self.current_player not in self.remaining_players:\n",
    "            for player in self.remaining_players:\n",
    "                if player > self.current_player:\n",
    "                    return player\n",
    "                else:\n",
    "                    return self.remaining_players[0]\n",
    "        return self.remaining_players[(self.remaining_players.index(self.current_player) + 1) % len(self.remaining_players)]\n",
    "        \n",
    "    # Render the environment\n",
    "    def render(self, mode):\n",
    "        screen = pygame.display.set_mode((650, 750))\n",
    "        screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the game board\n",
    "        board = pygame.image.load(\"TsuroImages/board.png\")\n",
    "        board = pygame.transform.scale(board, (600, 600))\n",
    "        screen.blit(board, (25,25))\n",
    "        \n",
    "        # Draw current players hand\n",
    "        for i in range (len(self.player_tiles[self.current_player-1])):\n",
    "            tile = self.player_tiles[self.current_player-1][i]\n",
    "            screen.blit(self.tiles[tile].image, (75 + (i * 200), 635))\n",
    "        \n",
    "        # Draw the tiles on the board\n",
    "        for x in range(self.tile_board_size[0]):\n",
    "            for y in range(self.tile_board_size[1]):\n",
    "                val = self.tile_board[x][y]\n",
    "                if val != 0:\n",
    "                    tile = self.tiles[val-1]\n",
    "                    screen.blit(tile.image, (25 + x * 100, 25 + y * 100))\n",
    "                    \n",
    "        # Draw the players' pieces on the board\n",
    "        for i in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == i)\n",
    "            y_add = 0\n",
    "            x_add = 0\n",
    "            y_mult = 0\n",
    "            x_mult = 0\n",
    "            \n",
    "            match node_number[0]:\n",
    "                case 0:\n",
    "                    y_add = 35\n",
    "                case 1:\n",
    "                    x_add = 35\n",
    "                case 2:\n",
    "                    x_add = 70\n",
    "                case 3:\n",
    "                    x_add = 100\n",
    "                    y_add = 35\n",
    "                case 4:\n",
    "                    x_add = 100\n",
    "                    y_add = 70\n",
    "                case 5:\n",
    "                    x_add = 70\n",
    "                    y_add = 100\n",
    "                case 6:\n",
    "                    x_add = 35\n",
    "                    y_add = 100\n",
    "                case 7:\n",
    "                     y_add = 70\n",
    "                case _:\n",
    "                    raise Exception(\"Issue in drawing the player board\")\n",
    "                    \n",
    "            if tile_number[0] != 0:\n",
    "                x_mult, y_mult = TsuroEnv.euclidean_division(self, tile_number[0])\n",
    "            \n",
    "            pygame.draw.circle(screen, colors[i-1], (25 + x_add + (100 * x_mult), 25 + y_add + (100 * y_mult)), 5)\n",
    "            \n",
    "        # Draw text to show who won when game is over\n",
    "        if self.game_is_over() or self.current_player == -1:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            text = font.render('Player ' + str(self.current_player) + ' wins', True, '#00FF00')\n",
    "            textRect = text.get_rect()\n",
    "            textRect.center = (650 // 2, 750 // 2)\n",
    "            screen.blit(text, textRect)\n",
    "            \n",
    "        pygame.display.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8016cc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2049dbccd90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training hyper parameters\n",
    "num_iterations = 25000000\n",
    "initial_collect_steps = 5000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "log_interval = 200\n",
    "num_eval_episodes = 20\n",
    "eval_interval = 5000\n",
    "\n",
    "# Register the environment through gym\n",
    "register(\n",
    "    id='TsuroEnvBasic',\n",
    "    entry_point=TsuroEnv,\n",
    ")\n",
    "\n",
    "env_name = \"TsuroEnvBasic\"\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "fc_layer_params = (200, 100)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(num_units, activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(num_actions, activation=None, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03), bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step)\n",
    "    # commenting out render will make training quicker\n",
    "    # environment.render(\"human\")\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=\"Checkpoints2/\",\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step_counter\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a4868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "returns = []\n",
    "\n",
    "with open(\"Checkpoints2/returns.txt\", \"r\") as txt:\n",
    "    for line in txt:\n",
    "        returns.append(line)\n",
    "        \n",
    "for i in range(len(returns)):\n",
    "    returns[i] = returns[i].strip()\n",
    "    returns[i] = float(returns[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9565a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\\nagent.train_step_counter.assign(0)\\n\\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\\nreturns = [avg_return]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Collection for new agent\n",
    "'''\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016920e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2510200: loss = 4.048866271972656\n",
      "step = 2510400: loss = 2.711169481277466\n",
      "step = 2510600: loss = 2.831785202026367\n",
      "step = 2510800: loss = 4.045544147491455\n",
      "step = 2511000: loss = 3.755446434020996\n",
      "step = 2511200: loss = 3.416694164276123\n",
      "step = 2511400: loss = 2.1397054195404053\n",
      "step = 2511600: loss = 3.3178164958953857\n",
      "step = 2511800: loss = 3.104382038116455\n",
      "step = 2512000: loss = 4.317492485046387\n",
      "step = 2512200: loss = 3.75129771232605\n",
      "step = 2512400: loss = 3.5067083835601807\n",
      "step = 2512600: loss = 2.9723024368286133\n",
      "step = 2512800: loss = 3.127715587615967\n",
      "step = 2513000: loss = 3.5993614196777344\n",
      "step = 2513200: loss = 5.463475227355957\n",
      "step = 2513400: loss = 2.5705113410949707\n",
      "step = 2513600: loss = 3.66930890083313\n",
      "step = 2513800: loss = 2.8041768074035645\n",
      "step = 2514000: loss = 2.736694574356079\n",
      "step = 2514200: loss = 4.450831890106201\n",
      "step = 2514400: loss = 2.989251136779785\n",
      "step = 2514600: loss = 3.52043080329895\n",
      "step = 2514800: loss = 3.02284574508667\n",
      "step = 2515000: loss = 3.754302978515625\n",
      "step = 2515000: Average Return = 4.699999809265137\n",
      "step = 2515200: loss = 2.983152389526367\n",
      "step = 2515400: loss = 2.6826205253601074\n",
      "step = 2515600: loss = 4.352460861206055\n",
      "step = 2515800: loss = 3.1966161727905273\n",
      "step = 2516000: loss = 2.805415391921997\n",
      "step = 2516200: loss = 2.8587355613708496\n",
      "step = 2516400: loss = 2.349121570587158\n",
      "step = 2516600: loss = 2.678030252456665\n",
      "step = 2516800: loss = 2.4769325256347656\n",
      "step = 2517000: loss = 3.440321207046509\n",
      "step = 2517200: loss = 2.573575735092163\n",
      "step = 2517400: loss = 3.2729899883270264\n",
      "step = 2517600: loss = 2.524068593978882\n",
      "step = 2517800: loss = 3.775114059448242\n",
      "step = 2518000: loss = 3.1765947341918945\n",
      "step = 2518200: loss = 2.408351421356201\n",
      "step = 2518400: loss = 4.378289222717285\n",
      "step = 2518600: loss = 2.9384281635284424\n",
      "step = 2518800: loss = 2.583876371383667\n",
      "step = 2519000: loss = 5.176297664642334\n",
      "step = 2519200: loss = 3.444094657897949\n",
      "step = 2519400: loss = 3.547954559326172\n",
      "step = 2519600: loss = 3.4652891159057617\n",
      "step = 2519800: loss = 1.9941669702529907\n",
      "step = 2520000: loss = 4.3132829666137695\n",
      "step = 2520000: Average Return = 3.700000047683716\n",
      "step = 2520200: loss = 2.780553102493286\n",
      "step = 2520400: loss = 3.6197385787963867\n",
      "step = 2520600: loss = 4.203067302703857\n",
      "step = 2520800: loss = 2.924628257751465\n",
      "step = 2521000: loss = 3.869755983352661\n",
      "step = 2521200: loss = 3.8715462684631348\n",
      "step = 2521400: loss = 3.316432476043701\n",
      "step = 2521600: loss = 3.4886388778686523\n",
      "step = 2521800: loss = 3.820312023162842\n",
      "step = 2522000: loss = 3.794753074645996\n",
      "step = 2522200: loss = 2.782756805419922\n",
      "step = 2522400: loss = 3.6670384407043457\n",
      "step = 2522600: loss = 2.6060194969177246\n",
      "step = 2522800: loss = 2.639091730117798\n",
      "step = 2523000: loss = 3.438981533050537\n",
      "step = 2523200: loss = 2.1086997985839844\n",
      "step = 2523400: loss = 2.1711626052856445\n",
      "step = 2523600: loss = 3.1843249797821045\n",
      "step = 2523800: loss = 2.8320744037628174\n",
      "step = 2524000: loss = 3.858715057373047\n",
      "step = 2524200: loss = 3.605786085128784\n",
      "step = 2524400: loss = 1.7199946641921997\n",
      "step = 2524600: loss = 3.476618528366089\n",
      "step = 2524800: loss = 4.525348663330078\n",
      "step = 2525000: loss = 3.062778949737549\n",
      "step = 2525000: Average Return = 2.950000047683716\n",
      "step = 2525200: loss = 2.3681695461273193\n",
      "step = 2525400: loss = 2.4640579223632812\n",
      "step = 2525600: loss = 3.0813751220703125\n",
      "step = 2525800: loss = 4.509304523468018\n",
      "step = 2526000: loss = 4.041559219360352\n",
      "step = 2526200: loss = 2.4780752658843994\n",
      "step = 2526400: loss = 2.4379332065582275\n",
      "step = 2526600: loss = 2.828145742416382\n",
      "step = 2526800: loss = 2.6526544094085693\n",
      "step = 2527000: loss = 3.2931125164031982\n",
      "step = 2527200: loss = 4.044671535491943\n",
      "step = 2527400: loss = 3.6213762760162354\n",
      "step = 2527600: loss = 3.696730852127075\n",
      "step = 2527800: loss = 3.996687650680542\n",
      "step = 2528000: loss = 5.113580703735352\n",
      "step = 2528200: loss = 3.251490592956543\n",
      "step = 2528400: loss = 3.402810573577881\n",
      "step = 2528600: loss = 2.7695329189300537\n",
      "step = 2528800: loss = 3.02221417427063\n",
      "step = 2529000: loss = 3.001028060913086\n",
      "step = 2529200: loss = 3.198845148086548\n",
      "step = 2529400: loss = 3.114100694656372\n",
      "step = 2529600: loss = 4.034008026123047\n",
      "step = 2529800: loss = 3.073429584503174\n",
      "step = 2530000: loss = 2.7824320793151855\n",
      "step = 2530000: Average Return = 5.650000095367432\n",
      "step = 2530200: loss = 5.183856010437012\n",
      "step = 2530400: loss = 3.012233018875122\n",
      "step = 2530600: loss = 2.4791738986968994\n",
      "step = 2530800: loss = 3.276658773422241\n",
      "step = 2531000: loss = 3.0959649085998535\n",
      "step = 2531200: loss = 5.133964538574219\n",
      "step = 2531400: loss = 2.4820547103881836\n",
      "step = 2531600: loss = 3.025139093399048\n",
      "step = 2531800: loss = 2.5497801303863525\n",
      "step = 2532000: loss = 3.4396355152130127\n",
      "step = 2532200: loss = 1.9487533569335938\n",
      "step = 2532400: loss = 3.445547342300415\n",
      "step = 2532600: loss = 3.631969928741455\n",
      "step = 2532800: loss = 3.3676891326904297\n",
      "step = 2533000: loss = 2.873720407485962\n",
      "step = 2533200: loss = 2.655543804168701\n",
      "step = 2533400: loss = 2.572047710418701\n",
      "step = 2533600: loss = 3.395796775817871\n",
      "step = 2533800: loss = 3.655442953109741\n",
      "step = 2534000: loss = 3.040015459060669\n",
      "step = 2534200: loss = 2.5280580520629883\n",
      "step = 2534400: loss = 2.984426736831665\n",
      "step = 2534600: loss = 2.8893487453460693\n",
      "step = 2534800: loss = 3.1424245834350586\n",
      "step = 2535000: loss = 3.4416797161102295\n",
      "step = 2535000: Average Return = 3.9000000953674316\n",
      "step = 2535200: loss = 3.571686267852783\n",
      "step = 2535400: loss = 3.2182605266571045\n",
      "step = 2535600: loss = 4.134555339813232\n",
      "step = 2535800: loss = 2.4794468879699707\n",
      "step = 2536000: loss = 3.2600064277648926\n",
      "step = 2536200: loss = 2.4885916709899902\n",
      "step = 2536400: loss = 2.2713699340820312\n",
      "step = 2536600: loss = 2.5634026527404785\n",
      "step = 2536800: loss = 2.9722564220428467\n",
      "step = 2537000: loss = 3.9433627128601074\n",
      "step = 2537200: loss = 3.2647149562835693\n",
      "step = 2537400: loss = 3.7936112880706787\n",
      "step = 2537600: loss = 3.9075305461883545\n",
      "step = 2537800: loss = 3.5120091438293457\n",
      "step = 2538000: loss = 3.5812182426452637\n",
      "step = 2538200: loss = 3.6450023651123047\n",
      "step = 2538400: loss = 2.178851842880249\n",
      "step = 2538600: loss = 2.605455160140991\n",
      "step = 2538800: loss = 3.496431827545166\n",
      "step = 2539000: loss = 3.170987844467163\n",
      "step = 2539200: loss = 3.1542739868164062\n",
      "step = 2539400: loss = 4.212996006011963\n",
      "step = 2539600: loss = 2.581435441970825\n",
      "step = 2539800: loss = 3.6710081100463867\n",
      "step = 2540000: loss = 2.600569725036621\n",
      "step = 2540000: Average Return = 3.6500000953674316\n",
      "step = 2540200: loss = 2.145824432373047\n",
      "step = 2540400: loss = 2.417119026184082\n",
      "step = 2540600: loss = 2.61268949508667\n",
      "step = 2540800: loss = 2.4912285804748535\n",
      "step = 2541000: loss = 1.9122377634048462\n",
      "step = 2541200: loss = 2.092041254043579\n",
      "step = 2541400: loss = 3.043485641479492\n",
      "step = 2541600: loss = 3.329883337020874\n",
      "step = 2541800: loss = 4.097257137298584\n",
      "step = 2542000: loss = 2.5173678398132324\n",
      "step = 2542200: loss = 2.7603111267089844\n",
      "step = 2542400: loss = 2.7083232402801514\n",
      "step = 2542600: loss = 3.1194169521331787\n",
      "step = 2542800: loss = 2.2497076988220215\n",
      "step = 2543000: loss = 3.2573931217193604\n",
      "step = 2543200: loss = 3.000303268432617\n",
      "step = 2543400: loss = 3.3331310749053955\n",
      "step = 2543600: loss = 3.473004102706909\n",
      "step = 2543800: loss = 4.044963359832764\n",
      "step = 2544000: loss = 3.1985065937042236\n",
      "step = 2544200: loss = 3.9725399017333984\n",
      "step = 2544400: loss = 2.8703572750091553\n",
      "step = 2544600: loss = 1.877419352531433\n",
      "step = 2544800: loss = 4.2362823486328125\n",
      "step = 2545000: loss = 3.011528253555298\n",
      "step = 2545000: Average Return = 3.3499999046325684\n",
      "step = 2545200: loss = 3.0163328647613525\n",
      "step = 2545400: loss = 3.6147642135620117\n",
      "step = 2545600: loss = 3.0856645107269287\n",
      "step = 2545800: loss = 2.435086727142334\n",
      "step = 2546000: loss = 2.944413423538208\n",
      "step = 2546200: loss = 3.043491840362549\n",
      "step = 2546400: loss = 3.1491050720214844\n",
      "step = 2546600: loss = 3.1655092239379883\n",
      "step = 2546800: loss = 2.345473289489746\n",
      "step = 2547000: loss = 4.534078121185303\n",
      "step = 2547200: loss = 3.0926709175109863\n",
      "step = 2547400: loss = 2.0543243885040283\n",
      "step = 2547600: loss = 2.9020471572875977\n",
      "step = 2547800: loss = 3.1002187728881836\n",
      "step = 2548000: loss = 5.061479091644287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2548200: loss = 2.157727003097534\n",
      "step = 2548400: loss = 3.6423330307006836\n",
      "step = 2548600: loss = 3.1738977432250977\n",
      "step = 2548800: loss = 2.767191171646118\n",
      "step = 2549000: loss = 5.5469841957092285\n",
      "step = 2549200: loss = 2.707580804824829\n",
      "step = 2549400: loss = 3.2502684593200684\n",
      "step = 2549600: loss = 2.703089475631714\n",
      "step = 2549800: loss = 3.366084575653076\n",
      "step = 2550000: loss = 2.7421765327453613\n",
      "step = 2550000: Average Return = 4.75\n",
      "step = 2550200: loss = 3.4008564949035645\n",
      "step = 2550400: loss = 4.933030128479004\n",
      "step = 2550600: loss = 2.678490161895752\n",
      "step = 2550800: loss = 2.9693222045898438\n",
      "step = 2551000: loss = 3.899578809738159\n",
      "step = 2551200: loss = 2.7278358936309814\n",
      "step = 2551400: loss = 2.2267513275146484\n",
      "step = 2551600: loss = 4.242767810821533\n",
      "step = 2551800: loss = 3.538522243499756\n",
      "step = 2552000: loss = 3.027844190597534\n",
      "step = 2552200: loss = 4.073752403259277\n",
      "step = 2552400: loss = 2.639488697052002\n",
      "step = 2552600: loss = 2.7111473083496094\n",
      "step = 2552800: loss = 2.9539880752563477\n",
      "step = 2553000: loss = 2.7203760147094727\n",
      "step = 2553200: loss = 2.4124655723571777\n",
      "step = 2553400: loss = 3.2920336723327637\n",
      "step = 2553600: loss = 3.2838010787963867\n",
      "step = 2553800: loss = 2.854430675506592\n",
      "step = 2554000: loss = 2.683708429336548\n",
      "step = 2554200: loss = 3.785637140274048\n",
      "step = 2554400: loss = 2.875148296356201\n",
      "step = 2554600: loss = 3.786322832107544\n",
      "step = 2554800: loss = 2.5242364406585693\n",
      "step = 2555000: loss = 3.384127140045166\n",
      "step = 2555000: Average Return = 1.2999999523162842\n",
      "step = 2555200: loss = 3.180656671524048\n",
      "step = 2555400: loss = 3.035728931427002\n",
      "step = 2555600: loss = 5.232999801635742\n",
      "step = 2555800: loss = 3.182072162628174\n",
      "step = 2556000: loss = 2.852134943008423\n",
      "step = 2556200: loss = 3.306628942489624\n",
      "step = 2556400: loss = 3.237386703491211\n",
      "step = 2556600: loss = 3.22042179107666\n",
      "step = 2556800: loss = 5.39756965637207\n",
      "step = 2557000: loss = 2.834174633026123\n",
      "step = 2557200: loss = 3.0708086490631104\n",
      "step = 2557400: loss = 2.92333984375\n",
      "step = 2557600: loss = 2.2552924156188965\n",
      "step = 2557800: loss = 2.595263719558716\n",
      "step = 2558000: loss = 3.4781792163848877\n",
      "step = 2558200: loss = 2.7889654636383057\n",
      "step = 2558400: loss = 1.9131379127502441\n",
      "step = 2558600: loss = 2.8846495151519775\n",
      "step = 2558800: loss = 4.089016437530518\n",
      "step = 2559000: loss = 3.52498459815979\n",
      "step = 2559200: loss = 2.3408899307250977\n",
      "step = 2559400: loss = 2.5924463272094727\n",
      "step = 2559600: loss = 3.978482484817505\n",
      "step = 2559800: loss = 3.3767175674438477\n",
      "step = 2560000: loss = 3.517881393432617\n",
      "step = 2560000: Average Return = 4.849999904632568\n",
      "step = 2560200: loss = 2.903163433074951\n",
      "step = 2560400: loss = 2.938253164291382\n",
      "step = 2560600: loss = 2.806258201599121\n",
      "step = 2560800: loss = 2.3011434078216553\n",
      "step = 2561000: loss = 3.4746313095092773\n",
      "step = 2561200: loss = 4.0382981300354\n",
      "step = 2561400: loss = 2.275674343109131\n",
      "step = 2561600: loss = 2.8413586616516113\n",
      "step = 2561800: loss = 3.4707555770874023\n",
      "step = 2562000: loss = 2.868138313293457\n",
      "step = 2562200: loss = 2.5977783203125\n",
      "step = 2562400: loss = 3.6240899562835693\n",
      "step = 2562600: loss = 3.393066883087158\n",
      "step = 2562800: loss = 2.49025559425354\n",
      "step = 2563000: loss = 3.034165620803833\n",
      "step = 2563200: loss = 3.3355910778045654\n",
      "step = 2563400: loss = 2.5300188064575195\n",
      "step = 2563600: loss = 2.9522111415863037\n",
      "step = 2563800: loss = 4.290963649749756\n",
      "step = 2564000: loss = 3.207181930541992\n",
      "step = 2564200: loss = 3.175830364227295\n",
      "step = 2564400: loss = 3.3634214401245117\n",
      "step = 2564600: loss = 4.1009626388549805\n",
      "step = 2564800: loss = 3.843636989593506\n",
      "step = 2565000: loss = 2.581023693084717\n",
      "step = 2565000: Average Return = 4.349999904632568\n",
      "step = 2565200: loss = 2.5394134521484375\n",
      "step = 2565400: loss = 2.801079750061035\n",
      "step = 2565600: loss = 4.694975852966309\n",
      "step = 2565800: loss = 3.2045719623565674\n",
      "step = 2566000: loss = 3.42287278175354\n",
      "step = 2566200: loss = 3.482339382171631\n",
      "step = 2566400: loss = 2.4397706985473633\n",
      "step = 2566600: loss = 2.474801540374756\n",
      "step = 2566800: loss = 3.131460189819336\n",
      "step = 2567000: loss = 2.7583913803100586\n",
      "step = 2567200: loss = 3.766700267791748\n",
      "step = 2567400: loss = 1.4704444408416748\n",
      "step = 2567600: loss = 3.048006296157837\n",
      "step = 2567800: loss = 3.6578752994537354\n",
      "step = 2568000: loss = 2.544522762298584\n",
      "step = 2568200: loss = 3.232739210128784\n",
      "step = 2568400: loss = 2.6071126461029053\n",
      "step = 2568600: loss = 3.10225510597229\n",
      "step = 2568800: loss = 2.601170539855957\n",
      "step = 2569000: loss = 3.2503764629364014\n",
      "step = 2569200: loss = 2.882903575897217\n",
      "step = 2569400: loss = 3.065610885620117\n",
      "step = 2569600: loss = 2.479851245880127\n",
      "step = 2569800: loss = 3.483098030090332\n",
      "step = 2570000: loss = 2.8082330226898193\n",
      "step = 2570000: Average Return = 3.9000000953674316\n",
      "step = 2570200: loss = 4.542226791381836\n",
      "step = 2570400: loss = 3.569118022918701\n",
      "step = 2570600: loss = 3.835624933242798\n",
      "step = 2570800: loss = 2.329474925994873\n",
      "step = 2571000: loss = 5.807024955749512\n",
      "step = 2571200: loss = 2.320188283920288\n",
      "step = 2571400: loss = 2.8554461002349854\n",
      "step = 2571600: loss = 2.3713271617889404\n",
      "step = 2571800: loss = 3.0530803203582764\n",
      "step = 2572000: loss = 2.712104320526123\n",
      "step = 2572200: loss = 2.592913866043091\n",
      "step = 2572400: loss = 2.9099581241607666\n",
      "step = 2572600: loss = 4.173134803771973\n",
      "step = 2572800: loss = 2.6067123413085938\n",
      "step = 2573000: loss = 2.9093685150146484\n",
      "step = 2573200: loss = 3.532362699508667\n",
      "step = 2573400: loss = 3.0078160762786865\n",
      "step = 2573600: loss = 3.7404513359069824\n",
      "step = 2573800: loss = 2.2761361598968506\n",
      "step = 2574000: loss = 3.896977424621582\n",
      "step = 2574200: loss = 2.6883397102355957\n",
      "step = 2574400: loss = 3.6417131423950195\n",
      "step = 2574600: loss = 3.404956579208374\n",
      "step = 2574800: loss = 2.2370009422302246\n",
      "step = 2575000: loss = 3.3383116722106934\n",
      "step = 2575000: Average Return = 3.5999999046325684\n",
      "step = 2575200: loss = 3.5930984020233154\n",
      "step = 2575400: loss = 3.577713966369629\n",
      "step = 2575600: loss = 3.6379144191741943\n",
      "step = 2575800: loss = 3.0825982093811035\n",
      "step = 2576000: loss = 5.103298187255859\n",
      "step = 2576200: loss = 4.501641273498535\n",
      "step = 2576400: loss = 3.849961996078491\n",
      "step = 2576600: loss = 2.663909435272217\n",
      "step = 2576800: loss = 5.798645973205566\n",
      "step = 2577000: loss = 2.0700531005859375\n",
      "step = 2577200: loss = 3.818547248840332\n",
      "step = 2577400: loss = 3.537877321243286\n",
      "step = 2577600: loss = 2.289361000061035\n",
      "step = 2577800: loss = 2.5515480041503906\n",
      "step = 2578000: loss = 3.5155162811279297\n",
      "step = 2578200: loss = 3.6273794174194336\n",
      "step = 2578400: loss = 3.4107067584991455\n",
      "step = 2578600: loss = 3.396782875061035\n",
      "step = 2578800: loss = 1.8304729461669922\n",
      "step = 2579000: loss = 3.4184155464172363\n",
      "step = 2579200: loss = 3.153393507003784\n",
      "step = 2579400: loss = 2.9528918266296387\n",
      "step = 2579600: loss = 2.9620680809020996\n",
      "step = 2579800: loss = 2.9190304279327393\n",
      "step = 2580000: loss = 1.9821195602416992\n",
      "step = 2580000: Average Return = 4.050000190734863\n",
      "step = 2580200: loss = 2.8522188663482666\n",
      "step = 2580400: loss = 2.7526774406433105\n",
      "step = 2580600: loss = 3.998023509979248\n",
      "step = 2580800: loss = 4.060287952423096\n",
      "step = 2581000: loss = 3.166186571121216\n",
      "step = 2581200: loss = 3.371513605117798\n",
      "step = 2581400: loss = 4.8859944343566895\n",
      "step = 2581600: loss = 4.149587631225586\n",
      "step = 2581800: loss = 5.418549537658691\n",
      "step = 2582000: loss = 4.214967727661133\n",
      "step = 2582200: loss = 3.893944263458252\n",
      "step = 2582400: loss = 2.070871591567993\n",
      "step = 2582600: loss = 2.8146700859069824\n",
      "step = 2582800: loss = 3.5653491020202637\n",
      "step = 2583000: loss = 2.441519021987915\n",
      "step = 2583200: loss = 3.34954833984375\n",
      "step = 2583400: loss = 3.9225378036499023\n",
      "step = 2583600: loss = 4.540678024291992\n",
      "step = 2583800: loss = 3.827134609222412\n",
      "step = 2584000: loss = 2.0070488452911377\n",
      "step = 2584200: loss = 3.967257022857666\n",
      "step = 2584400: loss = 2.9294421672821045\n",
      "step = 2584600: loss = 2.36090350151062\n",
      "step = 2584800: loss = 3.502753734588623\n",
      "step = 2585000: loss = 4.439610958099365\n",
      "step = 2585000: Average Return = 3.450000047683716\n",
      "step = 2585200: loss = 3.345573663711548\n",
      "step = 2585400: loss = 3.819833278656006\n",
      "step = 2585600: loss = 2.4396209716796875\n",
      "step = 2585800: loss = 3.7213551998138428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2586000: loss = 3.124817371368408\n",
      "step = 2586200: loss = 3.1143736839294434\n",
      "step = 2586400: loss = 3.2300448417663574\n",
      "step = 2586600: loss = 2.7157094478607178\n",
      "step = 2586800: loss = 4.058740615844727\n",
      "step = 2587000: loss = 3.338604688644409\n",
      "step = 2587200: loss = 3.1129956245422363\n",
      "step = 2587400: loss = 2.9943435192108154\n",
      "step = 2587600: loss = 2.627328634262085\n",
      "step = 2587800: loss = 4.344545364379883\n",
      "step = 2588000: loss = 3.4886484146118164\n",
      "step = 2588200: loss = 3.652071714401245\n",
      "step = 2588400: loss = 2.7342641353607178\n",
      "step = 2588600: loss = 3.734679698944092\n",
      "step = 2588800: loss = 2.646418333053589\n",
      "step = 2589000: loss = 3.9943525791168213\n",
      "step = 2589200: loss = 3.67970609664917\n",
      "step = 2589400: loss = 3.430471658706665\n",
      "step = 2589600: loss = 3.288140296936035\n",
      "step = 2589800: loss = 4.000049114227295\n",
      "step = 2590000: loss = 4.310790538787842\n",
      "step = 2590000: Average Return = 4.349999904632568\n",
      "step = 2590200: loss = 3.606550931930542\n",
      "step = 2590400: loss = 3.739993095397949\n",
      "step = 2590600: loss = 3.8434460163116455\n",
      "step = 2590800: loss = 4.019853591918945\n",
      "step = 2591000: loss = 3.307361364364624\n",
      "step = 2591200: loss = 2.84106183052063\n",
      "step = 2591400: loss = 2.1593213081359863\n",
      "step = 2591600: loss = 3.6228020191192627\n",
      "step = 2591800: loss = 4.951951026916504\n",
      "step = 2592000: loss = 3.5733489990234375\n",
      "step = 2592200: loss = 4.100741386413574\n",
      "step = 2592400: loss = 2.854994535446167\n",
      "step = 2592600: loss = 3.066526174545288\n",
      "step = 2592800: loss = 4.056799411773682\n",
      "step = 2593000: loss = 3.141193151473999\n",
      "step = 2593200: loss = 3.2243947982788086\n",
      "step = 2593400: loss = 3.1708247661590576\n",
      "step = 2593600: loss = 5.441360950469971\n",
      "step = 2593800: loss = 4.287004470825195\n",
      "step = 2594000: loss = 1.927344799041748\n",
      "step = 2594200: loss = 2.1400249004364014\n",
      "step = 2594400: loss = 3.572166919708252\n",
      "step = 2594600: loss = 2.2742974758148193\n",
      "step = 2594800: loss = 3.3639636039733887\n",
      "step = 2595000: loss = 2.9865667819976807\n",
      "step = 2595000: Average Return = 3.8499999046325684\n",
      "step = 2595200: loss = 2.981539249420166\n",
      "step = 2595400: loss = 3.4504714012145996\n",
      "step = 2595600: loss = 3.1865921020507812\n",
      "step = 2595800: loss = 4.264174938201904\n",
      "step = 2596000: loss = 2.827592611312866\n",
      "step = 2596200: loss = 3.0720083713531494\n",
      "step = 2596400: loss = 3.8455147743225098\n",
      "step = 2596600: loss = 3.1027331352233887\n",
      "step = 2596800: loss = 3.300527572631836\n",
      "step = 2597000: loss = 3.5591509342193604\n",
      "step = 2597200: loss = 2.3290562629699707\n",
      "step = 2597400: loss = 3.2435591220855713\n",
      "step = 2597600: loss = 3.662696361541748\n",
      "step = 2597800: loss = 3.58158278465271\n",
      "step = 2598000: loss = 4.196507453918457\n",
      "step = 2598200: loss = 3.396332263946533\n",
      "step = 2598400: loss = 2.616223096847534\n",
      "step = 2598600: loss = 2.30102801322937\n",
      "step = 2598800: loss = 3.2588648796081543\n",
      "step = 2599000: loss = 4.1188788414001465\n",
      "step = 2599200: loss = 3.1542773246765137\n",
      "step = 2599400: loss = 3.759061098098755\n",
      "step = 2599600: loss = 2.3279688358306885\n",
      "step = 2599800: loss = 3.155390977859497\n",
      "step = 2600000: loss = 3.4088656902313232\n",
      "step = 2600000: Average Return = 3.8499999046325684\n",
      "step = 2600200: loss = 4.351844310760498\n",
      "step = 2600400: loss = 4.075575828552246\n",
      "step = 2600600: loss = 3.8388125896453857\n",
      "step = 2600800: loss = 2.9966444969177246\n",
      "step = 2601000: loss = 3.8615260124206543\n",
      "step = 2601200: loss = 2.756019115447998\n",
      "step = 2601400: loss = 2.8015027046203613\n",
      "step = 2601600: loss = 2.8370048999786377\n",
      "step = 2601800: loss = 3.4798054695129395\n",
      "step = 2602000: loss = 3.743516206741333\n",
      "step = 2602200: loss = 3.099982738494873\n",
      "step = 2602400: loss = 3.7286601066589355\n",
      "step = 2602600: loss = 3.866877317428589\n",
      "step = 2602800: loss = 3.3679451942443848\n",
      "step = 2603000: loss = 2.805908441543579\n",
      "step = 2603200: loss = 3.1826608180999756\n",
      "step = 2603400: loss = 2.4155819416046143\n",
      "step = 2603600: loss = 3.2923924922943115\n",
      "step = 2603800: loss = 3.3166592121124268\n",
      "step = 2604000: loss = 2.3821804523468018\n",
      "step = 2604200: loss = 4.0264410972595215\n",
      "step = 2604400: loss = 3.292389392852783\n",
      "step = 2604600: loss = 5.101463317871094\n",
      "step = 2604800: loss = 2.4745137691497803\n",
      "step = 2605000: loss = 1.9205013513565063\n",
      "step = 2605000: Average Return = 4.25\n",
      "step = 2605200: loss = 2.52530574798584\n",
      "step = 2605400: loss = 3.618117094039917\n",
      "step = 2605600: loss = 2.6934750080108643\n",
      "step = 2605800: loss = 3.067697763442993\n",
      "step = 2606000: loss = 3.8582987785339355\n",
      "step = 2606200: loss = 2.9374780654907227\n",
      "step = 2606400: loss = 4.224032878875732\n",
      "step = 2606600: loss = 4.0546875\n",
      "step = 2606800: loss = 3.098480224609375\n",
      "step = 2607000: loss = 3.175356149673462\n",
      "step = 2607200: loss = 2.889399766921997\n",
      "step = 2607400: loss = 2.8778293132781982\n",
      "step = 2607600: loss = 2.8622002601623535\n",
      "step = 2607800: loss = 3.0604207515716553\n",
      "step = 2608000: loss = 2.900759220123291\n",
      "step = 2608200: loss = 3.331301689147949\n",
      "step = 2608400: loss = 2.8676774501800537\n",
      "step = 2608600: loss = 2.9185054302215576\n",
      "step = 2608800: loss = 2.2249486446380615\n",
      "step = 2609000: loss = 2.953718423843384\n",
      "step = 2609200: loss = 3.499323606491089\n",
      "step = 2609400: loss = 3.2644145488739014\n",
      "step = 2609600: loss = 3.188983201980591\n",
      "step = 2609800: loss = 3.250295639038086\n",
      "step = 2610000: loss = 2.8817479610443115\n",
      "step = 2610000: Average Return = 5.5\n",
      "step = 2610200: loss = 2.979844808578491\n",
      "step = 2610400: loss = 2.6471147537231445\n",
      "step = 2610600: loss = 2.9089012145996094\n",
      "step = 2610800: loss = 2.907595157623291\n",
      "step = 2611000: loss = 4.532231330871582\n",
      "step = 2611200: loss = 2.2177770137786865\n",
      "step = 2611400: loss = 2.876161813735962\n",
      "step = 2611600: loss = 5.900273323059082\n",
      "step = 2611800: loss = 4.482089519500732\n",
      "step = 2612000: loss = 3.7654151916503906\n",
      "step = 2612200: loss = 2.8729960918426514\n",
      "step = 2612400: loss = 2.667628526687622\n",
      "step = 2612600: loss = 1.6602123975753784\n",
      "step = 2612800: loss = 3.1293818950653076\n",
      "step = 2613000: loss = 2.418743133544922\n",
      "step = 2613200: loss = 3.2316365242004395\n",
      "step = 2613400: loss = 2.5661797523498535\n",
      "step = 2613600: loss = 3.3146257400512695\n",
      "step = 2613800: loss = 2.8308002948760986\n",
      "step = 2614000: loss = 2.698840856552124\n",
      "step = 2614200: loss = 3.575057029724121\n",
      "step = 2614400: loss = 3.269261360168457\n",
      "step = 2614600: loss = 4.862120151519775\n",
      "step = 2614800: loss = 2.1156389713287354\n",
      "step = 2615000: loss = 4.802717208862305\n",
      "step = 2615000: Average Return = 4.0\n",
      "step = 2615200: loss = 2.7473487854003906\n",
      "step = 2615400: loss = 3.085514783859253\n",
      "step = 2615600: loss = 2.550123929977417\n",
      "step = 2615800: loss = 4.355884552001953\n",
      "step = 2616000: loss = 2.7393722534179688\n",
      "step = 2616200: loss = 2.923224925994873\n",
      "step = 2616400: loss = 4.037541389465332\n",
      "step = 2616600: loss = 3.1813085079193115\n",
      "step = 2616800: loss = 5.095340251922607\n",
      "step = 2617000: loss = 3.903132915496826\n",
      "step = 2617200: loss = 3.8347740173339844\n",
      "step = 2617400: loss = 3.26112699508667\n",
      "step = 2617600: loss = 3.9705140590667725\n",
      "step = 2617800: loss = 3.816950559616089\n",
      "step = 2618000: loss = 4.054581165313721\n",
      "step = 2618200: loss = 3.268099784851074\n",
      "step = 2618400: loss = 2.7499465942382812\n",
      "step = 2618600: loss = 3.4469072818756104\n",
      "step = 2618800: loss = 4.382693767547607\n",
      "step = 2619000: loss = 2.711224317550659\n",
      "step = 2619200: loss = 3.387923002243042\n",
      "step = 2619400: loss = 3.045719861984253\n",
      "step = 2619600: loss = 3.0662498474121094\n",
      "step = 2619800: loss = 2.4703030586242676\n",
      "step = 2620000: loss = 3.7255043983459473\n",
      "step = 2620000: Average Return = 3.5999999046325684\n",
      "step = 2620200: loss = 3.1554930210113525\n",
      "step = 2620400: loss = 3.4287617206573486\n",
      "step = 2620600: loss = 3.2986369132995605\n",
      "step = 2620800: loss = 3.715527296066284\n",
      "step = 2621000: loss = 3.2691988945007324\n",
      "step = 2621200: loss = 3.0794084072113037\n",
      "step = 2621400: loss = 3.340615749359131\n",
      "step = 2621600: loss = 3.614901542663574\n",
      "step = 2621800: loss = 4.80238676071167\n",
      "step = 2622000: loss = 2.2785143852233887\n",
      "step = 2622200: loss = 3.050407648086548\n",
      "step = 2622400: loss = 3.2573115825653076\n",
      "step = 2622600: loss = 4.559831142425537\n",
      "step = 2622800: loss = 4.142269134521484\n",
      "step = 2623000: loss = 3.27760910987854\n",
      "step = 2623200: loss = 4.182963848114014\n",
      "step = 2623400: loss = 4.008200645446777\n",
      "step = 2623600: loss = 2.2480082511901855\n",
      "step = 2623800: loss = 2.772205114364624\n",
      "step = 2624000: loss = 3.772427797317505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2624200: loss = 2.5450093746185303\n",
      "step = 2624400: loss = 2.220459461212158\n",
      "step = 2624600: loss = 3.50508713722229\n",
      "step = 2624800: loss = 2.0219886302948\n",
      "step = 2625000: loss = 3.335392475128174\n",
      "step = 2625000: Average Return = 3.75\n",
      "step = 2625200: loss = 2.863750457763672\n",
      "step = 2625400: loss = 3.366926908493042\n",
      "step = 2625600: loss = 2.711125135421753\n",
      "step = 2625800: loss = 3.3607935905456543\n",
      "step = 2626000: loss = 3.473520517349243\n",
      "step = 2626200: loss = 3.7116847038269043\n",
      "step = 2626400: loss = 2.9988527297973633\n",
      "step = 2626600: loss = 3.612630844116211\n",
      "step = 2626800: loss = 2.929960012435913\n",
      "step = 2627000: loss = 2.065577983856201\n",
      "step = 2627200: loss = 2.9000494480133057\n",
      "step = 2627400: loss = 4.394617080688477\n",
      "step = 2627600: loss = 3.0426340103149414\n",
      "step = 2627800: loss = 3.6971001625061035\n",
      "step = 2628000: loss = 1.785845160484314\n",
      "step = 2628200: loss = 2.8554821014404297\n",
      "step = 2628400: loss = 2.101391315460205\n",
      "step = 2628600: loss = 5.1456499099731445\n",
      "step = 2628800: loss = 3.254955768585205\n",
      "step = 2629000: loss = 4.755754470825195\n",
      "step = 2629200: loss = 3.2851641178131104\n",
      "step = 2629400: loss = 3.2908129692077637\n",
      "step = 2629600: loss = 3.5122148990631104\n",
      "step = 2629800: loss = 3.060420513153076\n",
      "step = 2630000: loss = 3.1102070808410645\n",
      "step = 2630000: Average Return = 5.0\n",
      "step = 2630200: loss = 2.3795411586761475\n",
      "step = 2630400: loss = 2.689077377319336\n",
      "step = 2630600: loss = 1.7125164270401\n",
      "step = 2630800: loss = 3.131211996078491\n",
      "step = 2631000: loss = 2.92991304397583\n",
      "step = 2631200: loss = 3.366910219192505\n",
      "step = 2631400: loss = 3.96061110496521\n",
      "step = 2631600: loss = 3.8855581283569336\n",
      "step = 2631800: loss = 2.546550750732422\n",
      "step = 2632000: loss = 2.3969693183898926\n",
      "step = 2632200: loss = 2.3967225551605225\n",
      "step = 2632400: loss = 2.7617082595825195\n",
      "step = 2632600: loss = 4.33066463470459\n",
      "step = 2632800: loss = 3.608995199203491\n",
      "step = 2633000: loss = 3.3879313468933105\n",
      "step = 2633200: loss = 2.5629940032958984\n",
      "step = 2633400: loss = 2.496039628982544\n",
      "step = 2633600: loss = 4.836911678314209\n",
      "step = 2633800: loss = 3.320669174194336\n",
      "step = 2634000: loss = 3.225412607192993\n",
      "step = 2634200: loss = 3.146535634994507\n",
      "step = 2634400: loss = 1.4284545183181763\n",
      "step = 2634600: loss = 2.9072372913360596\n",
      "step = 2634800: loss = 2.3057308197021484\n",
      "step = 2635000: loss = 3.668440580368042\n",
      "step = 2635000: Average Return = 5.849999904632568\n",
      "step = 2635200: loss = 2.5672895908355713\n",
      "step = 2635400: loss = 2.8810513019561768\n",
      "step = 2635600: loss = 3.774172306060791\n",
      "step = 2635800: loss = 2.768422842025757\n",
      "step = 2636000: loss = 2.511256456375122\n",
      "step = 2636200: loss = 3.2959673404693604\n",
      "step = 2636400: loss = 2.683986186981201\n",
      "step = 2636600: loss = 4.322569370269775\n",
      "step = 2636800: loss = 2.748591184616089\n",
      "step = 2637000: loss = 2.710049629211426\n",
      "step = 2637200: loss = 2.1566383838653564\n",
      "step = 2637400: loss = 2.1614203453063965\n",
      "step = 2637600: loss = 4.066450595855713\n",
      "step = 2637800: loss = 3.1537623405456543\n",
      "step = 2638000: loss = 2.4954140186309814\n",
      "step = 2638200: loss = 3.739762544631958\n",
      "step = 2638400: loss = 2.8831212520599365\n",
      "step = 2638600: loss = 4.1846795082092285\n",
      "step = 2638800: loss = 2.513256788253784\n",
      "step = 2639000: loss = 3.012207269668579\n",
      "step = 2639200: loss = 3.172718048095703\n",
      "step = 2639400: loss = 4.30919075012207\n",
      "step = 2639600: loss = 2.794417142868042\n",
      "step = 2639800: loss = 1.8965301513671875\n",
      "step = 2640000: loss = 2.74678635597229\n",
      "step = 2640000: Average Return = 5.75\n",
      "step = 2640200: loss = 3.2184526920318604\n",
      "step = 2640400: loss = 3.8730359077453613\n",
      "step = 2640600: loss = 4.297857284545898\n",
      "step = 2640800: loss = 4.529715538024902\n",
      "step = 2641000: loss = 3.2133145332336426\n",
      "step = 2641200: loss = 2.148454427719116\n",
      "step = 2641400: loss = 3.361114501953125\n",
      "step = 2641600: loss = 3.5493972301483154\n",
      "step = 2641800: loss = 2.8242571353912354\n",
      "step = 2642000: loss = 2.287452220916748\n",
      "step = 2642200: loss = 3.811924934387207\n",
      "step = 2642400: loss = 2.933173418045044\n",
      "step = 2642600: loss = 2.872856616973877\n",
      "step = 2642800: loss = 2.914618968963623\n",
      "step = 2643000: loss = 2.8784680366516113\n",
      "step = 2643200: loss = 2.9274656772613525\n",
      "step = 2643400: loss = 1.9482390880584717\n",
      "step = 2643600: loss = 1.5346038341522217\n",
      "step = 2643800: loss = 4.1667704582214355\n",
      "step = 2644000: loss = 3.114436626434326\n",
      "step = 2644200: loss = 3.215304136276245\n",
      "step = 2644400: loss = 3.221616744995117\n",
      "step = 2644600: loss = 3.1603708267211914\n",
      "step = 2644800: loss = 2.6090710163116455\n",
      "step = 2645000: loss = 4.263606548309326\n",
      "step = 2645000: Average Return = 4.449999809265137\n",
      "step = 2645200: loss = 3.746547222137451\n",
      "step = 2645400: loss = 2.7451319694519043\n",
      "step = 2645600: loss = 4.042950630187988\n",
      "step = 2645800: loss = 3.533142566680908\n",
      "step = 2646000: loss = 2.9295754432678223\n",
      "step = 2646200: loss = 4.239272594451904\n",
      "step = 2646400: loss = 3.5648930072784424\n",
      "step = 2646600: loss = 4.520744800567627\n",
      "step = 2646800: loss = 2.700594425201416\n",
      "step = 2647000: loss = 3.5233383178710938\n",
      "step = 2647200: loss = 2.8565428256988525\n",
      "step = 2647400: loss = 4.005746841430664\n",
      "step = 2647600: loss = 2.929716110229492\n",
      "step = 2647800: loss = 3.397803544998169\n",
      "step = 2648000: loss = 2.983541250228882\n",
      "step = 2648200: loss = 3.359163522720337\n",
      "step = 2648400: loss = 2.6022753715515137\n",
      "step = 2648600: loss = 2.8276968002319336\n",
      "step = 2648800: loss = 3.859143018722534\n",
      "step = 2649000: loss = 2.9748058319091797\n",
      "step = 2649200: loss = 5.470757961273193\n",
      "step = 2649400: loss = 3.38247013092041\n",
      "step = 2649600: loss = 3.066997528076172\n",
      "step = 2649800: loss = 2.584365129470825\n",
      "step = 2650000: loss = 3.4948718547821045\n",
      "step = 2650000: Average Return = 4.0\n",
      "step = 2650200: loss = 3.3213119506835938\n",
      "step = 2650400: loss = 3.633350372314453\n",
      "step = 2650600: loss = 2.532336950302124\n",
      "step = 2650800: loss = 2.9018750190734863\n",
      "step = 2651000: loss = 2.830808639526367\n",
      "step = 2651200: loss = 2.413022518157959\n",
      "step = 2651400: loss = 2.9996917247772217\n",
      "step = 2651600: loss = 3.119302272796631\n",
      "step = 2651800: loss = 2.5721821784973145\n",
      "step = 2652000: loss = 2.6007702350616455\n",
      "step = 2652200: loss = 5.074376583099365\n",
      "step = 2652400: loss = 3.670802593231201\n",
      "step = 2652600: loss = 2.130711078643799\n",
      "step = 2652800: loss = 2.463942527770996\n",
      "step = 2653000: loss = 3.229720115661621\n",
      "step = 2653200: loss = 2.4451282024383545\n",
      "step = 2653400: loss = 3.289780855178833\n",
      "step = 2653600: loss = 2.507436513900757\n",
      "step = 2653800: loss = 2.2177674770355225\n",
      "step = 2654000: loss = 3.2440578937530518\n",
      "step = 2654200: loss = 3.7253129482269287\n",
      "step = 2654400: loss = 3.100937843322754\n",
      "step = 2654600: loss = 3.4278759956359863\n",
      "step = 2654800: loss = 4.103967666625977\n",
      "step = 2655000: loss = 2.1328511238098145\n",
      "step = 2655000: Average Return = 3.700000047683716\n",
      "step = 2655200: loss = 3.531998872756958\n",
      "step = 2655400: loss = 3.072545051574707\n",
      "step = 2655600: loss = 3.258782386779785\n",
      "step = 2655800: loss = 3.0391368865966797\n",
      "step = 2656000: loss = 3.2496023178100586\n",
      "step = 2656200: loss = 3.4231176376342773\n",
      "step = 2656400: loss = 3.334969997406006\n",
      "step = 2656600: loss = 3.206676483154297\n",
      "step = 2656800: loss = 4.165678024291992\n",
      "step = 2657000: loss = 3.1245055198669434\n",
      "step = 2657200: loss = 1.933330774307251\n",
      "step = 2657400: loss = 2.757948160171509\n",
      "step = 2657600: loss = 2.681810140609741\n",
      "step = 2657800: loss = 3.7243003845214844\n",
      "step = 2658000: loss = 3.9057469367980957\n",
      "step = 2658200: loss = 3.4347660541534424\n",
      "step = 2658400: loss = 2.2117817401885986\n",
      "step = 2658600: loss = 4.943643093109131\n",
      "step = 2658800: loss = 5.31367301940918\n",
      "step = 2659000: loss = 3.32897686958313\n",
      "step = 2659200: loss = 2.6138603687286377\n",
      "step = 2659400: loss = 3.193146228790283\n",
      "step = 2659600: loss = 3.286572217941284\n",
      "step = 2659800: loss = 4.034150123596191\n",
      "step = 2660000: loss = 3.749387502670288\n",
      "step = 2660000: Average Return = 5.5\n",
      "step = 2660200: loss = 2.9762909412384033\n",
      "step = 2660400: loss = 3.1595776081085205\n",
      "step = 2660600: loss = 1.8428705930709839\n",
      "step = 2660800: loss = 2.756783962249756\n",
      "step = 2661000: loss = 2.900974988937378\n",
      "step = 2661200: loss = 3.0199902057647705\n",
      "step = 2661400: loss = 3.100248098373413\n",
      "step = 2661600: loss = 2.9045491218566895\n",
      "step = 2661800: loss = 2.778062582015991\n",
      "step = 2662000: loss = 4.120983123779297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2662200: loss = 3.381899356842041\n",
      "step = 2662400: loss = 3.0700621604919434\n",
      "step = 2662600: loss = 3.7657225131988525\n",
      "step = 2662800: loss = 2.6732499599456787\n",
      "step = 2663000: loss = 2.8280718326568604\n",
      "step = 2663200: loss = 2.2484099864959717\n",
      "step = 2663400: loss = 2.8268446922302246\n",
      "step = 2663600: loss = 1.8743817806243896\n",
      "step = 2663800: loss = 2.409698963165283\n",
      "step = 2664000: loss = 2.595264196395874\n",
      "step = 2664200: loss = 2.50045108795166\n",
      "step = 2664400: loss = 2.4013671875\n",
      "step = 2664600: loss = 2.384348154067993\n",
      "step = 2664800: loss = 3.5517003536224365\n",
      "step = 2665000: loss = 3.056777238845825\n",
      "step = 2665000: Average Return = 3.5\n",
      "step = 2665200: loss = 2.942471504211426\n",
      "step = 2665400: loss = 3.0386221408843994\n",
      "step = 2665600: loss = 2.343806028366089\n",
      "step = 2665800: loss = 3.346177816390991\n",
      "step = 2666000: loss = 3.195348024368286\n",
      "step = 2666200: loss = 3.4184746742248535\n",
      "step = 2666400: loss = 4.262916088104248\n",
      "step = 2666600: loss = 3.9704906940460205\n",
      "step = 2666800: loss = 2.6194286346435547\n",
      "step = 2667000: loss = 2.770228862762451\n",
      "step = 2667200: loss = 2.938922882080078\n",
      "step = 2667400: loss = 2.2622461318969727\n",
      "step = 2667600: loss = 2.2256531715393066\n",
      "step = 2667800: loss = 4.260989189147949\n",
      "step = 2668000: loss = 3.1480114459991455\n",
      "step = 2668200: loss = 2.4143753051757812\n",
      "step = 2668400: loss = 2.994425058364868\n",
      "step = 2668600: loss = 3.961399793624878\n",
      "step = 2668800: loss = 2.7805747985839844\n",
      "step = 2669000: loss = 4.768553733825684\n",
      "step = 2669200: loss = 4.184675693511963\n",
      "step = 2669400: loss = 2.795485734939575\n",
      "step = 2669600: loss = 3.3139190673828125\n",
      "step = 2669800: loss = 3.8798301219940186\n",
      "step = 2670000: loss = 3.352180004119873\n",
      "step = 2670000: Average Return = 3.549999952316284\n",
      "step = 2670200: loss = 3.2355449199676514\n",
      "step = 2670400: loss = 2.800168037414551\n",
      "step = 2670600: loss = 3.555231809616089\n",
      "step = 2670800: loss = 3.751843214035034\n",
      "step = 2671000: loss = 3.4922497272491455\n",
      "step = 2671200: loss = 2.9885339736938477\n",
      "step = 2671400: loss = 3.4569740295410156\n",
      "step = 2671600: loss = 3.861064910888672\n",
      "step = 2671800: loss = 2.934626340866089\n",
      "step = 2672000: loss = 3.3090999126434326\n",
      "step = 2672200: loss = 2.9529714584350586\n",
      "step = 2672400: loss = 2.650132179260254\n",
      "step = 2672600: loss = 2.719390392303467\n",
      "step = 2672800: loss = 3.8043575286865234\n",
      "step = 2673000: loss = 2.915375232696533\n",
      "step = 2673200: loss = 3.3847177028656006\n",
      "step = 2673400: loss = 2.8520634174346924\n",
      "step = 2673600: loss = 3.7849433422088623\n",
      "step = 2673800: loss = 2.5572469234466553\n",
      "step = 2674000: loss = 2.209087371826172\n",
      "step = 2674200: loss = 2.3608200550079346\n",
      "step = 2674400: loss = 3.3780059814453125\n",
      "step = 2674600: loss = 1.9027208089828491\n",
      "step = 2674800: loss = 2.6571004390716553\n",
      "step = 2675000: loss = 3.2016332149505615\n",
      "step = 2675000: Average Return = 4.599999904632568\n",
      "step = 2675200: loss = 3.371317148208618\n",
      "step = 2675400: loss = 2.2963266372680664\n",
      "step = 2675600: loss = 3.003960609436035\n",
      "step = 2675800: loss = 2.4490318298339844\n",
      "step = 2676000: loss = 2.8087010383605957\n",
      "step = 2676200: loss = 2.7442150115966797\n",
      "step = 2676400: loss = 2.9602456092834473\n",
      "step = 2676600: loss = 2.450150489807129\n",
      "step = 2676800: loss = 3.8586413860321045\n",
      "step = 2677000: loss = 3.706840753555298\n",
      "step = 2677200: loss = 2.4350438117980957\n",
      "step = 2677400: loss = 3.7026755809783936\n",
      "step = 2677600: loss = 3.1079089641571045\n",
      "step = 2677800: loss = 2.5711495876312256\n",
      "step = 2678000: loss = 3.4541265964508057\n",
      "step = 2678200: loss = 3.0183393955230713\n",
      "step = 2678400: loss = 2.7786455154418945\n",
      "step = 2678600: loss = 2.9806370735168457\n",
      "step = 2678800: loss = 2.338613271713257\n",
      "step = 2679000: loss = 4.203543186187744\n",
      "step = 2679200: loss = 2.5064239501953125\n",
      "step = 2679400: loss = 3.3871612548828125\n",
      "step = 2679600: loss = 3.2871720790863037\n",
      "step = 2679800: loss = 3.1041948795318604\n",
      "step = 2680000: loss = 2.2394425868988037\n",
      "step = 2680000: Average Return = 4.099999904632568\n",
      "step = 2680200: loss = 3.1059906482696533\n",
      "step = 2680400: loss = 3.568953275680542\n",
      "step = 2680600: loss = 3.7626359462738037\n",
      "step = 2680800: loss = 3.3936290740966797\n",
      "step = 2681000: loss = 2.91355037689209\n",
      "step = 2681200: loss = 4.564621925354004\n",
      "step = 2681400: loss = 3.7260661125183105\n",
      "step = 2681600: loss = 2.7096033096313477\n",
      "step = 2681800: loss = 5.463368892669678\n",
      "step = 2682000: loss = 2.3923287391662598\n",
      "step = 2682200: loss = 2.8955399990081787\n",
      "step = 2682400: loss = 3.9767394065856934\n",
      "step = 2682600: loss = 3.7391765117645264\n",
      "step = 2682800: loss = 3.1017370223999023\n",
      "step = 2683000: loss = 2.474860191345215\n",
      "step = 2683200: loss = 4.746003150939941\n",
      "step = 2683400: loss = 2.665147542953491\n",
      "step = 2683600: loss = 2.4208288192749023\n",
      "step = 2683800: loss = 3.413586378097534\n",
      "step = 2684000: loss = 3.006354570388794\n",
      "step = 2684200: loss = 2.871650218963623\n",
      "step = 2684400: loss = 3.0956695079803467\n",
      "step = 2684600: loss = 2.2934834957122803\n",
      "step = 2684800: loss = 3.6510865688323975\n",
      "step = 2685000: loss = 3.341996431350708\n",
      "step = 2685000: Average Return = 4.599999904632568\n",
      "step = 2685200: loss = 3.3906168937683105\n",
      "step = 2685400: loss = 2.563298225402832\n",
      "step = 2685600: loss = 4.726405620574951\n",
      "step = 2685800: loss = 3.3379814624786377\n",
      "step = 2686000: loss = 3.5859463214874268\n",
      "step = 2686200: loss = 4.738236427307129\n",
      "step = 2686400: loss = 3.134965419769287\n",
      "step = 2686600: loss = 1.890655517578125\n",
      "step = 2686800: loss = 2.9527740478515625\n",
      "step = 2687000: loss = 3.79449462890625\n",
      "step = 2687200: loss = 2.5925304889678955\n",
      "step = 2687400: loss = 2.720487117767334\n",
      "step = 2687600: loss = 5.994176387786865\n",
      "step = 2687800: loss = 3.4655044078826904\n",
      "step = 2688000: loss = 3.203735828399658\n",
      "step = 2688200: loss = 3.2530603408813477\n",
      "step = 2688400: loss = 2.325385332107544\n",
      "step = 2688600: loss = 2.498973846435547\n",
      "step = 2688800: loss = 3.9333760738372803\n",
      "step = 2689000: loss = 1.878605604171753\n",
      "step = 2689200: loss = 3.274029493331909\n",
      "step = 2689400: loss = 4.532575607299805\n",
      "step = 2689600: loss = 2.3669471740722656\n",
      "step = 2689800: loss = 3.017273426055908\n",
      "step = 2690000: loss = 3.0951740741729736\n",
      "step = 2690000: Average Return = 4.800000190734863\n",
      "step = 2690200: loss = 4.085434436798096\n",
      "step = 2690400: loss = 4.223200798034668\n",
      "step = 2690600: loss = 2.561260938644409\n",
      "step = 2690800: loss = 2.945758581161499\n",
      "step = 2691000: loss = 3.955347776412964\n",
      "step = 2691200: loss = 2.0095393657684326\n",
      "step = 2691400: loss = 2.8882710933685303\n",
      "step = 2691600: loss = 2.438883066177368\n",
      "step = 2691800: loss = 1.7097469568252563\n",
      "step = 2692000: loss = 3.243697166442871\n",
      "step = 2692200: loss = 3.383784055709839\n",
      "step = 2692400: loss = 3.5972869396209717\n",
      "step = 2692600: loss = 3.335552215576172\n",
      "step = 2692800: loss = 3.6442928314208984\n",
      "step = 2693000: loss = 3.0106146335601807\n",
      "step = 2693200: loss = 2.7730724811553955\n",
      "step = 2693400: loss = 3.6604597568511963\n",
      "step = 2693600: loss = 2.9532737731933594\n",
      "step = 2693800: loss = 2.925166606903076\n",
      "step = 2694000: loss = 3.4728426933288574\n",
      "step = 2694200: loss = 3.5721652507781982\n",
      "step = 2694400: loss = 3.832695722579956\n",
      "step = 2694600: loss = 4.284354209899902\n",
      "step = 2694800: loss = 4.13366174697876\n",
      "step = 2695000: loss = 1.787719964981079\n",
      "step = 2695000: Average Return = 5.050000190734863\n",
      "step = 2695200: loss = 1.9409658908843994\n",
      "step = 2695400: loss = 3.815962076187134\n",
      "step = 2695600: loss = 3.0504837036132812\n",
      "step = 2695800: loss = 2.499859571456909\n",
      "step = 2696000: loss = 3.5085344314575195\n",
      "step = 2696200: loss = 2.9112040996551514\n",
      "step = 2696400: loss = 2.9280765056610107\n",
      "step = 2696600: loss = 2.7122838497161865\n",
      "step = 2696800: loss = 2.966953754425049\n",
      "step = 2697000: loss = 2.9458258152008057\n",
      "step = 2697200: loss = 3.1717593669891357\n",
      "step = 2697400: loss = 3.161702871322632\n",
      "step = 2697600: loss = 3.5007617473602295\n",
      "step = 2697800: loss = 3.0839970111846924\n",
      "step = 2698000: loss = 1.9970847368240356\n",
      "step = 2698200: loss = 4.257762908935547\n",
      "step = 2698400: loss = 4.321879863739014\n",
      "step = 2698600: loss = 3.362691640853882\n",
      "step = 2698800: loss = 2.784468650817871\n",
      "step = 2699000: loss = 3.516807794570923\n",
      "step = 2699200: loss = 2.7070696353912354\n",
      "step = 2699400: loss = 3.447770595550537\n",
      "step = 2699600: loss = 3.0293116569519043\n",
      "step = 2699800: loss = 1.9495046138763428\n",
      "step = 2700000: loss = 2.9956791400909424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2700000: Average Return = 3.549999952316284\n",
      "step = 2700200: loss = 4.511680603027344\n",
      "step = 2700400: loss = 3.3166093826293945\n",
      "step = 2700600: loss = 3.0568673610687256\n",
      "step = 2700800: loss = 4.413233757019043\n",
      "step = 2701000: loss = 3.651874303817749\n",
      "step = 2701200: loss = 2.8103344440460205\n",
      "step = 2701400: loss = 3.664243698120117\n",
      "step = 2701600: loss = 3.8443148136138916\n",
      "step = 2701800: loss = 2.878333330154419\n",
      "step = 2702000: loss = 3.4415996074676514\n",
      "step = 2702200: loss = 1.7710567712783813\n",
      "step = 2702400: loss = 2.1470940113067627\n",
      "step = 2702600: loss = 2.5661418437957764\n",
      "step = 2702800: loss = 2.5534543991088867\n",
      "step = 2703000: loss = 2.5576584339141846\n",
      "step = 2703200: loss = 3.0795092582702637\n",
      "step = 2703400: loss = 4.28232479095459\n",
      "step = 2703600: loss = 2.898602247238159\n",
      "step = 2703800: loss = 2.697442054748535\n",
      "step = 2704000: loss = 4.617446422576904\n",
      "step = 2704200: loss = 2.418473958969116\n",
      "step = 2704400: loss = 3.5156755447387695\n",
      "step = 2704600: loss = 3.8872387409210205\n",
      "step = 2704800: loss = 2.253645181655884\n",
      "step = 2705000: loss = 3.437830686569214\n",
      "step = 2705000: Average Return = 5.550000190734863\n",
      "step = 2705200: loss = 3.2599937915802\n",
      "step = 2705400: loss = 5.159420013427734\n",
      "step = 2705600: loss = 2.3178648948669434\n",
      "step = 2705800: loss = 2.7492728233337402\n",
      "step = 2706000: loss = 3.593186616897583\n",
      "step = 2706200: loss = 3.1233513355255127\n",
      "step = 2706400: loss = 5.002429962158203\n",
      "step = 2706600: loss = 3.222975015640259\n",
      "step = 2706800: loss = 3.4102654457092285\n",
      "step = 2707000: loss = 3.437697172164917\n",
      "step = 2707200: loss = 3.3955821990966797\n",
      "step = 2707400: loss = 4.608124732971191\n",
      "step = 2707600: loss = 2.775977611541748\n",
      "step = 2707800: loss = 3.006931781768799\n",
      "step = 2708000: loss = 2.1306216716766357\n",
      "step = 2708200: loss = 3.921743631362915\n",
      "step = 2708400: loss = 3.4816722869873047\n",
      "step = 2708600: loss = 3.1295952796936035\n",
      "step = 2708800: loss = 3.9256060123443604\n",
      "step = 2709000: loss = 2.9392848014831543\n",
      "step = 2709200: loss = 4.271347522735596\n",
      "step = 2709400: loss = 2.448543071746826\n",
      "step = 2709600: loss = 3.421475648880005\n",
      "step = 2709800: loss = 2.6844916343688965\n",
      "step = 2710000: loss = 3.47820782661438\n",
      "step = 2710000: Average Return = 0.949999988079071\n",
      "step = 2710200: loss = 3.79179310798645\n",
      "step = 2710400: loss = 3.600374937057495\n",
      "step = 2710600: loss = 4.989474773406982\n",
      "step = 2710800: loss = 3.446751356124878\n",
      "step = 2711000: loss = 3.36185359954834\n",
      "step = 2711200: loss = 2.1521897315979004\n",
      "step = 2711400: loss = 3.162410259246826\n",
      "step = 2711600: loss = 3.0589818954467773\n",
      "step = 2711800: loss = 3.13914155960083\n",
      "step = 2712000: loss = 3.048083543777466\n",
      "step = 2712200: loss = 2.0612332820892334\n",
      "step = 2712400: loss = 2.662825584411621\n",
      "step = 2712600: loss = 3.732022285461426\n",
      "step = 2712800: loss = 3.4005038738250732\n",
      "step = 2713000: loss = 1.9090948104858398\n",
      "step = 2713200: loss = 3.777757167816162\n",
      "step = 2713400: loss = 3.5602781772613525\n",
      "step = 2713600: loss = 4.1258368492126465\n",
      "step = 2713800: loss = 2.5379583835601807\n",
      "step = 2714000: loss = 2.592918634414673\n",
      "step = 2714200: loss = 2.7770466804504395\n",
      "step = 2714400: loss = 4.002924919128418\n",
      "step = 2714600: loss = 2.5168983936309814\n",
      "step = 2714800: loss = 2.869581937789917\n",
      "step = 2715000: loss = 2.301671028137207\n",
      "step = 2715000: Average Return = 5.150000095367432\n",
      "step = 2715200: loss = 3.3340771198272705\n",
      "step = 2715400: loss = 3.6239938735961914\n",
      "step = 2715600: loss = 3.42872953414917\n",
      "step = 2715800: loss = 3.766693353652954\n",
      "step = 2716000: loss = 2.865171194076538\n",
      "step = 2716200: loss = 2.688307762145996\n",
      "step = 2716400: loss = 3.1288914680480957\n",
      "step = 2716600: loss = 3.1243736743927\n",
      "step = 2716800: loss = 2.099616289138794\n",
      "step = 2717000: loss = 3.3068108558654785\n",
      "step = 2717200: loss = 2.571824312210083\n",
      "step = 2717400: loss = 4.702369689941406\n",
      "step = 2717600: loss = 3.0319626331329346\n",
      "step = 2717800: loss = 4.506685256958008\n",
      "step = 2718000: loss = 3.8941380977630615\n",
      "step = 2718200: loss = 3.9604265689849854\n",
      "step = 2718400: loss = 3.6559267044067383\n",
      "step = 2718600: loss = 2.916285991668701\n",
      "step = 2718800: loss = 3.3442986011505127\n",
      "step = 2719000: loss = 4.195385932922363\n",
      "step = 2719200: loss = 4.970653057098389\n",
      "step = 2719400: loss = 2.6303627490997314\n",
      "step = 2719600: loss = 3.8874449729919434\n",
      "step = 2719800: loss = 4.20845365524292\n",
      "step = 2720000: loss = 2.685220956802368\n",
      "step = 2720000: Average Return = 7.050000190734863\n",
      "step = 2720200: loss = 2.3785948753356934\n",
      "step = 2720400: loss = 4.478903293609619\n",
      "step = 2720600: loss = 4.764553546905518\n",
      "step = 2720800: loss = 3.13909649848938\n",
      "step = 2721000: loss = 3.400613307952881\n",
      "step = 2721200: loss = 3.8178253173828125\n",
      "step = 2721400: loss = 3.1683387756347656\n",
      "step = 2721600: loss = 3.2234857082366943\n",
      "step = 2721800: loss = 3.9340062141418457\n",
      "step = 2722000: loss = 2.9903576374053955\n",
      "step = 2722200: loss = 5.48046875\n",
      "step = 2722400: loss = 4.648021221160889\n",
      "step = 2722600: loss = 3.3425886631011963\n",
      "step = 2722800: loss = 4.0674729347229\n",
      "step = 2723000: loss = 3.633700132369995\n",
      "step = 2723200: loss = 4.299262523651123\n",
      "step = 2723400: loss = 2.8935155868530273\n",
      "step = 2723600: loss = 3.8772172927856445\n",
      "step = 2723800: loss = 2.980532169342041\n",
      "step = 2724000: loss = 2.785560131072998\n",
      "step = 2724200: loss = 3.7991976737976074\n",
      "step = 2724400: loss = 2.9375174045562744\n",
      "step = 2724600: loss = 3.9492058753967285\n",
      "step = 2724800: loss = 2.499682664871216\n",
      "step = 2725000: loss = 2.2447235584259033\n",
      "step = 2725000: Average Return = 4.599999904632568\n",
      "step = 2725200: loss = 3.3072967529296875\n",
      "step = 2725400: loss = 2.5057713985443115\n",
      "step = 2725600: loss = 4.688323497772217\n",
      "step = 2725800: loss = 2.7581024169921875\n",
      "step = 2726000: loss = 2.817190408706665\n",
      "step = 2726200: loss = 2.46390700340271\n",
      "step = 2726400: loss = 3.9668993949890137\n",
      "step = 2726600: loss = 4.11452054977417\n",
      "step = 2726800: loss = 3.090965509414673\n",
      "step = 2727000: loss = 3.4047234058380127\n",
      "step = 2727200: loss = 2.7204675674438477\n",
      "step = 2727400: loss = 2.401371955871582\n",
      "step = 2727600: loss = 3.680114269256592\n",
      "step = 2727800: loss = 2.9422221183776855\n",
      "step = 2728000: loss = 2.444599151611328\n",
      "step = 2728200: loss = 2.8426811695098877\n",
      "step = 2728400: loss = 3.6722142696380615\n",
      "step = 2728600: loss = 2.9201500415802\n",
      "step = 2728800: loss = 2.8794381618499756\n",
      "step = 2729000: loss = 4.775984764099121\n",
      "step = 2729200: loss = 2.8784282207489014\n",
      "step = 2729400: loss = 3.4464492797851562\n",
      "step = 2729600: loss = 2.3342697620391846\n",
      "step = 2729800: loss = 3.2928850650787354\n",
      "step = 2730000: loss = 3.477241039276123\n",
      "step = 2730000: Average Return = 6.099999904632568\n",
      "step = 2730200: loss = 2.870051145553589\n",
      "step = 2730400: loss = 3.8230183124542236\n",
      "step = 2730600: loss = 5.0330071449279785\n",
      "step = 2730800: loss = 3.8525469303131104\n",
      "step = 2731000: loss = 5.375370979309082\n",
      "step = 2731200: loss = 4.245909690856934\n",
      "step = 2731400: loss = 3.27675724029541\n",
      "step = 2731600: loss = 2.7735595703125\n",
      "step = 2731800: loss = 3.333918333053589\n",
      "step = 2732000: loss = 3.86735200881958\n",
      "step = 2732200: loss = 3.204435348510742\n",
      "step = 2732400: loss = 3.8324782848358154\n",
      "step = 2732600: loss = 2.5615997314453125\n",
      "step = 2732800: loss = 3.596763849258423\n",
      "step = 2733000: loss = 3.565912961959839\n",
      "step = 2733200: loss = 4.051551818847656\n",
      "step = 2733400: loss = 3.006525754928589\n",
      "step = 2733600: loss = 2.712991237640381\n",
      "step = 2733800: loss = 3.509068489074707\n",
      "step = 2734000: loss = 3.097276449203491\n",
      "step = 2734200: loss = 3.6572892665863037\n",
      "step = 2734400: loss = 2.233713150024414\n",
      "step = 2734600: loss = 4.614232540130615\n",
      "step = 2734800: loss = 2.5193800926208496\n",
      "step = 2735000: loss = 3.4854350090026855\n",
      "step = 2735000: Average Return = 3.200000047683716\n",
      "step = 2735200: loss = 1.9891796112060547\n",
      "step = 2735400: loss = 2.3527140617370605\n",
      "step = 2735600: loss = 3.343238353729248\n",
      "step = 2735800: loss = 3.245238780975342\n",
      "step = 2736000: loss = 4.545127868652344\n",
      "step = 2736200: loss = 3.8834962844848633\n",
      "step = 2736400: loss = 3.292459011077881\n",
      "step = 2736600: loss = 2.6663811206817627\n",
      "step = 2736800: loss = 2.234675884246826\n",
      "step = 2737000: loss = 2.8127920627593994\n",
      "step = 2737200: loss = 3.476233959197998\n",
      "step = 2737400: loss = 4.684295654296875\n",
      "step = 2737600: loss = 1.9484902620315552\n",
      "step = 2737800: loss = 2.1915855407714844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2738000: loss = 7.657233715057373\n",
      "step = 2738200: loss = 3.3923444747924805\n",
      "step = 2738400: loss = 4.633528709411621\n",
      "step = 2738600: loss = 2.853543519973755\n",
      "step = 2738800: loss = 3.637834310531616\n",
      "step = 2739000: loss = 3.856658935546875\n",
      "step = 2739200: loss = 2.886582612991333\n",
      "step = 2739400: loss = 2.696573495864868\n",
      "step = 2739600: loss = 3.340240001678467\n",
      "step = 2739800: loss = 5.548066139221191\n",
      "step = 2740000: loss = 3.750075578689575\n",
      "step = 2740000: Average Return = 5.550000190734863\n",
      "step = 2740200: loss = 4.199072360992432\n",
      "step = 2740400: loss = 5.223312854766846\n",
      "step = 2740600: loss = 2.3409347534179688\n",
      "step = 2740800: loss = 3.807110548019409\n",
      "step = 2741000: loss = 3.9105961322784424\n",
      "step = 2741200: loss = 5.43275260925293\n",
      "step = 2741400: loss = 3.4452085494995117\n",
      "step = 2741600: loss = 3.8545193672180176\n",
      "step = 2741800: loss = 2.4824037551879883\n",
      "step = 2742000: loss = 2.819568157196045\n",
      "step = 2742200: loss = 2.6137218475341797\n",
      "step = 2742400: loss = 2.116792917251587\n",
      "step = 2742600: loss = 4.2126288414001465\n",
      "step = 2742800: loss = 3.475175619125366\n",
      "step = 2743000: loss = 3.27484130859375\n",
      "step = 2743200: loss = 4.76401948928833\n",
      "step = 2743400: loss = 4.353160381317139\n",
      "step = 2743600: loss = 3.236579418182373\n",
      "step = 2743800: loss = 3.1845266819000244\n",
      "step = 2744000: loss = 4.036520481109619\n",
      "step = 2744200: loss = 4.131401062011719\n",
      "step = 2744400: loss = 3.4960620403289795\n",
      "step = 2744600: loss = 4.711899280548096\n",
      "step = 2744800: loss = 2.772423028945923\n",
      "step = 2745000: loss = 2.132781744003296\n",
      "step = 2745000: Average Return = 3.1500000953674316\n",
      "step = 2745200: loss = 4.038936138153076\n",
      "step = 2745400: loss = 2.443103551864624\n",
      "step = 2745600: loss = 3.1136608123779297\n",
      "step = 2745800: loss = 3.4976322650909424\n",
      "step = 2746000: loss = 3.260667085647583\n",
      "step = 2746200: loss = 2.008197546005249\n",
      "step = 2746400: loss = 2.1948750019073486\n",
      "step = 2746600: loss = 3.1369574069976807\n",
      "step = 2746800: loss = 3.8783745765686035\n",
      "step = 2747000: loss = 3.778846025466919\n",
      "step = 2747200: loss = 4.558791637420654\n",
      "step = 2747400: loss = 3.946364164352417\n",
      "step = 2747600: loss = 4.648532390594482\n",
      "step = 2747800: loss = 3.72910737991333\n",
      "step = 2748000: loss = 3.2877776622772217\n",
      "step = 2748200: loss = 2.7019670009613037\n",
      "step = 2748400: loss = 4.4705023765563965\n",
      "step = 2748600: loss = 5.301200866699219\n",
      "step = 2748800: loss = 4.44580602645874\n",
      "step = 2749000: loss = 2.7442567348480225\n",
      "step = 2749200: loss = 3.927692174911499\n",
      "step = 2749400: loss = 3.341374158859253\n",
      "step = 2749600: loss = 3.0983974933624268\n",
      "step = 2749800: loss = 6.1613898277282715\n",
      "step = 2750000: loss = 3.361751079559326\n",
      "step = 2750000: Average Return = 3.450000047683716\n",
      "step = 2750200: loss = 3.731128692626953\n",
      "step = 2750400: loss = 1.917521595954895\n",
      "step = 2750600: loss = 3.75243878364563\n",
      "step = 2750800: loss = 3.1319353580474854\n",
      "step = 2751000: loss = 3.096747875213623\n",
      "step = 2751200: loss = 3.1612346172332764\n",
      "step = 2751400: loss = 2.531256675720215\n",
      "step = 2751600: loss = 4.032698631286621\n",
      "step = 2751800: loss = 2.4297385215759277\n",
      "step = 2752000: loss = 2.99111270904541\n",
      "step = 2752200: loss = 3.483187198638916\n",
      "step = 2752400: loss = 2.9215197563171387\n",
      "step = 2752600: loss = 4.164958477020264\n",
      "step = 2752800: loss = 2.584275484085083\n",
      "step = 2753000: loss = 3.142083168029785\n",
      "step = 2753200: loss = 3.1631500720977783\n",
      "step = 2753400: loss = 3.4248290061950684\n",
      "step = 2753600: loss = 3.5468504428863525\n",
      "step = 2753800: loss = 3.4616081714630127\n",
      "step = 2754000: loss = 3.8359837532043457\n",
      "step = 2754200: loss = 2.1644320487976074\n",
      "step = 2754400: loss = 3.3005175590515137\n",
      "step = 2754600: loss = 3.00459623336792\n",
      "step = 2754800: loss = 3.9334404468536377\n",
      "step = 2755000: loss = 3.6589245796203613\n",
      "step = 2755000: Average Return = 4.550000190734863\n",
      "step = 2755200: loss = 3.185460090637207\n",
      "step = 2755400: loss = 3.969067335128784\n",
      "step = 2755600: loss = 3.74686861038208\n",
      "step = 2755800: loss = 3.818707227706909\n",
      "step = 2756000: loss = 2.5461368560791016\n",
      "step = 2756200: loss = 3.160667657852173\n",
      "step = 2756400: loss = 4.4782233238220215\n",
      "step = 2756600: loss = 3.847151517868042\n",
      "step = 2756800: loss = 3.3485469818115234\n",
      "step = 2757000: loss = 5.725784778594971\n",
      "step = 2757200: loss = 3.4136900901794434\n",
      "step = 2757400: loss = 2.3721365928649902\n",
      "step = 2757600: loss = 3.5335488319396973\n",
      "step = 2757800: loss = 2.7136480808258057\n",
      "step = 2758000: loss = 3.1314773559570312\n",
      "step = 2758200: loss = 2.8853728771209717\n",
      "step = 2758400: loss = 3.413015604019165\n",
      "step = 2758600: loss = 2.5038771629333496\n",
      "step = 2758800: loss = 2.793532133102417\n",
      "step = 2759000: loss = 2.763554334640503\n",
      "step = 2759200: loss = 2.9162304401397705\n",
      "step = 2759400: loss = 4.481089115142822\n",
      "step = 2759600: loss = 2.7067601680755615\n",
      "step = 2759800: loss = 2.6018500328063965\n",
      "step = 2760000: loss = 2.854771137237549\n",
      "step = 2760000: Average Return = 7.449999809265137\n",
      "step = 2760200: loss = 3.6581835746765137\n",
      "step = 2760400: loss = 4.02993106842041\n",
      "step = 2760600: loss = 3.175290107727051\n",
      "step = 2760800: loss = 2.192426919937134\n",
      "step = 2761000: loss = 4.263362884521484\n",
      "step = 2761200: loss = 3.116124153137207\n",
      "step = 2761400: loss = 4.1047892570495605\n",
      "step = 2761600: loss = 2.7674999237060547\n",
      "step = 2761800: loss = 2.518733263015747\n",
      "step = 2762000: loss = 3.440904378890991\n",
      "step = 2762200: loss = 3.3960750102996826\n",
      "step = 2762400: loss = 3.821230411529541\n",
      "step = 2762600: loss = 3.4059898853302\n",
      "step = 2762800: loss = 2.8461296558380127\n",
      "step = 2763000: loss = 2.980038642883301\n",
      "step = 2763200: loss = 3.0988097190856934\n",
      "step = 2763400: loss = 3.9456725120544434\n",
      "step = 2763600: loss = 3.1055638790130615\n",
      "step = 2763800: loss = 3.111363410949707\n",
      "step = 2764000: loss = 3.6333611011505127\n",
      "step = 2764200: loss = 2.5469110012054443\n",
      "step = 2764400: loss = 3.8447258472442627\n",
      "step = 2764600: loss = 2.6506762504577637\n",
      "step = 2764800: loss = 2.6438920497894287\n",
      "step = 2765000: loss = 4.1115546226501465\n",
      "step = 2765000: Average Return = 3.0999999046325684\n",
      "step = 2765200: loss = 2.9915966987609863\n",
      "step = 2765400: loss = 3.6779563426971436\n",
      "step = 2765600: loss = 3.013467311859131\n",
      "step = 2765800: loss = 2.756143808364868\n",
      "step = 2766000: loss = 3.4153501987457275\n",
      "step = 2766200: loss = 3.5524792671203613\n",
      "step = 2766400: loss = 2.516486644744873\n",
      "step = 2766600: loss = 3.9176998138427734\n",
      "step = 2766800: loss = 3.3108503818511963\n",
      "step = 2767000: loss = 3.6601521968841553\n",
      "step = 2767200: loss = 3.6553597450256348\n",
      "step = 2767400: loss = 3.073558807373047\n",
      "step = 2767600: loss = 3.309776544570923\n",
      "step = 2767800: loss = 4.1426849365234375\n",
      "step = 2768000: loss = 3.964822292327881\n",
      "step = 2768200: loss = 2.6950507164001465\n",
      "step = 2768400: loss = 6.200536251068115\n",
      "step = 2768600: loss = 3.254024028778076\n",
      "step = 2768800: loss = 2.964020013809204\n",
      "step = 2769000: loss = 3.068732261657715\n",
      "step = 2769200: loss = 3.901716947555542\n",
      "step = 2769400: loss = 2.6779017448425293\n",
      "step = 2769600: loss = 3.603688955307007\n",
      "step = 2769800: loss = 3.09901762008667\n",
      "step = 2770000: loss = 2.5543179512023926\n",
      "step = 2770000: Average Return = 2.4000000953674316\n",
      "step = 2770200: loss = 2.779045581817627\n",
      "step = 2770400: loss = 6.23335075378418\n",
      "step = 2770600: loss = 3.2575602531433105\n",
      "step = 2770800: loss = 3.516165018081665\n",
      "step = 2771000: loss = 2.6985321044921875\n",
      "step = 2771200: loss = 2.8938839435577393\n",
      "step = 2771400: loss = 3.5019781589508057\n",
      "step = 2771600: loss = 2.3081724643707275\n",
      "step = 2771800: loss = 2.802342176437378\n",
      "step = 2772000: loss = 4.272367000579834\n",
      "step = 2772200: loss = 5.540268898010254\n",
      "step = 2772400: loss = 3.210477828979492\n",
      "step = 2772600: loss = 2.692295551300049\n",
      "step = 2772800: loss = 3.51141095161438\n",
      "step = 2773000: loss = 2.1779608726501465\n",
      "step = 2773200: loss = 3.100428342819214\n",
      "step = 2773400: loss = 2.5962178707122803\n",
      "step = 2773600: loss = 3.8119659423828125\n",
      "step = 2773800: loss = 4.144641876220703\n",
      "step = 2774000: loss = 2.882035732269287\n",
      "step = 2774200: loss = 3.703253746032715\n",
      "step = 2774400: loss = 3.461747884750366\n",
      "step = 2774600: loss = 4.379998207092285\n",
      "step = 2774800: loss = 4.608229160308838\n",
      "step = 2775000: loss = 4.351942539215088\n",
      "step = 2775000: Average Return = 3.700000047683716\n",
      "step = 2775200: loss = 3.1799700260162354\n",
      "step = 2775400: loss = 3.6212682723999023\n",
      "step = 2775600: loss = 2.8375279903411865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2775800: loss = 3.0471179485321045\n",
      "step = 2776000: loss = 3.096787452697754\n",
      "step = 2776200: loss = 4.0198845863342285\n",
      "step = 2776400: loss = 3.907235622406006\n",
      "step = 2776600: loss = 3.9241185188293457\n",
      "step = 2776800: loss = 3.9264659881591797\n",
      "step = 2777000: loss = 1.4314799308776855\n",
      "step = 2777200: loss = 5.775545597076416\n",
      "step = 2777400: loss = 3.4712228775024414\n",
      "step = 2777600: loss = 4.659541606903076\n",
      "step = 2777800: loss = 3.090432643890381\n",
      "step = 2778000: loss = 4.867247104644775\n",
      "step = 2778200: loss = 4.301934719085693\n",
      "step = 2778400: loss = 2.3440403938293457\n",
      "step = 2778600: loss = 4.046743869781494\n",
      "step = 2778800: loss = 3.991426944732666\n",
      "step = 2779000: loss = 2.945340394973755\n",
      "step = 2779200: loss = 2.7819125652313232\n",
      "step = 2779400: loss = 2.421208381652832\n",
      "step = 2779600: loss = 3.5682647228240967\n",
      "step = 2779800: loss = 4.330775737762451\n",
      "step = 2780000: loss = 2.4547760486602783\n",
      "step = 2780000: Average Return = 3.049999952316284\n",
      "step = 2780200: loss = 2.7472176551818848\n",
      "step = 2780400: loss = 3.277081251144409\n",
      "step = 2780600: loss = 3.212559938430786\n",
      "step = 2780800: loss = 3.3508152961730957\n",
      "step = 2781000: loss = 4.488531589508057\n",
      "step = 2781200: loss = 3.018270254135132\n",
      "step = 2781400: loss = 3.0775392055511475\n",
      "step = 2781600: loss = 4.156526565551758\n",
      "step = 2781800: loss = 3.2085728645324707\n",
      "step = 2782000: loss = 2.7962706089019775\n",
      "step = 2782200: loss = 4.180670261383057\n",
      "step = 2782400: loss = 2.6519060134887695\n",
      "step = 2782600: loss = 3.8842451572418213\n",
      "step = 2782800: loss = 2.762298583984375\n",
      "step = 2783000: loss = 3.248884916305542\n",
      "step = 2783200: loss = 3.2601189613342285\n",
      "step = 2783400: loss = 3.1339967250823975\n",
      "step = 2783600: loss = 2.885739326477051\n",
      "step = 2783800: loss = 3.035047769546509\n",
      "step = 2784000: loss = 3.6670525074005127\n",
      "step = 2784200: loss = 2.395272970199585\n",
      "step = 2784400: loss = 2.6170663833618164\n",
      "step = 2784600: loss = 3.959094762802124\n",
      "step = 2784800: loss = 2.676543712615967\n",
      "step = 2785000: loss = 3.8890326023101807\n",
      "step = 2785000: Average Return = 5.349999904632568\n",
      "step = 2785200: loss = 3.597412586212158\n",
      "step = 2785400: loss = 2.67523455619812\n",
      "step = 2785600: loss = 3.8662519454956055\n",
      "step = 2785800: loss = 3.5519633293151855\n",
      "step = 2786000: loss = 3.3308093547821045\n",
      "step = 2786200: loss = 4.571948528289795\n",
      "step = 2786400: loss = 4.663545608520508\n",
      "step = 2786600: loss = 3.361720085144043\n",
      "step = 2786800: loss = 3.9793293476104736\n",
      "step = 2787000: loss = 3.5111260414123535\n",
      "step = 2787200: loss = 2.895695209503174\n",
      "step = 2787400: loss = 3.4506776332855225\n",
      "step = 2787600: loss = 3.4008171558380127\n",
      "step = 2787800: loss = 4.041881561279297\n",
      "step = 2788000: loss = 4.035005569458008\n",
      "step = 2788200: loss = 3.446075916290283\n",
      "step = 2788400: loss = 2.852105140686035\n",
      "step = 2788600: loss = 3.1870079040527344\n",
      "step = 2788800: loss = 3.2862203121185303\n",
      "step = 2789000: loss = 3.1131012439727783\n",
      "step = 2789200: loss = 3.7717418670654297\n",
      "step = 2789400: loss = 2.0586135387420654\n",
      "step = 2789600: loss = 3.0235326290130615\n",
      "step = 2789800: loss = 3.538283109664917\n",
      "step = 2790000: loss = 3.8269731998443604\n",
      "step = 2790000: Average Return = 5.349999904632568\n",
      "step = 2790200: loss = 4.262544631958008\n",
      "step = 2790400: loss = 3.173165798187256\n",
      "step = 2790600: loss = 2.2318270206451416\n",
      "step = 2790800: loss = 3.499614715576172\n",
      "step = 2791000: loss = 3.9672458171844482\n",
      "step = 2791200: loss = 2.214155435562134\n",
      "step = 2791400: loss = 3.6384007930755615\n",
      "step = 2791600: loss = 2.745091438293457\n",
      "step = 2791800: loss = 2.500370740890503\n",
      "step = 2792000: loss = 3.217806816101074\n",
      "step = 2792200: loss = 3.343925714492798\n",
      "step = 2792400: loss = 3.795353889465332\n",
      "step = 2792600: loss = 4.0691986083984375\n",
      "step = 2792800: loss = 3.530505895614624\n",
      "step = 2793000: loss = 3.254539728164673\n",
      "step = 2793200: loss = 5.768341064453125\n",
      "step = 2793400: loss = 3.6184206008911133\n",
      "step = 2793600: loss = 4.259666919708252\n",
      "step = 2793800: loss = 3.9812541007995605\n",
      "step = 2794000: loss = 2.600574254989624\n",
      "step = 2794200: loss = 4.01275634765625\n",
      "step = 2794400: loss = 3.9454667568206787\n",
      "step = 2794600: loss = 3.4010632038116455\n",
      "step = 2794800: loss = 2.8879826068878174\n",
      "step = 2795000: loss = 3.6473805904388428\n",
      "step = 2795000: Average Return = 4.650000095367432\n",
      "step = 2795200: loss = 5.157832145690918\n",
      "step = 2795400: loss = 3.309553384780884\n",
      "step = 2795600: loss = 4.1136016845703125\n",
      "step = 2795800: loss = 2.9874212741851807\n",
      "step = 2796000: loss = 3.2318532466888428\n",
      "step = 2796200: loss = 2.6727163791656494\n",
      "step = 2796400: loss = 4.2918620109558105\n",
      "step = 2796600: loss = 2.4246723651885986\n",
      "step = 2796800: loss = 4.439345359802246\n",
      "step = 2797000: loss = 5.024698734283447\n",
      "step = 2797200: loss = 3.0280020236968994\n",
      "step = 2797400: loss = 3.3952860832214355\n",
      "step = 2797600: loss = 2.9026055335998535\n",
      "step = 2797800: loss = 2.5239744186401367\n",
      "step = 2798000: loss = 2.686918258666992\n",
      "step = 2798200: loss = 3.0337109565734863\n",
      "step = 2798400: loss = 2.789266586303711\n",
      "step = 2798600: loss = 3.20448899269104\n",
      "step = 2798800: loss = 2.6136555671691895\n",
      "step = 2799000: loss = 2.7344601154327393\n",
      "step = 2799200: loss = 5.720197677612305\n",
      "step = 2799400: loss = 3.204716682434082\n",
      "step = 2799600: loss = 4.532166481018066\n",
      "step = 2799800: loss = 3.1871392726898193\n",
      "step = 2800000: loss = 2.494378089904785\n",
      "step = 2800000: Average Return = 4.150000095367432\n",
      "step = 2800200: loss = 4.037224769592285\n",
      "step = 2800400: loss = 2.879364013671875\n",
      "step = 2800600: loss = 4.52980375289917\n",
      "step = 2800800: loss = 3.453716993331909\n",
      "step = 2801000: loss = 4.489490032196045\n",
      "step = 2801200: loss = 3.3514018058776855\n",
      "step = 2801400: loss = 2.9408929347991943\n",
      "step = 2801600: loss = 3.658550977706909\n",
      "step = 2801800: loss = 3.4790215492248535\n",
      "step = 2802000: loss = 3.941188335418701\n",
      "step = 2802200: loss = 2.1673455238342285\n",
      "step = 2802400: loss = 3.508176803588867\n",
      "step = 2802600: loss = 4.221991539001465\n",
      "step = 2802800: loss = 2.8204219341278076\n",
      "step = 2803000: loss = 3.6533195972442627\n",
      "step = 2803200: loss = 4.1621623039245605\n",
      "step = 2803400: loss = 3.1714389324188232\n",
      "step = 2803600: loss = 2.8928446769714355\n",
      "step = 2803800: loss = 3.5785772800445557\n",
      "step = 2804000: loss = 3.73411226272583\n",
      "step = 2804200: loss = 3.6469030380249023\n",
      "step = 2804400: loss = 3.0226552486419678\n",
      "step = 2804600: loss = 3.7842538356781006\n",
      "step = 2804800: loss = 3.047868490219116\n",
      "step = 2805000: loss = 4.533174514770508\n",
      "step = 2805000: Average Return = 4.699999809265137\n",
      "step = 2805200: loss = 2.762347936630249\n",
      "step = 2805400: loss = 3.9792375564575195\n",
      "step = 2805600: loss = 2.7032558917999268\n",
      "step = 2805800: loss = 4.8194427490234375\n",
      "step = 2806000: loss = 3.050724744796753\n",
      "step = 2806200: loss = 4.096034526824951\n",
      "step = 2806400: loss = 2.414266347885132\n",
      "step = 2806600: loss = 4.120960712432861\n",
      "step = 2806800: loss = 2.9042391777038574\n",
      "step = 2807000: loss = 3.9683079719543457\n",
      "step = 2807200: loss = 3.2515063285827637\n",
      "step = 2807400: loss = 3.9426963329315186\n",
      "step = 2807600: loss = 4.733244895935059\n",
      "step = 2807800: loss = 3.5913445949554443\n",
      "step = 2808000: loss = 4.104026794433594\n",
      "step = 2808200: loss = 2.6727428436279297\n",
      "step = 2808400: loss = 4.34082555770874\n",
      "step = 2808600: loss = 2.862468719482422\n",
      "step = 2808800: loss = 2.652787446975708\n",
      "step = 2809000: loss = 2.2088418006896973\n",
      "step = 2809200: loss = 3.0593185424804688\n",
      "step = 2809400: loss = 3.5303661823272705\n",
      "step = 2809600: loss = 3.075343370437622\n",
      "step = 2809800: loss = 2.737389087677002\n",
      "step = 2810000: loss = 4.5916547775268555\n",
      "step = 2810000: Average Return = 3.299999952316284\n",
      "step = 2810200: loss = 3.7497191429138184\n",
      "step = 2810400: loss = 3.652834892272949\n",
      "step = 2810600: loss = 2.7907633781433105\n",
      "step = 2810800: loss = 2.0714542865753174\n",
      "step = 2811000: loss = 4.133660316467285\n",
      "step = 2811200: loss = 3.045117139816284\n",
      "step = 2811400: loss = 2.812636137008667\n",
      "step = 2811600: loss = 4.739466667175293\n",
      "step = 2811800: loss = 4.414465427398682\n",
      "step = 2812000: loss = 5.107624530792236\n",
      "step = 2812200: loss = 2.584340810775757\n",
      "step = 2812400: loss = 4.0045928955078125\n",
      "step = 2812600: loss = 4.259158611297607\n",
      "step = 2812800: loss = 4.34881591796875\n",
      "step = 2813000: loss = 1.9152034521102905\n",
      "step = 2813200: loss = 3.893763303756714\n",
      "step = 2813400: loss = 2.579474925994873\n",
      "step = 2813600: loss = 3.333531379699707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2813800: loss = 3.839897394180298\n",
      "step = 2814000: loss = 2.832841396331787\n",
      "step = 2814200: loss = 3.38753342628479\n",
      "step = 2814400: loss = 3.8953981399536133\n",
      "step = 2814600: loss = 3.943134069442749\n",
      "step = 2814800: loss = 3.4579973220825195\n",
      "step = 2815000: loss = 3.223841428756714\n",
      "step = 2815000: Average Return = 6.25\n",
      "step = 2815200: loss = 3.5930662155151367\n",
      "step = 2815400: loss = 2.1957461833953857\n",
      "step = 2815600: loss = 2.9757869243621826\n",
      "step = 2815800: loss = 4.219380855560303\n",
      "step = 2816000: loss = 4.381907939910889\n",
      "step = 2816200: loss = 3.195312976837158\n",
      "step = 2816400: loss = 4.447340488433838\n",
      "step = 2816600: loss = 2.293313503265381\n",
      "step = 2816800: loss = 3.9432103633880615\n",
      "step = 2817000: loss = 3.8291921615600586\n",
      "step = 2817200: loss = 2.4946484565734863\n",
      "step = 2817400: loss = 4.158511638641357\n",
      "step = 2817600: loss = 2.430424213409424\n",
      "step = 2817800: loss = 3.6725873947143555\n",
      "step = 2818000: loss = 2.8682169914245605\n",
      "step = 2818200: loss = 3.1790220737457275\n",
      "step = 2818400: loss = 2.5655391216278076\n",
      "step = 2818600: loss = 3.509064197540283\n",
      "step = 2818800: loss = 2.919774293899536\n",
      "step = 2819000: loss = 2.9733285903930664\n",
      "step = 2819200: loss = 2.4824931621551514\n",
      "step = 2819400: loss = 3.9744856357574463\n",
      "step = 2819600: loss = 3.166929244995117\n",
      "step = 2819800: loss = 3.1972267627716064\n",
      "step = 2820000: loss = 4.337849140167236\n",
      "step = 2820000: Average Return = 2.5999999046325684\n",
      "step = 2820200: loss = 2.8219549655914307\n",
      "step = 2820400: loss = 2.647444486618042\n",
      "step = 2820600: loss = 3.306173086166382\n",
      "step = 2820800: loss = 3.3474552631378174\n",
      "step = 2821000: loss = 4.3021745681762695\n",
      "step = 2821200: loss = 3.0394585132598877\n",
      "step = 2821400: loss = 3.3074071407318115\n",
      "step = 2821600: loss = 3.607177972793579\n",
      "step = 2821800: loss = 3.744741678237915\n",
      "step = 2822000: loss = 3.7317333221435547\n",
      "step = 2822200: loss = 3.8357954025268555\n",
      "step = 2822400: loss = 3.6772308349609375\n",
      "step = 2822600: loss = 4.168479919433594\n",
      "step = 2822800: loss = 2.6668498516082764\n",
      "step = 2823000: loss = 2.787109851837158\n",
      "step = 2823200: loss = 2.922147750854492\n",
      "step = 2823400: loss = 3.62701153755188\n",
      "step = 2823600: loss = 3.1295111179351807\n",
      "step = 2823800: loss = 2.909437417984009\n",
      "step = 2824000: loss = 4.378776550292969\n",
      "step = 2824200: loss = 3.683807373046875\n",
      "step = 2824400: loss = 2.525580644607544\n",
      "step = 2824600: loss = 2.46146559715271\n",
      "step = 2824800: loss = 3.0429627895355225\n",
      "step = 2825000: loss = 2.5263655185699463\n",
      "step = 2825000: Average Return = 3.299999952316284\n",
      "step = 2825200: loss = 3.369964599609375\n",
      "step = 2825400: loss = 3.136012315750122\n",
      "step = 2825600: loss = 3.3207337856292725\n",
      "step = 2825800: loss = 3.460726737976074\n",
      "step = 2826000: loss = 3.533115863800049\n",
      "step = 2826200: loss = 3.116016387939453\n",
      "step = 2826400: loss = 3.938715696334839\n",
      "step = 2826600: loss = 3.0676519870758057\n",
      "step = 2826800: loss = 4.236595630645752\n",
      "step = 2827000: loss = 2.8288896083831787\n",
      "step = 2827200: loss = 5.120553970336914\n",
      "step = 2827400: loss = 2.3847899436950684\n",
      "step = 2827600: loss = 3.0987532138824463\n",
      "step = 2827800: loss = 3.1171483993530273\n",
      "step = 2828000: loss = 4.807959079742432\n",
      "step = 2828200: loss = 3.1135172843933105\n",
      "step = 2828400: loss = 4.222359657287598\n",
      "step = 2828600: loss = 2.121904134750366\n",
      "step = 2828800: loss = 3.1469621658325195\n",
      "step = 2829000: loss = 3.0051231384277344\n",
      "step = 2829200: loss = 4.246245861053467\n",
      "step = 2829400: loss = 2.6993887424468994\n",
      "step = 2829600: loss = 2.6845948696136475\n",
      "step = 2829800: loss = 3.5419321060180664\n",
      "step = 2830000: loss = 2.786377429962158\n",
      "step = 2830000: Average Return = 5.150000095367432\n",
      "step = 2830200: loss = 2.347191333770752\n",
      "step = 2830400: loss = 4.603576183319092\n",
      "step = 2830600: loss = 3.1295573711395264\n",
      "step = 2830800: loss = 3.2329933643341064\n",
      "step = 2831000: loss = 2.97084903717041\n",
      "step = 2831200: loss = 3.977997064590454\n",
      "step = 2831400: loss = 4.244034767150879\n",
      "step = 2831600: loss = 2.690455198287964\n",
      "step = 2831800: loss = 2.5470733642578125\n",
      "step = 2832000: loss = 3.197796583175659\n",
      "step = 2832200: loss = 5.620516300201416\n",
      "step = 2832400: loss = 3.298953056335449\n",
      "step = 2832600: loss = 3.6173362731933594\n",
      "step = 2832800: loss = 2.579371213912964\n",
      "step = 2833000: loss = 2.628406286239624\n",
      "step = 2833200: loss = 2.641490936279297\n",
      "step = 2833400: loss = 3.206519603729248\n",
      "step = 2833600: loss = 3.1208934783935547\n",
      "step = 2833800: loss = 3.961266279220581\n",
      "step = 2834000: loss = 2.666276216506958\n",
      "step = 2834200: loss = 3.2893247604370117\n",
      "step = 2834400: loss = 3.1830132007598877\n",
      "step = 2834600: loss = 3.027700185775757\n",
      "step = 2834800: loss = 3.1390697956085205\n",
      "step = 2835000: loss = 3.002185583114624\n",
      "step = 2835000: Average Return = 5.599999904632568\n",
      "step = 2835200: loss = 3.4425978660583496\n",
      "step = 2835400: loss = 3.2784974575042725\n",
      "step = 2835600: loss = 3.475285053253174\n",
      "step = 2835800: loss = 3.5582258701324463\n",
      "step = 2836000: loss = 3.1713414192199707\n",
      "step = 2836200: loss = 3.4650747776031494\n",
      "step = 2836400: loss = 3.4893243312835693\n",
      "step = 2836600: loss = 3.091317892074585\n",
      "step = 2836800: loss = 3.061227321624756\n",
      "step = 2837000: loss = 3.3730528354644775\n",
      "step = 2837200: loss = 2.8399531841278076\n",
      "step = 2837400: loss = 3.5827174186706543\n",
      "step = 2837600: loss = 3.535944700241089\n",
      "step = 2837800: loss = 3.6373348236083984\n",
      "step = 2838000: loss = 3.444736957550049\n",
      "step = 2838200: loss = 3.6538844108581543\n",
      "step = 2838400: loss = 4.618093967437744\n",
      "step = 2838600: loss = 4.962942600250244\n",
      "step = 2838800: loss = 1.8654003143310547\n",
      "step = 2839000: loss = 4.551827430725098\n",
      "step = 2839200: loss = 2.7780706882476807\n",
      "step = 2839400: loss = 4.004354000091553\n",
      "step = 2839600: loss = 2.4485645294189453\n",
      "step = 2839800: loss = 2.627535104751587\n",
      "step = 2840000: loss = 3.3399581909179688\n",
      "step = 2840000: Average Return = 5.400000095367432\n",
      "step = 2840200: loss = 3.732938051223755\n",
      "step = 2840400: loss = 5.34416389465332\n",
      "step = 2840600: loss = 3.232180595397949\n",
      "step = 2840800: loss = 3.9721453189849854\n",
      "step = 2841000: loss = 4.207447528839111\n",
      "step = 2841200: loss = 2.384512186050415\n",
      "step = 2841400: loss = 3.4641661643981934\n",
      "step = 2841600: loss = 3.4639623165130615\n",
      "step = 2841800: loss = 3.7703001499176025\n",
      "step = 2842000: loss = 2.9583096504211426\n",
      "step = 2842200: loss = 2.6832528114318848\n",
      "step = 2842400: loss = 3.036956787109375\n",
      "step = 2842600: loss = 3.7861132621765137\n",
      "step = 2842800: loss = 4.075228691101074\n",
      "step = 2843000: loss = 4.045968055725098\n",
      "step = 2843200: loss = 2.689805746078491\n",
      "step = 2843400: loss = 3.015721559524536\n",
      "step = 2843600: loss = 3.6368048191070557\n",
      "step = 2843800: loss = 3.437760591506958\n",
      "step = 2844000: loss = 2.7842631340026855\n",
      "step = 2844200: loss = 4.588944435119629\n",
      "step = 2844400: loss = 4.563174247741699\n",
      "step = 2844600: loss = 3.858978748321533\n",
      "step = 2844800: loss = 2.712675094604492\n",
      "step = 2845000: loss = 3.198702335357666\n",
      "step = 2845000: Average Return = 5.199999809265137\n",
      "step = 2845200: loss = 2.457655668258667\n",
      "step = 2845400: loss = 2.8165764808654785\n",
      "step = 2845600: loss = 2.484006404876709\n",
      "step = 2845800: loss = 3.864758253097534\n",
      "step = 2846000: loss = 2.3732922077178955\n",
      "step = 2846200: loss = 4.535395622253418\n",
      "step = 2846400: loss = 3.579599142074585\n",
      "step = 2846600: loss = 2.7861039638519287\n",
      "step = 2846800: loss = 3.5000157356262207\n",
      "step = 2847000: loss = 3.812248945236206\n",
      "step = 2847200: loss = 3.1869938373565674\n",
      "step = 2847400: loss = 3.916938304901123\n",
      "step = 2847600: loss = 4.504390239715576\n",
      "step = 2847800: loss = 2.81772780418396\n",
      "step = 2848000: loss = 3.302988290786743\n",
      "step = 2848200: loss = 4.243031978607178\n",
      "step = 2848400: loss = 3.672646999359131\n",
      "step = 2848600: loss = 5.887497425079346\n",
      "step = 2848800: loss = 3.987722158432007\n",
      "step = 2849000: loss = 3.3654613494873047\n",
      "step = 2849200: loss = 3.1267638206481934\n",
      "step = 2849400: loss = 3.7814652919769287\n",
      "step = 2849600: loss = 4.501937389373779\n",
      "step = 2849800: loss = 3.5573079586029053\n",
      "step = 2850000: loss = 3.743528127670288\n",
      "step = 2850000: Average Return = 3.450000047683716\n",
      "step = 2850200: loss = 2.6741766929626465\n",
      "step = 2850400: loss = 2.772149085998535\n",
      "step = 2850600: loss = 2.90480637550354\n",
      "step = 2850800: loss = 3.415790557861328\n",
      "step = 2851000: loss = 4.196531295776367\n",
      "step = 2851200: loss = 3.3983967304229736\n",
      "step = 2851400: loss = 2.450608730316162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2851600: loss = 3.2105870246887207\n",
      "step = 2851800: loss = 3.2431609630584717\n",
      "step = 2852000: loss = 3.383578300476074\n",
      "step = 2852200: loss = 2.6810412406921387\n",
      "step = 2852400: loss = 3.2870864868164062\n",
      "step = 2852600: loss = 3.4920177459716797\n",
      "step = 2852800: loss = 3.1592302322387695\n",
      "step = 2853000: loss = 3.9330732822418213\n",
      "step = 2853200: loss = 3.434661865234375\n",
      "step = 2853400: loss = 3.715247631072998\n",
      "step = 2853600: loss = 3.587968111038208\n",
      "step = 2853800: loss = 3.8207855224609375\n",
      "step = 2854000: loss = 4.7128095626831055\n",
      "step = 2854200: loss = 3.8896923065185547\n",
      "step = 2854400: loss = 2.1688575744628906\n",
      "step = 2854600: loss = 3.444999933242798\n",
      "step = 2854800: loss = 2.44864559173584\n",
      "step = 2855000: loss = 3.5471179485321045\n",
      "step = 2855000: Average Return = 1.399999976158142\n",
      "step = 2855200: loss = 3.760585308074951\n",
      "step = 2855400: loss = 4.154103755950928\n",
      "step = 2855600: loss = 6.029599666595459\n",
      "step = 2855800: loss = 3.551370143890381\n",
      "step = 2856000: loss = 4.204890251159668\n",
      "step = 2856200: loss = 3.8580198287963867\n",
      "step = 2856400: loss = 3.8142943382263184\n",
      "step = 2856600: loss = 2.8414525985717773\n",
      "step = 2856800: loss = 2.351844310760498\n",
      "step = 2857000: loss = 3.36315655708313\n",
      "step = 2857200: loss = 4.934335708618164\n",
      "step = 2857400: loss = 4.850041389465332\n",
      "step = 2857600: loss = 3.388885736465454\n",
      "step = 2857800: loss = 3.4646382331848145\n",
      "step = 2858000: loss = 2.345219612121582\n",
      "step = 2858200: loss = 4.087808609008789\n",
      "step = 2858400: loss = 3.4587316513061523\n",
      "step = 2858600: loss = 3.5574724674224854\n",
      "step = 2858800: loss = 3.804917573928833\n",
      "step = 2859000: loss = 4.150183200836182\n",
      "step = 2859200: loss = 3.3324480056762695\n",
      "step = 2859400: loss = 3.786127805709839\n",
      "step = 2859600: loss = 4.4794487953186035\n",
      "step = 2859800: loss = 3.914889335632324\n",
      "step = 2860000: loss = 4.335786819458008\n",
      "step = 2860000: Average Return = 3.0999999046325684\n",
      "step = 2860200: loss = 3.7117857933044434\n",
      "step = 2860400: loss = 1.9138463735580444\n",
      "step = 2860600: loss = 4.212063312530518\n",
      "step = 2860800: loss = 2.863347053527832\n",
      "step = 2861000: loss = 4.120721817016602\n",
      "step = 2861200: loss = 3.274785041809082\n",
      "step = 2861400: loss = 3.758096218109131\n",
      "step = 2861600: loss = 3.809516429901123\n",
      "step = 2861800: loss = 3.234375238418579\n",
      "step = 2862000: loss = 4.225972652435303\n",
      "step = 2862200: loss = 5.457847595214844\n",
      "step = 2862400: loss = 4.547698020935059\n",
      "step = 2862600: loss = 3.30721116065979\n",
      "step = 2862800: loss = 5.532597541809082\n",
      "step = 2863000: loss = 2.6685054302215576\n",
      "step = 2863200: loss = 3.3949971199035645\n",
      "step = 2863400: loss = 3.185476064682007\n",
      "step = 2863600: loss = 5.116026401519775\n",
      "step = 2863800: loss = 2.8170647621154785\n",
      "step = 2864000: loss = 3.556594133377075\n",
      "step = 2864200: loss = 2.3397819995880127\n",
      "step = 2864400: loss = 3.5534770488739014\n",
      "step = 2864600: loss = 4.59050989151001\n",
      "step = 2864800: loss = 2.6952497959136963\n",
      "step = 2865000: loss = 3.8384459018707275\n",
      "step = 2865000: Average Return = 4.150000095367432\n",
      "step = 2865200: loss = 2.9572479724884033\n",
      "step = 2865400: loss = 2.1948235034942627\n",
      "step = 2865600: loss = 3.5596892833709717\n",
      "step = 2865800: loss = 3.712597370147705\n",
      "step = 2866000: loss = 2.6857738494873047\n",
      "step = 2866200: loss = 2.6514382362365723\n",
      "step = 2866400: loss = 2.914984703063965\n",
      "step = 2866600: loss = 3.154405355453491\n",
      "step = 2866800: loss = 4.303988456726074\n",
      "step = 2867000: loss = 3.42297101020813\n",
      "step = 2867200: loss = 3.3193583488464355\n",
      "step = 2867400: loss = 2.5853970050811768\n",
      "step = 2867600: loss = 3.9407408237457275\n",
      "step = 2867800: loss = 2.2039990425109863\n",
      "step = 2868000: loss = 3.7356910705566406\n",
      "step = 2868200: loss = 2.261263132095337\n",
      "step = 2868400: loss = 5.566298484802246\n",
      "step = 2868600: loss = 3.954667568206787\n",
      "step = 2868800: loss = 3.660447359085083\n",
      "step = 2869000: loss = 2.833422899246216\n",
      "step = 2869200: loss = 3.385545492172241\n",
      "step = 2869400: loss = 5.7995285987854\n",
      "step = 2869600: loss = 4.019569396972656\n",
      "step = 2869800: loss = 4.1013922691345215\n",
      "step = 2870000: loss = 3.723531484603882\n",
      "step = 2870000: Average Return = 5.949999809265137\n",
      "step = 2870200: loss = 4.0535688400268555\n",
      "step = 2870400: loss = 3.3582396507263184\n",
      "step = 2870600: loss = 5.026003360748291\n",
      "step = 2870800: loss = 3.872685432434082\n",
      "step = 2871000: loss = 3.0633392333984375\n",
      "step = 2871200: loss = 3.9917397499084473\n",
      "step = 2871400: loss = 2.844703197479248\n",
      "step = 2871600: loss = 3.211554527282715\n",
      "step = 2871800: loss = 2.5393309593200684\n",
      "step = 2872000: loss = 3.1145641803741455\n",
      "step = 2872200: loss = 3.6720378398895264\n",
      "step = 2872400: loss = 3.0190048217773438\n",
      "step = 2872600: loss = 2.779461622238159\n",
      "step = 2872800: loss = 3.2478160858154297\n",
      "step = 2873000: loss = 4.802853107452393\n",
      "step = 2873200: loss = 3.5407557487487793\n",
      "step = 2873400: loss = 3.3018674850463867\n",
      "step = 2873600: loss = 5.319234371185303\n",
      "step = 2873800: loss = 5.264412879943848\n",
      "step = 2874000: loss = 4.110105514526367\n",
      "step = 2874200: loss = 3.500486373901367\n",
      "step = 2874400: loss = 3.1392455101013184\n",
      "step = 2874600: loss = 5.526134967803955\n",
      "step = 2874800: loss = 2.864750623703003\n",
      "step = 2875000: loss = 3.983433485031128\n",
      "step = 2875000: Average Return = 4.900000095367432\n",
      "step = 2875200: loss = 3.5268640518188477\n",
      "step = 2875400: loss = 3.2308759689331055\n",
      "step = 2875600: loss = 2.4399023056030273\n",
      "step = 2875800: loss = 3.582951307296753\n",
      "step = 2876000: loss = 4.856563091278076\n",
      "step = 2876200: loss = 3.044276475906372\n",
      "step = 2876400: loss = 3.303711175918579\n",
      "step = 2876600: loss = 4.337863445281982\n",
      "step = 2876800: loss = 3.801666736602783\n",
      "step = 2877000: loss = 2.443190097808838\n",
      "step = 2877200: loss = 2.91672945022583\n",
      "step = 2877400: loss = 3.3800811767578125\n",
      "step = 2877600: loss = 3.11199688911438\n",
      "step = 2877800: loss = 3.8429601192474365\n",
      "step = 2878000: loss = 3.618948221206665\n",
      "step = 2878200: loss = 3.116464138031006\n",
      "step = 2878400: loss = 4.358007431030273\n",
      "step = 2878600: loss = 3.7011425495147705\n",
      "step = 2878800: loss = 3.715000629425049\n",
      "step = 2879000: loss = 3.3159632682800293\n",
      "step = 2879200: loss = 4.116362571716309\n",
      "step = 2879400: loss = 3.0559887886047363\n",
      "step = 2879600: loss = 2.9966697692871094\n",
      "step = 2879800: loss = 4.815385341644287\n",
      "step = 2880000: loss = 3.605835437774658\n",
      "step = 2880000: Average Return = 4.449999809265137\n",
      "step = 2880200: loss = 3.1666133403778076\n",
      "step = 2880400: loss = 4.347137451171875\n",
      "step = 2880600: loss = 4.028041839599609\n",
      "step = 2880800: loss = 5.278152942657471\n",
      "step = 2881000: loss = 4.718750953674316\n",
      "step = 2881200: loss = 3.4683384895324707\n",
      "step = 2881400: loss = 3.408055067062378\n",
      "step = 2881600: loss = 3.6505987644195557\n",
      "step = 2881800: loss = 3.916846752166748\n",
      "step = 2882000: loss = 3.547916889190674\n",
      "step = 2882200: loss = 2.79766845703125\n",
      "step = 2882400: loss = 2.6595876216888428\n",
      "step = 2882600: loss = 4.556924819946289\n",
      "step = 2882800: loss = 3.3774430751800537\n",
      "step = 2883000: loss = 3.447960138320923\n",
      "step = 2883200: loss = 2.7840805053710938\n",
      "step = 2883400: loss = 3.385737180709839\n",
      "step = 2883600: loss = 4.357395172119141\n",
      "step = 2883800: loss = 3.2407760620117188\n",
      "step = 2884000: loss = 3.0596816539764404\n",
      "step = 2884200: loss = 3.3840620517730713\n",
      "step = 2884400: loss = 3.2276391983032227\n",
      "step = 2884600: loss = 3.8426058292388916\n",
      "step = 2884800: loss = 4.289844989776611\n",
      "step = 2885000: loss = 3.748511791229248\n",
      "step = 2885000: Average Return = 5.150000095367432\n",
      "step = 2885200: loss = 4.127717971801758\n",
      "step = 2885400: loss = 3.1659841537475586\n",
      "step = 2885600: loss = 3.702711820602417\n",
      "step = 2885800: loss = 3.423032760620117\n",
      "step = 2886000: loss = 3.8401100635528564\n",
      "step = 2886200: loss = 3.4299535751342773\n",
      "step = 2886400: loss = 3.269113779067993\n",
      "step = 2886600: loss = 2.8153514862060547\n",
      "step = 2886800: loss = 5.315888404846191\n",
      "step = 2887000: loss = 2.691760778427124\n",
      "step = 2887200: loss = 3.7327091693878174\n",
      "step = 2887400: loss = 4.041630744934082\n",
      "step = 2887600: loss = 3.979196310043335\n",
      "step = 2887800: loss = 3.574666738510132\n",
      "step = 2888000: loss = 4.674137115478516\n",
      "step = 2888200: loss = 2.5291948318481445\n",
      "step = 2888400: loss = 3.250537633895874\n",
      "step = 2888600: loss = 2.558089017868042\n",
      "step = 2888800: loss = 3.913357973098755\n",
      "step = 2889000: loss = 4.332282543182373\n",
      "step = 2889200: loss = 2.4607012271881104\n",
      "step = 2889400: loss = 4.745522499084473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2889600: loss = 4.105504035949707\n",
      "step = 2889800: loss = 3.762542724609375\n",
      "step = 2890000: loss = 3.8663837909698486\n",
      "step = 2890000: Average Return = 2.549999952316284\n",
      "step = 2890200: loss = 3.348818302154541\n",
      "step = 2890400: loss = 3.6847853660583496\n",
      "step = 2890600: loss = 2.9363768100738525\n",
      "step = 2890800: loss = 3.7503325939178467\n",
      "step = 2891000: loss = 1.561279058456421\n",
      "step = 2891200: loss = 3.8533549308776855\n",
      "step = 2891400: loss = 3.624660015106201\n",
      "step = 2891600: loss = 4.434710502624512\n",
      "step = 2891800: loss = 3.3921895027160645\n",
      "step = 2892000: loss = 4.484179496765137\n",
      "step = 2892200: loss = 6.055502414703369\n",
      "step = 2892400: loss = 2.849550485610962\n",
      "step = 2892600: loss = 3.903944730758667\n",
      "step = 2892800: loss = 3.546522855758667\n",
      "step = 2893000: loss = 3.403686761856079\n",
      "step = 2893200: loss = 3.2271111011505127\n",
      "step = 2893400: loss = 3.1027445793151855\n",
      "step = 2893600: loss = 4.222938537597656\n",
      "step = 2893800: loss = 3.405149459838867\n",
      "step = 2894000: loss = 3.7732701301574707\n",
      "step = 2894200: loss = 3.50700306892395\n",
      "step = 2894400: loss = 4.289352893829346\n",
      "step = 2894600: loss = 2.968376398086548\n",
      "step = 2894800: loss = 3.758124351501465\n",
      "step = 2895000: loss = 3.6741626262664795\n",
      "step = 2895000: Average Return = 3.4000000953674316\n",
      "step = 2895200: loss = 3.010096788406372\n",
      "step = 2895400: loss = 3.1431167125701904\n",
      "step = 2895600: loss = 4.15164041519165\n",
      "step = 2895800: loss = 3.037071466445923\n",
      "step = 2896000: loss = 4.4111456871032715\n",
      "step = 2896200: loss = 4.0474534034729\n",
      "step = 2896400: loss = 4.859786033630371\n",
      "step = 2896600: loss = 3.834946870803833\n",
      "step = 2896800: loss = 3.1528680324554443\n",
      "step = 2897000: loss = 4.369678020477295\n",
      "step = 2897200: loss = 4.057513236999512\n",
      "step = 2897400: loss = 4.700647354125977\n",
      "step = 2897600: loss = 4.343300819396973\n",
      "step = 2897800: loss = 4.010574817657471\n",
      "step = 2898000: loss = 4.2230000495910645\n",
      "step = 2898200: loss = 2.577312469482422\n",
      "step = 2898400: loss = 2.7590582370758057\n",
      "step = 2898600: loss = 4.4799017906188965\n",
      "step = 2898800: loss = 3.7566473484039307\n",
      "step = 2899000: loss = 4.240652561187744\n",
      "step = 2899200: loss = 2.8930041790008545\n",
      "step = 2899400: loss = 3.8761956691741943\n",
      "step = 2899600: loss = 4.2458367347717285\n",
      "step = 2899800: loss = 3.093938112258911\n",
      "step = 2900000: loss = 2.811793804168701\n",
      "step = 2900000: Average Return = 4.300000190734863\n",
      "step = 2900200: loss = 3.351600170135498\n",
      "step = 2900400: loss = 4.920687198638916\n",
      "step = 2900600: loss = 4.301951885223389\n",
      "step = 2900800: loss = 2.6841156482696533\n",
      "step = 2901000: loss = 4.477691650390625\n",
      "step = 2901200: loss = 2.569261312484741\n",
      "step = 2901400: loss = 3.761747360229492\n",
      "step = 2901600: loss = 2.132456064224243\n",
      "step = 2901800: loss = 2.659768581390381\n",
      "step = 2902000: loss = 3.828568696975708\n",
      "step = 2902200: loss = 3.2190818786621094\n",
      "step = 2902400: loss = 2.559736728668213\n",
      "step = 2902600: loss = 2.8348774909973145\n",
      "step = 2902800: loss = 3.315809965133667\n",
      "step = 2903000: loss = 3.674360990524292\n",
      "step = 2903200: loss = 2.918079376220703\n",
      "step = 2903400: loss = 2.9624316692352295\n",
      "step = 2903600: loss = 3.218526601791382\n",
      "step = 2903800: loss = 4.833218097686768\n",
      "step = 2904000: loss = 2.529653787612915\n",
      "step = 2904200: loss = 4.834175109863281\n",
      "step = 2904400: loss = 2.7248942852020264\n",
      "step = 2904600: loss = 3.575479030609131\n",
      "step = 2904800: loss = 2.656278133392334\n",
      "step = 2905000: loss = 2.734534740447998\n",
      "step = 2905000: Average Return = 3.5999999046325684\n",
      "step = 2905200: loss = 3.0865800380706787\n",
      "step = 2905400: loss = 2.76369309425354\n",
      "step = 2905600: loss = 3.5555882453918457\n",
      "step = 2905800: loss = 4.947300434112549\n",
      "step = 2906000: loss = 3.8666276931762695\n",
      "step = 2906200: loss = 4.017066955566406\n",
      "step = 2906400: loss = 3.837348461151123\n",
      "step = 2906600: loss = 2.6511080265045166\n",
      "step = 2906800: loss = 3.488408088684082\n",
      "step = 2907000: loss = 3.013151168823242\n",
      "step = 2907200: loss = 3.6591780185699463\n",
      "step = 2907400: loss = 4.548677444458008\n",
      "step = 2907600: loss = 3.836765766143799\n",
      "step = 2907800: loss = 4.5362467765808105\n",
      "step = 2908000: loss = 3.1875722408294678\n",
      "step = 2908200: loss = 3.6583328247070312\n",
      "step = 2908400: loss = 2.9906413555145264\n",
      "step = 2908600: loss = 3.511735200881958\n",
      "step = 2908800: loss = 3.677454948425293\n",
      "step = 2909000: loss = 3.8475897312164307\n",
      "step = 2909200: loss = 2.98525333404541\n",
      "step = 2909400: loss = 3.031594753265381\n",
      "step = 2909600: loss = 5.214141845703125\n",
      "step = 2909800: loss = 3.2288708686828613\n",
      "step = 2910000: loss = 3.2910594940185547\n",
      "step = 2910000: Average Return = 3.700000047683716\n",
      "step = 2910200: loss = 2.719744920730591\n",
      "step = 2910400: loss = 3.937253952026367\n",
      "step = 2910600: loss = 3.2052972316741943\n",
      "step = 2910800: loss = 4.201887130737305\n",
      "step = 2911000: loss = 3.0097992420196533\n",
      "step = 2911200: loss = 2.81770920753479\n",
      "step = 2911400: loss = 2.9994871616363525\n",
      "step = 2911600: loss = 3.100264549255371\n",
      "step = 2911800: loss = 4.385867118835449\n",
      "step = 2912000: loss = 3.311875820159912\n",
      "step = 2912200: loss = 3.6055943965911865\n",
      "step = 2912400: loss = 4.539213180541992\n",
      "step = 2912600: loss = 3.5379791259765625\n",
      "step = 2912800: loss = 3.8886053562164307\n",
      "step = 2913000: loss = 2.9770376682281494\n",
      "step = 2913200: loss = 4.358254909515381\n",
      "step = 2913400: loss = 2.3108906745910645\n",
      "step = 2913600: loss = 6.299922943115234\n",
      "step = 2913800: loss = 2.8232195377349854\n",
      "step = 2914000: loss = 3.8430614471435547\n",
      "step = 2914200: loss = 2.3182148933410645\n",
      "step = 2914400: loss = 3.5243024826049805\n",
      "step = 2914600: loss = 4.504244327545166\n",
      "step = 2914800: loss = 2.8965673446655273\n",
      "step = 2915000: loss = 3.397799015045166\n",
      "step = 2915000: Average Return = 4.150000095367432\n",
      "step = 2915200: loss = 4.8271098136901855\n",
      "step = 2915400: loss = 3.21911358833313\n",
      "step = 2915600: loss = 5.236834526062012\n",
      "step = 2915800: loss = 4.367012023925781\n",
      "step = 2916000: loss = 3.3626537322998047\n",
      "step = 2916200: loss = 2.1845147609710693\n",
      "step = 2916400: loss = 2.830646276473999\n",
      "step = 2916600: loss = 3.7127058506011963\n",
      "step = 2916800: loss = 4.883697986602783\n",
      "step = 2917000: loss = 2.7259085178375244\n",
      "step = 2917200: loss = 3.9149436950683594\n",
      "step = 2917400: loss = 2.8146655559539795\n",
      "step = 2917600: loss = 3.866701602935791\n",
      "step = 2917800: loss = 3.219808340072632\n",
      "step = 2918000: loss = 3.799001932144165\n",
      "step = 2918200: loss = 3.192676067352295\n",
      "step = 2918400: loss = 2.666642904281616\n",
      "step = 2918600: loss = 3.3569014072418213\n",
      "step = 2918800: loss = 2.922255754470825\n",
      "step = 2919000: loss = 2.430107355117798\n",
      "step = 2919200: loss = 3.8429107666015625\n",
      "step = 2919400: loss = 3.699186325073242\n",
      "step = 2919600: loss = 4.119043350219727\n",
      "step = 2919800: loss = 3.675712823867798\n",
      "step = 2920000: loss = 4.137717247009277\n",
      "step = 2920000: Average Return = 4.0\n",
      "step = 2920200: loss = 3.5147581100463867\n",
      "step = 2920400: loss = 4.992697715759277\n",
      "step = 2920600: loss = 5.073354244232178\n",
      "step = 2920800: loss = 2.082843780517578\n",
      "step = 2921000: loss = 3.1846745014190674\n",
      "step = 2921200: loss = 2.6740832328796387\n",
      "step = 2921400: loss = 2.58444881439209\n",
      "step = 2921600: loss = 5.145052909851074\n",
      "step = 2921800: loss = 4.353059768676758\n",
      "step = 2922000: loss = 2.600236177444458\n",
      "step = 2922200: loss = 4.496820449829102\n",
      "step = 2922400: loss = 3.3008501529693604\n",
      "step = 2922600: loss = 2.731433629989624\n",
      "step = 2922800: loss = 3.488149881362915\n",
      "step = 2923000: loss = 4.8721137046813965\n",
      "step = 2923200: loss = 3.3683178424835205\n",
      "step = 2923400: loss = 2.5864369869232178\n",
      "step = 2923600: loss = 3.2130486965179443\n",
      "step = 2923800: loss = 3.2046639919281006\n",
      "step = 2924000: loss = 3.2113866806030273\n",
      "step = 2924200: loss = 2.9939708709716797\n",
      "step = 2924400: loss = 2.410043954849243\n",
      "step = 2924600: loss = 3.709911823272705\n",
      "step = 2924800: loss = 2.4705002307891846\n",
      "step = 2925000: loss = 2.7072255611419678\n",
      "step = 2925000: Average Return = 2.9000000953674316\n",
      "step = 2925200: loss = 3.9837615489959717\n",
      "step = 2925400: loss = 2.6770410537719727\n",
      "step = 2925600: loss = 4.474247455596924\n",
      "step = 2925800: loss = 3.661027193069458\n",
      "step = 2926000: loss = 3.068286657333374\n",
      "step = 2926200: loss = 2.590916395187378\n",
      "step = 2926400: loss = 2.962333917617798\n",
      "step = 2926600: loss = 3.6807217597961426\n",
      "step = 2926800: loss = 4.439573764801025\n",
      "step = 2927000: loss = 3.351783275604248\n",
      "step = 2927200: loss = 4.900755405426025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2927400: loss = 3.51348876953125\n",
      "step = 2927600: loss = 3.1322381496429443\n",
      "step = 2927800: loss = 2.8963210582733154\n",
      "step = 2928000: loss = 2.919368267059326\n",
      "step = 2928200: loss = 2.470257043838501\n",
      "step = 2928400: loss = 2.602710485458374\n",
      "step = 2928600: loss = 3.0272581577301025\n",
      "step = 2928800: loss = 3.2868359088897705\n",
      "step = 2929000: loss = 3.757803201675415\n",
      "step = 2929200: loss = 2.262524366378784\n",
      "step = 2929400: loss = 5.772360801696777\n",
      "step = 2929600: loss = 3.6523406505584717\n",
      "step = 2929800: loss = 3.3765218257904053\n",
      "step = 2930000: loss = 2.9159319400787354\n",
      "step = 2930000: Average Return = 5.099999904632568\n",
      "step = 2930200: loss = 3.6547088623046875\n",
      "step = 2930400: loss = 4.010072231292725\n",
      "step = 2930600: loss = 6.074926853179932\n",
      "step = 2930800: loss = 2.704352855682373\n",
      "step = 2931000: loss = 4.0349884033203125\n",
      "step = 2931200: loss = 3.933281660079956\n",
      "step = 2931400: loss = 3.552727699279785\n",
      "step = 2931600: loss = 4.976625442504883\n",
      "step = 2931800: loss = 2.608661413192749\n",
      "step = 2932000: loss = 3.8579156398773193\n",
      "step = 2932200: loss = 3.8141162395477295\n",
      "step = 2932400: loss = 3.1201062202453613\n",
      "step = 2932600: loss = 2.7161617279052734\n",
      "step = 2932800: loss = 3.7933437824249268\n",
      "step = 2933000: loss = 3.531834363937378\n",
      "step = 2933200: loss = 3.0416064262390137\n",
      "step = 2933400: loss = 2.8948824405670166\n",
      "step = 2933600: loss = 4.708038330078125\n",
      "step = 2933800: loss = 3.1111903190612793\n",
      "step = 2934000: loss = 3.772603750228882\n",
      "step = 2934200: loss = 2.998246669769287\n",
      "step = 2934400: loss = 5.178865909576416\n",
      "step = 2934600: loss = 3.291951894760132\n",
      "step = 2934800: loss = 3.0122129917144775\n",
      "step = 2935000: loss = 4.563179969787598\n",
      "step = 2935000: Average Return = 5.699999809265137\n",
      "step = 2935200: loss = 2.665562152862549\n",
      "step = 2935400: loss = 4.924633979797363\n",
      "step = 2935600: loss = 2.499218463897705\n",
      "step = 2935800: loss = 3.8563146591186523\n",
      "step = 2936000: loss = 3.2295589447021484\n",
      "step = 2936200: loss = 3.2527806758880615\n",
      "step = 2936400: loss = 3.049589157104492\n",
      "step = 2936600: loss = 3.0517234802246094\n",
      "step = 2936800: loss = 4.169764995574951\n",
      "step = 2937000: loss = 2.6467862129211426\n",
      "step = 2937200: loss = 2.3017349243164062\n",
      "step = 2937400: loss = 3.961845636367798\n",
      "step = 2937600: loss = 2.7277965545654297\n",
      "step = 2937800: loss = 4.345386028289795\n",
      "step = 2938000: loss = 4.4309401512146\n",
      "step = 2938200: loss = 4.188518524169922\n",
      "step = 2938400: loss = 4.577401638031006\n",
      "step = 2938600: loss = 2.903163433074951\n",
      "step = 2938800: loss = 3.581294536590576\n",
      "step = 2939000: loss = 3.0197038650512695\n",
      "step = 2939200: loss = 2.4585464000701904\n",
      "step = 2939400: loss = 2.540682077407837\n",
      "step = 2939600: loss = 4.188559055328369\n",
      "step = 2939800: loss = 3.7685999870300293\n",
      "step = 2940000: loss = 3.7200400829315186\n",
      "step = 2940000: Average Return = 6.900000095367432\n",
      "step = 2940200: loss = 3.563079833984375\n",
      "step = 2940400: loss = 2.163466215133667\n",
      "step = 2940600: loss = 5.284202575683594\n",
      "step = 2940800: loss = 4.043740272521973\n",
      "step = 2941000: loss = 2.9250314235687256\n",
      "step = 2941200: loss = 3.5263097286224365\n",
      "step = 2941400: loss = 3.1360487937927246\n",
      "step = 2941600: loss = 2.962435245513916\n",
      "step = 2941800: loss = 4.457802772521973\n",
      "step = 2942000: loss = 3.224534749984741\n",
      "step = 2942200: loss = 2.821803331375122\n",
      "step = 2942400: loss = 3.203441619873047\n",
      "step = 2942600: loss = 3.8547239303588867\n",
      "step = 2942800: loss = 3.138023614883423\n",
      "step = 2943000: loss = 4.224987030029297\n",
      "step = 2943200: loss = 3.917938709259033\n",
      "step = 2943400: loss = 2.8189101219177246\n",
      "step = 2943600: loss = 2.8525850772857666\n",
      "step = 2943800: loss = 4.869943618774414\n",
      "step = 2944000: loss = 2.7839958667755127\n",
      "step = 2944200: loss = 3.0526859760284424\n",
      "step = 2944400: loss = 2.8853111267089844\n",
      "step = 2944600: loss = 3.7164204120635986\n",
      "step = 2944800: loss = 3.791278600692749\n",
      "step = 2945000: loss = 3.97568416595459\n",
      "step = 2945000: Average Return = 5.5\n",
      "step = 2945200: loss = 3.1839942932128906\n",
      "step = 2945400: loss = 2.351198196411133\n",
      "step = 2945600: loss = 2.669093608856201\n",
      "step = 2945800: loss = 3.1871330738067627\n",
      "step = 2946000: loss = 3.747049570083618\n",
      "step = 2946200: loss = 3.4407122135162354\n",
      "step = 2946400: loss = 2.552974224090576\n",
      "step = 2946600: loss = 4.549535274505615\n",
      "step = 2946800: loss = 2.746799945831299\n",
      "step = 2947000: loss = 3.414818286895752\n",
      "step = 2947200: loss = 3.569295644760132\n",
      "step = 2947400: loss = 3.4996557235717773\n",
      "step = 2947600: loss = 3.1899478435516357\n",
      "step = 2947800: loss = 2.763845682144165\n",
      "step = 2948000: loss = 4.1504034996032715\n",
      "step = 2948200: loss = 2.7305896282196045\n",
      "step = 2948400: loss = 3.0590782165527344\n",
      "step = 2948600: loss = 3.8661210536956787\n",
      "step = 2948800: loss = 3.204238176345825\n",
      "step = 2949000: loss = 3.0745952129364014\n",
      "step = 2949200: loss = 2.897123336791992\n",
      "step = 2949400: loss = 2.8827333450317383\n",
      "step = 2949600: loss = 3.973644971847534\n",
      "step = 2949800: loss = 2.5618579387664795\n",
      "step = 2950000: loss = 2.7831010818481445\n",
      "step = 2950000: Average Return = 3.5999999046325684\n",
      "step = 2950200: loss = 3.093498468399048\n",
      "step = 2950400: loss = 1.943034291267395\n",
      "step = 2950600: loss = 3.482283353805542\n",
      "step = 2950800: loss = 3.3212878704071045\n",
      "step = 2951000: loss = 2.717802047729492\n",
      "step = 2951200: loss = 2.7948131561279297\n",
      "step = 2951400: loss = 3.820746421813965\n",
      "step = 2951600: loss = 3.153625965118408\n",
      "step = 2951800: loss = 4.5558977127075195\n",
      "step = 2952000: loss = 4.0095415115356445\n",
      "step = 2952200: loss = 3.430157423019409\n",
      "step = 2952400: loss = 2.6844863891601562\n",
      "step = 2952600: loss = 2.5352418422698975\n",
      "step = 2952800: loss = 3.2759640216827393\n",
      "step = 2953000: loss = 2.34718656539917\n",
      "step = 2953200: loss = 4.611250400543213\n",
      "step = 2953400: loss = 4.340432643890381\n",
      "step = 2953600: loss = 2.8793859481811523\n",
      "step = 2953800: loss = 3.084332227706909\n",
      "step = 2954000: loss = 4.481940746307373\n",
      "step = 2954200: loss = 3.685124158859253\n",
      "step = 2954400: loss = 4.91077184677124\n",
      "step = 2954600: loss = 2.73367977142334\n",
      "step = 2954800: loss = 4.916285037994385\n",
      "step = 2955000: loss = 2.726954221725464\n",
      "step = 2955000: Average Return = 3.9000000953674316\n",
      "step = 2955200: loss = 2.752265453338623\n",
      "step = 2955400: loss = 3.7412540912628174\n",
      "step = 2955600: loss = 5.171245098114014\n",
      "step = 2955800: loss = 2.8435750007629395\n",
      "step = 2956000: loss = 3.706766366958618\n",
      "step = 2956200: loss = 2.8252816200256348\n",
      "step = 2956400: loss = 2.4917407035827637\n",
      "step = 2956600: loss = 3.093689441680908\n",
      "step = 2956800: loss = 2.7911291122436523\n",
      "step = 2957000: loss = 3.53886342048645\n",
      "step = 2957200: loss = 4.4869513511657715\n",
      "step = 2957400: loss = 2.876680850982666\n",
      "step = 2957600: loss = 4.132976531982422\n",
      "step = 2957800: loss = 2.8666677474975586\n",
      "step = 2958000: loss = 3.7896363735198975\n",
      "step = 2958200: loss = 2.8913259506225586\n",
      "step = 2958400: loss = 2.997584819793701\n",
      "step = 2958600: loss = 3.099069356918335\n",
      "step = 2958800: loss = 3.708242416381836\n",
      "step = 2959000: loss = 3.6230268478393555\n",
      "step = 2959200: loss = 2.7689764499664307\n",
      "step = 2959400: loss = 2.619948625564575\n",
      "step = 2959600: loss = 2.6490674018859863\n",
      "step = 2959800: loss = 3.2675414085388184\n",
      "step = 2960000: loss = 3.216553211212158\n",
      "step = 2960000: Average Return = 3.799999952316284\n",
      "step = 2960200: loss = 1.948377013206482\n",
      "step = 2960400: loss = 3.677415609359741\n",
      "step = 2960600: loss = 3.183018684387207\n",
      "step = 2960800: loss = 3.0598344802856445\n",
      "step = 2961000: loss = 4.3593339920043945\n",
      "step = 2961200: loss = 2.46968936920166\n",
      "step = 2961400: loss = 3.147542715072632\n",
      "step = 2961600: loss = 3.8617348670959473\n",
      "step = 2961800: loss = 4.251417636871338\n",
      "step = 2962000: loss = 3.7351865768432617\n",
      "step = 2962200: loss = 2.9518914222717285\n",
      "step = 2962400: loss = 3.6237010955810547\n",
      "step = 2962600: loss = 3.7283809185028076\n",
      "step = 2962800: loss = 4.024477481842041\n",
      "step = 2963000: loss = 5.55161714553833\n",
      "step = 2963200: loss = 3.951803207397461\n",
      "step = 2963400: loss = 3.5614187717437744\n",
      "step = 2963600: loss = 2.7529003620147705\n",
      "step = 2963800: loss = 3.3795924186706543\n",
      "step = 2964000: loss = 2.0105440616607666\n",
      "step = 2964200: loss = 6.276337146759033\n",
      "step = 2964400: loss = 3.369805335998535\n",
      "step = 2964600: loss = 3.330655336380005\n",
      "step = 2964800: loss = 3.9850056171417236\n",
      "step = 2965000: loss = 2.539029359817505\n",
      "step = 2965000: Average Return = 6.050000190734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 2965200: loss = 4.190619945526123\n",
      "step = 2965400: loss = 2.6807568073272705\n",
      "step = 2965600: loss = 4.84339714050293\n",
      "step = 2965800: loss = 2.648712396621704\n",
      "step = 2966000: loss = 4.1675543785095215\n",
      "step = 2966200: loss = 4.129074573516846\n",
      "step = 2966400: loss = 3.0912322998046875\n",
      "step = 2966600: loss = 3.9739274978637695\n",
      "step = 2966800: loss = 3.183173656463623\n",
      "step = 2967000: loss = 3.7416930198669434\n",
      "step = 2967200: loss = 2.7887589931488037\n",
      "step = 2967400: loss = 4.59602165222168\n",
      "step = 2967600: loss = 3.681481122970581\n",
      "step = 2967800: loss = 2.83400821685791\n",
      "step = 2968000: loss = 3.674056053161621\n",
      "step = 2968200: loss = 2.241224765777588\n",
      "step = 2968400: loss = 3.467829465866089\n",
      "step = 2968600: loss = 2.8963494300842285\n",
      "step = 2968800: loss = 2.74027156829834\n",
      "step = 2969000: loss = 3.6749589443206787\n",
      "step = 2969200: loss = 3.168048620223999\n",
      "step = 2969400: loss = 2.720005512237549\n",
      "step = 2969600: loss = 3.6850907802581787\n",
      "step = 2969800: loss = 2.7566606998443604\n",
      "step = 2970000: loss = 2.6229605674743652\n",
      "step = 2970000: Average Return = 3.0999999046325684\n",
      "step = 2970200: loss = 7.007210731506348\n",
      "step = 2970400: loss = 2.277998208999634\n",
      "step = 2970600: loss = 3.8324503898620605\n",
      "step = 2970800: loss = 3.616309404373169\n",
      "step = 2971000: loss = 3.3059346675872803\n",
      "step = 2971200: loss = 3.2779324054718018\n",
      "step = 2971400: loss = 3.814427137374878\n",
      "step = 2971600: loss = 3.328599691390991\n",
      "step = 2971800: loss = 3.0910496711730957\n",
      "step = 2972000: loss = 3.500488758087158\n",
      "step = 2972200: loss = 4.442681789398193\n",
      "step = 2972400: loss = 2.5493340492248535\n",
      "step = 2972600: loss = 3.405266761779785\n",
      "step = 2972800: loss = 3.6792895793914795\n",
      "step = 2973000: loss = 3.2110447883605957\n",
      "step = 2973200: loss = 3.340580463409424\n",
      "step = 2973400: loss = 2.28729510307312\n",
      "step = 2973600: loss = 2.9174911975860596\n",
      "step = 2973800: loss = 3.0072360038757324\n",
      "step = 2974000: loss = 2.8594930171966553\n",
      "step = 2974200: loss = 2.0328736305236816\n",
      "step = 2974400: loss = 2.8961002826690674\n",
      "step = 2974600: loss = 3.9806153774261475\n",
      "step = 2974800: loss = 3.6199498176574707\n",
      "step = 2975000: loss = 3.000375986099243\n",
      "step = 2975000: Average Return = 4.800000190734863\n",
      "step = 2975200: loss = 4.138092994689941\n",
      "step = 2975400: loss = 2.902306079864502\n",
      "step = 2975600: loss = 4.161994457244873\n",
      "step = 2975800: loss = 3.3934786319732666\n",
      "step = 2976000: loss = 3.8985095024108887\n",
      "step = 2976200: loss = 4.243504047393799\n",
      "step = 2976400: loss = 4.33091402053833\n",
      "step = 2976600: loss = 2.900589942932129\n",
      "step = 2976800: loss = 2.9997332096099854\n",
      "step = 2977000: loss = 3.424915313720703\n",
      "step = 2977200: loss = 4.191025733947754\n",
      "step = 2977400: loss = 2.9683916568756104\n",
      "step = 2977600: loss = 3.0299956798553467\n",
      "step = 2977800: loss = 2.359618663787842\n",
      "step = 2978000: loss = 3.2411279678344727\n",
      "step = 2978200: loss = 4.337963581085205\n",
      "step = 2978400: loss = 3.816791296005249\n",
      "step = 2978600: loss = 2.8776302337646484\n",
      "step = 2978800: loss = 4.015893459320068\n",
      "step = 2979000: loss = 3.029738426208496\n",
      "step = 2979200: loss = 2.17034912109375\n",
      "step = 2979400: loss = 3.8551113605499268\n",
      "step = 2979600: loss = 3.7934930324554443\n",
      "step = 2979800: loss = 3.4511449337005615\n",
      "step = 2980000: loss = 3.9605839252471924\n",
      "step = 2980000: Average Return = 4.349999904632568\n",
      "step = 2980200: loss = 3.709498882293701\n",
      "step = 2980400: loss = 2.8846068382263184\n",
      "step = 2980600: loss = 2.540102243423462\n",
      "step = 2980800: loss = 4.82411527633667\n",
      "step = 2981000: loss = 2.6619038581848145\n",
      "step = 2981200: loss = 3.558314800262451\n",
      "step = 2981400: loss = 4.142001628875732\n",
      "step = 2981600: loss = 2.8442814350128174\n",
      "step = 2981800: loss = 2.901946544647217\n",
      "step = 2982000: loss = 3.671703815460205\n",
      "step = 2982200: loss = 2.4736785888671875\n",
      "step = 2982400: loss = 2.864948034286499\n",
      "step = 2982600: loss = 3.570995330810547\n",
      "step = 2982800: loss = 3.1588211059570312\n",
      "step = 2983000: loss = 3.266387939453125\n",
      "step = 2983200: loss = 3.813356876373291\n",
      "step = 2983400: loss = 3.1474838256835938\n",
      "step = 2983600: loss = 4.524556636810303\n",
      "step = 2983800: loss = 2.744434118270874\n",
      "step = 2984000: loss = 2.860477924346924\n",
      "step = 2984200: loss = 5.242371559143066\n",
      "step = 2984400: loss = 3.912977933883667\n",
      "step = 2984600: loss = 3.4112932682037354\n",
      "step = 2984800: loss = 2.477475166320801\n",
      "step = 2985000: loss = 2.7826714515686035\n",
      "step = 2985000: Average Return = 4.300000190734863\n",
      "step = 2985200: loss = 2.575161933898926\n",
      "step = 2985400: loss = 3.5849249362945557\n",
      "step = 2985600: loss = 3.5145251750946045\n",
      "step = 2985800: loss = 3.532172441482544\n",
      "step = 2986000: loss = 4.686147689819336\n",
      "step = 2986200: loss = 2.6165413856506348\n",
      "step = 2986400: loss = 2.8117854595184326\n",
      "step = 2986600: loss = 4.29415225982666\n",
      "step = 2986800: loss = 3.0454630851745605\n",
      "step = 2987000: loss = 3.7204277515411377\n",
      "step = 2987200: loss = 2.6371896266937256\n",
      "step = 2987400: loss = 3.7496871948242188\n",
      "step = 2987600: loss = 4.066523551940918\n",
      "step = 2987800: loss = 3.163526773452759\n",
      "step = 2988000: loss = 3.954235076904297\n",
      "step = 2988200: loss = 3.3289692401885986\n",
      "step = 2988400: loss = 3.8175549507141113\n",
      "step = 2988600: loss = 3.905343770980835\n",
      "step = 2988800: loss = 3.8185946941375732\n",
      "step = 2989000: loss = 2.9968926906585693\n",
      "step = 2989200: loss = 2.897444725036621\n",
      "step = 2989400: loss = 4.154800891876221\n",
      "step = 2989600: loss = 3.0518412590026855\n",
      "step = 2989800: loss = 2.667534351348877\n",
      "step = 2990000: loss = 3.4044816493988037\n",
      "step = 2990000: Average Return = 3.5\n",
      "step = 2990200: loss = 3.120119571685791\n",
      "step = 2990400: loss = 2.417402505874634\n",
      "step = 2990600: loss = 3.401364326477051\n",
      "step = 2990800: loss = 3.2966434955596924\n",
      "step = 2991000: loss = 3.3267531394958496\n",
      "step = 2991200: loss = 3.577143430709839\n",
      "step = 2991400: loss = 2.5391502380371094\n",
      "step = 2991600: loss = 2.8027544021606445\n",
      "step = 2991800: loss = 3.656980514526367\n",
      "step = 2992000: loss = 2.379371166229248\n",
      "step = 2992200: loss = 3.220829486846924\n",
      "step = 2992400: loss = 3.453148365020752\n",
      "step = 2992600: loss = 3.3899755477905273\n",
      "step = 2992800: loss = 3.328127145767212\n",
      "step = 2993000: loss = 3.232991933822632\n",
      "step = 2993200: loss = 2.978148937225342\n",
      "step = 2993400: loss = 2.017350196838379\n",
      "step = 2993600: loss = 3.3058483600616455\n",
      "step = 2993800: loss = 4.297574520111084\n",
      "step = 2994000: loss = 4.648866653442383\n",
      "step = 2994200: loss = 3.575157403945923\n",
      "step = 2994400: loss = 3.913041591644287\n",
      "step = 2994600: loss = 3.7875449657440186\n",
      "step = 2994800: loss = 4.832546234130859\n",
      "step = 2995000: loss = 4.135653972625732\n",
      "step = 2995000: Average Return = 5.900000095367432\n",
      "step = 2995200: loss = 4.252837657928467\n",
      "step = 2995400: loss = 2.987457275390625\n",
      "step = 2995600: loss = 3.5569350719451904\n",
      "step = 2995800: loss = 2.1522858142852783\n",
      "step = 2996000: loss = 2.600348472595215\n",
      "step = 2996200: loss = 3.3427700996398926\n",
      "step = 2996400: loss = 3.0072460174560547\n",
      "step = 2996600: loss = 5.736633777618408\n",
      "step = 2996800: loss = 2.741762399673462\n",
      "step = 2997000: loss = 2.1856026649475098\n",
      "step = 2997200: loss = 2.9943270683288574\n",
      "step = 2997400: loss = 4.697501182556152\n",
      "step = 2997600: loss = 3.4788689613342285\n",
      "step = 2997800: loss = 3.413445472717285\n",
      "step = 2998000: loss = 2.8921051025390625\n",
      "step = 2998200: loss = 3.871326446533203\n",
      "step = 2998400: loss = 3.9247500896453857\n",
      "step = 2998600: loss = 2.2630727291107178\n",
      "step = 2998800: loss = 3.523608922958374\n",
      "step = 2999000: loss = 3.7284538745880127\n",
      "step = 2999200: loss = 2.972203016281128\n",
      "step = 2999400: loss = 2.8726463317871094\n",
      "step = 2999600: loss = 2.6088674068450928\n",
      "step = 2999800: loss = 2.8976855278015137\n",
      "step = 3000000: loss = 2.3258283138275146\n",
      "step = 3000000: Average Return = 5.099999904632568\n",
      "step = 3000200: loss = 3.1317901611328125\n",
      "step = 3000400: loss = 3.9775826930999756\n",
      "step = 3000600: loss = 4.697494029998779\n",
      "step = 3000800: loss = 3.9804131984710693\n",
      "step = 3001000: loss = 2.601651906967163\n",
      "step = 3001200: loss = 2.9372057914733887\n",
      "step = 3001400: loss = 3.6854124069213867\n",
      "step = 3001600: loss = 3.233757972717285\n",
      "step = 3001800: loss = 3.6650407314300537\n",
      "step = 3002000: loss = 3.194868803024292\n",
      "step = 3002200: loss = 3.718276262283325\n",
      "step = 3002400: loss = 2.0159871578216553\n",
      "step = 3002600: loss = 4.848633766174316\n",
      "step = 3002800: loss = 3.752741575241089\n",
      "step = 3003000: loss = 2.6290390491485596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3003200: loss = 3.2824318408966064\n",
      "step = 3003400: loss = 3.576420783996582\n",
      "step = 3003600: loss = 2.591184616088867\n",
      "step = 3003800: loss = 4.559401988983154\n",
      "step = 3004000: loss = 2.6515402793884277\n",
      "step = 3004200: loss = 3.326951742172241\n",
      "step = 3004400: loss = 3.6440367698669434\n",
      "step = 3004600: loss = 2.6922929286956787\n",
      "step = 3004800: loss = 3.26889705657959\n",
      "step = 3005000: loss = 3.2423949241638184\n",
      "step = 3005000: Average Return = 5.050000190734863\n",
      "step = 3005200: loss = 4.1907525062561035\n",
      "step = 3005400: loss = 2.6395366191864014\n",
      "step = 3005600: loss = 2.6265881061553955\n",
      "step = 3005800: loss = 2.199538469314575\n",
      "step = 3006000: loss = 3.307614326477051\n",
      "step = 3006200: loss = 2.8010690212249756\n",
      "step = 3006400: loss = 2.899445056915283\n",
      "step = 3006600: loss = 3.0155751705169678\n",
      "step = 3006800: loss = 3.9581639766693115\n",
      "step = 3007000: loss = 3.4799859523773193\n",
      "step = 3007200: loss = 3.7997965812683105\n",
      "step = 3007400: loss = 3.7727468013763428\n",
      "step = 3007600: loss = 2.652789354324341\n",
      "step = 3007800: loss = 3.409345865249634\n",
      "step = 3008000: loss = 3.0422496795654297\n",
      "step = 3008200: loss = 3.5619072914123535\n",
      "step = 3008400: loss = 3.069805145263672\n",
      "step = 3008600: loss = 2.7194857597351074\n",
      "step = 3008800: loss = 4.253057956695557\n",
      "step = 3009000: loss = 3.6226484775543213\n",
      "step = 3009200: loss = 4.462186813354492\n",
      "step = 3009400: loss = 3.433194398880005\n",
      "step = 3009600: loss = 4.207220077514648\n",
      "step = 3009800: loss = 3.0710573196411133\n",
      "step = 3010000: loss = 2.4918768405914307\n",
      "step = 3010000: Average Return = 4.449999809265137\n",
      "step = 3010200: loss = 2.7004003524780273\n",
      "step = 3010400: loss = 3.6638548374176025\n",
      "step = 3010600: loss = 2.9370129108428955\n",
      "step = 3010800: loss = 4.391274929046631\n",
      "step = 3011000: loss = 4.895482540130615\n",
      "step = 3011200: loss = 3.634899616241455\n",
      "step = 3011400: loss = 2.4801197052001953\n",
      "step = 3011600: loss = 3.003349781036377\n",
      "step = 3011800: loss = 3.1197428703308105\n",
      "step = 3012000: loss = 2.6657519340515137\n",
      "step = 3012200: loss = 4.815412521362305\n",
      "step = 3012400: loss = 4.096329689025879\n",
      "step = 3012600: loss = 3.5693044662475586\n",
      "step = 3012800: loss = 2.0972509384155273\n",
      "step = 3013000: loss = 4.44971227645874\n",
      "step = 3013200: loss = 2.974003791809082\n",
      "step = 3013400: loss = 3.122019052505493\n",
      "step = 3013600: loss = 2.283775568008423\n",
      "step = 3013800: loss = 2.805684804916382\n",
      "step = 3014000: loss = 2.6240530014038086\n",
      "step = 3014200: loss = 2.8633177280426025\n",
      "step = 3014400: loss = 3.4350192546844482\n",
      "step = 3014600: loss = 2.224597215652466\n",
      "step = 3014800: loss = 2.9798779487609863\n",
      "step = 3015000: loss = 3.314401149749756\n",
      "step = 3015000: Average Return = 4.800000190734863\n",
      "step = 3015200: loss = 3.812408447265625\n",
      "step = 3015400: loss = 3.009644031524658\n",
      "step = 3015600: loss = 3.661698579788208\n",
      "step = 3015800: loss = 2.3120696544647217\n",
      "step = 3016000: loss = 3.133073329925537\n",
      "step = 3016200: loss = 3.5139894485473633\n",
      "step = 3016400: loss = 2.604062557220459\n",
      "step = 3016600: loss = 2.3727972507476807\n",
      "step = 3016800: loss = 5.347902774810791\n",
      "step = 3017000: loss = 3.4537734985351562\n",
      "step = 3017200: loss = 3.0963079929351807\n",
      "step = 3017400: loss = 2.269604206085205\n",
      "step = 3017600: loss = 2.0506558418273926\n",
      "step = 3017800: loss = 4.136358737945557\n",
      "step = 3018000: loss = 2.5642833709716797\n",
      "step = 3018200: loss = 3.8979427814483643\n",
      "step = 3018400: loss = 3.6546571254730225\n",
      "step = 3018600: loss = 3.409860849380493\n",
      "step = 3018800: loss = 2.807420015335083\n",
      "step = 3019000: loss = 2.6677660942077637\n",
      "step = 3019200: loss = 2.8473117351531982\n",
      "step = 3019400: loss = 2.675640106201172\n",
      "step = 3019600: loss = 3.336979389190674\n",
      "step = 3019800: loss = 2.864393472671509\n",
      "step = 3020000: loss = 2.4408106803894043\n",
      "step = 3020000: Average Return = 6.949999809265137\n",
      "step = 3020200: loss = 2.315647840499878\n",
      "step = 3020400: loss = 2.353149890899658\n",
      "step = 3020600: loss = 4.655168533325195\n",
      "step = 3020800: loss = 4.352168560028076\n",
      "step = 3021000: loss = 3.0385072231292725\n",
      "step = 3021200: loss = 2.422713279724121\n",
      "step = 3021400: loss = 2.8059334754943848\n",
      "step = 3021600: loss = 3.1932873725891113\n",
      "step = 3021800: loss = 3.546055316925049\n",
      "step = 3022000: loss = 2.960428476333618\n",
      "step = 3022200: loss = 2.3617658615112305\n",
      "step = 3022400: loss = 2.937945604324341\n",
      "step = 3022600: loss = 2.6189029216766357\n",
      "step = 3022800: loss = 3.411120891571045\n",
      "step = 3023000: loss = 3.961714744567871\n",
      "step = 3023200: loss = 3.2683422565460205\n",
      "step = 3023400: loss = 2.84371280670166\n",
      "step = 3023600: loss = 2.6706395149230957\n",
      "step = 3023800: loss = 2.0487706661224365\n",
      "step = 3024000: loss = 3.0001537799835205\n",
      "step = 3024200: loss = 3.45623517036438\n",
      "step = 3024400: loss = 3.584172487258911\n",
      "step = 3024600: loss = 3.3159854412078857\n",
      "step = 3024800: loss = 3.2648046016693115\n",
      "step = 3025000: loss = 3.0279033184051514\n",
      "step = 3025000: Average Return = 4.050000190734863\n",
      "step = 3025200: loss = 3.3002641201019287\n",
      "step = 3025400: loss = 2.98513126373291\n",
      "step = 3025600: loss = 4.010860919952393\n",
      "step = 3025800: loss = 2.6299920082092285\n",
      "step = 3026000: loss = 3.606121778488159\n",
      "step = 3026200: loss = 4.372017860412598\n",
      "step = 3026400: loss = 4.42002010345459\n",
      "step = 3026600: loss = 3.3277971744537354\n",
      "step = 3026800: loss = 3.0588786602020264\n",
      "step = 3027000: loss = 2.6630029678344727\n",
      "step = 3027200: loss = 4.293675422668457\n",
      "step = 3027400: loss = 3.5812175273895264\n",
      "step = 3027600: loss = 3.572780132293701\n",
      "step = 3027800: loss = 2.36590838432312\n",
      "step = 3028000: loss = 4.884669780731201\n",
      "step = 3028200: loss = 3.961883783340454\n",
      "step = 3028400: loss = 2.5227508544921875\n",
      "step = 3028600: loss = 2.2731587886810303\n",
      "step = 3028800: loss = 2.7462551593780518\n",
      "step = 3029000: loss = 2.0536696910858154\n",
      "step = 3029200: loss = 2.664790391921997\n",
      "step = 3029400: loss = 2.876434803009033\n",
      "step = 3029600: loss = 3.1901235580444336\n",
      "step = 3029800: loss = 3.6608712673187256\n",
      "step = 3030000: loss = 4.348643779754639\n",
      "step = 3030000: Average Return = 5.199999809265137\n",
      "step = 3030200: loss = 2.8597989082336426\n",
      "step = 3030400: loss = 2.690778970718384\n",
      "step = 3030600: loss = 2.6078972816467285\n",
      "step = 3030800: loss = 3.5792694091796875\n",
      "step = 3031000: loss = 3.8254945278167725\n",
      "step = 3031200: loss = 2.7891273498535156\n",
      "step = 3031400: loss = 3.949455499649048\n",
      "step = 3031600: loss = 2.237168788909912\n",
      "step = 3031800: loss = 3.520193099975586\n",
      "step = 3032000: loss = 2.8776772022247314\n",
      "step = 3032200: loss = 2.8317601680755615\n",
      "step = 3032400: loss = 2.4129228591918945\n",
      "step = 3032600: loss = 3.2518999576568604\n",
      "step = 3032800: loss = 1.7779113054275513\n",
      "step = 3033000: loss = 2.9201412200927734\n",
      "step = 3033200: loss = 3.5402214527130127\n",
      "step = 3033400: loss = 2.708873748779297\n",
      "step = 3033600: loss = 2.6661479473114014\n",
      "step = 3033800: loss = 3.4956986904144287\n",
      "step = 3034000: loss = 2.7430877685546875\n",
      "step = 3034200: loss = 3.2241148948669434\n",
      "step = 3034400: loss = 2.7615020275115967\n",
      "step = 3034600: loss = 2.4991912841796875\n",
      "step = 3034800: loss = 3.608248233795166\n",
      "step = 3035000: loss = 2.797468900680542\n",
      "step = 3035000: Average Return = 3.75\n",
      "step = 3035200: loss = 2.756451368331909\n",
      "step = 3035400: loss = 2.448239803314209\n",
      "step = 3035600: loss = 2.666841983795166\n",
      "step = 3035800: loss = 2.4137187004089355\n",
      "step = 3036000: loss = 1.9572945833206177\n",
      "step = 3036200: loss = 3.1379504203796387\n",
      "step = 3036400: loss = 2.681467294692993\n",
      "step = 3036600: loss = 3.380284309387207\n",
      "step = 3036800: loss = 3.0929205417633057\n",
      "step = 3037000: loss = 2.462462902069092\n",
      "step = 3037200: loss = 2.8894474506378174\n",
      "step = 3037400: loss = 2.0916128158569336\n",
      "step = 3037600: loss = 3.0249345302581787\n",
      "step = 3037800: loss = 3.3758864402770996\n",
      "step = 3038000: loss = 3.333803653717041\n",
      "step = 3038200: loss = 1.9173705577850342\n",
      "step = 3038400: loss = 2.3381142616271973\n",
      "step = 3038600: loss = 2.7779266834259033\n",
      "step = 3038800: loss = 4.064661026000977\n",
      "step = 3039000: loss = 2.587677240371704\n",
      "step = 3039200: loss = 2.8025195598602295\n",
      "step = 3039400: loss = 1.6594489812850952\n",
      "step = 3039600: loss = 2.842302083969116\n",
      "step = 3039800: loss = 3.5244243144989014\n",
      "step = 3040000: loss = 2.695913076400757\n",
      "step = 3040000: Average Return = 4.0\n",
      "step = 3040200: loss = 4.097627639770508\n",
      "step = 3040400: loss = 2.3378846645355225\n",
      "step = 3040600: loss = 3.428217887878418\n",
      "step = 3040800: loss = 3.434685707092285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3041000: loss = 3.1887645721435547\n",
      "step = 3041200: loss = 5.02045202255249\n",
      "step = 3041400: loss = 2.806558132171631\n",
      "step = 3041600: loss = 2.227179527282715\n",
      "step = 3041800: loss = 4.70228385925293\n",
      "step = 3042000: loss = 3.1681621074676514\n",
      "step = 3042200: loss = 3.60589599609375\n",
      "step = 3042400: loss = 4.1343994140625\n",
      "step = 3042600: loss = 2.9895992279052734\n",
      "step = 3042800: loss = 2.661558151245117\n",
      "step = 3043000: loss = 2.9671428203582764\n",
      "step = 3043200: loss = 4.0919294357299805\n",
      "step = 3043400: loss = 2.461470365524292\n",
      "step = 3043600: loss = 3.975863456726074\n",
      "step = 3043800: loss = 2.3004002571105957\n",
      "step = 3044000: loss = 2.9385650157928467\n",
      "step = 3044200: loss = 2.635371685028076\n",
      "step = 3044400: loss = 2.209726572036743\n",
      "step = 3044600: loss = 3.3036155700683594\n",
      "step = 3044800: loss = 3.1754631996154785\n",
      "step = 3045000: loss = 1.9701892137527466\n",
      "step = 3045000: Average Return = 3.799999952316284\n",
      "step = 3045200: loss = 3.0442543029785156\n",
      "step = 3045400: loss = 3.2580997943878174\n",
      "step = 3045600: loss = 2.2273221015930176\n",
      "step = 3045800: loss = 3.422152519226074\n",
      "step = 3046000: loss = 3.307199716567993\n",
      "step = 3046200: loss = 3.829396963119507\n",
      "step = 3046400: loss = 3.797762870788574\n",
      "step = 3046600: loss = 2.0324018001556396\n",
      "step = 3046800: loss = 3.119957208633423\n",
      "step = 3047000: loss = 3.1476669311523438\n",
      "step = 3047200: loss = 2.4265456199645996\n",
      "step = 3047400: loss = 3.876375913619995\n",
      "step = 3047600: loss = 3.2253952026367188\n",
      "step = 3047800: loss = 3.30745792388916\n",
      "step = 3048000: loss = 4.406805515289307\n",
      "step = 3048200: loss = 2.290825128555298\n",
      "step = 3048400: loss = 3.178457498550415\n",
      "step = 3048600: loss = 4.6852850914001465\n",
      "step = 3048800: loss = 3.3829336166381836\n",
      "step = 3049000: loss = 1.8793967962265015\n",
      "step = 3049200: loss = 4.540902137756348\n",
      "step = 3049400: loss = 2.7368240356445312\n",
      "step = 3049600: loss = 2.5088741779327393\n",
      "step = 3049800: loss = 4.265663146972656\n",
      "step = 3050000: loss = 3.9594645500183105\n",
      "step = 3050000: Average Return = 4.849999904632568\n",
      "step = 3050200: loss = 3.828932046890259\n",
      "step = 3050400: loss = 3.814964771270752\n",
      "step = 3050600: loss = 2.561781644821167\n",
      "step = 3050800: loss = 3.77449631690979\n",
      "step = 3051000: loss = 3.369619607925415\n",
      "step = 3051200: loss = 2.941810131072998\n",
      "step = 3051400: loss = 4.168766498565674\n",
      "step = 3051600: loss = 2.6638708114624023\n",
      "step = 3051800: loss = 3.145969867706299\n",
      "step = 3052000: loss = 3.711989641189575\n",
      "step = 3052200: loss = 4.257628917694092\n",
      "step = 3052400: loss = 2.0601913928985596\n",
      "step = 3052600: loss = 3.214380979537964\n",
      "step = 3052800: loss = 2.8014256954193115\n",
      "step = 3053000: loss = 3.516752243041992\n",
      "step = 3053200: loss = 2.4446074962615967\n",
      "step = 3053400: loss = 1.9950319528579712\n",
      "step = 3053600: loss = 3.1056413650512695\n",
      "step = 3053800: loss = 2.309222459793091\n",
      "step = 3054000: loss = 1.6998313665390015\n",
      "step = 3054200: loss = 2.5367279052734375\n",
      "step = 3054400: loss = 3.3195126056671143\n",
      "step = 3054600: loss = 5.29081916809082\n",
      "step = 3054800: loss = 2.9010345935821533\n",
      "step = 3055000: loss = 3.3808751106262207\n",
      "step = 3055000: Average Return = 4.900000095367432\n",
      "step = 3055200: loss = 2.81866192817688\n",
      "step = 3055400: loss = 3.3252294063568115\n",
      "step = 3055600: loss = 2.9839513301849365\n",
      "step = 3055800: loss = 3.3034613132476807\n",
      "step = 3056000: loss = 3.0419061183929443\n",
      "step = 3056200: loss = 2.577150821685791\n",
      "step = 3056400: loss = 3.0755093097686768\n",
      "step = 3056600: loss = 2.9896035194396973\n",
      "step = 3056800: loss = 1.7553815841674805\n",
      "step = 3057000: loss = 2.3043692111968994\n",
      "step = 3057200: loss = 3.013845443725586\n",
      "step = 3057400: loss = 3.2085964679718018\n",
      "step = 3057600: loss = 2.9296038150787354\n",
      "step = 3057800: loss = 3.3336095809936523\n",
      "step = 3058000: loss = 2.7941529750823975\n",
      "step = 3058200: loss = 2.806061267852783\n",
      "step = 3058400: loss = 1.9032824039459229\n",
      "step = 3058600: loss = 3.292971134185791\n",
      "step = 3058800: loss = 3.1357555389404297\n",
      "step = 3059000: loss = 3.6488773822784424\n",
      "step = 3059200: loss = 3.074939489364624\n",
      "step = 3059400: loss = 2.9921133518218994\n",
      "step = 3059600: loss = 3.4916481971740723\n",
      "step = 3059800: loss = 4.349680423736572\n",
      "step = 3060000: loss = 2.29636812210083\n",
      "step = 3060000: Average Return = 5.25\n",
      "step = 3060200: loss = 4.032864093780518\n",
      "step = 3060400: loss = 3.2163586616516113\n",
      "step = 3060600: loss = 3.60788631439209\n",
      "step = 3060800: loss = 2.7028987407684326\n",
      "step = 3061000: loss = 3.395331382751465\n",
      "step = 3061200: loss = 3.0606398582458496\n",
      "step = 3061400: loss = 3.6420516967773438\n",
      "step = 3061600: loss = 3.20121693611145\n",
      "step = 3061800: loss = 3.6563708782196045\n",
      "step = 3062000: loss = 2.786949396133423\n",
      "step = 3062200: loss = 2.7435238361358643\n",
      "step = 3062400: loss = 2.991356134414673\n",
      "step = 3062600: loss = 3.4703946113586426\n",
      "step = 3062800: loss = 2.664590358734131\n",
      "step = 3063000: loss = 4.322096347808838\n",
      "step = 3063200: loss = 3.1546196937561035\n",
      "step = 3063400: loss = 4.07096529006958\n",
      "step = 3063600: loss = 5.068145751953125\n",
      "step = 3063800: loss = 4.856527805328369\n",
      "step = 3064000: loss = 2.1138322353363037\n",
      "step = 3064200: loss = 4.650155544281006\n",
      "step = 3064400: loss = 3.4441258907318115\n",
      "step = 3064600: loss = 2.996202230453491\n",
      "step = 3064800: loss = 3.897296905517578\n",
      "step = 3065000: loss = 2.9209790229797363\n",
      "step = 3065000: Average Return = 5.099999904632568\n",
      "step = 3065200: loss = 2.2005298137664795\n",
      "step = 3065400: loss = 2.1916093826293945\n",
      "step = 3065600: loss = 2.3229727745056152\n",
      "step = 3065800: loss = 1.7304869890213013\n",
      "step = 3066000: loss = 3.394012689590454\n",
      "step = 3066200: loss = 3.7393815517425537\n",
      "step = 3066400: loss = 3.048593759536743\n",
      "step = 3066600: loss = 2.0523934364318848\n",
      "step = 3066800: loss = 3.1288065910339355\n",
      "step = 3067000: loss = 2.0088045597076416\n",
      "step = 3067200: loss = 2.4939072132110596\n",
      "step = 3067400: loss = 3.9095287322998047\n",
      "step = 3067600: loss = 2.7445292472839355\n",
      "step = 3067800: loss = 3.1910336017608643\n",
      "step = 3068000: loss = 3.0413973331451416\n",
      "step = 3068200: loss = 2.696852922439575\n",
      "step = 3068400: loss = 3.6168322563171387\n",
      "step = 3068600: loss = 4.347949028015137\n",
      "step = 3068800: loss = 3.005277633666992\n",
      "step = 3069000: loss = 3.5446367263793945\n",
      "step = 3069200: loss = 4.638856410980225\n",
      "step = 3069400: loss = 3.469332695007324\n",
      "step = 3069600: loss = 4.023392677307129\n",
      "step = 3069800: loss = 3.3762779235839844\n",
      "step = 3070000: loss = 2.9028966426849365\n",
      "step = 3070000: Average Return = 4.849999904632568\n",
      "step = 3070200: loss = 3.637878894805908\n",
      "step = 3070400: loss = 2.537987232208252\n",
      "step = 3070600: loss = 2.901726722717285\n",
      "step = 3070800: loss = 2.642631769180298\n",
      "step = 3071000: loss = 3.551548957824707\n",
      "step = 3071200: loss = 4.076873302459717\n",
      "step = 3071400: loss = 2.4746932983398438\n",
      "step = 3071600: loss = 2.983719825744629\n",
      "step = 3071800: loss = 2.7570016384124756\n",
      "step = 3072000: loss = 2.1543502807617188\n",
      "step = 3072200: loss = 3.2687511444091797\n",
      "step = 3072400: loss = 3.2664403915405273\n",
      "step = 3072600: loss = 3.7678134441375732\n",
      "step = 3072800: loss = 5.153257369995117\n",
      "step = 3073000: loss = 3.6374499797821045\n",
      "step = 3073200: loss = 3.534019947052002\n",
      "step = 3073400: loss = 2.492814064025879\n",
      "step = 3073600: loss = 4.507008075714111\n",
      "step = 3073800: loss = 3.805593252182007\n",
      "step = 3074000: loss = 4.006992816925049\n",
      "step = 3074200: loss = 4.153111934661865\n",
      "step = 3074400: loss = 3.1911401748657227\n",
      "step = 3074600: loss = 3.22308349609375\n",
      "step = 3074800: loss = 2.8872203826904297\n",
      "step = 3075000: loss = 4.1194963455200195\n",
      "step = 3075000: Average Return = 7.300000190734863\n",
      "step = 3075200: loss = 3.404219388961792\n",
      "step = 3075400: loss = 4.197834491729736\n",
      "step = 3075600: loss = 2.6745331287384033\n",
      "step = 3075800: loss = 2.689354419708252\n",
      "step = 3076000: loss = 1.5721594095230103\n",
      "step = 3076200: loss = 4.185059070587158\n",
      "step = 3076400: loss = 4.757390975952148\n",
      "step = 3076600: loss = 3.134842872619629\n",
      "step = 3076800: loss = 3.511691093444824\n",
      "step = 3077000: loss = 3.0229058265686035\n",
      "step = 3077200: loss = 3.5905601978302\n",
      "step = 3077400: loss = 3.0453505516052246\n",
      "step = 3077600: loss = 2.025489091873169\n",
      "step = 3077800: loss = 3.46694278717041\n",
      "step = 3078000: loss = 3.59240984916687\n",
      "step = 3078200: loss = 2.8570544719696045\n",
      "step = 3078400: loss = 3.024747610092163\n",
      "step = 3078600: loss = 2.7871294021606445\n",
      "step = 3078800: loss = 3.585336208343506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3079000: loss = 2.6715328693389893\n",
      "step = 3079200: loss = 4.155250549316406\n",
      "step = 3079400: loss = 2.400670289993286\n",
      "step = 3079600: loss = 4.840860366821289\n",
      "step = 3079800: loss = 2.4447455406188965\n",
      "step = 3080000: loss = 2.952083110809326\n",
      "step = 3080000: Average Return = 4.650000095367432\n",
      "step = 3080200: loss = 3.175210475921631\n",
      "step = 3080400: loss = 2.456993579864502\n",
      "step = 3080600: loss = 4.0108160972595215\n",
      "step = 3080800: loss = 3.182685375213623\n",
      "step = 3081000: loss = 3.723376989364624\n",
      "step = 3081200: loss = 3.6359148025512695\n",
      "step = 3081400: loss = 3.1508216857910156\n",
      "step = 3081600: loss = 3.8351211547851562\n",
      "step = 3081800: loss = 3.7077105045318604\n",
      "step = 3082000: loss = 4.144174575805664\n",
      "step = 3082200: loss = 2.6624979972839355\n",
      "step = 3082400: loss = 3.213247299194336\n",
      "step = 3082600: loss = 2.7304155826568604\n",
      "step = 3082800: loss = 2.9067089557647705\n",
      "step = 3083000: loss = 3.330641746520996\n",
      "step = 3083200: loss = 2.5812389850616455\n",
      "step = 3083400: loss = 4.939980983734131\n",
      "step = 3083600: loss = 3.380141019821167\n",
      "step = 3083800: loss = 4.377374649047852\n",
      "step = 3084000: loss = 4.129754066467285\n",
      "step = 3084200: loss = 4.204545974731445\n",
      "step = 3084400: loss = 5.33128023147583\n",
      "step = 3084600: loss = 2.9163944721221924\n",
      "step = 3084800: loss = 2.5471549034118652\n",
      "step = 3085000: loss = 3.2866923809051514\n",
      "step = 3085000: Average Return = 4.0\n",
      "step = 3085200: loss = 2.7038629055023193\n",
      "step = 3085400: loss = 3.6442434787750244\n",
      "step = 3085600: loss = 3.336899995803833\n",
      "step = 3085800: loss = 3.1786324977874756\n",
      "step = 3086000: loss = 2.9789302349090576\n",
      "step = 3086200: loss = 3.1524174213409424\n",
      "step = 3086400: loss = 4.286012172698975\n",
      "step = 3086600: loss = 3.2739031314849854\n",
      "step = 3086800: loss = 2.787184953689575\n",
      "step = 3087000: loss = 3.4506781101226807\n",
      "step = 3087200: loss = 3.272799253463745\n",
      "step = 3087400: loss = 3.2619171142578125\n",
      "step = 3087600: loss = 3.649721622467041\n",
      "step = 3087800: loss = 3.2661869525909424\n",
      "step = 3088000: loss = 3.4637651443481445\n",
      "step = 3088200: loss = 2.9421043395996094\n",
      "step = 3088400: loss = 3.165971517562866\n",
      "step = 3088600: loss = 3.433243989944458\n",
      "step = 3088800: loss = 4.600217342376709\n",
      "step = 3089000: loss = 3.342623233795166\n",
      "step = 3089200: loss = 2.503899574279785\n",
      "step = 3089400: loss = 2.8587944507598877\n",
      "step = 3089600: loss = 3.3885247707366943\n",
      "step = 3089800: loss = 2.9468188285827637\n",
      "step = 3090000: loss = 2.4151129722595215\n",
      "step = 3090000: Average Return = 5.400000095367432\n",
      "step = 3090200: loss = 3.219010591506958\n",
      "step = 3090400: loss = 2.861820697784424\n",
      "step = 3090600: loss = 3.8742740154266357\n",
      "step = 3090800: loss = 4.546336650848389\n",
      "step = 3091000: loss = 3.1524035930633545\n",
      "step = 3091200: loss = 2.942833185195923\n",
      "step = 3091400: loss = 3.8810906410217285\n",
      "step = 3091600: loss = 4.53474760055542\n",
      "step = 3091800: loss = 2.3457205295562744\n",
      "step = 3092000: loss = 4.041059494018555\n",
      "step = 3092200: loss = 3.5074784755706787\n",
      "step = 3092400: loss = 2.5181517601013184\n",
      "step = 3092600: loss = 3.167664051055908\n",
      "step = 3092800: loss = 2.5682919025421143\n",
      "step = 3093000: loss = 2.806819200515747\n",
      "step = 3093200: loss = 3.143048048019409\n",
      "step = 3093400: loss = 3.231537342071533\n",
      "step = 3093600: loss = 3.337552309036255\n",
      "step = 3093800: loss = 2.8377795219421387\n",
      "step = 3094000: loss = 4.39838171005249\n",
      "step = 3094200: loss = 3.642951488494873\n",
      "step = 3094400: loss = 3.2596113681793213\n",
      "step = 3094600: loss = 2.9043190479278564\n",
      "step = 3094800: loss = 3.2213144302368164\n",
      "step = 3095000: loss = 2.4739816188812256\n",
      "step = 3095000: Average Return = 5.199999809265137\n",
      "step = 3095200: loss = 2.884462833404541\n",
      "step = 3095400: loss = 3.037412643432617\n",
      "step = 3095600: loss = 2.407834053039551\n",
      "step = 3095800: loss = 2.2409512996673584\n",
      "step = 3096000: loss = 3.014894723892212\n",
      "step = 3096200: loss = 4.349373817443848\n",
      "step = 3096400: loss = 2.7570552825927734\n",
      "step = 3096600: loss = 5.3101325035095215\n",
      "step = 3096800: loss = 2.30867862701416\n",
      "step = 3097000: loss = 4.052567005157471\n",
      "step = 3097200: loss = 3.4017300605773926\n",
      "step = 3097400: loss = 2.669642210006714\n",
      "step = 3097600: loss = 3.001436710357666\n",
      "step = 3097800: loss = 3.355107545852661\n",
      "step = 3098000: loss = 2.83878493309021\n",
      "step = 3098200: loss = 4.11200475692749\n",
      "step = 3098400: loss = 3.4361541271209717\n",
      "step = 3098600: loss = 3.6064982414245605\n",
      "step = 3098800: loss = 2.69316029548645\n",
      "step = 3099000: loss = 3.1354494094848633\n",
      "step = 3099200: loss = 3.7272114753723145\n",
      "step = 3099400: loss = 2.7337679862976074\n",
      "step = 3099600: loss = 4.5586771965026855\n",
      "step = 3099800: loss = 2.469527244567871\n",
      "step = 3100000: loss = 3.2848522663116455\n",
      "step = 3100000: Average Return = 4.0\n",
      "step = 3100200: loss = 3.5257527828216553\n",
      "step = 3100400: loss = 2.9684948921203613\n",
      "step = 3100600: loss = 3.6513617038726807\n",
      "step = 3100800: loss = 4.265652179718018\n",
      "step = 3101000: loss = 3.2600951194763184\n",
      "step = 3101200: loss = 3.968230724334717\n",
      "step = 3101400: loss = 3.095160484313965\n",
      "step = 3101600: loss = 4.186561107635498\n",
      "step = 3101800: loss = 2.6856329441070557\n",
      "step = 3102000: loss = 2.918863534927368\n",
      "step = 3102200: loss = 3.0400919914245605\n",
      "step = 3102400: loss = 3.1883997917175293\n",
      "step = 3102600: loss = 3.090871572494507\n",
      "step = 3102800: loss = 4.1114912033081055\n",
      "step = 3103000: loss = 2.8897976875305176\n",
      "step = 3103200: loss = 2.5774126052856445\n",
      "step = 3103400: loss = 3.5595414638519287\n",
      "step = 3103600: loss = 2.9043004512786865\n",
      "step = 3103800: loss = 2.6814606189727783\n",
      "step = 3104000: loss = 2.415524482727051\n",
      "step = 3104200: loss = 3.002439498901367\n",
      "step = 3104400: loss = 5.6184983253479\n",
      "step = 3104600: loss = 3.86537766456604\n",
      "step = 3104800: loss = 2.9885847568511963\n",
      "step = 3105000: loss = 4.116578102111816\n",
      "step = 3105000: Average Return = 4.550000190734863\n",
      "step = 3105200: loss = 2.628854274749756\n",
      "step = 3105400: loss = 2.7779934406280518\n",
      "step = 3105600: loss = 3.522634267807007\n",
      "step = 3105800: loss = 2.992691993713379\n",
      "step = 3106000: loss = 3.7187907695770264\n",
      "step = 3106200: loss = 2.7332122325897217\n",
      "step = 3106400: loss = 2.5365796089172363\n",
      "step = 3106600: loss = 3.2121636867523193\n",
      "step = 3106800: loss = 3.105581283569336\n",
      "step = 3107000: loss = 2.758159637451172\n",
      "step = 3107200: loss = 3.596954345703125\n",
      "step = 3107400: loss = 2.2576048374176025\n",
      "step = 3107600: loss = 3.6210641860961914\n",
      "step = 3107800: loss = 2.6467156410217285\n",
      "step = 3108000: loss = 3.1797664165496826\n",
      "step = 3108200: loss = 3.1392831802368164\n",
      "step = 3108400: loss = 3.7440240383148193\n",
      "step = 3108600: loss = 3.1273438930511475\n",
      "step = 3108800: loss = 3.9116930961608887\n",
      "step = 3109000: loss = 4.216527462005615\n",
      "step = 3109200: loss = 4.90273904800415\n",
      "step = 3109400: loss = 3.6151256561279297\n",
      "step = 3109600: loss = 3.854459762573242\n",
      "step = 3109800: loss = 2.9936113357543945\n",
      "step = 3110000: loss = 4.078995704650879\n",
      "step = 3110000: Average Return = 5.25\n",
      "step = 3110200: loss = 4.412973403930664\n",
      "step = 3110400: loss = 2.0814871788024902\n",
      "step = 3110600: loss = 3.712618589401245\n",
      "step = 3110800: loss = 2.938425302505493\n",
      "step = 3111000: loss = 3.728233814239502\n",
      "step = 3111200: loss = 4.165012359619141\n",
      "step = 3111400: loss = 2.608588218688965\n",
      "step = 3111600: loss = 3.833408832550049\n",
      "step = 3111800: loss = 2.900606870651245\n",
      "step = 3112000: loss = 3.6556994915008545\n",
      "step = 3112200: loss = 2.5435657501220703\n",
      "step = 3112400: loss = 2.7317662239074707\n",
      "step = 3112600: loss = 3.840947389602661\n",
      "step = 3112800: loss = 3.50528621673584\n",
      "step = 3113000: loss = 2.6440107822418213\n",
      "step = 3113200: loss = 3.3616185188293457\n",
      "step = 3113400: loss = 2.2962887287139893\n",
      "step = 3113600: loss = 3.362462043762207\n",
      "step = 3113800: loss = 3.1102888584136963\n",
      "step = 3114000: loss = 2.2969560623168945\n",
      "step = 3114200: loss = 1.9796254634857178\n",
      "step = 3114400: loss = 3.3487484455108643\n",
      "step = 3114600: loss = 3.9608211517333984\n",
      "step = 3114800: loss = 3.49267315864563\n",
      "step = 3115000: loss = 3.00449538230896\n",
      "step = 3115000: Average Return = 5.550000190734863\n",
      "step = 3115200: loss = 2.8828184604644775\n",
      "step = 3115400: loss = 2.15745210647583\n",
      "step = 3115600: loss = 2.813244581222534\n",
      "step = 3115800: loss = 3.7692885398864746\n",
      "step = 3116000: loss = 3.717258930206299\n",
      "step = 3116200: loss = 3.904984712600708\n",
      "step = 3116400: loss = 2.289628028869629\n",
      "step = 3116600: loss = 2.4176509380340576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3116800: loss = 2.4213128089904785\n",
      "step = 3117000: loss = 2.9162211418151855\n",
      "step = 3117200: loss = 3.3262815475463867\n",
      "step = 3117400: loss = 2.668717861175537\n",
      "step = 3117600: loss = 3.692255735397339\n",
      "step = 3117800: loss = 2.7398157119750977\n",
      "step = 3118000: loss = 3.150330066680908\n",
      "step = 3118200: loss = 4.091987609863281\n",
      "step = 3118400: loss = 2.710566282272339\n",
      "step = 3118600: loss = 3.8198819160461426\n",
      "step = 3118800: loss = 3.202409505844116\n",
      "step = 3119000: loss = 2.5471277236938477\n",
      "step = 3119200: loss = 2.9545793533325195\n",
      "step = 3119400: loss = 2.6886205673217773\n",
      "step = 3119600: loss = 2.717970609664917\n",
      "step = 3119800: loss = 3.655224561691284\n",
      "step = 3120000: loss = 5.076142311096191\n",
      "step = 3120000: Average Return = 3.5999999046325684\n",
      "step = 3120200: loss = 3.7852818965911865\n",
      "step = 3120400: loss = 3.3174102306365967\n",
      "step = 3120600: loss = 3.1277880668640137\n",
      "step = 3120800: loss = 3.0552620887756348\n",
      "step = 3121000: loss = 3.024383068084717\n",
      "step = 3121200: loss = 3.148430824279785\n",
      "step = 3121400: loss = 2.917163133621216\n",
      "step = 3121600: loss = 3.5332415103912354\n",
      "step = 3121800: loss = 5.210625648498535\n",
      "step = 3122000: loss = 4.148870944976807\n",
      "step = 3122200: loss = 2.9008102416992188\n",
      "step = 3122400: loss = 3.500490665435791\n",
      "step = 3122600: loss = 4.178193092346191\n",
      "step = 3122800: loss = 2.6729753017425537\n",
      "step = 3123000: loss = 3.2980294227600098\n",
      "step = 3123200: loss = 2.7377769947052\n",
      "step = 3123400: loss = 2.5622453689575195\n",
      "step = 3123600: loss = 2.697570323944092\n",
      "step = 3123800: loss = 1.6763098239898682\n",
      "step = 3124000: loss = 3.7837846279144287\n",
      "step = 3124200: loss = 3.617363929748535\n",
      "step = 3124400: loss = 2.8605291843414307\n",
      "step = 3124600: loss = 2.7771403789520264\n",
      "step = 3124800: loss = 3.545396327972412\n",
      "step = 3125000: loss = 4.667296886444092\n",
      "step = 3125000: Average Return = 4.650000095367432\n",
      "step = 3125200: loss = 3.5390400886535645\n",
      "step = 3125400: loss = 3.032179832458496\n",
      "step = 3125600: loss = 2.6079673767089844\n",
      "step = 3125800: loss = 3.8572757244110107\n",
      "step = 3126000: loss = 3.3991143703460693\n",
      "step = 3126200: loss = 3.03200101852417\n",
      "step = 3126400: loss = 2.5705361366271973\n",
      "step = 3126600: loss = 3.233032703399658\n",
      "step = 3126800: loss = 2.521091938018799\n",
      "step = 3127000: loss = 2.6623148918151855\n",
      "step = 3127200: loss = 3.9755699634552\n",
      "step = 3127400: loss = 4.371654987335205\n",
      "step = 3127600: loss = 3.8638572692871094\n",
      "step = 3127800: loss = 2.822484254837036\n",
      "step = 3128000: loss = 3.3002147674560547\n",
      "step = 3128200: loss = 3.099910259246826\n",
      "step = 3128400: loss = 3.316570997238159\n",
      "step = 3128600: loss = 2.5640392303466797\n",
      "step = 3128800: loss = 2.598151445388794\n",
      "step = 3129000: loss = 3.4436683654785156\n",
      "step = 3129200: loss = 2.527765989303589\n",
      "step = 3129400: loss = 2.3392772674560547\n",
      "step = 3129600: loss = 2.604522705078125\n",
      "step = 3129800: loss = 3.6585464477539062\n",
      "step = 3130000: loss = 3.368234157562256\n",
      "step = 3130000: Average Return = 4.599999904632568\n",
      "step = 3130200: loss = 2.7991931438446045\n",
      "step = 3130400: loss = 2.9362566471099854\n",
      "step = 3130600: loss = 2.7665512561798096\n",
      "step = 3130800: loss = 3.68465256690979\n",
      "step = 3131000: loss = 3.206817626953125\n",
      "step = 3131200: loss = 2.5223464965820312\n",
      "step = 3131400: loss = 2.5200188159942627\n",
      "step = 3131600: loss = 3.072558879852295\n",
      "step = 3131800: loss = 3.29354190826416\n",
      "step = 3132000: loss = 3.8951051235198975\n",
      "step = 3132200: loss = 3.686110496520996\n",
      "step = 3132400: loss = 3.7133538722991943\n",
      "step = 3132600: loss = 5.226031303405762\n",
      "step = 3132800: loss = 3.924238443374634\n",
      "step = 3133000: loss = 2.6897215843200684\n",
      "step = 3133200: loss = 3.661069631576538\n",
      "step = 3133400: loss = 3.144719123840332\n",
      "step = 3133600: loss = 3.5050694942474365\n",
      "step = 3133800: loss = 4.10502290725708\n",
      "step = 3134000: loss = 3.206040620803833\n",
      "step = 3134200: loss = 4.085883617401123\n",
      "step = 3134400: loss = 2.8810579776763916\n",
      "step = 3134600: loss = 3.6733062267303467\n",
      "step = 3134800: loss = 3.574861764907837\n",
      "step = 3135000: loss = 3.0111732482910156\n",
      "step = 3135000: Average Return = 5.699999809265137\n",
      "step = 3135200: loss = 4.292327404022217\n",
      "step = 3135400: loss = 3.0217087268829346\n",
      "step = 3135600: loss = 4.784559726715088\n",
      "step = 3135800: loss = 3.051873207092285\n",
      "step = 3136000: loss = 2.8461430072784424\n",
      "step = 3136200: loss = 2.593599796295166\n",
      "step = 3136400: loss = 3.297239303588867\n",
      "step = 3136600: loss = 4.685507774353027\n",
      "step = 3136800: loss = 2.9296188354492188\n",
      "step = 3137000: loss = 3.4980995655059814\n",
      "step = 3137200: loss = 3.058647632598877\n",
      "step = 3137400: loss = 3.6952733993530273\n",
      "step = 3137600: loss = 3.6309754848480225\n",
      "step = 3137800: loss = 3.9749813079833984\n",
      "step = 3138000: loss = 3.0815086364746094\n",
      "step = 3138200: loss = 3.33453369140625\n",
      "step = 3138400: loss = 2.790562629699707\n",
      "step = 3138600: loss = 4.057631969451904\n",
      "step = 3138800: loss = 4.076176166534424\n",
      "step = 3139000: loss = 4.309878349304199\n",
      "step = 3139200: loss = 3.229668617248535\n",
      "step = 3139400: loss = 3.95931077003479\n",
      "step = 3139600: loss = 3.815605401992798\n",
      "step = 3139800: loss = 2.914581537246704\n",
      "step = 3140000: loss = 4.608865261077881\n",
      "step = 3140000: Average Return = 2.299999952316284\n",
      "step = 3140200: loss = 2.3880198001861572\n",
      "step = 3140400: loss = 3.7449560165405273\n",
      "step = 3140600: loss = 3.3084630966186523\n",
      "step = 3140800: loss = 2.93472957611084\n",
      "step = 3141000: loss = 2.2977957725524902\n",
      "step = 3141200: loss = 4.546177387237549\n",
      "step = 3141400: loss = 3.9404008388519287\n",
      "step = 3141600: loss = 3.275644063949585\n",
      "step = 3141800: loss = 3.1988584995269775\n",
      "step = 3142000: loss = 3.010349988937378\n",
      "step = 3142200: loss = 3.1897528171539307\n",
      "step = 3142400: loss = 3.7000887393951416\n",
      "step = 3142600: loss = 3.8437418937683105\n",
      "step = 3142800: loss = 2.335569381713867\n",
      "step = 3143000: loss = 2.977736711502075\n",
      "step = 3143200: loss = 3.217595100402832\n",
      "step = 3143400: loss = 3.333526611328125\n",
      "step = 3143600: loss = 3.48354172706604\n",
      "step = 3143800: loss = 2.5999996662139893\n",
      "step = 3144000: loss = 3.3922953605651855\n",
      "step = 3144200: loss = 2.935748815536499\n",
      "step = 3144400: loss = 3.3404130935668945\n",
      "step = 3144600: loss = 3.5421974658966064\n",
      "step = 3144800: loss = 6.879479885101318\n",
      "step = 3145000: loss = 2.0942041873931885\n",
      "step = 3145000: Average Return = 3.700000047683716\n",
      "step = 3145200: loss = 3.6670193672180176\n",
      "step = 3145400: loss = 4.546485424041748\n",
      "step = 3145600: loss = 4.015129566192627\n",
      "step = 3145800: loss = 2.850148916244507\n",
      "step = 3146000: loss = 3.3544256687164307\n",
      "step = 3146200: loss = 5.125505447387695\n",
      "step = 3146400: loss = 3.2474148273468018\n",
      "step = 3146600: loss = 4.072646141052246\n",
      "step = 3146800: loss = 3.0965263843536377\n",
      "step = 3147000: loss = 4.865119934082031\n",
      "step = 3147200: loss = 3.4712791442871094\n",
      "step = 3147400: loss = 5.329247951507568\n",
      "step = 3147600: loss = 2.7051806449890137\n",
      "step = 3147800: loss = 3.051725149154663\n",
      "step = 3148000: loss = 3.451552629470825\n",
      "step = 3148200: loss = 3.604273796081543\n",
      "step = 3148400: loss = 2.3109078407287598\n",
      "step = 3148600: loss = 3.805804967880249\n",
      "step = 3148800: loss = 3.9147472381591797\n",
      "step = 3149000: loss = 2.867173433303833\n",
      "step = 3149200: loss = 3.9663939476013184\n",
      "step = 3149400: loss = 2.8204991817474365\n",
      "step = 3149600: loss = 3.159050941467285\n",
      "step = 3149800: loss = 3.5469157695770264\n",
      "step = 3150000: loss = 3.688917875289917\n",
      "step = 3150000: Average Return = 6.400000095367432\n",
      "step = 3150200: loss = 5.303110122680664\n",
      "step = 3150400: loss = 2.9857144355773926\n",
      "step = 3150600: loss = 2.8678135871887207\n",
      "step = 3150800: loss = 2.515007257461548\n",
      "step = 3151000: loss = 3.112895965576172\n",
      "step = 3151200: loss = 3.371112585067749\n",
      "step = 3151400: loss = 4.458399772644043\n",
      "step = 3151600: loss = 3.080428123474121\n",
      "step = 3151800: loss = 5.001713752746582\n",
      "step = 3152000: loss = 2.2905807495117188\n",
      "step = 3152200: loss = 4.453503131866455\n",
      "step = 3152400: loss = 3.574192523956299\n",
      "step = 3152600: loss = 3.887860059738159\n",
      "step = 3152800: loss = 4.473894119262695\n",
      "step = 3153000: loss = 3.6694440841674805\n",
      "step = 3153200: loss = 4.537514686584473\n",
      "step = 3153400: loss = 3.395479440689087\n",
      "step = 3153600: loss = 4.857903480529785\n",
      "step = 3153800: loss = 3.889711856842041\n",
      "step = 3154000: loss = 2.9468111991882324\n",
      "step = 3154200: loss = 2.5461480617523193\n",
      "step = 3154400: loss = 2.5946733951568604\n",
      "step = 3154600: loss = 1.9012072086334229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3154800: loss = 2.808832883834839\n",
      "step = 3155000: loss = 2.7001516819000244\n",
      "step = 3155000: Average Return = 4.550000190734863\n",
      "step = 3155200: loss = 4.375783443450928\n",
      "step = 3155400: loss = 4.09182071685791\n",
      "step = 3155600: loss = 2.421567916870117\n",
      "step = 3155800: loss = 2.675203800201416\n",
      "step = 3156000: loss = 4.514044761657715\n",
      "step = 3156200: loss = 3.1010329723358154\n",
      "step = 3156400: loss = 3.193666934967041\n",
      "step = 3156600: loss = 3.2446610927581787\n",
      "step = 3156800: loss = 3.128676176071167\n",
      "step = 3157000: loss = 2.97686505317688\n",
      "step = 3157200: loss = 3.184847354888916\n",
      "step = 3157400: loss = 2.7527413368225098\n",
      "step = 3157600: loss = 4.125449180603027\n",
      "step = 3157800: loss = 3.625737428665161\n",
      "step = 3158000: loss = 3.3333795070648193\n",
      "step = 3158200: loss = 3.144260883331299\n",
      "step = 3158400: loss = 4.210293292999268\n",
      "step = 3158600: loss = 3.0293571949005127\n",
      "step = 3158800: loss = 2.936622142791748\n",
      "step = 3159000: loss = 2.9321532249450684\n",
      "step = 3159200: loss = 4.219292640686035\n",
      "step = 3159400: loss = 3.711094856262207\n",
      "step = 3159600: loss = 3.098644971847534\n",
      "step = 3159800: loss = 5.272672176361084\n",
      "step = 3160000: loss = 4.664938449859619\n",
      "step = 3160000: Average Return = 5.099999904632568\n",
      "step = 3160200: loss = 3.977525472640991\n",
      "step = 3160400: loss = 5.21564245223999\n",
      "step = 3160600: loss = 2.9839577674865723\n",
      "step = 3160800: loss = 3.0714757442474365\n",
      "step = 3161000: loss = 3.433617115020752\n",
      "step = 3161200: loss = 3.4957878589630127\n",
      "step = 3161400: loss = 3.992818832397461\n",
      "step = 3161600: loss = 1.9137418270111084\n",
      "step = 3161800: loss = 4.342437744140625\n",
      "step = 3162000: loss = 3.4761576652526855\n",
      "step = 3162200: loss = 3.102030038833618\n",
      "step = 3162400: loss = 3.0687029361724854\n",
      "step = 3162600: loss = 4.07678747177124\n",
      "step = 3162800: loss = 3.7653725147247314\n",
      "step = 3163000: loss = 2.062911033630371\n",
      "step = 3163200: loss = 2.6153526306152344\n",
      "step = 3163400: loss = 2.423478603363037\n",
      "step = 3163600: loss = 3.304924964904785\n",
      "step = 3163800: loss = 4.3975629806518555\n",
      "step = 3164000: loss = 3.744004487991333\n",
      "step = 3164200: loss = 3.4291868209838867\n",
      "step = 3164400: loss = 3.4496946334838867\n",
      "step = 3164600: loss = 4.365321636199951\n",
      "step = 3164800: loss = 2.9856350421905518\n",
      "step = 3165000: loss = 4.108108997344971\n",
      "step = 3165000: Average Return = 6.900000095367432\n",
      "step = 3165200: loss = 3.29085636138916\n",
      "step = 3165400: loss = 3.9348514080047607\n",
      "step = 3165600: loss = 2.8995659351348877\n",
      "step = 3165800: loss = 3.0944175720214844\n",
      "step = 3166000: loss = 3.2671289443969727\n",
      "step = 3166200: loss = 3.211824655532837\n",
      "step = 3166400: loss = 3.1884889602661133\n",
      "step = 3166600: loss = 2.55810284614563\n",
      "step = 3166800: loss = 2.676222562789917\n",
      "step = 3167000: loss = 3.1843299865722656\n",
      "step = 3167200: loss = 2.429340362548828\n",
      "step = 3167400: loss = 1.988486886024475\n",
      "step = 3167600: loss = 3.703195810317993\n",
      "step = 3167800: loss = 3.2798190116882324\n",
      "step = 3168000: loss = 3.643003463745117\n",
      "step = 3168200: loss = 3.310950756072998\n",
      "step = 3168400: loss = 3.889465808868408\n",
      "step = 3168600: loss = 3.995265007019043\n",
      "step = 3168800: loss = 2.176258087158203\n",
      "step = 3169000: loss = 3.1804919242858887\n",
      "step = 3169200: loss = 4.0204925537109375\n",
      "step = 3169400: loss = 3.56689715385437\n",
      "step = 3169600: loss = 4.230006694793701\n",
      "step = 3169800: loss = 3.5516717433929443\n",
      "step = 3170000: loss = 4.328074932098389\n",
      "step = 3170000: Average Return = 4.800000190734863\n",
      "step = 3170200: loss = 2.533825397491455\n",
      "step = 3170400: loss = 3.388908624649048\n",
      "step = 3170600: loss = 3.4945597648620605\n",
      "step = 3170800: loss = 3.7148220539093018\n",
      "step = 3171000: loss = 4.395171642303467\n",
      "step = 3171200: loss = 2.6983327865600586\n",
      "step = 3171400: loss = 4.32032585144043\n",
      "step = 3171600: loss = 4.035064220428467\n",
      "step = 3171800: loss = 2.9795382022857666\n",
      "step = 3172000: loss = 4.263469219207764\n",
      "step = 3172200: loss = 2.5609686374664307\n",
      "step = 3172400: loss = 4.299172878265381\n",
      "step = 3172600: loss = 2.729978084564209\n",
      "step = 3172800: loss = 2.7242677211761475\n",
      "step = 3173000: loss = 2.8594970703125\n",
      "step = 3173200: loss = 2.625842571258545\n",
      "step = 3173400: loss = 4.332192897796631\n",
      "step = 3173600: loss = 3.669701337814331\n",
      "step = 3173800: loss = 2.2429699897766113\n",
      "step = 3174000: loss = 3.854445695877075\n",
      "step = 3174200: loss = 5.837226867675781\n",
      "step = 3174400: loss = 4.144351005554199\n",
      "step = 3174600: loss = 3.392672538757324\n",
      "step = 3174800: loss = 2.78475284576416\n",
      "step = 3175000: loss = 3.567415714263916\n",
      "step = 3175000: Average Return = 5.900000095367432\n",
      "step = 3175200: loss = 3.5352659225463867\n",
      "step = 3175400: loss = 4.312415599822998\n",
      "step = 3175600: loss = 5.516110897064209\n",
      "step = 3175800: loss = 2.218184471130371\n",
      "step = 3176000: loss = 3.509324550628662\n",
      "step = 3176200: loss = 2.733643054962158\n",
      "step = 3176400: loss = 3.6146440505981445\n",
      "step = 3176600: loss = 2.853616952896118\n",
      "step = 3176800: loss = 3.3618953227996826\n",
      "step = 3177000: loss = 3.5754494667053223\n",
      "step = 3177200: loss = 2.3205156326293945\n",
      "step = 3177400: loss = 4.021549701690674\n",
      "step = 3177600: loss = 4.09999418258667\n",
      "step = 3177800: loss = 3.1349945068359375\n",
      "step = 3178000: loss = 4.866459846496582\n",
      "step = 3178200: loss = 4.204646587371826\n",
      "step = 3178400: loss = 3.0426018238067627\n",
      "step = 3178600: loss = 2.881789445877075\n",
      "step = 3178800: loss = 3.3644285202026367\n",
      "step = 3179000: loss = 5.465478420257568\n",
      "step = 3179200: loss = 5.3501996994018555\n",
      "step = 3179400: loss = 4.377130508422852\n",
      "step = 3179600: loss = 4.764823913574219\n",
      "step = 3179800: loss = 3.570920944213867\n",
      "step = 3180000: loss = 4.149831771850586\n",
      "step = 3180000: Average Return = 5.75\n",
      "step = 3180200: loss = 5.593481540679932\n",
      "step = 3180400: loss = 2.419250011444092\n",
      "step = 3180600: loss = 2.8701438903808594\n",
      "step = 3180800: loss = 3.115715265274048\n",
      "step = 3181000: loss = 2.9363176822662354\n",
      "step = 3181200: loss = 4.188539505004883\n",
      "step = 3181400: loss = 2.9093546867370605\n",
      "step = 3181600: loss = 4.269148349761963\n",
      "step = 3181800: loss = 3.532717704772949\n",
      "step = 3182000: loss = 3.4404313564300537\n",
      "step = 3182200: loss = 3.185940742492676\n",
      "step = 3182400: loss = 3.6082992553710938\n",
      "step = 3182600: loss = 4.502495765686035\n",
      "step = 3182800: loss = 2.9960591793060303\n",
      "step = 3183000: loss = 4.357200622558594\n",
      "step = 3183200: loss = 3.4898529052734375\n",
      "step = 3183400: loss = 3.1364150047302246\n",
      "step = 3183600: loss = 2.849153995513916\n",
      "step = 3183800: loss = 2.248394727706909\n",
      "step = 3184000: loss = 6.106066703796387\n",
      "step = 3184200: loss = 3.4018847942352295\n",
      "step = 3184400: loss = 2.7508437633514404\n",
      "step = 3184600: loss = 2.3234167098999023\n",
      "step = 3184800: loss = 3.033313512802124\n",
      "step = 3185000: loss = 3.9699764251708984\n",
      "step = 3185000: Average Return = 4.400000095367432\n",
      "step = 3185200: loss = 2.7638580799102783\n",
      "step = 3185400: loss = 4.067709445953369\n",
      "step = 3185600: loss = 2.1672251224517822\n",
      "step = 3185800: loss = 4.1886091232299805\n",
      "step = 3186000: loss = 4.350999355316162\n",
      "step = 3186200: loss = 2.8240857124328613\n",
      "step = 3186400: loss = 4.588958263397217\n",
      "step = 3186600: loss = 4.406707286834717\n",
      "step = 3186800: loss = 3.2156364917755127\n",
      "step = 3187000: loss = 4.283589839935303\n",
      "step = 3187200: loss = 2.9275002479553223\n",
      "step = 3187400: loss = 3.533588171005249\n",
      "step = 3187600: loss = 3.3727171421051025\n",
      "step = 3187800: loss = 3.5927352905273438\n",
      "step = 3188000: loss = 2.6001906394958496\n",
      "step = 3188200: loss = 3.111346960067749\n",
      "step = 3188400: loss = 3.1960885524749756\n",
      "step = 3188600: loss = 3.1063222885131836\n",
      "step = 3188800: loss = 2.745441436767578\n",
      "step = 3189000: loss = 2.569999933242798\n",
      "step = 3189200: loss = 2.998627185821533\n",
      "step = 3189400: loss = 3.3127856254577637\n",
      "step = 3189600: loss = 3.435980796813965\n",
      "step = 3189800: loss = 5.206939697265625\n",
      "step = 3190000: loss = 2.4300758838653564\n",
      "step = 3190000: Average Return = 5.0\n",
      "step = 3190200: loss = 2.963735818862915\n",
      "step = 3190400: loss = 2.1211612224578857\n",
      "step = 3190600: loss = 3.051255941390991\n",
      "step = 3190800: loss = 2.4913148880004883\n",
      "step = 3191000: loss = 3.9353880882263184\n",
      "step = 3191200: loss = 2.5802135467529297\n",
      "step = 3191400: loss = 2.864377021789551\n",
      "step = 3191600: loss = 3.0500218868255615\n",
      "step = 3191800: loss = 4.839663982391357\n",
      "step = 3192000: loss = 3.724175214767456\n",
      "step = 3192200: loss = 3.605867862701416\n",
      "step = 3192400: loss = 3.439293146133423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3192600: loss = 2.5499207973480225\n",
      "step = 3192800: loss = 4.500619888305664\n",
      "step = 3193000: loss = 2.609356641769409\n",
      "step = 3193200: loss = 2.97987961769104\n",
      "step = 3193400: loss = 2.348095655441284\n",
      "step = 3193600: loss = 3.1818981170654297\n",
      "step = 3193800: loss = 2.8279812335968018\n",
      "step = 3194000: loss = 3.328369617462158\n",
      "step = 3194200: loss = 3.7336156368255615\n",
      "step = 3194400: loss = 2.546710252761841\n",
      "step = 3194600: loss = 3.486632823944092\n",
      "step = 3194800: loss = 3.2114062309265137\n",
      "step = 3195000: loss = 3.6860976219177246\n",
      "step = 3195000: Average Return = 3.799999952316284\n",
      "step = 3195200: loss = 2.759636163711548\n",
      "step = 3195400: loss = 2.5208332538604736\n",
      "step = 3195600: loss = 4.607468128204346\n",
      "step = 3195800: loss = 2.6758065223693848\n",
      "step = 3196000: loss = 3.9206793308258057\n",
      "step = 3196200: loss = 4.0325212478637695\n",
      "step = 3196400: loss = 3.565516233444214\n",
      "step = 3196600: loss = 2.7635958194732666\n",
      "step = 3196800: loss = 3.1287059783935547\n",
      "step = 3197000: loss = 4.104075908660889\n",
      "step = 3197200: loss = 4.386481761932373\n",
      "step = 3197400: loss = 4.052806854248047\n",
      "step = 3197600: loss = 3.783372640609741\n",
      "step = 3197800: loss = 2.75814151763916\n",
      "step = 3198000: loss = 2.5912129878997803\n",
      "step = 3198200: loss = 2.2714128494262695\n",
      "step = 3198400: loss = 3.9039409160614014\n",
      "step = 3198600: loss = 3.776289701461792\n",
      "step = 3198800: loss = 3.372451066970825\n",
      "step = 3199000: loss = 2.8143107891082764\n",
      "step = 3199200: loss = 4.247512340545654\n",
      "step = 3199400: loss = 3.5583465099334717\n",
      "step = 3199600: loss = 2.199061632156372\n",
      "step = 3199800: loss = 5.992798328399658\n",
      "step = 3200000: loss = 2.430654287338257\n",
      "step = 3200000: Average Return = 3.950000047683716\n",
      "step = 3200200: loss = 2.644599437713623\n",
      "step = 3200400: loss = 2.53436017036438\n",
      "step = 3200600: loss = 3.5748167037963867\n",
      "step = 3200800: loss = 2.643609046936035\n",
      "step = 3201000: loss = 3.361769437789917\n",
      "step = 3201200: loss = 3.961127758026123\n",
      "step = 3201400: loss = 4.917200088500977\n",
      "step = 3201600: loss = 3.5788073539733887\n",
      "step = 3201800: loss = 5.88941764831543\n",
      "step = 3202000: loss = 4.064615249633789\n",
      "step = 3202200: loss = 3.890259027481079\n",
      "step = 3202400: loss = 2.9072906970977783\n",
      "step = 3202600: loss = 4.036592483520508\n",
      "step = 3202800: loss = 2.51959228515625\n",
      "step = 3203000: loss = 3.4031550884246826\n",
      "step = 3203200: loss = 3.021566152572632\n",
      "step = 3203400: loss = 2.803370237350464\n",
      "step = 3203600: loss = 3.581507444381714\n",
      "step = 3203800: loss = 4.312668800354004\n",
      "step = 3204000: loss = 3.4469995498657227\n",
      "step = 3204200: loss = 2.615532875061035\n",
      "step = 3204400: loss = 3.1932177543640137\n",
      "step = 3204600: loss = 5.0244927406311035\n",
      "step = 3204800: loss = 3.991563081741333\n",
      "step = 3205000: loss = 2.7439162731170654\n",
      "step = 3205000: Average Return = 4.900000095367432\n",
      "step = 3205200: loss = 4.73126745223999\n",
      "step = 3205400: loss = 2.752528667449951\n",
      "step = 3205600: loss = 2.6032772064208984\n",
      "step = 3205800: loss = 3.293522357940674\n",
      "step = 3206000: loss = 2.526243209838867\n",
      "step = 3206200: loss = 3.0947601795196533\n",
      "step = 3206400: loss = 4.15626859664917\n",
      "step = 3206600: loss = 2.518630266189575\n",
      "step = 3206800: loss = 4.406199932098389\n",
      "step = 3207000: loss = 2.9876549243927\n",
      "step = 3207200: loss = 2.645322322845459\n",
      "step = 3207400: loss = 3.572575807571411\n",
      "step = 3207600: loss = 3.3344240188598633\n",
      "step = 3207800: loss = 3.6765477657318115\n",
      "step = 3208000: loss = 3.250016450881958\n",
      "step = 3208200: loss = 2.7904272079467773\n",
      "step = 3208400: loss = 4.8830976486206055\n",
      "step = 3208600: loss = 2.0821380615234375\n",
      "step = 3208800: loss = 1.7885525226593018\n",
      "step = 3209000: loss = 1.694840669631958\n",
      "step = 3209200: loss = 4.56887149810791\n",
      "step = 3209400: loss = 2.904419183731079\n",
      "step = 3209600: loss = 3.3455779552459717\n",
      "step = 3209800: loss = 4.257716178894043\n",
      "step = 3210000: loss = 2.698946714401245\n",
      "step = 3210000: Average Return = 5.5\n",
      "step = 3210200: loss = 4.285516738891602\n",
      "step = 3210400: loss = 3.369147539138794\n",
      "step = 3210600: loss = 4.405592441558838\n",
      "step = 3210800: loss = 2.1492269039154053\n",
      "step = 3211000: loss = 3.073498249053955\n",
      "step = 3211200: loss = 2.9789936542510986\n",
      "step = 3211400: loss = 3.081939220428467\n",
      "step = 3211600: loss = 2.974433183670044\n",
      "step = 3211800: loss = 4.942866325378418\n",
      "step = 3212000: loss = 3.234703302383423\n",
      "step = 3212200: loss = 2.7775399684906006\n",
      "step = 3212400: loss = 4.212921619415283\n",
      "step = 3212600: loss = 2.586270809173584\n",
      "step = 3212800: loss = 3.4380311965942383\n",
      "step = 3213000: loss = 3.168259859085083\n",
      "step = 3213200: loss = 3.2193212509155273\n",
      "step = 3213400: loss = 2.4325037002563477\n",
      "step = 3213600: loss = 4.471614837646484\n",
      "step = 3213800: loss = 2.8397505283355713\n",
      "step = 3214000: loss = 3.055283784866333\n",
      "step = 3214200: loss = 2.983522891998291\n",
      "step = 3214400: loss = 4.787301540374756\n",
      "step = 3214600: loss = 3.2890007495880127\n",
      "step = 3214800: loss = 4.449929714202881\n",
      "step = 3215000: loss = 3.51924729347229\n",
      "step = 3215000: Average Return = 5.150000095367432\n",
      "step = 3215200: loss = 3.2198235988616943\n",
      "step = 3215400: loss = 3.3343193531036377\n",
      "step = 3215600: loss = 3.0486929416656494\n",
      "step = 3215800: loss = 2.6986825466156006\n",
      "step = 3216000: loss = 4.634894847869873\n",
      "step = 3216200: loss = 3.260692596435547\n",
      "step = 3216400: loss = 4.273193359375\n",
      "step = 3216600: loss = 3.243375062942505\n",
      "step = 3216800: loss = 2.774799108505249\n",
      "step = 3217000: loss = 3.0981760025024414\n",
      "step = 3217200: loss = 2.204144239425659\n",
      "step = 3217400: loss = 2.3555092811584473\n",
      "step = 3217600: loss = 3.5389885902404785\n",
      "step = 3217800: loss = 4.3970046043396\n",
      "step = 3218000: loss = 4.206143856048584\n",
      "step = 3218200: loss = 2.8625762462615967\n",
      "step = 3218400: loss = 2.0611445903778076\n",
      "step = 3218600: loss = 2.471726894378662\n",
      "step = 3218800: loss = 3.5426313877105713\n",
      "step = 3219000: loss = 2.591970443725586\n",
      "step = 3219200: loss = 4.548902988433838\n",
      "step = 3219400: loss = 2.111130475997925\n",
      "step = 3219600: loss = 2.966552972793579\n",
      "step = 3219800: loss = 3.8537685871124268\n",
      "step = 3220000: loss = 3.6443493366241455\n",
      "step = 3220000: Average Return = 4.300000190734863\n",
      "step = 3220200: loss = 3.2378015518188477\n",
      "step = 3220400: loss = 3.7353978157043457\n",
      "step = 3220600: loss = 2.49153470993042\n",
      "step = 3220800: loss = 3.5438597202301025\n",
      "step = 3221000: loss = 3.2096753120422363\n",
      "step = 3221200: loss = 3.3311259746551514\n",
      "step = 3221400: loss = 2.1752419471740723\n",
      "step = 3221600: loss = 2.901721477508545\n",
      "step = 3221800: loss = 4.042196750640869\n",
      "step = 3222000: loss = 3.176330089569092\n",
      "step = 3222200: loss = 4.142913341522217\n",
      "step = 3222400: loss = 4.008698463439941\n",
      "step = 3222600: loss = 3.6499264240264893\n",
      "step = 3222800: loss = 4.8351874351501465\n",
      "step = 3223000: loss = 3.7066187858581543\n",
      "step = 3223200: loss = 3.2743585109710693\n",
      "step = 3223400: loss = 3.8465993404388428\n",
      "step = 3223600: loss = 3.682274580001831\n",
      "step = 3223800: loss = 2.6701462268829346\n",
      "step = 3224000: loss = 3.8423051834106445\n",
      "step = 3224200: loss = 3.711995840072632\n",
      "step = 3224400: loss = 3.1513915061950684\n",
      "step = 3224600: loss = 4.139535903930664\n",
      "step = 3224800: loss = 2.550464153289795\n",
      "step = 3225000: loss = 3.6894774436950684\n",
      "step = 3225000: Average Return = 6.25\n",
      "step = 3225200: loss = 2.78432297706604\n",
      "step = 3225400: loss = 2.4882113933563232\n",
      "step = 3225600: loss = 3.8596489429473877\n",
      "step = 3225800: loss = 2.566477060317993\n",
      "step = 3226000: loss = 2.7057929039001465\n",
      "step = 3226200: loss = 2.827091693878174\n",
      "step = 3226400: loss = 3.2351760864257812\n",
      "step = 3226600: loss = 3.657719373703003\n",
      "step = 3226800: loss = 4.191439628601074\n",
      "step = 3227000: loss = 3.11672306060791\n",
      "step = 3227200: loss = 3.2559525966644287\n",
      "step = 3227400: loss = 3.544857978820801\n",
      "step = 3227600: loss = 3.365272045135498\n",
      "step = 3227800: loss = 3.718539237976074\n",
      "step = 3228000: loss = 2.3351082801818848\n",
      "step = 3228200: loss = 3.385047435760498\n",
      "step = 3228400: loss = 5.134018898010254\n",
      "step = 3228600: loss = 2.4969098567962646\n",
      "step = 3228800: loss = 4.362322807312012\n",
      "step = 3229000: loss = 3.1557698249816895\n",
      "step = 3229200: loss = 2.5482864379882812\n",
      "step = 3229400: loss = 2.225712776184082\n",
      "step = 3229600: loss = 2.3892691135406494\n",
      "step = 3229800: loss = 4.1862592697143555\n",
      "step = 3230000: loss = 4.02425479888916\n",
      "step = 3230000: Average Return = 5.599999904632568\n",
      "step = 3230200: loss = 3.2912797927856445\n",
      "step = 3230400: loss = 2.6708006858825684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3230600: loss = 3.7366583347320557\n",
      "step = 3230800: loss = 3.493878126144409\n",
      "step = 3231000: loss = 4.770025730133057\n",
      "step = 3231200: loss = 2.73630690574646\n",
      "step = 3231400: loss = 3.916651487350464\n",
      "step = 3231600: loss = 2.7277016639709473\n",
      "step = 3231800: loss = 3.5528924465179443\n",
      "step = 3232000: loss = 2.871662139892578\n",
      "step = 3232200: loss = 4.149856090545654\n",
      "step = 3232400: loss = 3.0401365756988525\n",
      "step = 3232600: loss = 3.320990562438965\n",
      "step = 3232800: loss = 4.350433826446533\n",
      "step = 3233000: loss = 2.244493007659912\n",
      "step = 3233200: loss = 3.397700071334839\n",
      "step = 3233400: loss = 4.257744312286377\n",
      "step = 3233600: loss = 2.4333882331848145\n",
      "step = 3233800: loss = 3.9494597911834717\n",
      "step = 3234000: loss = 2.2448389530181885\n",
      "step = 3234200: loss = 2.684147357940674\n",
      "step = 3234400: loss = 3.0021448135375977\n",
      "step = 3234600: loss = 3.1514244079589844\n",
      "step = 3234800: loss = 3.772486448287964\n",
      "step = 3235000: loss = 3.0578114986419678\n",
      "step = 3235000: Average Return = 3.75\n",
      "step = 3235200: loss = 3.751549005508423\n",
      "step = 3235400: loss = 2.5012288093566895\n",
      "step = 3235600: loss = 3.075291156768799\n",
      "step = 3235800: loss = 2.5633158683776855\n",
      "step = 3236000: loss = 2.50470232963562\n",
      "step = 3236200: loss = 3.1757044792175293\n",
      "step = 3236400: loss = 3.8228752613067627\n",
      "step = 3236600: loss = 3.5575408935546875\n",
      "step = 3236800: loss = 3.6258742809295654\n",
      "step = 3237000: loss = 3.878175973892212\n",
      "step = 3237200: loss = 3.528165817260742\n",
      "step = 3237400: loss = 3.0461175441741943\n",
      "step = 3237600: loss = 3.2988250255584717\n",
      "step = 3237800: loss = 3.0584442615509033\n",
      "step = 3238000: loss = 2.4273459911346436\n",
      "step = 3238200: loss = 3.22255802154541\n",
      "step = 3238400: loss = 3.5453250408172607\n",
      "step = 3238600: loss = 5.192995548248291\n",
      "step = 3238800: loss = 2.771474599838257\n",
      "step = 3239000: loss = 2.937915086746216\n",
      "step = 3239200: loss = 2.281773567199707\n",
      "step = 3239400: loss = 2.4723870754241943\n",
      "step = 3239600: loss = 3.8897483348846436\n",
      "step = 3239800: loss = 5.5490403175354\n",
      "step = 3240000: loss = 3.425662040710449\n",
      "step = 3240000: Average Return = 2.950000047683716\n",
      "step = 3240200: loss = 3.9520423412323\n",
      "step = 3240400: loss = 3.562825918197632\n",
      "step = 3240600: loss = 2.971083402633667\n",
      "step = 3240800: loss = 3.986098051071167\n",
      "step = 3241000: loss = 4.487909317016602\n",
      "step = 3241200: loss = 4.4155449867248535\n",
      "step = 3241400: loss = 2.5534820556640625\n",
      "step = 3241600: loss = 3.5723392963409424\n",
      "step = 3241800: loss = 3.14560604095459\n",
      "step = 3242000: loss = 4.002806186676025\n",
      "step = 3242200: loss = 2.8957841396331787\n",
      "step = 3242400: loss = 3.4295997619628906\n",
      "step = 3242600: loss = 2.6978044509887695\n",
      "step = 3242800: loss = 2.332359552383423\n",
      "step = 3243000: loss = 2.283738136291504\n",
      "step = 3243200: loss = 4.215234756469727\n",
      "step = 3243400: loss = 4.214635372161865\n",
      "step = 3243600: loss = 3.225856304168701\n",
      "step = 3243800: loss = 2.6876068115234375\n",
      "step = 3244000: loss = 3.824514627456665\n",
      "step = 3244200: loss = 3.550205707550049\n",
      "step = 3244400: loss = 2.7580840587615967\n",
      "step = 3244600: loss = 3.9216203689575195\n",
      "step = 3244800: loss = 3.0931499004364014\n",
      "step = 3245000: loss = 3.292767286300659\n",
      "step = 3245000: Average Return = 2.950000047683716\n",
      "step = 3245200: loss = 2.8430209159851074\n",
      "step = 3245400: loss = 3.149322748184204\n",
      "step = 3245600: loss = 3.0657567977905273\n",
      "step = 3245800: loss = 2.672957181930542\n",
      "step = 3246000: loss = 2.5139479637145996\n",
      "step = 3246200: loss = 2.07265305519104\n",
      "step = 3246400: loss = 4.306312084197998\n",
      "step = 3246600: loss = 2.860816478729248\n",
      "step = 3246800: loss = 4.271585941314697\n",
      "step = 3247000: loss = 2.3579862117767334\n",
      "step = 3247200: loss = 3.0013530254364014\n",
      "step = 3247400: loss = 2.7359750270843506\n",
      "step = 3247600: loss = 2.9995217323303223\n",
      "step = 3247800: loss = 2.7289161682128906\n",
      "step = 3248000: loss = 3.883453369140625\n",
      "step = 3248200: loss = 2.564760684967041\n",
      "step = 3248400: loss = 3.2090940475463867\n",
      "step = 3248600: loss = 2.2427055835723877\n",
      "step = 3248800: loss = 2.5665740966796875\n",
      "step = 3249000: loss = 3.8391599655151367\n",
      "step = 3249200: loss = 3.967454433441162\n",
      "step = 3249400: loss = 2.6955113410949707\n",
      "step = 3249600: loss = 3.6124751567840576\n",
      "step = 3249800: loss = 2.8694255352020264\n",
      "step = 3250000: loss = 2.775402784347534\n",
      "step = 3250000: Average Return = 6.550000190734863\n",
      "step = 3250200: loss = 3.713141918182373\n",
      "step = 3250400: loss = 4.343935966491699\n",
      "step = 3250600: loss = 3.069546699523926\n",
      "step = 3250800: loss = 3.6498985290527344\n",
      "step = 3251000: loss = 1.9532607793807983\n",
      "step = 3251200: loss = 3.106887102127075\n",
      "step = 3251400: loss = 4.048985958099365\n",
      "step = 3251600: loss = 5.136941909790039\n",
      "step = 3251800: loss = 4.413044452667236\n",
      "step = 3252000: loss = 3.44449520111084\n",
      "step = 3252200: loss = 3.47421932220459\n",
      "step = 3252400: loss = 3.7956695556640625\n",
      "step = 3252600: loss = 2.0853734016418457\n",
      "step = 3252800: loss = 2.7611052989959717\n",
      "step = 3253000: loss = 2.6044037342071533\n",
      "step = 3253200: loss = 3.1064605712890625\n",
      "step = 3253400: loss = 5.469273567199707\n",
      "step = 3253600: loss = 3.5048983097076416\n",
      "step = 3253800: loss = 2.2566847801208496\n",
      "step = 3254000: loss = 3.4090685844421387\n",
      "step = 3254200: loss = 3.275649309158325\n",
      "step = 3254400: loss = 2.723273277282715\n",
      "step = 3254600: loss = 2.7116339206695557\n",
      "step = 3254800: loss = 3.2667253017425537\n",
      "step = 3255000: loss = 4.2360005378723145\n",
      "step = 3255000: Average Return = 5.550000190734863\n",
      "step = 3255200: loss = 2.5342776775360107\n",
      "step = 3255400: loss = 3.516669750213623\n",
      "step = 3255600: loss = 3.529595375061035\n",
      "step = 3255800: loss = 4.980050563812256\n",
      "step = 3256000: loss = 3.5972986221313477\n",
      "step = 3256200: loss = 5.1597394943237305\n",
      "step = 3256400: loss = 4.092617511749268\n",
      "step = 3256600: loss = 2.86818790435791\n",
      "step = 3256800: loss = 4.890052318572998\n",
      "step = 3257000: loss = 3.79496169090271\n",
      "step = 3257200: loss = 2.762219190597534\n",
      "step = 3257400: loss = 3.1731395721435547\n",
      "step = 3257600: loss = 3.715883731842041\n",
      "step = 3257800: loss = 3.0680906772613525\n",
      "step = 3258000: loss = 3.980078935623169\n",
      "step = 3258200: loss = 2.3495333194732666\n",
      "step = 3258400: loss = 2.797532558441162\n",
      "step = 3258600: loss = 2.5362701416015625\n",
      "step = 3258800: loss = 2.985297679901123\n",
      "step = 3259000: loss = 3.7121496200561523\n",
      "step = 3259200: loss = 2.928995370864868\n",
      "step = 3259400: loss = 4.767399787902832\n",
      "step = 3259600: loss = 4.0571513175964355\n",
      "step = 3259800: loss = 2.9671411514282227\n",
      "step = 3260000: loss = 3.536963939666748\n",
      "step = 3260000: Average Return = 3.25\n",
      "step = 3260200: loss = 4.504929065704346\n",
      "step = 3260400: loss = 2.2529938220977783\n",
      "step = 3260600: loss = 4.4584832191467285\n",
      "step = 3260800: loss = 3.677804470062256\n",
      "step = 3261000: loss = 3.4505443572998047\n",
      "step = 3261200: loss = 3.0774354934692383\n",
      "step = 3261400: loss = 3.579357862472534\n",
      "step = 3261600: loss = 4.538191318511963\n",
      "step = 3261800: loss = 4.668342113494873\n",
      "step = 3262000: loss = 2.2885003089904785\n",
      "step = 3262200: loss = 2.8799235820770264\n",
      "step = 3262400: loss = 3.4862353801727295\n",
      "step = 3262600: loss = 2.968018054962158\n",
      "step = 3262800: loss = 4.3445916175842285\n",
      "step = 3263000: loss = 1.9624544382095337\n",
      "step = 3263200: loss = 3.2549870014190674\n",
      "step = 3263400: loss = 3.423762083053589\n",
      "step = 3263600: loss = 2.0057296752929688\n",
      "step = 3263800: loss = 4.355422496795654\n",
      "step = 3264000: loss = 3.2159764766693115\n",
      "step = 3264200: loss = 3.323368549346924\n",
      "step = 3264400: loss = 3.5755178928375244\n",
      "step = 3264600: loss = 2.1493141651153564\n",
      "step = 3264800: loss = 5.259842395782471\n",
      "step = 3265000: loss = 2.219024419784546\n",
      "step = 3265000: Average Return = 4.199999809265137\n",
      "step = 3265200: loss = 3.48515248298645\n",
      "step = 3265400: loss = 2.5448367595672607\n",
      "step = 3265600: loss = 3.3129239082336426\n",
      "step = 3265800: loss = 2.4359514713287354\n",
      "step = 3266000: loss = 3.263498544692993\n",
      "step = 3266200: loss = 2.8741002082824707\n",
      "step = 3266400: loss = 3.9204702377319336\n",
      "step = 3266600: loss = 2.6789822578430176\n",
      "step = 3266800: loss = 2.3986949920654297\n",
      "step = 3267000: loss = 3.083552122116089\n",
      "step = 3267200: loss = 2.3832919597625732\n",
      "step = 3267400: loss = 3.4166417121887207\n",
      "step = 3267600: loss = 2.8257861137390137\n",
      "step = 3267800: loss = 4.364803791046143\n",
      "step = 3268000: loss = 4.179811954498291\n",
      "step = 3268200: loss = 3.5440661907196045\n",
      "step = 3268400: loss = 3.7101047039031982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3268600: loss = 3.3776228427886963\n",
      "step = 3268800: loss = 3.007855176925659\n",
      "step = 3269000: loss = 3.6635169982910156\n",
      "step = 3269200: loss = 2.7121074199676514\n",
      "step = 3269400: loss = 2.991455316543579\n",
      "step = 3269600: loss = 2.8521058559417725\n",
      "step = 3269800: loss = 2.2301063537597656\n",
      "step = 3270000: loss = 2.5153393745422363\n",
      "step = 3270000: Average Return = 3.6500000953674316\n",
      "step = 3270200: loss = 3.6294188499450684\n",
      "step = 3270400: loss = 3.0456490516662598\n",
      "step = 3270600: loss = 3.005620002746582\n",
      "step = 3270800: loss = 3.343261241912842\n",
      "step = 3271000: loss = 4.619717121124268\n",
      "step = 3271200: loss = 4.069950103759766\n",
      "step = 3271400: loss = 4.117256164550781\n",
      "step = 3271600: loss = 3.3760251998901367\n",
      "step = 3271800: loss = 3.422974109649658\n",
      "step = 3272000: loss = 3.0275142192840576\n",
      "step = 3272200: loss = 5.160956859588623\n",
      "step = 3272400: loss = 3.485809326171875\n",
      "step = 3272600: loss = 3.746927499771118\n",
      "step = 3272800: loss = 2.3593149185180664\n",
      "step = 3273000: loss = 2.84379506111145\n",
      "step = 3273200: loss = 2.7363228797912598\n",
      "step = 3273400: loss = 2.397655487060547\n",
      "step = 3273600: loss = 4.055954933166504\n",
      "step = 3273800: loss = 3.4930779933929443\n",
      "step = 3274000: loss = 4.015021324157715\n",
      "step = 3274200: loss = 3.4088706970214844\n",
      "step = 3274400: loss = 3.483677387237549\n",
      "step = 3274600: loss = 3.855518102645874\n",
      "step = 3274800: loss = 4.054834365844727\n",
      "step = 3275000: loss = 4.117211818695068\n",
      "step = 3275000: Average Return = 4.900000095367432\n",
      "step = 3275200: loss = 2.773008346557617\n",
      "step = 3275400: loss = 3.4648022651672363\n",
      "step = 3275600: loss = 3.2565104961395264\n",
      "step = 3275800: loss = 3.4918696880340576\n",
      "step = 3276000: loss = 3.6791670322418213\n",
      "step = 3276200: loss = 3.0685698986053467\n",
      "step = 3276400: loss = 3.346813678741455\n",
      "step = 3276600: loss = 3.3225646018981934\n",
      "step = 3276800: loss = 2.905271530151367\n",
      "step = 3277000: loss = 2.5718438625335693\n",
      "step = 3277200: loss = 3.0145084857940674\n",
      "step = 3277400: loss = 4.562317848205566\n",
      "step = 3277600: loss = 5.391451358795166\n",
      "step = 3277800: loss = 2.734290361404419\n",
      "step = 3278000: loss = 4.031720161437988\n",
      "step = 3278200: loss = 2.868927240371704\n",
      "step = 3278400: loss = 1.9601083993911743\n",
      "step = 3278600: loss = 2.96352481842041\n",
      "step = 3278800: loss = 3.703054666519165\n",
      "step = 3279000: loss = 4.677978992462158\n",
      "step = 3279200: loss = 2.328056812286377\n",
      "step = 3279400: loss = 3.136930227279663\n",
      "step = 3279600: loss = 3.039118528366089\n",
      "step = 3279800: loss = 3.492847204208374\n",
      "step = 3280000: loss = 4.130672454833984\n",
      "step = 3280000: Average Return = 4.400000095367432\n",
      "step = 3280200: loss = 3.3192412853240967\n",
      "step = 3280400: loss = 4.136795520782471\n",
      "step = 3280600: loss = 5.72722864151001\n",
      "step = 3280800: loss = 2.7807605266571045\n",
      "step = 3281000: loss = 2.8107073307037354\n",
      "step = 3281200: loss = 3.7119364738464355\n",
      "step = 3281400: loss = 3.8479857444763184\n",
      "step = 3281600: loss = 2.534468412399292\n",
      "step = 3281800: loss = 4.323352813720703\n",
      "step = 3282000: loss = 3.954402446746826\n",
      "step = 3282200: loss = 2.6735785007476807\n",
      "step = 3282400: loss = 3.613719940185547\n",
      "step = 3282600: loss = 3.5371623039245605\n",
      "step = 3282800: loss = 3.770334005355835\n",
      "step = 3283000: loss = 2.8133602142333984\n",
      "step = 3283200: loss = 3.827237844467163\n",
      "step = 3283400: loss = 3.5457253456115723\n",
      "step = 3283600: loss = 2.9188485145568848\n",
      "step = 3283800: loss = 2.6997668743133545\n",
      "step = 3284000: loss = 2.6204543113708496\n",
      "step = 3284200: loss = 3.2577502727508545\n",
      "step = 3284400: loss = 3.234832763671875\n",
      "step = 3284600: loss = 4.081883907318115\n",
      "step = 3284800: loss = 3.051558256149292\n",
      "step = 3285000: loss = 2.9129977226257324\n",
      "step = 3285000: Average Return = 4.0\n",
      "step = 3285200: loss = 3.9156389236450195\n",
      "step = 3285400: loss = 3.0857386589050293\n",
      "step = 3285600: loss = 4.4874491691589355\n",
      "step = 3285800: loss = 3.241457939147949\n",
      "step = 3286000: loss = 4.602806568145752\n",
      "step = 3286200: loss = 2.5887365341186523\n",
      "step = 3286400: loss = 2.6510252952575684\n",
      "step = 3286600: loss = 3.495828151702881\n",
      "step = 3286800: loss = 2.8297393321990967\n",
      "step = 3287000: loss = 3.220900297164917\n",
      "step = 3287200: loss = 4.009354591369629\n",
      "step = 3287400: loss = 2.59849214553833\n",
      "step = 3287600: loss = 2.2780842781066895\n",
      "step = 3287800: loss = 2.55859637260437\n",
      "step = 3288000: loss = 3.83925199508667\n",
      "step = 3288200: loss = 3.317574977874756\n",
      "step = 3288400: loss = 4.309762954711914\n",
      "step = 3288600: loss = 3.6307144165039062\n",
      "step = 3288800: loss = 3.891486644744873\n",
      "step = 3289000: loss = 2.489872694015503\n",
      "step = 3289200: loss = 3.644285202026367\n",
      "step = 3289400: loss = 3.5826005935668945\n",
      "step = 3289600: loss = 3.2848706245422363\n",
      "step = 3289800: loss = 3.3034191131591797\n",
      "step = 3290000: loss = 2.6577510833740234\n",
      "step = 3290000: Average Return = 4.099999904632568\n",
      "step = 3290200: loss = 4.246567249298096\n",
      "step = 3290400: loss = 3.1421051025390625\n",
      "step = 3290600: loss = 3.929760694503784\n",
      "step = 3290800: loss = 2.3966667652130127\n",
      "step = 3291000: loss = 3.230217456817627\n",
      "step = 3291200: loss = 2.7815048694610596\n",
      "step = 3291400: loss = 3.1909902095794678\n",
      "step = 3291600: loss = 3.2583720684051514\n",
      "step = 3291800: loss = 3.5918383598327637\n",
      "step = 3292000: loss = 3.7169594764709473\n",
      "step = 3292200: loss = 2.647313117980957\n",
      "step = 3292400: loss = 3.0933611392974854\n",
      "step = 3292600: loss = 2.67476487159729\n",
      "step = 3292800: loss = 2.6980748176574707\n",
      "step = 3293000: loss = 2.537433624267578\n",
      "step = 3293200: loss = 4.786000728607178\n",
      "step = 3293400: loss = 2.7707414627075195\n",
      "step = 3293600: loss = 3.14713978767395\n",
      "step = 3293800: loss = 3.700087308883667\n",
      "step = 3294000: loss = 4.182860851287842\n",
      "step = 3294200: loss = 4.559919834136963\n",
      "step = 3294400: loss = 2.631108283996582\n",
      "step = 3294600: loss = 2.8703441619873047\n",
      "step = 3294800: loss = 4.126980304718018\n",
      "step = 3295000: loss = 3.198089599609375\n",
      "step = 3295000: Average Return = 4.300000190734863\n",
      "step = 3295200: loss = 3.9672911167144775\n",
      "step = 3295400: loss = 4.003884792327881\n",
      "step = 3295600: loss = 2.6978204250335693\n",
      "step = 3295800: loss = 3.3447282314300537\n",
      "step = 3296000: loss = 3.372843027114868\n",
      "step = 3296200: loss = 3.3639564514160156\n",
      "step = 3296400: loss = 2.61653995513916\n",
      "step = 3296600: loss = 3.863809108734131\n",
      "step = 3296800: loss = 3.350309133529663\n",
      "step = 3297000: loss = 3.935347557067871\n",
      "step = 3297200: loss = 3.2611052989959717\n",
      "step = 3297400: loss = 3.195024251937866\n",
      "step = 3297600: loss = 3.650472402572632\n",
      "step = 3297800: loss = 2.9309911727905273\n",
      "step = 3298000: loss = 3.6186676025390625\n",
      "step = 3298200: loss = 2.5531837940216064\n",
      "step = 3298400: loss = 2.0519063472747803\n",
      "step = 3298600: loss = 3.9754209518432617\n",
      "step = 3298800: loss = 5.2272257804870605\n",
      "step = 3299000: loss = 2.8540749549865723\n",
      "step = 3299200: loss = 3.011662483215332\n",
      "step = 3299400: loss = 2.739225387573242\n",
      "step = 3299600: loss = 3.178903102874756\n",
      "step = 3299800: loss = 5.049857139587402\n",
      "step = 3300000: loss = 3.4674110412597656\n",
      "step = 3300000: Average Return = 4.550000190734863\n",
      "step = 3300200: loss = 3.62080979347229\n",
      "step = 3300400: loss = 3.424638509750366\n",
      "step = 3300600: loss = 3.9933910369873047\n",
      "step = 3300800: loss = 3.1198599338531494\n",
      "step = 3301000: loss = 3.5803756713867188\n",
      "step = 3301200: loss = 3.6060078144073486\n",
      "step = 3301400: loss = 2.7535765171051025\n",
      "step = 3301600: loss = 2.370070695877075\n",
      "step = 3301800: loss = 3.6886608600616455\n",
      "step = 3302000: loss = 2.843961477279663\n",
      "step = 3302200: loss = 4.487793922424316\n",
      "step = 3302400: loss = 3.480865955352783\n",
      "step = 3302600: loss = 3.1892800331115723\n",
      "step = 3302800: loss = 5.345739841461182\n",
      "step = 3303000: loss = 3.2534570693969727\n",
      "step = 3303200: loss = 4.155810356140137\n",
      "step = 3303400: loss = 3.9124298095703125\n",
      "step = 3303600: loss = 2.441722869873047\n",
      "step = 3303800: loss = 4.766294002532959\n",
      "step = 3304000: loss = 4.860005855560303\n",
      "step = 3304200: loss = 2.9258828163146973\n",
      "step = 3304400: loss = 3.583883285522461\n",
      "step = 3304600: loss = 4.6369781494140625\n",
      "step = 3304800: loss = 3.615143299102783\n",
      "step = 3305000: loss = 3.4667162895202637\n",
      "step = 3305000: Average Return = 4.449999809265137\n",
      "step = 3305200: loss = 3.305967330932617\n",
      "step = 3305400: loss = 3.144047260284424\n",
      "step = 3305600: loss = 4.72059965133667\n",
      "step = 3305800: loss = 2.9611315727233887\n",
      "step = 3306000: loss = 3.7071940898895264\n",
      "step = 3306200: loss = 2.4395997524261475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3306400: loss = 2.568593740463257\n",
      "step = 3306600: loss = 3.2766642570495605\n",
      "step = 3306800: loss = 3.2660984992980957\n",
      "step = 3307000: loss = 4.416940689086914\n",
      "step = 3307200: loss = 2.3537023067474365\n",
      "step = 3307400: loss = 1.9392189979553223\n",
      "step = 3307600: loss = 3.04093861579895\n",
      "step = 3307800: loss = 4.041125774383545\n",
      "step = 3308000: loss = 3.803685426712036\n",
      "step = 3308200: loss = 3.001223087310791\n",
      "step = 3308400: loss = 3.945913076400757\n",
      "step = 3308600: loss = 2.6946229934692383\n",
      "step = 3308800: loss = 3.7394497394561768\n",
      "step = 3309000: loss = 3.8624870777130127\n",
      "step = 3309200: loss = 2.928513765335083\n",
      "step = 3309400: loss = 3.6554665565490723\n",
      "step = 3309600: loss = 2.603045701980591\n",
      "step = 3309800: loss = 2.858268976211548\n",
      "step = 3310000: loss = 2.663499116897583\n",
      "step = 3310000: Average Return = 5.0\n",
      "step = 3310200: loss = 3.2755510807037354\n",
      "step = 3310400: loss = 2.436558246612549\n",
      "step = 3310600: loss = 2.754950523376465\n",
      "step = 3310800: loss = 4.686131477355957\n",
      "step = 3311000: loss = 2.1717336177825928\n",
      "step = 3311200: loss = 3.2361934185028076\n",
      "step = 3311400: loss = 2.4953501224517822\n",
      "step = 3311600: loss = 3.219985246658325\n",
      "step = 3311800: loss = 3.975541353225708\n",
      "step = 3312000: loss = 3.1367015838623047\n",
      "step = 3312200: loss = 4.678707122802734\n",
      "step = 3312400: loss = 3.2106804847717285\n",
      "step = 3312600: loss = 4.677988052368164\n",
      "step = 3312800: loss = 1.784598469734192\n",
      "step = 3313000: loss = 3.353255033493042\n",
      "step = 3313200: loss = 2.374253988265991\n",
      "step = 3313400: loss = 3.1720573902130127\n",
      "step = 3313600: loss = 2.590552806854248\n",
      "step = 3313800: loss = 2.3981869220733643\n",
      "step = 3314000: loss = 3.7244343757629395\n",
      "step = 3314200: loss = 2.245344877243042\n",
      "step = 3314400: loss = 3.657939910888672\n",
      "step = 3314600: loss = 3.3297245502471924\n",
      "step = 3314800: loss = 2.2887725830078125\n",
      "step = 3315000: loss = 3.028690814971924\n",
      "step = 3315000: Average Return = 4.75\n",
      "step = 3315200: loss = 3.504528045654297\n",
      "step = 3315400: loss = 3.6090922355651855\n",
      "step = 3315600: loss = 2.88315486907959\n",
      "step = 3315800: loss = 2.6972768306732178\n",
      "step = 3316000: loss = 3.7470922470092773\n",
      "step = 3316200: loss = 2.543987274169922\n",
      "step = 3316400: loss = 3.0555989742279053\n",
      "step = 3316600: loss = 4.050507545471191\n",
      "step = 3316800: loss = 4.1667704582214355\n",
      "step = 3317000: loss = 4.28533935546875\n",
      "step = 3317200: loss = 4.199888229370117\n",
      "step = 3317400: loss = 4.141696453094482\n",
      "step = 3317600: loss = 2.784154176712036\n",
      "step = 3317800: loss = 2.818866491317749\n",
      "step = 3318000: loss = 3.4032468795776367\n",
      "step = 3318200: loss = 3.7320525646209717\n",
      "step = 3318400: loss = 4.173272609710693\n",
      "step = 3318600: loss = 3.6271920204162598\n",
      "step = 3318800: loss = 5.4681925773620605\n",
      "step = 3319000: loss = 4.685896873474121\n",
      "step = 3319200: loss = 3.626619815826416\n",
      "step = 3319400: loss = 3.445117712020874\n",
      "step = 3319600: loss = 5.528268814086914\n",
      "step = 3319800: loss = 3.1928045749664307\n",
      "step = 3320000: loss = 4.6085076332092285\n",
      "step = 3320000: Average Return = 3.0\n",
      "step = 3320200: loss = 3.943500280380249\n",
      "step = 3320400: loss = 3.8211772441864014\n",
      "step = 3320600: loss = 3.270897150039673\n",
      "step = 3320800: loss = 2.606297016143799\n",
      "step = 3321000: loss = 4.03491735458374\n",
      "step = 3321200: loss = 2.8603320121765137\n",
      "step = 3321400: loss = 3.600140333175659\n",
      "step = 3321600: loss = 4.7427473068237305\n",
      "step = 3321800: loss = 4.170177936553955\n",
      "step = 3322000: loss = 3.9253880977630615\n",
      "step = 3322200: loss = 2.1007919311523438\n",
      "step = 3322400: loss = 3.7850871086120605\n",
      "step = 3322600: loss = 3.410179376602173\n",
      "step = 3322800: loss = 2.5254626274108887\n",
      "step = 3323000: loss = 2.591493844985962\n",
      "step = 3323200: loss = 4.794985771179199\n",
      "step = 3323400: loss = 2.363538980484009\n",
      "step = 3323600: loss = 3.04986310005188\n",
      "step = 3323800: loss = 4.154003143310547\n",
      "step = 3324000: loss = 2.697341203689575\n",
      "step = 3324200: loss = 2.143218994140625\n",
      "step = 3324400: loss = 2.8162803649902344\n",
      "step = 3324600: loss = 2.954599618911743\n",
      "step = 3324800: loss = 2.659170150756836\n",
      "step = 3325000: loss = 3.540440797805786\n",
      "step = 3325000: Average Return = 7.050000190734863\n",
      "step = 3325200: loss = 4.259525299072266\n",
      "step = 3325400: loss = 3.614567756652832\n",
      "step = 3325600: loss = 3.472562551498413\n",
      "step = 3325800: loss = 5.211459636688232\n",
      "step = 3326000: loss = 3.86899471282959\n",
      "step = 3326200: loss = 3.047898530960083\n",
      "step = 3326400: loss = 4.287839412689209\n",
      "step = 3326600: loss = 2.5384862422943115\n",
      "step = 3326800: loss = 3.904595136642456\n",
      "step = 3327000: loss = 3.0334670543670654\n",
      "step = 3327200: loss = 4.320220470428467\n",
      "step = 3327400: loss = 2.7054402828216553\n",
      "step = 3327600: loss = 4.277900695800781\n",
      "step = 3327800: loss = 4.9673895835876465\n",
      "step = 3328000: loss = 2.50382924079895\n",
      "step = 3328200: loss = 3.170009136199951\n",
      "step = 3328400: loss = 4.3420305252075195\n",
      "step = 3328600: loss = 3.982501745223999\n",
      "step = 3328800: loss = 4.982890605926514\n",
      "step = 3329000: loss = 4.003909111022949\n",
      "step = 3329200: loss = 3.120184898376465\n",
      "step = 3329400: loss = 3.8262202739715576\n",
      "step = 3329600: loss = 3.862370491027832\n",
      "step = 3329800: loss = 4.956287860870361\n",
      "step = 3330000: loss = 3.3584911823272705\n",
      "step = 3330000: Average Return = 3.549999952316284\n",
      "step = 3330200: loss = 4.272669792175293\n",
      "step = 3330400: loss = 4.655304431915283\n",
      "step = 3330600: loss = 4.755566120147705\n",
      "step = 3330800: loss = 3.5999460220336914\n",
      "step = 3331000: loss = 3.9975762367248535\n",
      "step = 3331200: loss = 4.792300224304199\n",
      "step = 3331400: loss = 2.855145215988159\n",
      "step = 3331600: loss = 2.374936819076538\n",
      "step = 3331800: loss = 4.459009647369385\n",
      "step = 3332000: loss = 3.154262065887451\n",
      "step = 3332200: loss = 4.1343231201171875\n",
      "step = 3332400: loss = 2.1623668670654297\n",
      "step = 3332600: loss = 4.9145612716674805\n",
      "step = 3332800: loss = 2.6222543716430664\n",
      "step = 3333000: loss = 3.505753993988037\n",
      "step = 3333200: loss = 3.669487237930298\n",
      "step = 3333400: loss = 3.382544994354248\n",
      "step = 3333600: loss = 2.950762987136841\n",
      "step = 3333800: loss = 2.6493377685546875\n",
      "step = 3334000: loss = 4.407939434051514\n",
      "step = 3334200: loss = 4.149861812591553\n",
      "step = 3334400: loss = 2.197976589202881\n",
      "step = 3334600: loss = 3.4830987453460693\n",
      "step = 3334800: loss = 3.390066146850586\n",
      "step = 3335000: loss = 3.3167717456817627\n",
      "step = 3335000: Average Return = 5.5\n",
      "step = 3335200: loss = 3.623061418533325\n",
      "step = 3335400: loss = 4.26134729385376\n",
      "step = 3335600: loss = 4.30068302154541\n",
      "step = 3335800: loss = 2.8643972873687744\n",
      "step = 3336000: loss = 3.0658156871795654\n",
      "step = 3336200: loss = 3.1549572944641113\n",
      "step = 3336400: loss = 3.9705395698547363\n",
      "step = 3336600: loss = 2.965336799621582\n",
      "step = 3336800: loss = 2.308387041091919\n",
      "step = 3337000: loss = 4.960567951202393\n",
      "step = 3337200: loss = 3.296915292739868\n",
      "step = 3337400: loss = 3.246950626373291\n",
      "step = 3337600: loss = 4.221609592437744\n",
      "step = 3337800: loss = 3.9146981239318848\n",
      "step = 3338000: loss = 2.377933979034424\n",
      "step = 3338200: loss = 3.0232298374176025\n",
      "step = 3338400: loss = 3.86789608001709\n",
      "step = 3338600: loss = 3.1589324474334717\n",
      "step = 3338800: loss = 3.9229705333709717\n",
      "step = 3339000: loss = 4.30369758605957\n",
      "step = 3339200: loss = 4.78943395614624\n",
      "step = 3339400: loss = 2.6153318881988525\n",
      "step = 3339600: loss = 2.7824957370758057\n",
      "step = 3339800: loss = 3.626556396484375\n",
      "step = 3340000: loss = 3.915114164352417\n",
      "step = 3340000: Average Return = 5.650000095367432\n",
      "step = 3340200: loss = 3.959517240524292\n",
      "step = 3340400: loss = 3.5965664386749268\n",
      "step = 3340600: loss = 3.5081398487091064\n",
      "step = 3340800: loss = 3.4549450874328613\n",
      "step = 3341000: loss = 3.1946046352386475\n",
      "step = 3341200: loss = 3.3073441982269287\n",
      "step = 3341400: loss = 4.092236042022705\n",
      "step = 3341600: loss = 2.2613301277160645\n",
      "step = 3341800: loss = 5.187217712402344\n",
      "step = 3342000: loss = 3.890094518661499\n",
      "step = 3342200: loss = 4.133517742156982\n",
      "step = 3342400: loss = 4.869604110717773\n",
      "step = 3342600: loss = 3.3082971572875977\n",
      "step = 3342800: loss = 3.501715660095215\n",
      "step = 3343000: loss = 2.979551315307617\n",
      "step = 3343200: loss = 3.2108547687530518\n",
      "step = 3343400: loss = 4.513585567474365\n",
      "step = 3343600: loss = 3.3014848232269287\n",
      "step = 3343800: loss = 2.8396925926208496\n",
      "step = 3344000: loss = 4.137939929962158\n",
      "step = 3344200: loss = 4.478131294250488\n",
      "step = 3344400: loss = 3.239140510559082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3344600: loss = 4.017951965332031\n",
      "step = 3344800: loss = 3.7994158267974854\n",
      "step = 3345000: loss = 4.1921491622924805\n",
      "step = 3345000: Average Return = 2.700000047683716\n",
      "step = 3345200: loss = 2.9047672748565674\n",
      "step = 3345400: loss = 3.260714054107666\n",
      "step = 3345600: loss = 4.073824882507324\n",
      "step = 3345800: loss = 3.187847137451172\n",
      "step = 3346000: loss = 5.123801231384277\n",
      "step = 3346200: loss = 4.531785011291504\n",
      "step = 3346400: loss = 4.345210552215576\n",
      "step = 3346600: loss = 4.494755268096924\n",
      "step = 3346800: loss = 3.5486485958099365\n",
      "step = 3347000: loss = 4.892576694488525\n",
      "step = 3347200: loss = 4.611588478088379\n",
      "step = 3347400: loss = 3.8687379360198975\n",
      "step = 3347600: loss = 2.8864989280700684\n",
      "step = 3347800: loss = 3.883092164993286\n",
      "step = 3348000: loss = 3.3559093475341797\n",
      "step = 3348200: loss = 2.935800075531006\n",
      "step = 3348400: loss = 2.623154878616333\n",
      "step = 3348600: loss = 2.933520793914795\n",
      "step = 3348800: loss = 3.6234843730926514\n",
      "step = 3349000: loss = 3.067218065261841\n",
      "step = 3349200: loss = 3.553581476211548\n",
      "step = 3349400: loss = 4.036713600158691\n",
      "step = 3349600: loss = 3.172203302383423\n",
      "step = 3349800: loss = 3.7602195739746094\n",
      "step = 3350000: loss = 4.7678656578063965\n",
      "step = 3350000: Average Return = 5.949999809265137\n",
      "step = 3350200: loss = 3.2986388206481934\n",
      "step = 3350400: loss = 4.407126426696777\n",
      "step = 3350600: loss = 2.6125380992889404\n",
      "step = 3350800: loss = 4.048447132110596\n",
      "step = 3351000: loss = 3.8580758571624756\n",
      "step = 3351200: loss = 3.902264356613159\n",
      "step = 3351400: loss = 3.1915042400360107\n",
      "step = 3351600: loss = 3.556478261947632\n",
      "step = 3351800: loss = 4.077337741851807\n",
      "step = 3352000: loss = 4.992423057556152\n",
      "step = 3352200: loss = 4.016417980194092\n",
      "step = 3352400: loss = 3.476597309112549\n",
      "step = 3352600: loss = 3.0796568393707275\n",
      "step = 3352800: loss = 3.7649717330932617\n",
      "step = 3353000: loss = 3.499810218811035\n",
      "step = 3353200: loss = 4.520819664001465\n",
      "step = 3353400: loss = 3.224205255508423\n",
      "step = 3353600: loss = 3.6946587562561035\n",
      "step = 3353800: loss = 3.733549118041992\n",
      "step = 3354000: loss = 4.435559272766113\n",
      "step = 3354200: loss = 5.926788806915283\n",
      "step = 3354400: loss = 5.069096088409424\n",
      "step = 3354600: loss = 3.984992027282715\n",
      "step = 3354800: loss = 4.847349643707275\n",
      "step = 3355000: loss = 3.596508502960205\n",
      "step = 3355000: Average Return = 6.050000190734863\n",
      "step = 3355200: loss = 2.5312225818634033\n",
      "step = 3355400: loss = 6.612939834594727\n",
      "step = 3355600: loss = 3.102045774459839\n",
      "step = 3355800: loss = 4.238571643829346\n",
      "step = 3356000: loss = 3.355971574783325\n",
      "step = 3356200: loss = 4.523331642150879\n",
      "step = 3356400: loss = 3.732295036315918\n",
      "step = 3356600: loss = 2.2152602672576904\n",
      "step = 3356800: loss = 4.026827812194824\n",
      "step = 3357000: loss = 5.3510661125183105\n",
      "step = 3357200: loss = 3.006497621536255\n",
      "step = 3357400: loss = 4.212488651275635\n",
      "step = 3357600: loss = 3.7010908126831055\n",
      "step = 3357800: loss = 2.298815965652466\n",
      "step = 3358000: loss = 3.0424089431762695\n",
      "step = 3358200: loss = 2.3152456283569336\n",
      "step = 3358400: loss = 4.303142547607422\n",
      "step = 3358600: loss = 4.494235992431641\n",
      "step = 3358800: loss = 4.048498153686523\n",
      "step = 3359000: loss = 4.273129940032959\n",
      "step = 3359200: loss = 2.968194007873535\n",
      "step = 3359400: loss = 2.6588878631591797\n",
      "step = 3359600: loss = 3.241533041000366\n",
      "step = 3359800: loss = 4.09193229675293\n",
      "step = 3360000: loss = 3.4339964389801025\n",
      "step = 3360000: Average Return = 4.900000095367432\n",
      "step = 3360200: loss = 3.1792993545532227\n",
      "step = 3360400: loss = 4.176517009735107\n",
      "step = 3360600: loss = 4.698962211608887\n",
      "step = 3360800: loss = 4.491292953491211\n",
      "step = 3361000: loss = 3.839049816131592\n",
      "step = 3361200: loss = 2.362333059310913\n",
      "step = 3361400: loss = 3.4413859844207764\n",
      "step = 3361600: loss = 4.266602039337158\n",
      "step = 3361800: loss = 2.985868215560913\n",
      "step = 3362000: loss = 3.8793323040008545\n",
      "step = 3362200: loss = 3.404264450073242\n",
      "step = 3362400: loss = 3.2696588039398193\n",
      "step = 3362600: loss = 3.5386149883270264\n",
      "step = 3362800: loss = 3.9934093952178955\n",
      "step = 3363000: loss = 4.2358551025390625\n",
      "step = 3363200: loss = 3.494326114654541\n",
      "step = 3363400: loss = 2.0515637397766113\n",
      "step = 3363600: loss = 2.9198315143585205\n",
      "step = 3363800: loss = 2.3725297451019287\n",
      "step = 3364000: loss = 4.384674072265625\n",
      "step = 3364200: loss = 4.855890274047852\n",
      "step = 3364400: loss = 3.0142529010772705\n",
      "step = 3364600: loss = 2.838874340057373\n",
      "step = 3364800: loss = 2.488865852355957\n",
      "step = 3365000: loss = 4.197964191436768\n",
      "step = 3365000: Average Return = 3.1500000953674316\n",
      "step = 3365200: loss = 4.816129207611084\n",
      "step = 3365400: loss = 3.071622610092163\n",
      "step = 3365600: loss = 3.2272660732269287\n",
      "step = 3365800: loss = 2.8527073860168457\n",
      "step = 3366000: loss = 2.9067447185516357\n",
      "step = 3366200: loss = 3.174086570739746\n",
      "step = 3366400: loss = 3.831425189971924\n",
      "step = 3366600: loss = 2.8882458209991455\n",
      "step = 3366800: loss = 3.365375280380249\n",
      "step = 3367000: loss = 3.3597288131713867\n",
      "step = 3367200: loss = 4.59660530090332\n",
      "step = 3367400: loss = 4.010029315948486\n",
      "step = 3367600: loss = 3.9947893619537354\n",
      "step = 3367800: loss = 3.1752166748046875\n",
      "step = 3368000: loss = 3.616175413131714\n",
      "step = 3368200: loss = 3.692408323287964\n",
      "step = 3368400: loss = 3.7813994884490967\n",
      "step = 3368600: loss = 5.020105838775635\n",
      "step = 3368800: loss = 3.7959067821502686\n",
      "step = 3369000: loss = 2.875018358230591\n",
      "step = 3369200: loss = 3.0571653842926025\n",
      "step = 3369400: loss = 4.748040676116943\n",
      "step = 3369600: loss = 4.299383640289307\n",
      "step = 3369800: loss = 2.763437509536743\n",
      "step = 3370000: loss = 3.57741641998291\n",
      "step = 3370000: Average Return = 2.8499999046325684\n",
      "step = 3370200: loss = 3.6003780364990234\n",
      "step = 3370400: loss = 3.9942593574523926\n",
      "step = 3370600: loss = 4.369180202484131\n",
      "step = 3370800: loss = 3.7845187187194824\n",
      "step = 3371000: loss = 4.861280918121338\n",
      "step = 3371200: loss = 3.652742862701416\n",
      "step = 3371400: loss = 2.1153433322906494\n",
      "step = 3371600: loss = 2.9362926483154297\n",
      "step = 3371800: loss = 2.4923343658447266\n",
      "step = 3372000: loss = 4.341011047363281\n",
      "step = 3372200: loss = 2.3487651348114014\n",
      "step = 3372400: loss = 2.7843170166015625\n",
      "step = 3372600: loss = 4.856662273406982\n",
      "step = 3372800: loss = 4.031548500061035\n",
      "step = 3373000: loss = 3.215902090072632\n",
      "step = 3373200: loss = 4.1096343994140625\n",
      "step = 3373400: loss = 2.9662487506866455\n",
      "step = 3373600: loss = 3.891307830810547\n",
      "step = 3373800: loss = 3.473759174346924\n",
      "step = 3374000: loss = 5.090906143188477\n",
      "step = 3374200: loss = 2.608755111694336\n",
      "step = 3374400: loss = 4.848602771759033\n",
      "step = 3374600: loss = 3.276583671569824\n",
      "step = 3374800: loss = 3.2900328636169434\n",
      "step = 3375000: loss = 4.58203649520874\n",
      "step = 3375000: Average Return = 2.799999952316284\n",
      "step = 3375200: loss = 3.0662004947662354\n",
      "step = 3375400: loss = 3.8153083324432373\n",
      "step = 3375600: loss = 3.305353879928589\n",
      "step = 3375800: loss = 4.734864711761475\n",
      "step = 3376000: loss = 4.016838073730469\n",
      "step = 3376200: loss = 4.378815650939941\n",
      "step = 3376400: loss = 3.82675838470459\n",
      "step = 3376600: loss = 3.823707103729248\n",
      "step = 3376800: loss = 4.615965843200684\n",
      "step = 3377000: loss = 3.390902519226074\n",
      "step = 3377200: loss = 3.4443514347076416\n",
      "step = 3377400: loss = 6.1805243492126465\n",
      "step = 3377600: loss = 3.8552446365356445\n",
      "step = 3377800: loss = 2.902771472930908\n",
      "step = 3378000: loss = 6.188238620758057\n",
      "step = 3378200: loss = 3.4098973274230957\n",
      "step = 3378400: loss = 3.3851804733276367\n",
      "step = 3378600: loss = 4.929107666015625\n",
      "step = 3378800: loss = 2.5171029567718506\n",
      "step = 3379000: loss = 4.8621439933776855\n",
      "step = 3379200: loss = 2.702979564666748\n",
      "step = 3379400: loss = 4.70609188079834\n",
      "step = 3379600: loss = 3.9188756942749023\n",
      "step = 3379800: loss = 4.037765026092529\n",
      "step = 3380000: loss = 4.460701942443848\n",
      "step = 3380000: Average Return = 4.199999809265137\n",
      "step = 3380200: loss = 4.592920303344727\n",
      "step = 3380400: loss = 4.087644100189209\n",
      "step = 3380600: loss = 3.8388264179229736\n",
      "step = 3380800: loss = 3.9562838077545166\n",
      "step = 3381000: loss = 3.423434257507324\n",
      "step = 3381200: loss = 3.56227970123291\n",
      "step = 3381400: loss = 3.5063836574554443\n",
      "step = 3381600: loss = 4.716941833496094\n",
      "step = 3381800: loss = 3.6783461570739746\n",
      "step = 3382000: loss = 2.928493022918701\n",
      "step = 3382200: loss = 2.6581838130950928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3382400: loss = 3.9414873123168945\n",
      "step = 3382600: loss = 4.241152763366699\n",
      "step = 3382800: loss = 6.14728307723999\n",
      "step = 3383000: loss = 2.8730950355529785\n",
      "step = 3383200: loss = 3.0224626064300537\n",
      "step = 3383400: loss = 4.294897079467773\n",
      "step = 3383600: loss = 2.9893953800201416\n",
      "step = 3383800: loss = 3.7662694454193115\n",
      "step = 3384000: loss = 3.6611523628234863\n",
      "step = 3384200: loss = 3.5449914932250977\n",
      "step = 3384400: loss = 2.744903564453125\n",
      "step = 3384600: loss = 3.1041221618652344\n",
      "step = 3384800: loss = 2.986873149871826\n",
      "step = 3385000: loss = 3.087012767791748\n",
      "step = 3385000: Average Return = 2.799999952316284\n",
      "step = 3385200: loss = 4.210818767547607\n",
      "step = 3385400: loss = 3.1291213035583496\n",
      "step = 3385600: loss = 4.339074611663818\n",
      "step = 3385800: loss = 4.530116081237793\n",
      "step = 3386000: loss = 3.67010760307312\n",
      "step = 3386200: loss = 4.008792400360107\n",
      "step = 3386400: loss = 3.432772397994995\n",
      "step = 3386600: loss = 3.1097655296325684\n",
      "step = 3386800: loss = 4.124011516571045\n",
      "step = 3387000: loss = 3.9071433544158936\n",
      "step = 3387200: loss = 3.838192939758301\n",
      "step = 3387400: loss = 3.281151056289673\n",
      "step = 3387600: loss = 4.314485549926758\n",
      "step = 3387800: loss = 3.467571973800659\n",
      "step = 3388000: loss = 3.099884510040283\n",
      "step = 3388200: loss = 2.8359029293060303\n",
      "step = 3388400: loss = 3.8540093898773193\n",
      "step = 3388600: loss = 4.4737677574157715\n",
      "step = 3388800: loss = 3.8323781490325928\n",
      "step = 3389000: loss = 2.4881324768066406\n",
      "step = 3389200: loss = 3.622208595275879\n",
      "step = 3389400: loss = 4.96195125579834\n",
      "step = 3389600: loss = 4.457923412322998\n",
      "step = 3389800: loss = 3.7448418140411377\n",
      "step = 3390000: loss = 4.366996765136719\n",
      "step = 3390000: Average Return = 3.799999952316284\n",
      "step = 3390200: loss = 4.290675163269043\n",
      "step = 3390400: loss = 4.17781925201416\n",
      "step = 3390600: loss = 3.4331839084625244\n",
      "step = 3390800: loss = 3.609696626663208\n",
      "step = 3391000: loss = 4.833590984344482\n",
      "step = 3391200: loss = 4.612081050872803\n",
      "step = 3391400: loss = 4.160680294036865\n",
      "step = 3391600: loss = 3.8361244201660156\n",
      "step = 3391800: loss = 4.154490947723389\n",
      "step = 3392000: loss = 4.188563823699951\n",
      "step = 3392200: loss = 2.838165283203125\n",
      "step = 3392400: loss = 4.542392253875732\n",
      "step = 3392600: loss = 4.320659160614014\n",
      "step = 3392800: loss = 4.0204758644104\n",
      "step = 3393000: loss = 3.3118443489074707\n",
      "step = 3393200: loss = 3.8610007762908936\n",
      "step = 3393400: loss = 4.160030364990234\n",
      "step = 3393600: loss = 4.08595609664917\n",
      "step = 3393800: loss = 3.432307720184326\n",
      "step = 3394000: loss = 4.175628662109375\n",
      "step = 3394200: loss = 3.905294179916382\n",
      "step = 3394400: loss = 3.68792986869812\n",
      "step = 3394600: loss = 2.9939918518066406\n",
      "step = 3394800: loss = 4.144852638244629\n",
      "step = 3395000: loss = 3.9827308654785156\n",
      "step = 3395000: Average Return = 2.700000047683716\n",
      "step = 3395200: loss = 3.277399778366089\n",
      "step = 3395400: loss = 3.949841260910034\n",
      "step = 3395600: loss = 3.9676742553710938\n",
      "step = 3395800: loss = 3.4198596477508545\n",
      "step = 3396000: loss = 3.926368236541748\n",
      "step = 3396200: loss = 3.3828938007354736\n",
      "step = 3396400: loss = 3.7148451805114746\n",
      "step = 3396600: loss = 4.062103271484375\n",
      "step = 3396800: loss = 3.411292552947998\n",
      "step = 3397000: loss = 5.968005657196045\n",
      "step = 3397200: loss = 4.4579668045043945\n",
      "step = 3397400: loss = 4.808821201324463\n",
      "step = 3397600: loss = 2.8875627517700195\n",
      "step = 3397800: loss = 5.058882236480713\n",
      "step = 3398000: loss = 4.308685302734375\n",
      "step = 3398200: loss = 2.8087432384490967\n",
      "step = 3398400: loss = 3.7348203659057617\n",
      "step = 3398600: loss = 2.8888702392578125\n",
      "step = 3398800: loss = 3.647855758666992\n",
      "step = 3399000: loss = 2.768993377685547\n",
      "step = 3399200: loss = 3.0419883728027344\n",
      "step = 3399400: loss = 2.5361082553863525\n",
      "step = 3399600: loss = 3.3217594623565674\n",
      "step = 3399800: loss = 4.508610248565674\n",
      "step = 3400000: loss = 3.375218391418457\n",
      "step = 3400000: Average Return = 5.5\n",
      "step = 3400200: loss = 5.822643280029297\n",
      "step = 3400400: loss = 3.2891008853912354\n",
      "step = 3400600: loss = 3.089395761489868\n",
      "step = 3400800: loss = 3.4865918159484863\n",
      "step = 3401000: loss = 2.4988350868225098\n",
      "step = 3401200: loss = 4.252471923828125\n",
      "step = 3401400: loss = 3.184868812561035\n",
      "step = 3401600: loss = 3.526923656463623\n",
      "step = 3401800: loss = 4.866185665130615\n",
      "step = 3402000: loss = 4.112564563751221\n",
      "step = 3402200: loss = 2.5184292793273926\n",
      "step = 3402400: loss = 5.173616886138916\n",
      "step = 3402600: loss = 3.802666187286377\n",
      "step = 3402800: loss = 3.737323522567749\n",
      "step = 3403000: loss = 2.069363594055176\n",
      "step = 3403200: loss = 4.330711364746094\n",
      "step = 3403400: loss = 4.308384895324707\n",
      "step = 3403600: loss = 3.527350902557373\n",
      "step = 3403800: loss = 3.916470766067505\n",
      "step = 3404000: loss = 4.332983493804932\n",
      "step = 3404200: loss = 3.7633605003356934\n",
      "step = 3404400: loss = 4.3100786209106445\n",
      "step = 3404600: loss = 3.4917430877685547\n",
      "step = 3404800: loss = 4.201873779296875\n",
      "step = 3405000: loss = 3.264491558074951\n",
      "step = 3405000: Average Return = 4.599999904632568\n",
      "step = 3405200: loss = 4.737380027770996\n",
      "step = 3405400: loss = 3.5400097370147705\n",
      "step = 3405600: loss = 3.918187141418457\n",
      "step = 3405800: loss = 4.596882343292236\n",
      "step = 3406000: loss = 2.938615560531616\n",
      "step = 3406200: loss = 3.2348992824554443\n",
      "step = 3406400: loss = 4.734958171844482\n",
      "step = 3406600: loss = 3.3692309856414795\n",
      "step = 3406800: loss = 2.0789499282836914\n",
      "step = 3407000: loss = 4.758157253265381\n",
      "step = 3407200: loss = 3.503711462020874\n",
      "step = 3407400: loss = 4.315471649169922\n",
      "step = 3407600: loss = 4.508816242218018\n",
      "step = 3407800: loss = 3.280622720718384\n",
      "step = 3408000: loss = 4.142080783843994\n",
      "step = 3408200: loss = 3.291882276535034\n",
      "step = 3408400: loss = 4.662942409515381\n",
      "step = 3408600: loss = 4.584563732147217\n",
      "step = 3408800: loss = 3.8634450435638428\n",
      "step = 3409000: loss = 4.312100887298584\n",
      "step = 3409200: loss = 3.311624050140381\n",
      "step = 3409400: loss = 3.4212961196899414\n",
      "step = 3409600: loss = 3.549006223678589\n",
      "step = 3409800: loss = 2.9298064708709717\n",
      "step = 3410000: loss = 3.452584743499756\n",
      "step = 3410000: Average Return = 2.450000047683716\n",
      "step = 3410200: loss = 2.9826037883758545\n",
      "step = 3410400: loss = 4.708548069000244\n",
      "step = 3410600: loss = 2.6732442378997803\n",
      "step = 3410800: loss = 4.449335098266602\n",
      "step = 3411000: loss = 3.9938855171203613\n",
      "step = 3411200: loss = 3.593752384185791\n",
      "step = 3411400: loss = 3.493239402770996\n",
      "step = 3411600: loss = 4.616176128387451\n",
      "step = 3411800: loss = 4.469882011413574\n",
      "step = 3412000: loss = 4.216992378234863\n",
      "step = 3412200: loss = 3.257683753967285\n",
      "step = 3412400: loss = 3.5674962997436523\n",
      "step = 3412600: loss = 3.330768346786499\n",
      "step = 3412800: loss = 4.138522148132324\n",
      "step = 3413000: loss = 3.4119389057159424\n",
      "step = 3413200: loss = 3.7159903049468994\n",
      "step = 3413400: loss = 2.5941381454467773\n",
      "step = 3413600: loss = 3.5100297927856445\n",
      "step = 3413800: loss = 4.021642208099365\n",
      "step = 3414000: loss = 2.7434563636779785\n",
      "step = 3414200: loss = 3.370089054107666\n",
      "step = 3414400: loss = 3.1059837341308594\n",
      "step = 3414600: loss = 3.185666084289551\n",
      "step = 3414800: loss = 2.750821590423584\n",
      "step = 3415000: loss = 2.581040620803833\n",
      "step = 3415000: Average Return = 5.25\n",
      "step = 3415200: loss = 3.560687780380249\n",
      "step = 3415400: loss = 4.288146495819092\n",
      "step = 3415600: loss = 3.7531914710998535\n",
      "step = 3415800: loss = 3.065981149673462\n",
      "step = 3416000: loss = 2.572615385055542\n",
      "step = 3416200: loss = 2.779633045196533\n",
      "step = 3416400: loss = 3.0534026622772217\n",
      "step = 3416600: loss = 4.023649215698242\n",
      "step = 3416800: loss = 4.140490531921387\n",
      "step = 3417000: loss = 4.78825569152832\n",
      "step = 3417200: loss = 4.341702938079834\n",
      "step = 3417400: loss = 5.201442241668701\n",
      "step = 3417600: loss = 4.770771503448486\n",
      "step = 3417800: loss = 4.468645095825195\n",
      "step = 3418000: loss = 3.075254440307617\n",
      "step = 3418200: loss = 3.24822735786438\n",
      "step = 3418400: loss = 3.2244701385498047\n",
      "step = 3418600: loss = 2.835995674133301\n",
      "step = 3418800: loss = 2.959782600402832\n",
      "step = 3419000: loss = 4.272632122039795\n",
      "step = 3419200: loss = 3.205805540084839\n",
      "step = 3419400: loss = 5.02922248840332\n",
      "step = 3419600: loss = 2.6110973358154297\n",
      "step = 3419800: loss = 3.914492130279541\n",
      "step = 3420000: loss = 2.59647274017334\n",
      "step = 3420000: Average Return = 4.75\n",
      "step = 3420200: loss = 3.080268383026123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3420400: loss = 4.890122413635254\n",
      "step = 3420600: loss = 2.973360538482666\n",
      "step = 3420800: loss = 4.449543476104736\n",
      "step = 3421000: loss = 3.569755792617798\n",
      "step = 3421200: loss = 6.02864933013916\n",
      "step = 3421400: loss = 6.622134208679199\n",
      "step = 3421600: loss = 3.1400134563446045\n",
      "step = 3421800: loss = 2.234994649887085\n",
      "step = 3422000: loss = 3.5223281383514404\n",
      "step = 3422200: loss = 3.2545957565307617\n",
      "step = 3422400: loss = 2.6648101806640625\n",
      "step = 3422600: loss = 3.1530187129974365\n",
      "step = 3422800: loss = 3.2552928924560547\n",
      "step = 3423000: loss = 3.7408947944641113\n",
      "step = 3423200: loss = 4.426473140716553\n",
      "step = 3423400: loss = 4.50949764251709\n",
      "step = 3423600: loss = 4.290276527404785\n",
      "step = 3423800: loss = 3.240504026412964\n",
      "step = 3424000: loss = 3.5126593112945557\n",
      "step = 3424200: loss = 3.4335265159606934\n",
      "step = 3424400: loss = 3.7903404235839844\n",
      "step = 3424600: loss = 4.871089458465576\n",
      "step = 3424800: loss = 3.1852073669433594\n",
      "step = 3425000: loss = 2.722904682159424\n",
      "step = 3425000: Average Return = 6.099999904632568\n",
      "step = 3425200: loss = 3.442882537841797\n",
      "step = 3425400: loss = 2.6684253215789795\n",
      "step = 3425600: loss = 3.456589937210083\n",
      "step = 3425800: loss = 4.060242652893066\n",
      "step = 3426000: loss = 3.242159843444824\n",
      "step = 3426200: loss = 2.496044397354126\n",
      "step = 3426400: loss = 3.8205831050872803\n",
      "step = 3426600: loss = 3.477879047393799\n",
      "step = 3426800: loss = 3.3136966228485107\n",
      "step = 3427000: loss = 4.0068817138671875\n",
      "step = 3427200: loss = 3.413323402404785\n",
      "step = 3427400: loss = 2.8963394165039062\n",
      "step = 3427600: loss = 2.646374464035034\n",
      "step = 3427800: loss = 4.36091947555542\n",
      "step = 3428000: loss = 2.5787570476531982\n",
      "step = 3428200: loss = 2.9667043685913086\n",
      "step = 3428400: loss = 3.6417970657348633\n",
      "step = 3428600: loss = 3.6685807704925537\n",
      "step = 3428800: loss = 3.30784273147583\n",
      "step = 3429000: loss = 2.3957831859588623\n",
      "step = 3429200: loss = 2.882960796356201\n",
      "step = 3429400: loss = 5.130649089813232\n",
      "step = 3429600: loss = 4.773540496826172\n",
      "step = 3429800: loss = 2.456758737564087\n",
      "step = 3430000: loss = 3.321775436401367\n",
      "step = 3430000: Average Return = 3.3499999046325684\n",
      "step = 3430200: loss = 3.592230796813965\n",
      "step = 3430400: loss = 4.892970561981201\n",
      "step = 3430600: loss = 3.2205827236175537\n",
      "step = 3430800: loss = 3.338405132293701\n",
      "step = 3431000: loss = 3.4871110916137695\n",
      "step = 3431200: loss = 2.3140869140625\n",
      "step = 3431400: loss = 2.9056875705718994\n",
      "step = 3431600: loss = 5.267000675201416\n",
      "step = 3431800: loss = 3.809788227081299\n",
      "step = 3432000: loss = 4.124570846557617\n",
      "step = 3432200: loss = 3.5541584491729736\n",
      "step = 3432400: loss = 3.075162172317505\n",
      "step = 3432600: loss = 5.453226089477539\n",
      "step = 3432800: loss = 2.622556447982788\n",
      "step = 3433000: loss = 3.703972101211548\n",
      "step = 3433200: loss = 4.102386474609375\n",
      "step = 3433400: loss = 5.122326374053955\n",
      "step = 3433600: loss = 3.672004222869873\n",
      "step = 3433800: loss = 3.3195927143096924\n",
      "step = 3434000: loss = 2.164297103881836\n",
      "step = 3434200: loss = 2.577831983566284\n",
      "step = 3434400: loss = 4.339567184448242\n",
      "step = 3434600: loss = 4.769363880157471\n",
      "step = 3434800: loss = 3.0623104572296143\n",
      "step = 3435000: loss = 5.591464519500732\n",
      "step = 3435000: Average Return = 4.599999904632568\n",
      "step = 3435200: loss = 2.8886146545410156\n",
      "step = 3435400: loss = 3.365849733352661\n",
      "step = 3435600: loss = 3.7260539531707764\n",
      "step = 3435800: loss = 2.8573272228240967\n",
      "step = 3436000: loss = 4.462450981140137\n",
      "step = 3436200: loss = 3.9237279891967773\n",
      "step = 3436400: loss = 3.5737829208374023\n",
      "step = 3436600: loss = 3.003131628036499\n",
      "step = 3436800: loss = 4.7401299476623535\n",
      "step = 3437000: loss = 2.6600658893585205\n",
      "step = 3437200: loss = 3.6817774772644043\n",
      "step = 3437400: loss = 4.11004114151001\n",
      "step = 3437600: loss = 4.082591533660889\n",
      "step = 3437800: loss = 4.478899002075195\n",
      "step = 3438000: loss = 4.242284297943115\n",
      "step = 3438200: loss = 3.116492748260498\n",
      "step = 3438400: loss = 5.136882305145264\n",
      "step = 3438600: loss = 2.497027635574341\n",
      "step = 3438800: loss = 2.707672119140625\n",
      "step = 3439000: loss = 3.830381393432617\n",
      "step = 3439200: loss = 2.844212532043457\n",
      "step = 3439400: loss = 2.8019509315490723\n",
      "step = 3439600: loss = 3.261847972869873\n",
      "step = 3439800: loss = 3.542781352996826\n",
      "step = 3440000: loss = 2.566204786300659\n",
      "step = 3440000: Average Return = 4.849999904632568\n",
      "step = 3440200: loss = 2.738474130630493\n",
      "step = 3440400: loss = 3.9869744777679443\n",
      "step = 3440600: loss = 4.3848090171813965\n",
      "step = 3440800: loss = 2.979912042617798\n",
      "step = 3441000: loss = 5.374461650848389\n",
      "step = 3441200: loss = 4.129894733428955\n",
      "step = 3441400: loss = 3.0764992237091064\n",
      "step = 3441600: loss = 4.129549503326416\n",
      "step = 3441800: loss = 2.0110442638397217\n",
      "step = 3442000: loss = 4.689789295196533\n",
      "step = 3442200: loss = 3.420321464538574\n",
      "step = 3442400: loss = 4.618589878082275\n",
      "step = 3442600: loss = 3.0393288135528564\n",
      "step = 3442800: loss = 2.982189893722534\n",
      "step = 3443000: loss = 3.8962178230285645\n",
      "step = 3443200: loss = 5.782505989074707\n",
      "step = 3443400: loss = 5.173768997192383\n",
      "step = 3443600: loss = 3.8994579315185547\n",
      "step = 3443800: loss = 4.0998148918151855\n",
      "step = 3444000: loss = 4.849305152893066\n",
      "step = 3444200: loss = 3.50602650642395\n",
      "step = 3444400: loss = 4.333207130432129\n",
      "step = 3444600: loss = 3.505772590637207\n",
      "step = 3444800: loss = 4.4013190269470215\n",
      "step = 3445000: loss = 4.9018144607543945\n",
      "step = 3445000: Average Return = 2.950000047683716\n",
      "step = 3445200: loss = 3.517005443572998\n",
      "step = 3445400: loss = 5.084724426269531\n",
      "step = 3445600: loss = 3.4069879055023193\n",
      "step = 3445800: loss = 3.550175189971924\n",
      "step = 3446000: loss = 3.818535566329956\n",
      "step = 3446200: loss = 4.060276985168457\n",
      "step = 3446400: loss = 3.074110746383667\n",
      "step = 3446600: loss = 2.64174747467041\n",
      "step = 3446800: loss = 2.7625560760498047\n",
      "step = 3447000: loss = 3.927286386489868\n",
      "step = 3447200: loss = 3.123893976211548\n",
      "step = 3447400: loss = 4.3603057861328125\n",
      "step = 3447600: loss = 3.4285333156585693\n",
      "step = 3447800: loss = 4.330908298492432\n",
      "step = 3448000: loss = 3.2265772819519043\n",
      "step = 3448200: loss = 3.678657054901123\n",
      "step = 3448400: loss = 2.531080722808838\n",
      "step = 3448600: loss = 4.372574329376221\n",
      "step = 3448800: loss = 3.7238645553588867\n",
      "step = 3449000: loss = 4.121347427368164\n",
      "step = 3449200: loss = 3.4150731563568115\n",
      "step = 3449400: loss = 2.8692784309387207\n",
      "step = 3449600: loss = 3.189225435256958\n",
      "step = 3449800: loss = 3.482837438583374\n",
      "step = 3450000: loss = 3.0871849060058594\n",
      "step = 3450000: Average Return = 4.900000095367432\n",
      "step = 3450200: loss = 4.349659442901611\n",
      "step = 3450400: loss = 4.808563232421875\n",
      "step = 3450600: loss = 4.461898326873779\n",
      "step = 3450800: loss = 2.5031542778015137\n",
      "step = 3451000: loss = 4.475539684295654\n",
      "step = 3451200: loss = 2.9995357990264893\n",
      "step = 3451400: loss = 3.2080633640289307\n",
      "step = 3451600: loss = 3.8645100593566895\n",
      "step = 3451800: loss = 3.046299695968628\n",
      "step = 3452000: loss = 3.8330471515655518\n",
      "step = 3452200: loss = 2.365400552749634\n",
      "step = 3452400: loss = 3.2942538261413574\n",
      "step = 3452600: loss = 5.1776227951049805\n",
      "step = 3452800: loss = 2.7171411514282227\n",
      "step = 3453000: loss = 3.3954527378082275\n",
      "step = 3453200: loss = 3.045380115509033\n",
      "step = 3453400: loss = 2.9869823455810547\n",
      "step = 3453600: loss = 4.480556011199951\n",
      "step = 3453800: loss = 4.157146453857422\n",
      "step = 3454000: loss = 3.7812910079956055\n",
      "step = 3454200: loss = 2.983301877975464\n",
      "step = 3454400: loss = 3.6500494480133057\n",
      "step = 3454600: loss = 4.137868404388428\n",
      "step = 3454800: loss = 3.3007335662841797\n",
      "step = 3455000: loss = 3.072582960128784\n",
      "step = 3455000: Average Return = 6.5\n",
      "step = 3455200: loss = 4.61843729019165\n",
      "step = 3455400: loss = 2.765537738800049\n",
      "step = 3455600: loss = 3.301649808883667\n",
      "step = 3455800: loss = 6.411800861358643\n",
      "step = 3456000: loss = 3.499551296234131\n",
      "step = 3456200: loss = 2.569946765899658\n",
      "step = 3456400: loss = 3.7492969036102295\n",
      "step = 3456600: loss = 3.99639630317688\n",
      "step = 3456800: loss = 4.2838969230651855\n",
      "step = 3457000: loss = 3.3561527729034424\n",
      "step = 3457200: loss = 3.60943341255188\n",
      "step = 3457400: loss = 4.456185817718506\n",
      "step = 3457600: loss = 3.795994758605957\n",
      "step = 3457800: loss = 3.91005277633667\n",
      "step = 3458000: loss = 3.7516233921051025\n",
      "step = 3458200: loss = 4.028277397155762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3458400: loss = 3.0581254959106445\n",
      "step = 3458600: loss = 3.0743608474731445\n",
      "step = 3458800: loss = 3.6028642654418945\n",
      "step = 3459000: loss = 2.9064910411834717\n",
      "step = 3459200: loss = 2.2844316959381104\n",
      "step = 3459400: loss = 2.735285520553589\n",
      "step = 3459600: loss = 3.745630979537964\n",
      "step = 3459800: loss = 2.887094736099243\n",
      "step = 3460000: loss = 4.039612293243408\n",
      "step = 3460000: Average Return = 5.550000190734863\n",
      "step = 3460200: loss = 3.027116537094116\n",
      "step = 3460400: loss = 3.774600028991699\n",
      "step = 3460600: loss = 3.374187469482422\n",
      "step = 3460800: loss = 3.5898149013519287\n",
      "step = 3461000: loss = 3.2900822162628174\n",
      "step = 3461200: loss = 3.033090829849243\n",
      "step = 3461400: loss = 4.397043228149414\n",
      "step = 3461600: loss = 4.93563175201416\n",
      "step = 3461800: loss = 4.111385822296143\n",
      "step = 3462000: loss = 4.419529438018799\n",
      "step = 3462200: loss = 2.98974609375\n",
      "step = 3462400: loss = 2.842226505279541\n",
      "step = 3462600: loss = 3.613762140274048\n",
      "step = 3462800: loss = 3.4666314125061035\n",
      "step = 3463000: loss = 3.7276535034179688\n",
      "step = 3463200: loss = 3.611284017562866\n",
      "step = 3463400: loss = 4.553161144256592\n",
      "step = 3463600: loss = 3.723566770553589\n",
      "step = 3463800: loss = 2.85166072845459\n",
      "step = 3464000: loss = 5.017503261566162\n",
      "step = 3464200: loss = 4.149792194366455\n",
      "step = 3464400: loss = 3.148369073867798\n",
      "step = 3464600: loss = 5.7351226806640625\n",
      "step = 3464800: loss = 3.641442060470581\n",
      "step = 3465000: loss = 4.66013765335083\n",
      "step = 3465000: Average Return = 3.950000047683716\n",
      "step = 3465200: loss = 5.28126335144043\n",
      "step = 3465400: loss = 3.2849953174591064\n",
      "step = 3465600: loss = 3.1763341426849365\n",
      "step = 3465800: loss = 5.609938621520996\n",
      "step = 3466000: loss = 3.182206392288208\n",
      "step = 3466200: loss = 3.2441422939300537\n",
      "step = 3466400: loss = 4.208663463592529\n",
      "step = 3466600: loss = 3.511021614074707\n",
      "step = 3466800: loss = 5.572061061859131\n",
      "step = 3467000: loss = 2.740220785140991\n",
      "step = 3467200: loss = 4.074825286865234\n",
      "step = 3467400: loss = 2.128302574157715\n",
      "step = 3467600: loss = 2.6824378967285156\n",
      "step = 3467800: loss = 5.789998531341553\n",
      "step = 3468000: loss = 3.3956260681152344\n",
      "step = 3468200: loss = 4.113011837005615\n",
      "step = 3468400: loss = 2.960303783416748\n",
      "step = 3468600: loss = 2.9611010551452637\n",
      "step = 3468800: loss = 4.063661575317383\n",
      "step = 3469000: loss = 2.4939322471618652\n",
      "step = 3469200: loss = 3.241978406906128\n",
      "step = 3469400: loss = 2.9009664058685303\n",
      "step = 3469600: loss = 3.11533522605896\n",
      "step = 3469800: loss = 4.066905498504639\n",
      "step = 3470000: loss = 4.798165321350098\n",
      "step = 3470000: Average Return = 3.75\n",
      "step = 3470200: loss = 3.177029848098755\n",
      "step = 3470400: loss = 5.902416706085205\n",
      "step = 3470600: loss = 4.261614799499512\n",
      "step = 3470800: loss = 3.50600266456604\n",
      "step = 3471000: loss = 3.8135409355163574\n",
      "step = 3471200: loss = 3.8610570430755615\n",
      "step = 3471400: loss = 5.530548572540283\n",
      "step = 3471600: loss = 4.170907020568848\n",
      "step = 3471800: loss = 4.376383304595947\n",
      "step = 3472000: loss = 4.62078857421875\n",
      "step = 3472200: loss = 2.846271514892578\n",
      "step = 3472400: loss = 4.150046348571777\n",
      "step = 3472600: loss = 4.425732135772705\n",
      "step = 3472800: loss = 3.605825662612915\n",
      "step = 3473000: loss = 2.8806872367858887\n",
      "step = 3473200: loss = 3.111583709716797\n",
      "step = 3473400: loss = 3.8846147060394287\n",
      "step = 3473600: loss = 5.870687007904053\n",
      "step = 3473800: loss = 4.101417064666748\n",
      "step = 3474000: loss = 2.6686859130859375\n",
      "step = 3474200: loss = 3.5083088874816895\n",
      "step = 3474400: loss = 4.824276447296143\n",
      "step = 3474600: loss = 3.8969638347625732\n",
      "step = 3474800: loss = 4.0557756423950195\n",
      "step = 3475000: loss = 4.2423200607299805\n",
      "step = 3475000: Average Return = 4.0\n",
      "step = 3475200: loss = 4.956044673919678\n",
      "step = 3475400: loss = 3.973886013031006\n",
      "step = 3475600: loss = 5.833568096160889\n",
      "step = 3475800: loss = 6.24210786819458\n",
      "step = 3476000: loss = 2.9574837684631348\n",
      "step = 3476200: loss = 3.3593924045562744\n",
      "step = 3476400: loss = 3.531665563583374\n",
      "step = 3476600: loss = 3.231362819671631\n",
      "step = 3476800: loss = 4.6311163902282715\n",
      "step = 3477000: loss = 3.1496379375457764\n",
      "step = 3477200: loss = 3.286341428756714\n",
      "step = 3477400: loss = 2.684115171432495\n",
      "step = 3477600: loss = 3.596036195755005\n",
      "step = 3477800: loss = 4.3499627113342285\n",
      "step = 3478000: loss = 3.3576719760894775\n",
      "step = 3478200: loss = 3.3514933586120605\n",
      "step = 3478400: loss = 4.467535018920898\n",
      "step = 3478600: loss = 4.77397346496582\n",
      "step = 3478800: loss = 3.98838472366333\n",
      "step = 3479000: loss = 4.776399612426758\n",
      "step = 3479200: loss = 3.3040077686309814\n",
      "step = 3479400: loss = 2.7267684936523438\n",
      "step = 3479600: loss = 2.9150326251983643\n",
      "step = 3479800: loss = 4.993332862854004\n",
      "step = 3480000: loss = 4.299522876739502\n",
      "step = 3480000: Average Return = 5.349999904632568\n",
      "step = 3480200: loss = 3.8045482635498047\n",
      "step = 3480400: loss = 4.25731897354126\n",
      "step = 3480600: loss = 3.770115375518799\n",
      "step = 3480800: loss = 3.1127684116363525\n",
      "step = 3481000: loss = 4.919325828552246\n",
      "step = 3481200: loss = 2.905069589614868\n",
      "step = 3481400: loss = 3.56180477142334\n",
      "step = 3481600: loss = 3.912625551223755\n",
      "step = 3481800: loss = 3.396759033203125\n",
      "step = 3482000: loss = 3.0575451850891113\n",
      "step = 3482200: loss = 3.3483293056488037\n",
      "step = 3482400: loss = 2.893260955810547\n",
      "step = 3482600: loss = 4.593890190124512\n",
      "step = 3482800: loss = 3.9647750854492188\n",
      "step = 3483000: loss = 3.1688671112060547\n",
      "step = 3483200: loss = 4.674879550933838\n",
      "step = 3483400: loss = 3.407219171524048\n",
      "step = 3483600: loss = 3.0437991619110107\n",
      "step = 3483800: loss = 3.142702341079712\n",
      "step = 3484000: loss = 2.9455642700195312\n",
      "step = 3484200: loss = 3.575190544128418\n",
      "step = 3484400: loss = 2.8007261753082275\n",
      "step = 3484600: loss = 4.706719398498535\n",
      "step = 3484800: loss = 3.899813652038574\n",
      "step = 3485000: loss = 3.400956869125366\n",
      "step = 3485000: Average Return = 6.699999809265137\n",
      "step = 3485200: loss = 3.239478588104248\n",
      "step = 3485400: loss = 4.925522327423096\n",
      "step = 3485600: loss = 4.758527755737305\n",
      "step = 3485800: loss = 3.8256309032440186\n",
      "step = 3486000: loss = 2.511742115020752\n",
      "step = 3486200: loss = 2.953217029571533\n",
      "step = 3486400: loss = 2.795266628265381\n",
      "step = 3486600: loss = 3.697036027908325\n",
      "step = 3486800: loss = 3.3886961936950684\n",
      "step = 3487000: loss = 2.791717529296875\n",
      "step = 3487200: loss = 2.625286102294922\n",
      "step = 3487400: loss = 4.382622718811035\n",
      "step = 3487600: loss = 4.106273174285889\n",
      "step = 3487800: loss = 5.311388969421387\n",
      "step = 3488000: loss = 4.512060165405273\n",
      "step = 3488200: loss = 3.6060686111450195\n",
      "step = 3488400: loss = 2.3653407096862793\n",
      "step = 3488600: loss = 4.14389181137085\n",
      "step = 3488800: loss = 3.8854963779449463\n",
      "step = 3489000: loss = 3.8004989624023438\n",
      "step = 3489200: loss = 4.294625282287598\n",
      "step = 3489400: loss = 3.63783860206604\n",
      "step = 3489600: loss = 3.083991289138794\n",
      "step = 3489800: loss = 3.252265214920044\n",
      "step = 3490000: loss = 3.3388707637786865\n",
      "step = 3490000: Average Return = 4.849999904632568\n",
      "step = 3490200: loss = 3.0590550899505615\n",
      "step = 3490400: loss = 3.3120028972625732\n",
      "step = 3490600: loss = 3.4772682189941406\n",
      "step = 3490800: loss = 3.0490808486938477\n",
      "step = 3491000: loss = 3.3310813903808594\n",
      "step = 3491200: loss = 4.030544281005859\n",
      "step = 3491400: loss = 2.5188651084899902\n",
      "step = 3491600: loss = 2.5221612453460693\n",
      "step = 3491800: loss = 4.019008159637451\n",
      "step = 3492000: loss = 3.7937254905700684\n",
      "step = 3492200: loss = 3.824864387512207\n",
      "step = 3492400: loss = 3.5583131313323975\n",
      "step = 3492600: loss = 3.593064785003662\n",
      "step = 3492800: loss = 2.879899263381958\n",
      "step = 3493000: loss = 3.6035542488098145\n",
      "step = 3493200: loss = 2.848466157913208\n",
      "step = 3493400: loss = 3.558802366256714\n",
      "step = 3493600: loss = 2.6168854236602783\n",
      "step = 3493800: loss = 5.452174186706543\n",
      "step = 3494000: loss = 4.444273948669434\n",
      "step = 3494200: loss = 3.655987501144409\n",
      "step = 3494400: loss = 4.20122766494751\n",
      "step = 3494600: loss = 3.6737782955169678\n",
      "step = 3494800: loss = 3.7669668197631836\n",
      "step = 3495000: loss = 3.1904284954071045\n",
      "step = 3495000: Average Return = 3.700000047683716\n",
      "step = 3495200: loss = 3.0188024044036865\n",
      "step = 3495400: loss = 4.895132541656494\n",
      "step = 3495600: loss = 4.097772598266602\n",
      "step = 3495800: loss = 4.0918498039245605\n",
      "step = 3496000: loss = 2.9762234687805176\n",
      "step = 3496200: loss = 3.8951432704925537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3496400: loss = 3.900087833404541\n",
      "step = 3496600: loss = 3.525827646255493\n",
      "step = 3496800: loss = 5.180683612823486\n",
      "step = 3497000: loss = 3.6979362964630127\n",
      "step = 3497200: loss = 3.083252429962158\n",
      "step = 3497400: loss = 4.799710750579834\n",
      "step = 3497600: loss = 3.6291143894195557\n",
      "step = 3497800: loss = 3.0397515296936035\n",
      "step = 3498000: loss = 3.6495063304901123\n",
      "step = 3498200: loss = 3.533487558364868\n",
      "step = 3498400: loss = 3.14496111869812\n",
      "step = 3498600: loss = 4.955605506896973\n",
      "step = 3498800: loss = 4.690998554229736\n",
      "step = 3499000: loss = 3.8698625564575195\n",
      "step = 3499200: loss = 3.463017463684082\n",
      "step = 3499400: loss = 3.5557377338409424\n",
      "step = 3499600: loss = 3.4401607513427734\n",
      "step = 3499800: loss = 2.464143991470337\n",
      "step = 3500000: loss = 1.9352757930755615\n",
      "step = 3500000: Average Return = 3.049999952316284\n",
      "step = 3500200: loss = 4.178389072418213\n",
      "step = 3500400: loss = 2.5809342861175537\n",
      "step = 3500600: loss = 3.1166350841522217\n",
      "step = 3500800: loss = 2.7935967445373535\n",
      "step = 3501000: loss = 4.5958170890808105\n",
      "step = 3501200: loss = 4.7262797355651855\n",
      "step = 3501400: loss = 3.787702798843384\n",
      "step = 3501600: loss = 3.037616014480591\n",
      "step = 3501800: loss = 3.6502811908721924\n",
      "step = 3502000: loss = 3.4111833572387695\n",
      "step = 3502200: loss = 3.118234872817993\n",
      "step = 3502400: loss = 3.4672906398773193\n",
      "step = 3502600: loss = 2.21958327293396\n",
      "step = 3502800: loss = 3.7275876998901367\n",
      "step = 3503000: loss = 3.1593902111053467\n",
      "step = 3503200: loss = 3.586808919906616\n",
      "step = 3503400: loss = 2.8259239196777344\n",
      "step = 3503600: loss = 4.824082374572754\n",
      "step = 3503800: loss = 3.682973623275757\n",
      "step = 3504000: loss = 3.648393154144287\n",
      "step = 3504200: loss = 1.7988815307617188\n",
      "step = 3504400: loss = 3.8830649852752686\n",
      "step = 3504600: loss = 5.443583965301514\n",
      "step = 3504800: loss = 3.4545507431030273\n",
      "step = 3505000: loss = 3.6268131732940674\n",
      "step = 3505000: Average Return = 5.550000190734863\n",
      "step = 3505200: loss = 4.286429405212402\n",
      "step = 3505400: loss = 3.9606406688690186\n",
      "step = 3505600: loss = 5.269481182098389\n",
      "step = 3505800: loss = 3.455275774002075\n",
      "step = 3506000: loss = 5.110254764556885\n",
      "step = 3506200: loss = 3.0686419010162354\n",
      "step = 3506400: loss = 3.3686046600341797\n",
      "step = 3506600: loss = 3.283947706222534\n",
      "step = 3506800: loss = 2.8343701362609863\n",
      "step = 3507000: loss = 4.232394695281982\n",
      "step = 3507200: loss = 2.601457357406616\n",
      "step = 3507400: loss = 3.779521942138672\n",
      "step = 3507600: loss = 2.975292205810547\n",
      "step = 3507800: loss = 4.604457378387451\n",
      "step = 3508000: loss = 2.394991397857666\n",
      "step = 3508200: loss = 4.257434844970703\n",
      "step = 3508400: loss = 3.3461198806762695\n",
      "step = 3508600: loss = 4.850361347198486\n",
      "step = 3508800: loss = 3.150358200073242\n",
      "step = 3509000: loss = 2.8021998405456543\n",
      "step = 3509200: loss = 2.6973137855529785\n",
      "step = 3509400: loss = 3.34456205368042\n",
      "step = 3509600: loss = 4.053353309631348\n",
      "step = 3509800: loss = 2.3510425090789795\n",
      "step = 3510000: loss = 2.677539825439453\n",
      "step = 3510000: Average Return = 4.800000190734863\n",
      "step = 3510200: loss = 4.184572696685791\n",
      "step = 3510400: loss = 4.461914539337158\n",
      "step = 3510600: loss = 4.075778007507324\n",
      "step = 3510800: loss = 2.9306323528289795\n",
      "step = 3511000: loss = 3.573763608932495\n",
      "step = 3511200: loss = 3.295161008834839\n",
      "step = 3511400: loss = 1.9379650354385376\n",
      "step = 3511600: loss = 3.6361632347106934\n",
      "step = 3511800: loss = 3.783843994140625\n",
      "step = 3512000: loss = 3.5290117263793945\n",
      "step = 3512200: loss = 3.681135416030884\n",
      "step = 3512400: loss = 2.2726194858551025\n",
      "step = 3512600: loss = 2.1118781566619873\n",
      "step = 3512800: loss = 5.191748142242432\n",
      "step = 3513000: loss = 3.598473072052002\n",
      "step = 3513200: loss = 4.1875386238098145\n",
      "step = 3513400: loss = 4.524481773376465\n",
      "step = 3513600: loss = 3.5699920654296875\n",
      "step = 3513800: loss = 3.4752514362335205\n",
      "step = 3514000: loss = 3.254584312438965\n",
      "step = 3514200: loss = 3.6628270149230957\n",
      "step = 3514400: loss = 2.976989269256592\n",
      "step = 3514600: loss = 3.605250120162964\n",
      "step = 3514800: loss = 3.813382387161255\n",
      "step = 3515000: loss = 4.768095970153809\n",
      "step = 3515000: Average Return = 4.650000095367432\n",
      "step = 3515200: loss = 3.5023577213287354\n",
      "step = 3515400: loss = 3.0064287185668945\n",
      "step = 3515600: loss = 4.032878398895264\n",
      "step = 3515800: loss = 4.36606502532959\n",
      "step = 3516000: loss = 5.10670804977417\n",
      "step = 3516200: loss = 3.745699405670166\n",
      "step = 3516400: loss = 3.615499258041382\n",
      "step = 3516600: loss = 2.6484711170196533\n",
      "step = 3516800: loss = 3.3199124336242676\n",
      "step = 3517000: loss = 2.745198965072632\n",
      "step = 3517200: loss = 3.665926456451416\n",
      "step = 3517400: loss = 3.802738904953003\n",
      "step = 3517600: loss = 3.7048144340515137\n",
      "step = 3517800: loss = 2.3716988563537598\n",
      "step = 3518000: loss = 3.2420384883880615\n",
      "step = 3518200: loss = 3.680880546569824\n",
      "step = 3518400: loss = 3.7671334743499756\n",
      "step = 3518600: loss = 4.862055778503418\n",
      "step = 3518800: loss = 5.119063377380371\n",
      "step = 3519000: loss = 3.519587278366089\n",
      "step = 3519200: loss = 3.8355681896209717\n",
      "step = 3519400: loss = 2.5624377727508545\n",
      "step = 3519600: loss = 3.2729480266571045\n",
      "step = 3519800: loss = 3.9902195930480957\n",
      "step = 3520000: loss = 3.3068008422851562\n",
      "step = 3520000: Average Return = 6.25\n",
      "step = 3520200: loss = 3.927548408508301\n",
      "step = 3520400: loss = 2.2657573223114014\n",
      "step = 3520600: loss = 3.1340649127960205\n",
      "step = 3520800: loss = 3.7241835594177246\n",
      "step = 3521000: loss = 3.9168193340301514\n",
      "step = 3521200: loss = 4.436331272125244\n",
      "step = 3521400: loss = 2.946307897567749\n",
      "step = 3521600: loss = 3.2655346393585205\n",
      "step = 3521800: loss = 3.8319272994995117\n",
      "step = 3522000: loss = 3.814227819442749\n",
      "step = 3522200: loss = 2.7004611492156982\n",
      "step = 3522400: loss = 2.825026750564575\n",
      "step = 3522600: loss = 3.603285074234009\n",
      "step = 3522800: loss = 3.1271536350250244\n",
      "step = 3523000: loss = 3.156195640563965\n",
      "step = 3523200: loss = 3.3698067665100098\n",
      "step = 3523400: loss = 3.2546184062957764\n",
      "step = 3523600: loss = 3.379326105117798\n",
      "step = 3523800: loss = 3.4826066493988037\n",
      "step = 3524000: loss = 3.5265142917633057\n",
      "step = 3524200: loss = 3.9412362575531006\n",
      "step = 3524400: loss = 2.739858627319336\n",
      "step = 3524600: loss = 2.8610081672668457\n",
      "step = 3524800: loss = 2.997222423553467\n",
      "step = 3525000: loss = 3.3963661193847656\n",
      "step = 3525000: Average Return = 4.900000095367432\n",
      "step = 3525200: loss = 2.8001136779785156\n",
      "step = 3525400: loss = 3.7986767292022705\n",
      "step = 3525600: loss = 2.9318814277648926\n",
      "step = 3525800: loss = 3.2961955070495605\n",
      "step = 3526000: loss = 3.7104198932647705\n",
      "step = 3526200: loss = 3.262458324432373\n",
      "step = 3526400: loss = 4.059782028198242\n",
      "step = 3526600: loss = 4.834231376647949\n",
      "step = 3526800: loss = 4.502960205078125\n",
      "step = 3527000: loss = 3.656101942062378\n",
      "step = 3527200: loss = 4.19749641418457\n",
      "step = 3527400: loss = 3.0997490882873535\n",
      "step = 3527600: loss = 4.204387664794922\n",
      "step = 3527800: loss = 3.8153369426727295\n",
      "step = 3528000: loss = 3.6703953742980957\n",
      "step = 3528200: loss = 3.4349584579467773\n",
      "step = 3528400: loss = 4.092372417449951\n",
      "step = 3528600: loss = 3.785038471221924\n",
      "step = 3528800: loss = 2.664329767227173\n",
      "step = 3529000: loss = 2.982682704925537\n",
      "step = 3529200: loss = 3.846405506134033\n",
      "step = 3529400: loss = 2.533447265625\n",
      "step = 3529600: loss = 3.3696584701538086\n",
      "step = 3529800: loss = 1.9372073411941528\n",
      "step = 3530000: loss = 3.906059741973877\n",
      "step = 3530000: Average Return = 4.0\n",
      "step = 3530200: loss = 2.8913683891296387\n",
      "step = 3530400: loss = 3.2700273990631104\n",
      "step = 3530600: loss = 3.237553834915161\n",
      "step = 3530800: loss = 3.065779447555542\n",
      "step = 3531000: loss = 3.0125813484191895\n",
      "step = 3531200: loss = 2.5441102981567383\n",
      "step = 3531400: loss = 3.251915216445923\n",
      "step = 3531600: loss = 4.489243030548096\n",
      "step = 3531800: loss = 2.5262370109558105\n",
      "step = 3532000: loss = 3.00095534324646\n",
      "step = 3532200: loss = 4.3938889503479\n",
      "step = 3532400: loss = 3.5115983486175537\n",
      "step = 3532600: loss = 4.717521667480469\n",
      "step = 3532800: loss = 3.557598829269409\n",
      "step = 3533000: loss = 3.8945350646972656\n",
      "step = 3533200: loss = 2.7110161781311035\n",
      "step = 3533400: loss = 4.517477989196777\n",
      "step = 3533600: loss = 3.5952043533325195\n",
      "step = 3533800: loss = 4.823156833648682\n",
      "step = 3534000: loss = 2.9853429794311523\n",
      "step = 3534200: loss = 3.403190851211548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3534400: loss = 3.668053388595581\n",
      "step = 3534600: loss = 2.788156270980835\n",
      "step = 3534800: loss = 2.9816882610321045\n",
      "step = 3535000: loss = 3.0504837036132812\n",
      "step = 3535000: Average Return = 4.400000095367432\n",
      "step = 3535200: loss = 3.2293286323547363\n",
      "step = 3535400: loss = 4.116815567016602\n",
      "step = 3535600: loss = 4.164984226226807\n",
      "step = 3535800: loss = 3.3003695011138916\n",
      "step = 3536000: loss = 3.4690158367156982\n",
      "step = 3536200: loss = 3.458704948425293\n",
      "step = 3536400: loss = 5.1625657081604\n",
      "step = 3536600: loss = 3.258537530899048\n",
      "step = 3536800: loss = 4.001347541809082\n",
      "step = 3537000: loss = 3.6915643215179443\n",
      "step = 3537200: loss = 3.2307708263397217\n",
      "step = 3537400: loss = 4.2666850090026855\n",
      "step = 3537600: loss = 3.6150922775268555\n",
      "step = 3537800: loss = 3.0178651809692383\n",
      "step = 3538000: loss = 3.282864809036255\n",
      "step = 3538200: loss = 3.34060001373291\n",
      "step = 3538400: loss = 2.2703566551208496\n",
      "step = 3538600: loss = 2.905729055404663\n",
      "step = 3538800: loss = 3.4471380710601807\n",
      "step = 3539000: loss = 3.4681854248046875\n",
      "step = 3539200: loss = 2.968653440475464\n",
      "step = 3539400: loss = 3.63547420501709\n",
      "step = 3539600: loss = 3.3301401138305664\n",
      "step = 3539800: loss = 5.025905132293701\n",
      "step = 3540000: loss = 3.0172271728515625\n",
      "step = 3540000: Average Return = 4.699999809265137\n",
      "step = 3540200: loss = 5.198883056640625\n",
      "step = 3540400: loss = 4.164994239807129\n",
      "step = 3540600: loss = 4.511536121368408\n",
      "step = 3540800: loss = 3.562591075897217\n",
      "step = 3541000: loss = 4.154086589813232\n",
      "step = 3541200: loss = 2.6040873527526855\n",
      "step = 3541400: loss = 2.819138765335083\n",
      "step = 3541600: loss = 3.5062291622161865\n",
      "step = 3541800: loss = 2.959991455078125\n",
      "step = 3542000: loss = 4.274118423461914\n",
      "step = 3542200: loss = 2.314640998840332\n",
      "step = 3542400: loss = 2.8575878143310547\n",
      "step = 3542600: loss = 4.3748273849487305\n",
      "step = 3542800: loss = 3.090348958969116\n",
      "step = 3543000: loss = 2.49456524848938\n",
      "step = 3543200: loss = 2.5413379669189453\n",
      "step = 3543400: loss = 2.101776599884033\n",
      "step = 3543600: loss = 3.3366355895996094\n",
      "step = 3543800: loss = 2.6916615962982178\n",
      "step = 3544000: loss = 3.354588031768799\n",
      "step = 3544200: loss = 2.855634927749634\n",
      "step = 3544400: loss = 4.109051704406738\n",
      "step = 3544600: loss = 4.1796441078186035\n",
      "step = 3544800: loss = 3.29202938079834\n",
      "step = 3545000: loss = 3.445485830307007\n",
      "step = 3545000: Average Return = 5.400000095367432\n",
      "step = 3545200: loss = 3.5005557537078857\n",
      "step = 3545400: loss = 2.869363307952881\n",
      "step = 3545600: loss = 3.498957395553589\n",
      "step = 3545800: loss = 3.203557014465332\n",
      "step = 3546000: loss = 3.6600162982940674\n",
      "step = 3546200: loss = 5.011872291564941\n",
      "step = 3546400: loss = 2.6396114826202393\n",
      "step = 3546600: loss = 3.131122350692749\n",
      "step = 3546800: loss = 3.1671459674835205\n",
      "step = 3547000: loss = 2.1840579509735107\n",
      "step = 3547200: loss = 4.036611080169678\n",
      "step = 3547400: loss = 4.4254255294799805\n",
      "step = 3547600: loss = 3.4557745456695557\n",
      "step = 3547800: loss = 3.366011381149292\n",
      "step = 3548000: loss = 3.081343412399292\n",
      "step = 3548200: loss = 2.5006773471832275\n",
      "step = 3548400: loss = 2.1039586067199707\n",
      "step = 3548600: loss = 3.7208192348480225\n",
      "step = 3548800: loss = 2.9336161613464355\n",
      "step = 3549000: loss = 2.8112614154815674\n",
      "step = 3549200: loss = 2.5025389194488525\n",
      "step = 3549400: loss = 3.5465214252471924\n",
      "step = 3549600: loss = 3.1030690670013428\n",
      "step = 3549800: loss = 3.309001922607422\n",
      "step = 3550000: loss = 2.237091541290283\n",
      "step = 3550000: Average Return = 6.300000190734863\n",
      "step = 3550200: loss = 3.119252920150757\n",
      "step = 3550400: loss = 3.889310836791992\n",
      "step = 3550600: loss = 3.5544872283935547\n",
      "step = 3550800: loss = 3.58331298828125\n",
      "step = 3551000: loss = 2.7290873527526855\n",
      "step = 3551200: loss = 3.295579195022583\n",
      "step = 3551400: loss = 4.94631290435791\n",
      "step = 3551600: loss = 2.921473264694214\n",
      "step = 3551800: loss = 3.497642755508423\n",
      "step = 3552000: loss = 3.5716471672058105\n",
      "step = 3552200: loss = 3.2128186225891113\n",
      "step = 3552400: loss = 3.7311596870422363\n",
      "step = 3552600: loss = 3.4161465167999268\n",
      "step = 3552800: loss = 4.236147880554199\n",
      "step = 3553000: loss = 2.7546184062957764\n",
      "step = 3553200: loss = 3.2474637031555176\n",
      "step = 3553400: loss = 4.365480422973633\n",
      "step = 3553600: loss = 2.8810489177703857\n",
      "step = 3553800: loss = 2.127129077911377\n",
      "step = 3554000: loss = 1.956457495689392\n",
      "step = 3554200: loss = 3.999295473098755\n",
      "step = 3554400: loss = 3.8121514320373535\n",
      "step = 3554600: loss = 3.546586275100708\n",
      "step = 3554800: loss = 2.836549997329712\n",
      "step = 3555000: loss = 2.9534032344818115\n",
      "step = 3555000: Average Return = 3.6500000953674316\n",
      "step = 3555200: loss = 2.8979811668395996\n",
      "step = 3555400: loss = 3.0040359497070312\n",
      "step = 3555600: loss = 2.709247350692749\n",
      "step = 3555800: loss = 2.8181099891662598\n",
      "step = 3556000: loss = 3.077354669570923\n",
      "step = 3556200: loss = 3.012972593307495\n",
      "step = 3556400: loss = 4.0302863121032715\n",
      "step = 3556600: loss = 3.2098803520202637\n",
      "step = 3556800: loss = 3.4477202892303467\n",
      "step = 3557000: loss = 3.6021673679351807\n",
      "step = 3557200: loss = 2.8449294567108154\n",
      "step = 3557400: loss = 2.1992146968841553\n",
      "step = 3557600: loss = 2.3983778953552246\n",
      "step = 3557800: loss = 3.6908013820648193\n",
      "step = 3558000: loss = 3.719322443008423\n",
      "step = 3558200: loss = 2.436128854751587\n",
      "step = 3558400: loss = 1.9329183101654053\n",
      "step = 3558600: loss = 2.4258615970611572\n",
      "step = 3558800: loss = 2.517345905303955\n",
      "step = 3559000: loss = 3.354954719543457\n",
      "step = 3559200: loss = 4.486088752746582\n",
      "step = 3559400: loss = 2.7590043544769287\n",
      "step = 3559600: loss = 3.982898712158203\n",
      "step = 3559800: loss = 4.735766410827637\n",
      "step = 3560000: loss = 1.7891241312026978\n",
      "step = 3560000: Average Return = 7.099999904632568\n",
      "step = 3560200: loss = 2.32957124710083\n",
      "step = 3560400: loss = 3.533620595932007\n",
      "step = 3560600: loss = 3.4534754753112793\n",
      "step = 3560800: loss = 2.7867507934570312\n",
      "step = 3561000: loss = 3.5369932651519775\n",
      "step = 3561200: loss = 2.9663288593292236\n",
      "step = 3561400: loss = 2.947690010070801\n",
      "step = 3561600: loss = 2.108430862426758\n",
      "step = 3561800: loss = 2.8078219890594482\n",
      "step = 3562000: loss = 3.2203657627105713\n",
      "step = 3562200: loss = 3.1143383979797363\n",
      "step = 3562400: loss = 3.4129838943481445\n",
      "step = 3562600: loss = 3.13285756111145\n",
      "step = 3562800: loss = 3.3999786376953125\n",
      "step = 3563000: loss = 3.8053274154663086\n",
      "step = 3563200: loss = 3.368873119354248\n",
      "step = 3563400: loss = 2.963330030441284\n",
      "step = 3563600: loss = 3.2000770568847656\n",
      "step = 3563800: loss = 3.4631567001342773\n",
      "step = 3564000: loss = 2.8785417079925537\n",
      "step = 3564200: loss = 4.059770584106445\n",
      "step = 3564400: loss = 2.677426815032959\n",
      "step = 3564600: loss = 2.2030413150787354\n",
      "step = 3564800: loss = 2.7630209922790527\n",
      "step = 3565000: loss = 3.3123793601989746\n",
      "step = 3565000: Average Return = 5.849999904632568\n",
      "step = 3565200: loss = 3.33695125579834\n",
      "step = 3565400: loss = 4.4854583740234375\n",
      "step = 3565600: loss = 2.710239887237549\n",
      "step = 3565800: loss = 2.838826894760132\n",
      "step = 3566000: loss = 2.6628081798553467\n",
      "step = 3566200: loss = 2.2443199157714844\n",
      "step = 3566400: loss = 4.621105670928955\n",
      "step = 3566600: loss = 3.0530333518981934\n",
      "step = 3566800: loss = 2.9610910415649414\n",
      "step = 3567000: loss = 3.3677313327789307\n",
      "step = 3567200: loss = 3.8624749183654785\n",
      "step = 3567400: loss = 1.8542060852050781\n",
      "step = 3567600: loss = 2.5553195476531982\n",
      "step = 3567800: loss = 3.6378462314605713\n",
      "step = 3568000: loss = 2.8029916286468506\n",
      "step = 3568200: loss = 2.5845284461975098\n",
      "step = 3568400: loss = 1.967761516571045\n",
      "step = 3568600: loss = 2.647101640701294\n",
      "step = 3568800: loss = 3.6616859436035156\n",
      "step = 3569000: loss = 3.24716854095459\n",
      "step = 3569200: loss = 3.78244686126709\n",
      "step = 3569400: loss = 3.1774842739105225\n",
      "step = 3569600: loss = 4.0229172706604\n",
      "step = 3569800: loss = 5.61338996887207\n",
      "step = 3570000: loss = 3.501781940460205\n",
      "step = 3570000: Average Return = 4.199999809265137\n",
      "step = 3570200: loss = 2.51011323928833\n",
      "step = 3570400: loss = 2.665452003479004\n",
      "step = 3570600: loss = 3.8561503887176514\n",
      "step = 3570800: loss = 2.1733248233795166\n",
      "step = 3571000: loss = 2.9159035682678223\n",
      "step = 3571200: loss = 2.33377742767334\n",
      "step = 3571400: loss = 3.8031044006347656\n",
      "step = 3571600: loss = 4.638829708099365\n",
      "step = 3571800: loss = 2.971364736557007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3572000: loss = 3.4677042961120605\n",
      "step = 3572200: loss = 3.7668895721435547\n",
      "step = 3572400: loss = 3.324186325073242\n",
      "step = 3572600: loss = 2.950462818145752\n",
      "step = 3572800: loss = 2.4828991889953613\n",
      "step = 3573000: loss = 3.5001168251037598\n",
      "step = 3573200: loss = 3.133633613586426\n",
      "step = 3573400: loss = 3.0600054264068604\n",
      "step = 3573600: loss = 3.4065709114074707\n",
      "step = 3573800: loss = 3.9815564155578613\n",
      "step = 3574000: loss = 3.8373842239379883\n",
      "step = 3574200: loss = 2.9295246601104736\n",
      "step = 3574400: loss = 3.7383806705474854\n",
      "step = 3574600: loss = 2.0963380336761475\n",
      "step = 3574800: loss = 3.876264810562134\n",
      "step = 3575000: loss = 2.6845343112945557\n",
      "step = 3575000: Average Return = 4.150000095367432\n",
      "step = 3575200: loss = 3.5218729972839355\n",
      "step = 3575400: loss = 4.134249687194824\n",
      "step = 3575600: loss = 3.115288019180298\n",
      "step = 3575800: loss = 3.735982894897461\n",
      "step = 3576000: loss = 2.8165369033813477\n",
      "step = 3576200: loss = 3.5725696086883545\n",
      "step = 3576400: loss = 3.8325109481811523\n",
      "step = 3576600: loss = 3.4590208530426025\n",
      "step = 3576800: loss = 4.276211738586426\n",
      "step = 3577000: loss = 3.0816256999969482\n",
      "step = 3577200: loss = 3.017474889755249\n",
      "step = 3577400: loss = 2.6003835201263428\n",
      "step = 3577600: loss = 3.267223834991455\n",
      "step = 3577800: loss = 3.0910298824310303\n",
      "step = 3578000: loss = 3.11637282371521\n",
      "step = 3578200: loss = 2.3864941596984863\n",
      "step = 3578400: loss = 2.094270706176758\n",
      "step = 3578600: loss = 4.355065822601318\n",
      "step = 3578800: loss = 3.1518447399139404\n",
      "step = 3579000: loss = 2.753713846206665\n",
      "step = 3579200: loss = 3.872654438018799\n",
      "step = 3579400: loss = 3.558727264404297\n",
      "step = 3579600: loss = 4.1830220222473145\n",
      "step = 3579800: loss = 3.446018695831299\n",
      "step = 3580000: loss = 3.0960803031921387\n",
      "step = 3580000: Average Return = 6.0\n",
      "step = 3580200: loss = 3.185134172439575\n",
      "step = 3580400: loss = 2.7791595458984375\n",
      "step = 3580600: loss = 2.868394613265991\n",
      "step = 3580800: loss = 3.2684314250946045\n",
      "step = 3581000: loss = 6.029951572418213\n",
      "step = 3581200: loss = 2.5931410789489746\n",
      "step = 3581400: loss = 2.882439613342285\n",
      "step = 3581600: loss = 2.5958468914031982\n",
      "step = 3581800: loss = 2.1538918018341064\n",
      "step = 3582000: loss = 3.8225579261779785\n",
      "step = 3582200: loss = 3.0920867919921875\n",
      "step = 3582400: loss = 3.0261759757995605\n",
      "step = 3582600: loss = 3.7267165184020996\n",
      "step = 3582800: loss = 3.2245235443115234\n",
      "step = 3583000: loss = 3.699502468109131\n",
      "step = 3583200: loss = 2.3130574226379395\n",
      "step = 3583400: loss = 3.1723568439483643\n",
      "step = 3583600: loss = 3.820955753326416\n",
      "step = 3583800: loss = 2.493825912475586\n",
      "step = 3584000: loss = 2.930060386657715\n",
      "step = 3584200: loss = 2.074856758117676\n",
      "step = 3584400: loss = 3.032813549041748\n",
      "step = 3584600: loss = 2.459453582763672\n",
      "step = 3584800: loss = 3.275221109390259\n",
      "step = 3585000: loss = 2.826076030731201\n",
      "step = 3585000: Average Return = 6.099999904632568\n",
      "step = 3585200: loss = 3.787524938583374\n",
      "step = 3585400: loss = 2.911064386367798\n",
      "step = 3585600: loss = 4.152338027954102\n",
      "step = 3585800: loss = 4.606420040130615\n",
      "step = 3586000: loss = 4.180017471313477\n",
      "step = 3586200: loss = 2.699249267578125\n",
      "step = 3586400: loss = 3.3935351371765137\n",
      "step = 3586600: loss = 4.081867694854736\n",
      "step = 3586800: loss = 3.9550905227661133\n",
      "step = 3587000: loss = 3.676687479019165\n",
      "step = 3587200: loss = 3.7120277881622314\n",
      "step = 3587400: loss = 2.922802686691284\n",
      "step = 3587600: loss = 2.3839774131774902\n",
      "step = 3587800: loss = 3.3102705478668213\n",
      "step = 3588000: loss = 2.9158971309661865\n",
      "step = 3588200: loss = 4.096621990203857\n",
      "step = 3588400: loss = 4.2179789543151855\n",
      "step = 3588600: loss = 3.4316954612731934\n",
      "step = 3588800: loss = 2.9785211086273193\n",
      "step = 3589000: loss = 3.081895589828491\n",
      "step = 3589200: loss = 3.5604028701782227\n",
      "step = 3589400: loss = 5.293556213378906\n",
      "step = 3589600: loss = 3.887197256088257\n",
      "step = 3589800: loss = 2.713681221008301\n",
      "step = 3590000: loss = 3.0040783882141113\n",
      "step = 3590000: Average Return = 4.0\n",
      "step = 3590200: loss = 2.6471779346466064\n",
      "step = 3590400: loss = 2.7008843421936035\n",
      "step = 3590600: loss = 2.7210757732391357\n",
      "step = 3590800: loss = 1.8480067253112793\n",
      "step = 3591000: loss = 4.194855213165283\n",
      "step = 3591200: loss = 3.5461549758911133\n",
      "step = 3591400: loss = 3.515501022338867\n",
      "step = 3591600: loss = 5.193222522735596\n",
      "step = 3591800: loss = 4.1038312911987305\n",
      "step = 3592000: loss = 4.521121978759766\n",
      "step = 3592200: loss = 4.3286919593811035\n",
      "step = 3592400: loss = 4.545816421508789\n",
      "step = 3592600: loss = 2.704153299331665\n",
      "step = 3592800: loss = 2.8540539741516113\n",
      "step = 3593000: loss = 2.3771464824676514\n",
      "step = 3593200: loss = 3.2344048023223877\n",
      "step = 3593400: loss = 3.211564779281616\n",
      "step = 3593600: loss = 2.7134523391723633\n",
      "step = 3593800: loss = 3.6799654960632324\n",
      "step = 3594000: loss = 2.616231679916382\n",
      "step = 3594200: loss = 1.8020057678222656\n",
      "step = 3594400: loss = 4.858449459075928\n",
      "step = 3594600: loss = 3.744325876235962\n",
      "step = 3594800: loss = 3.9291326999664307\n",
      "step = 3595000: loss = 4.005931854248047\n",
      "step = 3595000: Average Return = 3.6500000953674316\n",
      "step = 3595200: loss = 4.3682661056518555\n",
      "step = 3595400: loss = 3.500354766845703\n",
      "step = 3595600: loss = 3.051908493041992\n",
      "step = 3595800: loss = 2.6144604682922363\n",
      "step = 3596000: loss = 4.49718713760376\n",
      "step = 3596200: loss = 3.155412435531616\n",
      "step = 3596400: loss = 2.7556352615356445\n",
      "step = 3596600: loss = 3.9741880893707275\n",
      "step = 3596800: loss = 2.4274742603302\n",
      "step = 3597000: loss = 3.7281651496887207\n",
      "step = 3597200: loss = 2.749164342880249\n",
      "step = 3597400: loss = 3.1957342624664307\n",
      "step = 3597600: loss = 3.3135008811950684\n",
      "step = 3597800: loss = 2.693072557449341\n",
      "step = 3598000: loss = 3.959160327911377\n",
      "step = 3598200: loss = 3.374135732650757\n",
      "step = 3598400: loss = 2.6698226928710938\n",
      "step = 3598600: loss = 3.371613025665283\n",
      "step = 3598800: loss = 3.8390746116638184\n",
      "step = 3599000: loss = 3.1508586406707764\n",
      "step = 3599200: loss = 3.1419811248779297\n",
      "step = 3599400: loss = 4.030733108520508\n",
      "step = 3599600: loss = 3.675105571746826\n",
      "step = 3599800: loss = 3.6730618476867676\n",
      "step = 3600000: loss = 2.9404895305633545\n",
      "step = 3600000: Average Return = 3.799999952316284\n",
      "step = 3600200: loss = 2.1733360290527344\n",
      "step = 3600400: loss = 3.4256269931793213\n",
      "step = 3600600: loss = 3.031317710876465\n",
      "step = 3600800: loss = 3.67683744430542\n",
      "step = 3601000: loss = 4.861261367797852\n",
      "step = 3601200: loss = 2.9226903915405273\n",
      "step = 3601400: loss = 2.854238271713257\n",
      "step = 3601600: loss = 2.554398536682129\n",
      "step = 3601800: loss = 3.5511302947998047\n",
      "step = 3602000: loss = 4.323390483856201\n",
      "step = 3602200: loss = 3.158228635787964\n",
      "step = 3602400: loss = 2.9571187496185303\n",
      "step = 3602600: loss = 2.9383111000061035\n",
      "step = 3602800: loss = 4.0023088455200195\n",
      "step = 3603000: loss = 4.596029281616211\n",
      "step = 3603200: loss = 4.159968376159668\n",
      "step = 3603400: loss = 3.6182751655578613\n",
      "step = 3603600: loss = 4.0729079246521\n",
      "step = 3603800: loss = 4.006036758422852\n",
      "step = 3604000: loss = 2.4378819465637207\n",
      "step = 3604200: loss = 3.738969564437866\n",
      "step = 3604400: loss = 3.59291934967041\n",
      "step = 3604600: loss = 4.226312160491943\n",
      "step = 3604800: loss = 3.064373731613159\n",
      "step = 3605000: loss = 3.831028938293457\n",
      "step = 3605000: Average Return = 4.849999904632568\n",
      "step = 3605200: loss = 4.445126056671143\n",
      "step = 3605400: loss = 2.6384730339050293\n",
      "step = 3605600: loss = 4.092436790466309\n",
      "step = 3605800: loss = 2.9636995792388916\n",
      "step = 3606000: loss = 2.8232195377349854\n",
      "step = 3606200: loss = 2.8243649005889893\n",
      "step = 3606400: loss = 3.8375608921051025\n",
      "step = 3606600: loss = 3.9148106575012207\n",
      "step = 3606800: loss = 3.5312047004699707\n",
      "step = 3607000: loss = 2.925513505935669\n",
      "step = 3607200: loss = 2.2263543605804443\n",
      "step = 3607400: loss = 3.168632745742798\n",
      "step = 3607600: loss = 3.6861202716827393\n",
      "step = 3607800: loss = 2.4949893951416016\n",
      "step = 3608000: loss = 3.34005069732666\n",
      "step = 3608200: loss = 3.0745325088500977\n",
      "step = 3608400: loss = 2.636277198791504\n",
      "step = 3608600: loss = 2.9457123279571533\n",
      "step = 3608800: loss = 2.8592748641967773\n",
      "step = 3609000: loss = 3.9559924602508545\n",
      "step = 3609200: loss = 4.869050025939941\n",
      "step = 3609400: loss = 4.090341567993164\n",
      "step = 3609600: loss = 2.947174310684204\n",
      "step = 3609800: loss = 3.143399238586426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3610000: loss = 2.6585566997528076\n",
      "step = 3610000: Average Return = 2.049999952316284\n",
      "step = 3610200: loss = 3.659776210784912\n",
      "step = 3610400: loss = 3.1472854614257812\n",
      "step = 3610600: loss = 2.569208860397339\n",
      "step = 3610800: loss = 3.98736310005188\n",
      "step = 3611000: loss = 2.9009921550750732\n",
      "step = 3611200: loss = 3.7474708557128906\n",
      "step = 3611400: loss = 4.938913345336914\n",
      "step = 3611600: loss = 3.6494195461273193\n",
      "step = 3611800: loss = 3.194864511489868\n",
      "step = 3612000: loss = 3.2201101779937744\n",
      "step = 3612200: loss = 2.545665740966797\n",
      "step = 3612400: loss = 2.9692184925079346\n",
      "step = 3612600: loss = 5.136545181274414\n",
      "step = 3612800: loss = 3.051640033721924\n",
      "step = 3613000: loss = 4.490206718444824\n",
      "step = 3613200: loss = 3.426499605178833\n",
      "step = 3613400: loss = 3.4677722454071045\n",
      "step = 3613600: loss = 2.2849996089935303\n",
      "step = 3613800: loss = 2.918471574783325\n",
      "step = 3614000: loss = 3.2345573902130127\n",
      "step = 3614200: loss = 3.2243707180023193\n",
      "step = 3614400: loss = 2.9043643474578857\n",
      "step = 3614600: loss = 3.107741594314575\n",
      "step = 3614800: loss = 2.1643593311309814\n",
      "step = 3615000: loss = 2.928129196166992\n",
      "step = 3615000: Average Return = 3.549999952316284\n",
      "step = 3615200: loss = 3.223482608795166\n",
      "step = 3615400: loss = 2.770508289337158\n",
      "step = 3615600: loss = 4.8158979415893555\n",
      "step = 3615800: loss = 4.213324546813965\n",
      "step = 3616000: loss = 3.587400436401367\n",
      "step = 3616200: loss = 2.517961263656616\n",
      "step = 3616400: loss = 4.371265888214111\n",
      "step = 3616600: loss = 2.10258412361145\n",
      "step = 3616800: loss = 4.527235507965088\n",
      "step = 3617000: loss = 4.566603183746338\n",
      "step = 3617200: loss = 4.394820213317871\n",
      "step = 3617400: loss = 2.2091453075408936\n",
      "step = 3617600: loss = 3.619180917739868\n",
      "step = 3617800: loss = 3.4693827629089355\n",
      "step = 3618000: loss = 4.459583759307861\n",
      "step = 3618200: loss = 3.526339530944824\n",
      "step = 3618400: loss = 4.256139278411865\n",
      "step = 3618600: loss = 5.201279163360596\n",
      "step = 3618800: loss = 2.7045671939849854\n",
      "step = 3619000: loss = 3.821770191192627\n",
      "step = 3619200: loss = 3.853073835372925\n",
      "step = 3619400: loss = 2.6603167057037354\n",
      "step = 3619600: loss = 3.4537413120269775\n",
      "step = 3619800: loss = 3.2546496391296387\n",
      "step = 3620000: loss = 3.0936639308929443\n",
      "step = 3620000: Average Return = 5.199999809265137\n",
      "step = 3620200: loss = 3.6580922603607178\n",
      "step = 3620400: loss = 4.179835319519043\n",
      "step = 3620600: loss = 4.43094539642334\n",
      "step = 3620800: loss = 2.2999322414398193\n",
      "step = 3621000: loss = 3.3196098804473877\n",
      "step = 3621200: loss = 3.80423903465271\n",
      "step = 3621400: loss = 2.873061418533325\n",
      "step = 3621600: loss = 2.5833983421325684\n",
      "step = 3621800: loss = 3.8300838470458984\n",
      "step = 3622000: loss = 3.0025341510772705\n",
      "step = 3622200: loss = 3.5115599632263184\n",
      "step = 3622400: loss = 3.3490066528320312\n",
      "step = 3622600: loss = 3.3052618503570557\n",
      "step = 3622800: loss = 3.07690167427063\n",
      "step = 3623000: loss = 3.3931145668029785\n",
      "step = 3623200: loss = 2.8734045028686523\n",
      "step = 3623400: loss = 3.7258763313293457\n",
      "step = 3623600: loss = 3.988291025161743\n",
      "step = 3623800: loss = 2.7357988357543945\n",
      "step = 3624000: loss = 3.249298572540283\n",
      "step = 3624200: loss = 3.8730387687683105\n",
      "step = 3624400: loss = 3.9461357593536377\n",
      "step = 3624600: loss = 3.185635566711426\n",
      "step = 3624800: loss = 4.997605323791504\n",
      "step = 3625000: loss = 3.8368337154388428\n",
      "step = 3625000: Average Return = 3.3499999046325684\n",
      "step = 3625200: loss = 3.4765827655792236\n",
      "step = 3625400: loss = 5.5657639503479\n",
      "step = 3625600: loss = 2.930302143096924\n",
      "step = 3625800: loss = 2.8811535835266113\n",
      "step = 3626000: loss = 4.998071670532227\n",
      "step = 3626200: loss = 2.799548625946045\n",
      "step = 3626400: loss = 3.5086605548858643\n",
      "step = 3626600: loss = 2.8671481609344482\n",
      "step = 3626800: loss = 3.481130599975586\n",
      "step = 3627000: loss = 2.472766876220703\n",
      "step = 3627200: loss = 4.196938514709473\n",
      "step = 3627400: loss = 3.7727184295654297\n",
      "step = 3627600: loss = 3.5791943073272705\n",
      "step = 3627800: loss = 3.76847505569458\n",
      "step = 3628000: loss = 2.4089105129241943\n",
      "step = 3628200: loss = 2.17289400100708\n",
      "step = 3628400: loss = 3.7072629928588867\n",
      "step = 3628600: loss = 3.2714130878448486\n",
      "step = 3628800: loss = 2.8980205059051514\n",
      "step = 3629000: loss = 3.146860361099243\n",
      "step = 3629200: loss = 3.9387922286987305\n",
      "step = 3629400: loss = 3.787832021713257\n",
      "step = 3629600: loss = 2.007601261138916\n",
      "step = 3629800: loss = 1.9574320316314697\n",
      "step = 3630000: loss = 3.202040910720825\n",
      "step = 3630000: Average Return = 3.1500000953674316\n",
      "step = 3630200: loss = 3.6859283447265625\n",
      "step = 3630400: loss = 2.90708065032959\n",
      "step = 3630600: loss = 2.90328049659729\n",
      "step = 3630800: loss = 2.691053628921509\n",
      "step = 3631000: loss = 5.541704177856445\n",
      "step = 3631200: loss = 3.1064653396606445\n",
      "step = 3631400: loss = 3.9814937114715576\n",
      "step = 3631600: loss = 3.2861852645874023\n",
      "step = 3631800: loss = 4.177821636199951\n",
      "step = 3632000: loss = 2.4405648708343506\n",
      "step = 3632200: loss = 3.8056275844573975\n",
      "step = 3632400: loss = 5.50039005279541\n",
      "step = 3632600: loss = 3.9529571533203125\n",
      "step = 3632800: loss = 2.3481907844543457\n",
      "step = 3633000: loss = 3.1179487705230713\n",
      "step = 3633200: loss = 3.9284074306488037\n",
      "step = 3633400: loss = 3.895548105239868\n",
      "step = 3633600: loss = 2.5917041301727295\n",
      "step = 3633800: loss = 2.6459949016571045\n",
      "step = 3634000: loss = 2.690631151199341\n",
      "step = 3634200: loss = 4.375240325927734\n",
      "step = 3634400: loss = 4.8883466720581055\n",
      "step = 3634600: loss = 3.050008535385132\n",
      "step = 3634800: loss = 3.4477124214172363\n",
      "step = 3635000: loss = 4.805985450744629\n",
      "step = 3635000: Average Return = 5.050000190734863\n",
      "step = 3635200: loss = 3.7312817573547363\n",
      "step = 3635400: loss = 3.754509687423706\n",
      "step = 3635600: loss = 3.288177728652954\n",
      "step = 3635800: loss = 3.9619085788726807\n",
      "step = 3636000: loss = 4.023583889007568\n",
      "step = 3636200: loss = 3.080577850341797\n",
      "step = 3636400: loss = 5.461894512176514\n",
      "step = 3636600: loss = 3.451678991317749\n",
      "step = 3636800: loss = 3.202310085296631\n",
      "step = 3637000: loss = 3.641801118850708\n",
      "step = 3637200: loss = 2.7888259887695312\n",
      "step = 3637400: loss = 2.3651351928710938\n",
      "step = 3637600: loss = 2.819500684738159\n",
      "step = 3637800: loss = 2.234145402908325\n",
      "step = 3638000: loss = 4.087290287017822\n",
      "step = 3638200: loss = 3.11167049407959\n",
      "step = 3638400: loss = 2.422750473022461\n",
      "step = 3638600: loss = 3.228419065475464\n",
      "step = 3638800: loss = 4.333286762237549\n",
      "step = 3639000: loss = 4.815443515777588\n",
      "step = 3639200: loss = 2.778170108795166\n",
      "step = 3639400: loss = 3.8525445461273193\n",
      "step = 3639600: loss = 4.17747688293457\n",
      "step = 3639800: loss = 3.8345141410827637\n",
      "step = 3640000: loss = 3.752358913421631\n",
      "step = 3640000: Average Return = 3.6500000953674316\n",
      "step = 3640200: loss = 2.726155996322632\n",
      "step = 3640400: loss = 2.4634451866149902\n",
      "step = 3640600: loss = 3.7239785194396973\n",
      "step = 3640800: loss = 3.112682342529297\n",
      "step = 3641000: loss = 1.9943172931671143\n",
      "step = 3641200: loss = 4.5709052085876465\n",
      "step = 3641400: loss = 3.371058464050293\n",
      "step = 3641600: loss = 4.2950310707092285\n",
      "step = 3641800: loss = 3.808115243911743\n",
      "step = 3642000: loss = 2.854398727416992\n",
      "step = 3642200: loss = 4.079751968383789\n",
      "step = 3642400: loss = 2.668471574783325\n",
      "step = 3642600: loss = 3.6813926696777344\n",
      "step = 3642800: loss = 3.679236888885498\n",
      "step = 3643000: loss = 2.5649328231811523\n",
      "step = 3643200: loss = 3.342252492904663\n",
      "step = 3643400: loss = 3.6539790630340576\n",
      "step = 3643600: loss = 2.3151779174804688\n",
      "step = 3643800: loss = 5.301425457000732\n",
      "step = 3644000: loss = 4.7296905517578125\n",
      "step = 3644200: loss = 3.5867466926574707\n",
      "step = 3644400: loss = 3.4723196029663086\n",
      "step = 3644600: loss = 3.769808769226074\n",
      "step = 3644800: loss = 3.272778272628784\n",
      "step = 3645000: loss = 3.111884355545044\n",
      "step = 3645000: Average Return = 5.25\n",
      "step = 3645200: loss = 4.9474639892578125\n",
      "step = 3645400: loss = 2.716182231903076\n",
      "step = 3645600: loss = 4.658924102783203\n",
      "step = 3645800: loss = 3.92179799079895\n",
      "step = 3646000: loss = 2.788771390914917\n",
      "step = 3646200: loss = 2.2675530910491943\n",
      "step = 3646400: loss = 3.3813085556030273\n",
      "step = 3646600: loss = 3.304694652557373\n",
      "step = 3646800: loss = 2.832336902618408\n",
      "step = 3647000: loss = 2.8303446769714355\n",
      "step = 3647200: loss = 3.9982311725616455\n",
      "step = 3647400: loss = 3.8939802646636963\n",
      "step = 3647600: loss = 4.420057773590088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3647800: loss = 2.362133741378784\n",
      "step = 3648000: loss = 3.2859086990356445\n",
      "step = 3648200: loss = 3.70816707611084\n",
      "step = 3648400: loss = 3.088719367980957\n",
      "step = 3648600: loss = 2.7488460540771484\n",
      "step = 3648800: loss = 2.7402873039245605\n",
      "step = 3649000: loss = 3.2958438396453857\n",
      "step = 3649200: loss = 3.463226795196533\n",
      "step = 3649400: loss = 3.407548666000366\n",
      "step = 3649600: loss = 4.125080585479736\n",
      "step = 3649800: loss = 3.512789726257324\n",
      "step = 3650000: loss = 2.6286098957061768\n",
      "step = 3650000: Average Return = 6.099999904632568\n",
      "step = 3650200: loss = 2.8266453742980957\n",
      "step = 3650400: loss = 3.4758946895599365\n",
      "step = 3650600: loss = 2.8993256092071533\n",
      "step = 3650800: loss = 4.0609660148620605\n",
      "step = 3651000: loss = 2.73238468170166\n",
      "step = 3651200: loss = 3.0377776622772217\n",
      "step = 3651400: loss = 4.341207027435303\n",
      "step = 3651600: loss = 3.534048080444336\n",
      "step = 3651800: loss = 2.7369680404663086\n",
      "step = 3652000: loss = 2.764066219329834\n",
      "step = 3652200: loss = 3.300623655319214\n",
      "step = 3652400: loss = 4.688362121582031\n",
      "step = 3652600: loss = 2.488682746887207\n",
      "step = 3652800: loss = 3.310007333755493\n",
      "step = 3653000: loss = 3.54171085357666\n",
      "step = 3653200: loss = 2.4890620708465576\n",
      "step = 3653400: loss = 3.64324951171875\n",
      "step = 3653600: loss = 3.8169267177581787\n",
      "step = 3653800: loss = 5.038461208343506\n",
      "step = 3654000: loss = 3.3036131858825684\n",
      "step = 3654200: loss = 3.3419389724731445\n",
      "step = 3654400: loss = 3.8518779277801514\n",
      "step = 3654600: loss = 3.264021158218384\n",
      "step = 3654800: loss = 5.303647518157959\n",
      "step = 3655000: loss = 3.5908994674682617\n",
      "step = 3655000: Average Return = 3.299999952316284\n",
      "step = 3655200: loss = 3.7545387744903564\n",
      "step = 3655400: loss = 4.430145263671875\n",
      "step = 3655600: loss = 4.4252729415893555\n",
      "step = 3655800: loss = 2.794419527053833\n",
      "step = 3656000: loss = 2.8365185260772705\n",
      "step = 3656200: loss = 3.34384822845459\n",
      "step = 3656400: loss = 3.5685484409332275\n",
      "step = 3656600: loss = 3.1345508098602295\n",
      "step = 3656800: loss = 3.973700523376465\n",
      "step = 3657000: loss = 3.1132829189300537\n",
      "step = 3657200: loss = 4.474376678466797\n",
      "step = 3657400: loss = 4.299107074737549\n",
      "step = 3657600: loss = 3.6347103118896484\n",
      "step = 3657800: loss = 2.897235155105591\n",
      "step = 3658000: loss = 3.6512043476104736\n",
      "step = 3658200: loss = 3.132232189178467\n",
      "step = 3658400: loss = 3.7401483058929443\n",
      "step = 3658600: loss = 4.221489429473877\n",
      "step = 3658800: loss = 2.600663661956787\n",
      "step = 3659000: loss = 3.1537468433380127\n",
      "step = 3659200: loss = 4.46891450881958\n",
      "step = 3659400: loss = 3.21970272064209\n",
      "step = 3659600: loss = 2.998161792755127\n",
      "step = 3659800: loss = 2.43336820602417\n",
      "step = 3660000: loss = 3.804469585418701\n",
      "step = 3660000: Average Return = 4.0\n",
      "step = 3660200: loss = 4.274350643157959\n",
      "step = 3660400: loss = 3.829252243041992\n",
      "step = 3660600: loss = 3.239201545715332\n",
      "step = 3660800: loss = 3.391496419906616\n",
      "step = 3661000: loss = 3.9583041667938232\n",
      "step = 3661200: loss = 2.996551990509033\n",
      "step = 3661400: loss = 3.094757318496704\n",
      "step = 3661600: loss = 2.355644464492798\n",
      "step = 3661800: loss = 3.8947112560272217\n",
      "step = 3662000: loss = 3.1530730724334717\n",
      "step = 3662200: loss = 2.42350435256958\n",
      "step = 3662400: loss = 3.305645704269409\n",
      "step = 3662600: loss = 3.0360331535339355\n",
      "step = 3662800: loss = 3.767390727996826\n",
      "step = 3663000: loss = 2.86971378326416\n",
      "step = 3663200: loss = 3.048923969268799\n",
      "step = 3663400: loss = 3.062504291534424\n",
      "step = 3663600: loss = 3.4189724922180176\n",
      "step = 3663800: loss = 3.607880115509033\n",
      "step = 3664000: loss = 3.6627094745635986\n",
      "step = 3664200: loss = 2.6695032119750977\n",
      "step = 3664400: loss = 3.2050530910491943\n",
      "step = 3664600: loss = 3.0131616592407227\n",
      "step = 3664800: loss = 3.7559337615966797\n",
      "step = 3665000: loss = 3.9316442012786865\n",
      "step = 3665000: Average Return = 4.650000095367432\n",
      "step = 3665200: loss = 4.088226318359375\n",
      "step = 3665400: loss = 4.518945693969727\n",
      "step = 3665600: loss = 2.9387013912200928\n",
      "step = 3665800: loss = 3.439988374710083\n",
      "step = 3666000: loss = 4.356009006500244\n",
      "step = 3666200: loss = 3.289538860321045\n",
      "step = 3666400: loss = 4.002320289611816\n",
      "step = 3666600: loss = 3.7274670600891113\n",
      "step = 3666800: loss = 3.703512191772461\n",
      "step = 3667000: loss = 4.0665764808654785\n",
      "step = 3667200: loss = 3.725416898727417\n",
      "step = 3667400: loss = 2.755173921585083\n",
      "step = 3667600: loss = 4.429884433746338\n",
      "step = 3667800: loss = 4.229280471801758\n",
      "step = 3668000: loss = 2.890352487564087\n",
      "step = 3668200: loss = 4.036606788635254\n",
      "step = 3668400: loss = 3.5103280544281006\n",
      "step = 3668600: loss = 3.886411428451538\n",
      "step = 3668800: loss = 3.23958420753479\n",
      "step = 3669000: loss = 4.711009502410889\n",
      "step = 3669200: loss = 4.2774338722229\n",
      "step = 3669400: loss = 4.159128189086914\n",
      "step = 3669600: loss = 2.7782139778137207\n",
      "step = 3669800: loss = 3.6570885181427\n",
      "step = 3670000: loss = 3.162813663482666\n",
      "step = 3670000: Average Return = 3.0\n",
      "step = 3670200: loss = 4.321910858154297\n",
      "step = 3670400: loss = 4.511370658874512\n",
      "step = 3670600: loss = 3.4371440410614014\n",
      "step = 3670800: loss = 4.085186004638672\n",
      "step = 3671000: loss = 3.9992473125457764\n",
      "step = 3671200: loss = 4.418869972229004\n",
      "step = 3671400: loss = 4.329667091369629\n",
      "step = 3671600: loss = 3.1359047889709473\n",
      "step = 3671800: loss = 3.7825818061828613\n",
      "step = 3672000: loss = 3.5038511753082275\n",
      "step = 3672200: loss = 3.4680583477020264\n",
      "step = 3672400: loss = 2.8890044689178467\n",
      "step = 3672600: loss = 4.311932563781738\n",
      "step = 3672800: loss = 3.7958743572235107\n",
      "step = 3673000: loss = 2.9315056800842285\n",
      "step = 3673200: loss = 2.818298578262329\n",
      "step = 3673400: loss = 3.494425058364868\n",
      "step = 3673600: loss = 4.818362712860107\n",
      "step = 3673800: loss = 3.6989409923553467\n",
      "step = 3674000: loss = 3.1440694332122803\n",
      "step = 3674200: loss = 3.74686861038208\n",
      "step = 3674400: loss = 4.337705135345459\n",
      "step = 3674600: loss = 2.393277168273926\n",
      "step = 3674800: loss = 2.962040901184082\n",
      "step = 3675000: loss = 4.4305877685546875\n",
      "step = 3675000: Average Return = 3.5999999046325684\n",
      "step = 3675200: loss = 3.4354984760284424\n",
      "step = 3675400: loss = 4.539910316467285\n",
      "step = 3675600: loss = 4.8651957511901855\n",
      "step = 3675800: loss = 3.6066203117370605\n",
      "step = 3676000: loss = 4.441011428833008\n",
      "step = 3676200: loss = 2.5595030784606934\n",
      "step = 3676400: loss = 3.6431047916412354\n",
      "step = 3676600: loss = 2.8337881565093994\n",
      "step = 3676800: loss = 3.4441769123077393\n",
      "step = 3677000: loss = 3.6524548530578613\n",
      "step = 3677200: loss = 3.8230552673339844\n",
      "step = 3677400: loss = 4.823080062866211\n",
      "step = 3677600: loss = 3.5781426429748535\n",
      "step = 3677800: loss = 4.5259928703308105\n",
      "step = 3678000: loss = 4.702763557434082\n",
      "step = 3678200: loss = 3.9253904819488525\n",
      "step = 3678400: loss = 4.725721836090088\n",
      "step = 3678600: loss = 2.5700323581695557\n",
      "step = 3678800: loss = 3.791579008102417\n",
      "step = 3679000: loss = 3.831510305404663\n",
      "step = 3679200: loss = 4.633342742919922\n",
      "step = 3679400: loss = 4.300465106964111\n",
      "step = 3679600: loss = 3.2706828117370605\n",
      "step = 3679800: loss = 4.822629928588867\n",
      "step = 3680000: loss = 3.2110910415649414\n",
      "step = 3680000: Average Return = 3.5\n",
      "step = 3680200: loss = 4.352035045623779\n",
      "step = 3680400: loss = 4.547066688537598\n",
      "step = 3680600: loss = 4.096804618835449\n",
      "step = 3680800: loss = 4.631929397583008\n",
      "step = 3681000: loss = 3.547833204269409\n",
      "step = 3681200: loss = 4.460141658782959\n",
      "step = 3681400: loss = 4.756234645843506\n",
      "step = 3681600: loss = 4.047303199768066\n",
      "step = 3681800: loss = 3.276805877685547\n",
      "step = 3682000: loss = 3.8959197998046875\n",
      "step = 3682200: loss = 4.571154594421387\n",
      "step = 3682400: loss = 2.8819265365600586\n",
      "step = 3682600: loss = 3.4176559448242188\n",
      "step = 3682800: loss = 2.967132091522217\n",
      "step = 3683000: loss = 3.2096545696258545\n",
      "step = 3683200: loss = 4.207699298858643\n",
      "step = 3683400: loss = 2.79919695854187\n",
      "step = 3683600: loss = 5.295456409454346\n",
      "step = 3683800: loss = 5.070413589477539\n",
      "step = 3684000: loss = 3.6860594749450684\n",
      "step = 3684200: loss = 3.0603342056274414\n",
      "step = 3684400: loss = 4.07011604309082\n",
      "step = 3684600: loss = 3.639777183532715\n",
      "step = 3684800: loss = 4.671381950378418\n",
      "step = 3685000: loss = 2.2894294261932373\n",
      "step = 3685000: Average Return = 2.0\n",
      "step = 3685200: loss = 2.5051772594451904\n",
      "step = 3685400: loss = 3.80712628364563\n",
      "step = 3685600: loss = 4.765066623687744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3685800: loss = 3.1611955165863037\n",
      "step = 3686000: loss = 4.570422172546387\n",
      "step = 3686200: loss = 3.1965484619140625\n",
      "step = 3686400: loss = 2.974740505218506\n",
      "step = 3686600: loss = 2.944838762283325\n",
      "step = 3686800: loss = 2.862272024154663\n",
      "step = 3687000: loss = 2.3200223445892334\n",
      "step = 3687200: loss = 5.171318054199219\n",
      "step = 3687400: loss = 3.3974926471710205\n",
      "step = 3687600: loss = 3.4255151748657227\n",
      "step = 3687800: loss = 3.200570821762085\n",
      "step = 3688000: loss = 3.289844512939453\n",
      "step = 3688200: loss = 5.514200210571289\n",
      "step = 3688400: loss = 3.560537099838257\n",
      "step = 3688600: loss = 2.844874143600464\n",
      "step = 3688800: loss = 5.020442485809326\n",
      "step = 3689000: loss = 3.492079973220825\n",
      "step = 3689200: loss = 3.4324655532836914\n",
      "step = 3689400: loss = 4.820682048797607\n",
      "step = 3689600: loss = 3.347053050994873\n",
      "step = 3689800: loss = 2.884279489517212\n",
      "step = 3690000: loss = 3.916579008102417\n",
      "step = 3690000: Average Return = 4.849999904632568\n",
      "step = 3690200: loss = 3.9017608165740967\n",
      "step = 3690400: loss = 3.100269079208374\n",
      "step = 3690600: loss = 3.9574620723724365\n",
      "step = 3690800: loss = 4.13013219833374\n",
      "step = 3691000: loss = 4.304290294647217\n",
      "step = 3691200: loss = 3.568190813064575\n",
      "step = 3691400: loss = 3.1737325191497803\n",
      "step = 3691600: loss = 3.214519500732422\n",
      "step = 3691800: loss = 2.6109445095062256\n",
      "step = 3692000: loss = 2.9461488723754883\n",
      "step = 3692200: loss = 4.098266124725342\n",
      "step = 3692400: loss = 4.954890727996826\n",
      "step = 3692600: loss = 2.946115016937256\n",
      "step = 3692800: loss = 4.240832328796387\n",
      "step = 3693000: loss = 3.2696118354797363\n",
      "step = 3693200: loss = 3.7252421379089355\n",
      "step = 3693400: loss = 4.264190196990967\n",
      "step = 3693600: loss = 3.6211483478546143\n",
      "step = 3693800: loss = 2.286829710006714\n",
      "step = 3694000: loss = 3.1709954738616943\n",
      "step = 3694200: loss = 3.3392443656921387\n",
      "step = 3694400: loss = 3.861004590988159\n",
      "step = 3694600: loss = 3.251213550567627\n",
      "step = 3694800: loss = 2.9546351432800293\n",
      "step = 3695000: loss = 4.14599609375\n",
      "step = 3695000: Average Return = 3.700000047683716\n",
      "step = 3695200: loss = 4.735697269439697\n",
      "step = 3695400: loss = 3.1887569427490234\n",
      "step = 3695600: loss = 2.6874923706054688\n",
      "step = 3695800: loss = 2.7173144817352295\n",
      "step = 3696000: loss = 2.5647363662719727\n",
      "step = 3696200: loss = 2.182461738586426\n",
      "step = 3696400: loss = 3.377214193344116\n",
      "step = 3696600: loss = 3.9911816120147705\n",
      "step = 3696800: loss = 5.346465110778809\n",
      "step = 3697000: loss = 2.00191068649292\n",
      "step = 3697200: loss = 4.298678874969482\n",
      "step = 3697400: loss = 3.6993958950042725\n",
      "step = 3697600: loss = 2.8292016983032227\n",
      "step = 3697800: loss = 4.5323615074157715\n",
      "step = 3698000: loss = 2.533989906311035\n",
      "step = 3698200: loss = 3.793616533279419\n",
      "step = 3698400: loss = 4.323077201843262\n",
      "step = 3698600: loss = 3.599499464035034\n",
      "step = 3698800: loss = 5.716645240783691\n",
      "step = 3699000: loss = 3.9248769283294678\n",
      "step = 3699200: loss = 3.7667863368988037\n",
      "step = 3699400: loss = 1.932271122932434\n",
      "step = 3699600: loss = 3.355290651321411\n",
      "step = 3699800: loss = 3.3939547538757324\n",
      "step = 3700000: loss = 2.861865758895874\n",
      "step = 3700000: Average Return = 5.25\n",
      "step = 3700200: loss = 3.8526487350463867\n",
      "step = 3700400: loss = 3.401337146759033\n",
      "step = 3700600: loss = 4.278994560241699\n",
      "step = 3700800: loss = 3.567012310028076\n",
      "step = 3701000: loss = 3.4595603942871094\n",
      "step = 3701200: loss = 3.1697542667388916\n",
      "step = 3701400: loss = 5.112213611602783\n",
      "step = 3701600: loss = 3.995826244354248\n",
      "step = 3701800: loss = 2.5683319568634033\n",
      "step = 3702000: loss = 3.64231014251709\n",
      "step = 3702200: loss = 4.937686443328857\n",
      "step = 3702400: loss = 3.8739428520202637\n",
      "step = 3702600: loss = 3.9970498085021973\n",
      "step = 3702800: loss = 3.2635204792022705\n",
      "step = 3703000: loss = 3.4659698009490967\n",
      "step = 3703200: loss = 3.904862642288208\n",
      "step = 3703400: loss = 2.62753963470459\n",
      "step = 3703600: loss = 3.09323787689209\n",
      "step = 3703800: loss = 3.849705457687378\n",
      "step = 3704000: loss = 2.983210325241089\n",
      "step = 3704200: loss = 3.914151430130005\n",
      "step = 3704400: loss = 3.127669334411621\n",
      "step = 3704600: loss = 3.65520977973938\n",
      "step = 3704800: loss = 3.003272533416748\n",
      "step = 3705000: loss = 4.016725540161133\n",
      "step = 3705000: Average Return = 2.549999952316284\n",
      "step = 3705200: loss = 3.8119616508483887\n",
      "step = 3705400: loss = 3.0652923583984375\n",
      "step = 3705600: loss = 3.2180261611938477\n",
      "step = 3705800: loss = 4.176261901855469\n",
      "step = 3706000: loss = 4.945415496826172\n",
      "step = 3706200: loss = 2.8741908073425293\n",
      "step = 3706400: loss = 3.8919291496276855\n",
      "step = 3706600: loss = 3.6659116744995117\n",
      "step = 3706800: loss = 2.576643228530884\n",
      "step = 3707000: loss = 3.561676263809204\n",
      "step = 3707200: loss = 3.4189391136169434\n",
      "step = 3707400: loss = 2.5191454887390137\n",
      "step = 3707600: loss = 1.9950017929077148\n",
      "step = 3707800: loss = 3.5113213062286377\n",
      "step = 3708000: loss = 3.5941624641418457\n",
      "step = 3708200: loss = 3.909985303878784\n",
      "step = 3708400: loss = 3.021890163421631\n",
      "step = 3708600: loss = 4.022815227508545\n",
      "step = 3708800: loss = 4.844575881958008\n",
      "step = 3709000: loss = 3.6734862327575684\n",
      "step = 3709200: loss = 3.7013604640960693\n",
      "step = 3709400: loss = 4.026766300201416\n",
      "step = 3709600: loss = 3.1723358631134033\n",
      "step = 3709800: loss = 2.7563116550445557\n",
      "step = 3710000: loss = 3.558150053024292\n",
      "step = 3710000: Average Return = 5.349999904632568\n",
      "step = 3710200: loss = 1.71195650100708\n",
      "step = 3710400: loss = 4.5166754722595215\n",
      "step = 3710600: loss = 3.214250326156616\n",
      "step = 3710800: loss = 3.37903094291687\n",
      "step = 3711000: loss = 2.8024542331695557\n",
      "step = 3711200: loss = 3.7815630435943604\n",
      "step = 3711400: loss = 3.268641233444214\n",
      "step = 3711600: loss = 4.248954772949219\n",
      "step = 3711800: loss = 3.8362603187561035\n",
      "step = 3712000: loss = 5.271500110626221\n",
      "step = 3712200: loss = 3.043104410171509\n",
      "step = 3712400: loss = 2.6888041496276855\n",
      "step = 3712600: loss = 4.053296089172363\n",
      "step = 3712800: loss = 3.229292154312134\n",
      "step = 3713000: loss = 3.245438814163208\n",
      "step = 3713200: loss = 3.510624647140503\n",
      "step = 3713400: loss = 3.4352433681488037\n",
      "step = 3713600: loss = 4.974149703979492\n",
      "step = 3713800: loss = 3.151507616043091\n",
      "step = 3714000: loss = 4.9958696365356445\n",
      "step = 3714200: loss = 2.823073148727417\n",
      "step = 3714400: loss = 3.97883939743042\n",
      "step = 3714600: loss = 4.093338489532471\n",
      "step = 3714800: loss = 2.6499693393707275\n",
      "step = 3715000: loss = 3.625056743621826\n",
      "step = 3715000: Average Return = 3.8499999046325684\n",
      "step = 3715200: loss = 3.834888219833374\n",
      "step = 3715400: loss = 4.243794918060303\n",
      "step = 3715600: loss = 5.383032321929932\n",
      "step = 3715800: loss = 3.3468432426452637\n",
      "step = 3716000: loss = 5.508443355560303\n",
      "step = 3716200: loss = 4.036790370941162\n",
      "step = 3716400: loss = 3.523247718811035\n",
      "step = 3716600: loss = 3.353947639465332\n",
      "step = 3716800: loss = 3.881704330444336\n",
      "step = 3717000: loss = 3.693552255630493\n",
      "step = 3717200: loss = 3.504899740219116\n",
      "step = 3717400: loss = 2.9146482944488525\n",
      "step = 3717600: loss = 2.933547258377075\n",
      "step = 3717800: loss = 3.3965861797332764\n",
      "step = 3718000: loss = 2.5000085830688477\n",
      "step = 3718200: loss = 2.8785805702209473\n",
      "step = 3718400: loss = 2.68937611579895\n",
      "step = 3718600: loss = 4.296802043914795\n",
      "step = 3718800: loss = 2.934612274169922\n",
      "step = 3719000: loss = 3.7187447547912598\n",
      "step = 3719200: loss = 4.20421028137207\n",
      "step = 3719400: loss = 4.1763787269592285\n",
      "step = 3719600: loss = 4.206831932067871\n",
      "step = 3719800: loss = 2.6480071544647217\n",
      "step = 3720000: loss = 3.5670104026794434\n",
      "step = 3720000: Average Return = 6.150000095367432\n",
      "step = 3720200: loss = 2.955249547958374\n",
      "step = 3720400: loss = 3.761080265045166\n",
      "step = 3720600: loss = 4.091408729553223\n",
      "step = 3720800: loss = 3.290837287902832\n",
      "step = 3721000: loss = 2.243489980697632\n",
      "step = 3721200: loss = 3.999586343765259\n",
      "step = 3721400: loss = 4.268618106842041\n",
      "step = 3721600: loss = 2.754635810852051\n",
      "step = 3721800: loss = 2.397953987121582\n",
      "step = 3722000: loss = 3.469470500946045\n",
      "step = 3722200: loss = 2.4186487197875977\n",
      "step = 3722400: loss = 3.301758050918579\n",
      "step = 3722600: loss = 3.159353256225586\n",
      "step = 3722800: loss = 2.4038565158843994\n",
      "step = 3723000: loss = 4.67869758605957\n",
      "step = 3723200: loss = 3.1356170177459717\n",
      "step = 3723400: loss = 2.739392042160034\n",
      "step = 3723600: loss = 2.4057695865631104\n",
      "step = 3723800: loss = 3.183218240737915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3724000: loss = 3.611384153366089\n",
      "step = 3724200: loss = 2.9514904022216797\n",
      "step = 3724400: loss = 4.106415748596191\n",
      "step = 3724600: loss = 3.2138118743896484\n",
      "step = 3724800: loss = 3.402151346206665\n",
      "step = 3725000: loss = 3.3898589611053467\n",
      "step = 3725000: Average Return = 4.25\n",
      "step = 3725200: loss = 3.4489669799804688\n",
      "step = 3725400: loss = 2.996715545654297\n",
      "step = 3725600: loss = 3.9606921672821045\n",
      "step = 3725800: loss = 3.4716625213623047\n",
      "step = 3726000: loss = 4.517755031585693\n",
      "step = 3726200: loss = 3.7145402431488037\n",
      "step = 3726400: loss = 2.8105013370513916\n",
      "step = 3726600: loss = 3.3325488567352295\n",
      "step = 3726800: loss = 3.3554887771606445\n",
      "step = 3727000: loss = 3.0366103649139404\n",
      "step = 3727200: loss = 4.633983135223389\n",
      "step = 3727400: loss = 3.830410957336426\n",
      "step = 3727600: loss = 4.208041191101074\n",
      "step = 3727800: loss = 4.039253234863281\n",
      "step = 3728000: loss = 3.2919256687164307\n",
      "step = 3728200: loss = 3.2544050216674805\n",
      "step = 3728400: loss = 4.287342548370361\n",
      "step = 3728600: loss = 3.228527784347534\n",
      "step = 3728800: loss = 3.5570573806762695\n",
      "step = 3729000: loss = 4.358967304229736\n",
      "step = 3729200: loss = 3.3595030307769775\n",
      "step = 3729400: loss = 2.9603042602539062\n",
      "step = 3729600: loss = 3.620522975921631\n",
      "step = 3729800: loss = 3.419217348098755\n",
      "step = 3730000: loss = 3.313636302947998\n",
      "step = 3730000: Average Return = 3.549999952316284\n",
      "step = 3730200: loss = 4.66892671585083\n",
      "step = 3730400: loss = 3.3071348667144775\n",
      "step = 3730600: loss = 4.501468658447266\n",
      "step = 3730800: loss = 3.34586238861084\n",
      "step = 3731000: loss = 1.9632991552352905\n",
      "step = 3731200: loss = 4.569148063659668\n",
      "step = 3731400: loss = 4.813902378082275\n",
      "step = 3731600: loss = 4.040518283843994\n",
      "step = 3731800: loss = 2.854691743850708\n",
      "step = 3732000: loss = 2.947620153427124\n",
      "step = 3732200: loss = 3.4782304763793945\n",
      "step = 3732400: loss = 3.684762716293335\n",
      "step = 3732600: loss = 2.971888303756714\n",
      "step = 3732800: loss = 3.494626998901367\n",
      "step = 3733000: loss = 2.498849630355835\n",
      "step = 3733200: loss = 2.1848340034484863\n",
      "step = 3733400: loss = 2.2707247734069824\n",
      "step = 3733600: loss = 3.3424432277679443\n",
      "step = 3733800: loss = 4.595290660858154\n",
      "step = 3734000: loss = 3.3264362812042236\n",
      "step = 3734200: loss = 2.3032784461975098\n",
      "step = 3734400: loss = 4.609180927276611\n",
      "step = 3734600: loss = 4.511374473571777\n",
      "step = 3734800: loss = 3.1667003631591797\n",
      "step = 3735000: loss = 3.458413600921631\n",
      "step = 3735000: Average Return = 4.400000095367432\n",
      "step = 3735200: loss = 2.498887777328491\n",
      "step = 3735400: loss = 2.8389859199523926\n",
      "step = 3735600: loss = 3.128605365753174\n",
      "step = 3735800: loss = 3.3379967212677\n",
      "step = 3736000: loss = 2.68757963180542\n",
      "step = 3736200: loss = 3.8070483207702637\n",
      "step = 3736400: loss = 3.231714963912964\n",
      "step = 3736600: loss = 3.091383695602417\n",
      "step = 3736800: loss = 3.7898406982421875\n",
      "step = 3737000: loss = 4.018080234527588\n",
      "step = 3737200: loss = 4.84670352935791\n",
      "step = 3737400: loss = 5.3843994140625\n",
      "step = 3737600: loss = 3.3216145038604736\n",
      "step = 3737800: loss = 4.020503997802734\n",
      "step = 3738000: loss = 3.480544328689575\n",
      "step = 3738200: loss = 3.18673038482666\n",
      "step = 3738400: loss = 3.85908579826355\n",
      "step = 3738600: loss = 3.77828049659729\n",
      "step = 3738800: loss = 3.865506887435913\n",
      "step = 3739000: loss = 3.268249988555908\n",
      "step = 3739200: loss = 3.5756523609161377\n",
      "step = 3739400: loss = 4.6148786544799805\n",
      "step = 3739600: loss = 3.1615190505981445\n",
      "step = 3739800: loss = 4.39763069152832\n",
      "step = 3740000: loss = 3.2939867973327637\n",
      "step = 3740000: Average Return = 3.200000047683716\n",
      "step = 3740200: loss = 3.7897918224334717\n",
      "step = 3740400: loss = 2.9306530952453613\n",
      "step = 3740600: loss = 2.7592058181762695\n",
      "step = 3740800: loss = 3.707109212875366\n",
      "step = 3741000: loss = 3.292020082473755\n",
      "step = 3741200: loss = 3.654346227645874\n",
      "step = 3741400: loss = 3.1834187507629395\n",
      "step = 3741600: loss = 3.784144401550293\n",
      "step = 3741800: loss = 2.165461778640747\n",
      "step = 3742000: loss = 4.241425514221191\n",
      "step = 3742200: loss = 4.943454265594482\n",
      "step = 3742400: loss = 2.510453224182129\n",
      "step = 3742600: loss = 3.8615527153015137\n",
      "step = 3742800: loss = 4.50593900680542\n",
      "step = 3743000: loss = 5.005062103271484\n",
      "step = 3743200: loss = 2.822387933731079\n",
      "step = 3743400: loss = 3.5997226238250732\n",
      "step = 3743600: loss = 3.6812427043914795\n",
      "step = 3743800: loss = 2.0408272743225098\n",
      "step = 3744000: loss = 4.0873703956604\n",
      "step = 3744200: loss = 2.930868148803711\n",
      "step = 3744400: loss = 2.580739736557007\n",
      "step = 3744600: loss = 2.6673076152801514\n",
      "step = 3744800: loss = 3.375335693359375\n",
      "step = 3745000: loss = 3.6547906398773193\n",
      "step = 3745000: Average Return = 4.650000095367432\n",
      "step = 3745200: loss = 2.777905225753784\n",
      "step = 3745400: loss = 3.3211681842803955\n",
      "step = 3745600: loss = 3.6122381687164307\n",
      "step = 3745800: loss = 2.9790902137756348\n",
      "step = 3746000: loss = 4.153009414672852\n",
      "step = 3746200: loss = 1.9239202737808228\n",
      "step = 3746400: loss = 3.492926597595215\n",
      "step = 3746600: loss = 2.516329050064087\n",
      "step = 3746800: loss = 3.123762845993042\n",
      "step = 3747000: loss = 3.038578510284424\n",
      "step = 3747200: loss = 4.838734149932861\n",
      "step = 3747400: loss = 2.9330852031707764\n",
      "step = 3747600: loss = 3.770949602127075\n",
      "step = 3747800: loss = 3.439842462539673\n",
      "step = 3748000: loss = 3.510258436203003\n",
      "step = 3748200: loss = 3.363922595977783\n",
      "step = 3748400: loss = 3.5419933795928955\n",
      "step = 3748600: loss = 4.817994117736816\n",
      "step = 3748800: loss = 3.9671308994293213\n",
      "step = 3749000: loss = 2.68355655670166\n",
      "step = 3749200: loss = 3.4578664302825928\n",
      "step = 3749400: loss = 2.792368173599243\n",
      "step = 3749600: loss = 3.714120388031006\n",
      "step = 3749800: loss = 3.0258846282958984\n",
      "step = 3750000: loss = 4.1238884925842285\n",
      "step = 3750000: Average Return = 5.199999809265137\n",
      "step = 3750200: loss = 3.9418323040008545\n",
      "step = 3750400: loss = 3.2465922832489014\n",
      "step = 3750600: loss = 3.8352317810058594\n",
      "step = 3750800: loss = 4.510530471801758\n",
      "step = 3751000: loss = 3.2470178604125977\n",
      "step = 3751200: loss = 2.4649658203125\n",
      "step = 3751400: loss = 3.014718770980835\n",
      "step = 3751600: loss = 3.630295515060425\n",
      "step = 3751800: loss = 2.530425548553467\n",
      "step = 3752000: loss = 3.6229662895202637\n",
      "step = 3752200: loss = 3.0942726135253906\n",
      "step = 3752400: loss = 2.685358762741089\n",
      "step = 3752600: loss = 4.2611613273620605\n",
      "step = 3752800: loss = 2.5818095207214355\n",
      "step = 3753000: loss = 2.6503167152404785\n",
      "step = 3753200: loss = 2.8521666526794434\n",
      "step = 3753400: loss = 4.569141387939453\n",
      "step = 3753600: loss = 3.824754476547241\n",
      "step = 3753800: loss = 3.9642696380615234\n",
      "step = 3754000: loss = 3.874992609024048\n",
      "step = 3754200: loss = 3.6254067420959473\n",
      "step = 3754400: loss = 3.343255043029785\n",
      "step = 3754600: loss = 2.6315267086029053\n",
      "step = 3754800: loss = 2.6234219074249268\n",
      "step = 3755000: loss = 3.849233865737915\n",
      "step = 3755000: Average Return = 5.5\n",
      "step = 3755200: loss = 4.521345615386963\n",
      "step = 3755400: loss = 2.631673812866211\n",
      "step = 3755600: loss = 3.7050564289093018\n",
      "step = 3755800: loss = 4.895562648773193\n",
      "step = 3756000: loss = 4.007431983947754\n",
      "step = 3756200: loss = 3.04966402053833\n",
      "step = 3756400: loss = 3.6749565601348877\n",
      "step = 3756600: loss = 4.412698745727539\n",
      "step = 3756800: loss = 2.646149158477783\n",
      "step = 3757000: loss = 4.308483123779297\n",
      "step = 3757200: loss = 2.909581184387207\n",
      "step = 3757400: loss = 4.171324729919434\n",
      "step = 3757600: loss = 2.787166118621826\n",
      "step = 3757800: loss = 3.697582244873047\n",
      "step = 3758000: loss = 2.615443706512451\n",
      "step = 3758200: loss = 2.9055657386779785\n",
      "step = 3758400: loss = 3.1629443168640137\n",
      "step = 3758600: loss = 3.120864152908325\n",
      "step = 3758800: loss = 4.195976257324219\n",
      "step = 3759000: loss = 3.0086424350738525\n",
      "step = 3759200: loss = 3.2223052978515625\n",
      "step = 3759400: loss = 2.9687230587005615\n",
      "step = 3759600: loss = 3.1955296993255615\n",
      "step = 3759800: loss = 2.1306416988372803\n",
      "step = 3760000: loss = 2.900078773498535\n",
      "step = 3760000: Average Return = 4.900000095367432\n",
      "step = 3760200: loss = 3.7979118824005127\n",
      "step = 3760400: loss = 3.6536943912506104\n",
      "step = 3760600: loss = 2.689375638961792\n",
      "step = 3760800: loss = 4.7242112159729\n",
      "step = 3761000: loss = 3.1336312294006348\n",
      "step = 3761200: loss = 2.6353182792663574\n",
      "step = 3761400: loss = 3.724184513092041\n",
      "step = 3761600: loss = 3.1309051513671875\n",
      "step = 3761800: loss = 2.944126605987549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3762000: loss = 4.1406073570251465\n",
      "step = 3762200: loss = 3.3605308532714844\n",
      "step = 3762400: loss = 5.121596336364746\n",
      "step = 3762600: loss = 3.0508928298950195\n",
      "step = 3762800: loss = 3.286011219024658\n",
      "step = 3763000: loss = 2.0337021350860596\n",
      "step = 3763200: loss = 4.5180559158325195\n",
      "step = 3763400: loss = 3.207272529602051\n",
      "step = 3763600: loss = 3.9964277744293213\n",
      "step = 3763800: loss = 3.099693775177002\n",
      "step = 3764000: loss = 2.9820873737335205\n",
      "step = 3764200: loss = 4.314655780792236\n",
      "step = 3764400: loss = 3.0057320594787598\n",
      "step = 3764600: loss = 2.9106128215789795\n",
      "step = 3764800: loss = 3.3742733001708984\n",
      "step = 3765000: loss = 4.198757171630859\n",
      "step = 3765000: Average Return = 5.449999809265137\n",
      "step = 3765200: loss = 4.300223350524902\n",
      "step = 3765400: loss = 4.332664489746094\n",
      "step = 3765600: loss = 2.7302191257476807\n",
      "step = 3765800: loss = 3.087137460708618\n",
      "step = 3766000: loss = 5.823610782623291\n",
      "step = 3766200: loss = 3.8988213539123535\n",
      "step = 3766400: loss = 3.1856791973114014\n",
      "step = 3766600: loss = 2.7773725986480713\n",
      "step = 3766800: loss = 3.6935911178588867\n",
      "step = 3767000: loss = 2.523219347000122\n",
      "step = 3767200: loss = 2.5615005493164062\n",
      "step = 3767400: loss = 4.659461975097656\n",
      "step = 3767600: loss = 3.9729771614074707\n",
      "step = 3767800: loss = 4.063488960266113\n",
      "step = 3768000: loss = 3.3431243896484375\n",
      "step = 3768200: loss = 3.8324882984161377\n",
      "step = 3768400: loss = 2.877988815307617\n",
      "step = 3768600: loss = 2.6095547676086426\n",
      "step = 3768800: loss = 3.960174083709717\n",
      "step = 3769000: loss = 5.788487434387207\n",
      "step = 3769200: loss = 5.992157936096191\n",
      "step = 3769400: loss = 2.8593480587005615\n",
      "step = 3769600: loss = 3.524033784866333\n",
      "step = 3769800: loss = 3.455019474029541\n",
      "step = 3770000: loss = 3.758800506591797\n",
      "step = 3770000: Average Return = 3.950000047683716\n",
      "step = 3770200: loss = 3.524146318435669\n",
      "step = 3770400: loss = 5.780649185180664\n",
      "step = 3770600: loss = 2.61366605758667\n",
      "step = 3770800: loss = 3.1783556938171387\n",
      "step = 3771000: loss = 4.1412763595581055\n",
      "step = 3771200: loss = 3.999709129333496\n",
      "step = 3771400: loss = 3.294097900390625\n",
      "step = 3771600: loss = 3.9868807792663574\n",
      "step = 3771800: loss = 3.437264919281006\n",
      "step = 3772000: loss = 4.703517436981201\n",
      "step = 3772200: loss = 2.1189489364624023\n",
      "step = 3772400: loss = 4.2006144523620605\n",
      "step = 3772600: loss = 3.244094133377075\n",
      "step = 3772800: loss = 5.3993940353393555\n",
      "step = 3773000: loss = 4.391635417938232\n",
      "step = 3773200: loss = 4.11204195022583\n",
      "step = 3773400: loss = 5.20172119140625\n",
      "step = 3773600: loss = 4.629184722900391\n",
      "step = 3773800: loss = 1.9478646516799927\n",
      "step = 3774000: loss = 3.2724952697753906\n",
      "step = 3774200: loss = 2.41652250289917\n",
      "step = 3774400: loss = 4.25598669052124\n",
      "step = 3774600: loss = 3.2206640243530273\n",
      "step = 3774800: loss = 1.9617252349853516\n",
      "step = 3775000: loss = 3.7206339836120605\n",
      "step = 3775000: Average Return = 4.5\n",
      "step = 3775200: loss = 3.5928828716278076\n",
      "step = 3775400: loss = 4.670051574707031\n",
      "step = 3775600: loss = 4.006669044494629\n",
      "step = 3775800: loss = 3.20025897026062\n",
      "step = 3776000: loss = 4.319784641265869\n",
      "step = 3776200: loss = 3.249601125717163\n",
      "step = 3776400: loss = 4.446714401245117\n",
      "step = 3776600: loss = 3.4554905891418457\n",
      "step = 3776800: loss = 4.002073764801025\n",
      "step = 3777000: loss = 2.750863552093506\n",
      "step = 3777200: loss = 3.0391621589660645\n",
      "step = 3777400: loss = 3.942458391189575\n",
      "step = 3777600: loss = 4.0993571281433105\n",
      "step = 3777800: loss = 4.631803512573242\n",
      "step = 3778000: loss = 3.504706859588623\n",
      "step = 3778200: loss = 4.285104274749756\n",
      "step = 3778400: loss = 3.9143834114074707\n",
      "step = 3778600: loss = 3.7641990184783936\n",
      "step = 3778800: loss = 2.7383456230163574\n",
      "step = 3779000: loss = 3.048640012741089\n",
      "step = 3779200: loss = 2.5753378868103027\n",
      "step = 3779400: loss = 3.8961760997772217\n",
      "step = 3779600: loss = 3.7137105464935303\n",
      "step = 3779800: loss = 2.5327882766723633\n",
      "step = 3780000: loss = 2.7549469470977783\n",
      "step = 3780000: Average Return = 3.9000000953674316\n",
      "step = 3780200: loss = 3.066889524459839\n",
      "step = 3780400: loss = 2.7306926250457764\n",
      "step = 3780600: loss = 3.384772777557373\n",
      "step = 3780800: loss = 3.9732978343963623\n",
      "step = 3781000: loss = 5.168803691864014\n",
      "step = 3781200: loss = 2.58139967918396\n",
      "step = 3781400: loss = 4.368874549865723\n",
      "step = 3781600: loss = 3.301192283630371\n",
      "step = 3781800: loss = 3.116931915283203\n",
      "step = 3782000: loss = 3.6145505905151367\n",
      "step = 3782200: loss = 2.8967912197113037\n",
      "step = 3782400: loss = 4.640986919403076\n",
      "step = 3782600: loss = 4.995748996734619\n",
      "step = 3782800: loss = 2.686553955078125\n",
      "step = 3783000: loss = 2.693648099899292\n",
      "step = 3783200: loss = 3.710968017578125\n",
      "step = 3783400: loss = 3.2023961544036865\n",
      "step = 3783600: loss = 3.598294258117676\n",
      "step = 3783800: loss = 3.92073392868042\n",
      "step = 3784000: loss = 4.060031414031982\n",
      "step = 3784200: loss = 3.01297926902771\n",
      "step = 3784400: loss = 3.3101396560668945\n",
      "step = 3784600: loss = 3.7775213718414307\n",
      "step = 3784800: loss = 4.363575458526611\n",
      "step = 3785000: loss = 3.2577433586120605\n",
      "step = 3785000: Average Return = 3.9000000953674316\n",
      "step = 3785200: loss = 2.939225435256958\n",
      "step = 3785400: loss = 4.338926315307617\n",
      "step = 3785600: loss = 4.273245334625244\n",
      "step = 3785800: loss = 2.4586596488952637\n",
      "step = 3786000: loss = 3.141376256942749\n",
      "step = 3786200: loss = 4.160308361053467\n",
      "step = 3786400: loss = 3.6114559173583984\n",
      "step = 3786600: loss = 3.5983242988586426\n",
      "step = 3786800: loss = 3.469203472137451\n",
      "step = 3787000: loss = 3.206109046936035\n",
      "step = 3787200: loss = 4.561923027038574\n",
      "step = 3787400: loss = 3.0300891399383545\n",
      "step = 3787600: loss = 2.5022475719451904\n",
      "step = 3787800: loss = 3.9468257427215576\n",
      "step = 3788000: loss = 3.572073221206665\n",
      "step = 3788200: loss = 3.868098258972168\n",
      "step = 3788400: loss = 3.7785229682922363\n",
      "step = 3788600: loss = 3.1547882556915283\n",
      "step = 3788800: loss = 3.849841833114624\n",
      "step = 3789000: loss = 3.227318286895752\n",
      "step = 3789200: loss = 2.717133045196533\n",
      "step = 3789400: loss = 3.9340388774871826\n",
      "step = 3789600: loss = 2.943531036376953\n",
      "step = 3789800: loss = 4.026088714599609\n",
      "step = 3790000: loss = 2.4060986042022705\n",
      "step = 3790000: Average Return = 5.050000190734863\n",
      "step = 3790200: loss = 3.04152774810791\n",
      "step = 3790400: loss = 4.825857162475586\n",
      "step = 3790600: loss = 3.574448823928833\n",
      "step = 3790800: loss = 4.213562488555908\n",
      "step = 3791000: loss = 3.672839879989624\n",
      "step = 3791200: loss = 4.137177467346191\n",
      "step = 3791400: loss = 3.718599796295166\n",
      "step = 3791600: loss = 4.440621376037598\n",
      "step = 3791800: loss = 3.427856683731079\n",
      "step = 3792000: loss = 3.1030657291412354\n",
      "step = 3792200: loss = 2.8160479068756104\n",
      "step = 3792400: loss = 3.093906879425049\n",
      "step = 3792600: loss = 4.431252479553223\n",
      "step = 3792800: loss = 4.63493013381958\n",
      "step = 3793000: loss = 2.6020758152008057\n",
      "step = 3793200: loss = 4.131674289703369\n",
      "step = 3793400: loss = 11.409600257873535\n",
      "step = 3793600: loss = 3.1029014587402344\n",
      "step = 3793800: loss = 3.718797445297241\n",
      "step = 3794000: loss = 3.6552305221557617\n",
      "step = 3794200: loss = 3.4958648681640625\n",
      "step = 3794400: loss = 3.534961462020874\n",
      "step = 3794600: loss = 3.0142877101898193\n",
      "step = 3794800: loss = 4.126980304718018\n",
      "step = 3795000: loss = 3.5262064933776855\n",
      "step = 3795000: Average Return = 5.75\n",
      "step = 3795200: loss = 3.91548752784729\n",
      "step = 3795400: loss = 3.412449598312378\n",
      "step = 3795600: loss = 2.6925060749053955\n",
      "step = 3795800: loss = 3.965949535369873\n",
      "step = 3796000: loss = 3.5477232933044434\n",
      "step = 3796200: loss = 2.8633480072021484\n",
      "step = 3796400: loss = 2.901232957839966\n",
      "step = 3796600: loss = 5.759111404418945\n",
      "step = 3796800: loss = 3.9715404510498047\n",
      "step = 3797000: loss = 2.580287218093872\n",
      "step = 3797200: loss = 12.335396766662598\n",
      "step = 3797400: loss = 3.88865327835083\n",
      "step = 3797600: loss = 3.6971898078918457\n",
      "step = 3797800: loss = 2.926337480545044\n",
      "step = 3798000: loss = 3.581812620162964\n",
      "step = 3798200: loss = 4.176056861877441\n",
      "step = 3798400: loss = 3.7441325187683105\n",
      "step = 3798600: loss = 3.0366735458374023\n",
      "step = 3798800: loss = 2.840031623840332\n",
      "step = 3799000: loss = 3.6705844402313232\n",
      "step = 3799200: loss = 2.4662024974823\n",
      "step = 3799400: loss = 3.5237514972686768\n",
      "step = 3799600: loss = 2.904127836227417\n",
      "step = 3799800: loss = 3.6978330612182617\n",
      "step = 3800000: loss = 2.7680163383483887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3800000: Average Return = 6.050000190734863\n",
      "step = 3800200: loss = 4.187519550323486\n",
      "step = 3800400: loss = 2.323606014251709\n",
      "step = 3800600: loss = 2.8347320556640625\n",
      "step = 3800800: loss = 4.97605562210083\n",
      "step = 3801000: loss = 3.8010048866271973\n",
      "step = 3801200: loss = 2.2236168384552\n",
      "step = 3801400: loss = 2.394995927810669\n",
      "step = 3801600: loss = 3.439944267272949\n",
      "step = 3801800: loss = 4.960782051086426\n",
      "step = 3802000: loss = 2.8694000244140625\n",
      "step = 3802200: loss = 3.6375041007995605\n",
      "step = 3802400: loss = 2.952967405319214\n",
      "step = 3802600: loss = 6.276061058044434\n",
      "step = 3802800: loss = 3.552154779434204\n",
      "step = 3803000: loss = 3.5906624794006348\n",
      "step = 3803200: loss = 3.395392417907715\n",
      "step = 3803400: loss = 3.047852039337158\n",
      "step = 3803600: loss = 3.0159056186676025\n",
      "step = 3803800: loss = 6.139599323272705\n",
      "step = 3804000: loss = 4.347929954528809\n",
      "step = 3804200: loss = 4.107839107513428\n",
      "step = 3804400: loss = 3.617885112762451\n",
      "step = 3804600: loss = 2.6505911350250244\n",
      "step = 3804800: loss = 1.9160622358322144\n",
      "step = 3805000: loss = 3.0314674377441406\n",
      "step = 3805000: Average Return = 6.550000190734863\n",
      "step = 3805200: loss = 3.084164619445801\n",
      "step = 3805400: loss = 4.942646026611328\n",
      "step = 3805600: loss = 2.0450284481048584\n",
      "step = 3805800: loss = 2.82151460647583\n",
      "step = 3806000: loss = 4.4499125480651855\n",
      "step = 3806200: loss = 2.6408419609069824\n",
      "step = 3806400: loss = 3.07861065864563\n",
      "step = 3806600: loss = 3.862025737762451\n",
      "step = 3806800: loss = 2.981840133666992\n",
      "step = 3807000: loss = 3.2189602851867676\n",
      "step = 3807200: loss = 4.418328285217285\n",
      "step = 3807400: loss = 4.583780288696289\n",
      "step = 3807600: loss = 3.1803524494171143\n",
      "step = 3807800: loss = 3.691296339035034\n",
      "step = 3808000: loss = 3.860466241836548\n",
      "step = 3808200: loss = 3.7807395458221436\n",
      "step = 3808400: loss = 3.8847362995147705\n",
      "step = 3808600: loss = 4.5068278312683105\n",
      "step = 3808800: loss = 3.385420560836792\n",
      "step = 3809000: loss = 3.5832481384277344\n",
      "step = 3809200: loss = 4.826968193054199\n",
      "step = 3809400: loss = 2.9951272010803223\n",
      "step = 3809600: loss = 2.54818058013916\n",
      "step = 3809800: loss = 3.032179594039917\n",
      "step = 3810000: loss = 3.6073710918426514\n",
      "step = 3810000: Average Return = 4.599999904632568\n",
      "step = 3810200: loss = 3.795612335205078\n",
      "step = 3810400: loss = 5.3592529296875\n",
      "step = 3810600: loss = 3.739333391189575\n",
      "step = 3810800: loss = 3.840301990509033\n",
      "step = 3811000: loss = 3.950592041015625\n",
      "step = 3811200: loss = 3.8817760944366455\n",
      "step = 3811400: loss = 3.381974458694458\n",
      "step = 3811600: loss = 2.59440016746521\n",
      "step = 3811800: loss = 3.5989179611206055\n",
      "step = 3812000: loss = 3.5453550815582275\n",
      "step = 3812200: loss = 5.517055511474609\n",
      "step = 3812400: loss = 3.5916998386383057\n",
      "step = 3812600: loss = 5.22923469543457\n",
      "step = 3812800: loss = 3.40505313873291\n",
      "step = 3813000: loss = 2.9450531005859375\n",
      "step = 3813200: loss = 3.324660539627075\n",
      "step = 3813400: loss = 4.08195686340332\n",
      "step = 3813600: loss = 5.902148723602295\n",
      "step = 3813800: loss = 2.715876340866089\n",
      "step = 3814000: loss = 3.7785093784332275\n",
      "step = 3814200: loss = 3.103609561920166\n",
      "step = 3814400: loss = 4.169113636016846\n",
      "step = 3814600: loss = 3.315183401107788\n",
      "step = 3814800: loss = 2.7762346267700195\n",
      "step = 3815000: loss = 3.214189052581787\n",
      "step = 3815000: Average Return = 6.0\n",
      "step = 3815200: loss = 3.7915103435516357\n",
      "step = 3815400: loss = 2.9515743255615234\n",
      "step = 3815600: loss = 3.2912919521331787\n",
      "step = 3815800: loss = 3.3475279808044434\n",
      "step = 3816000: loss = 4.226557731628418\n",
      "step = 3816200: loss = 2.221370220184326\n",
      "step = 3816400: loss = 5.975553035736084\n",
      "step = 3816600: loss = 2.1754109859466553\n",
      "step = 3816800: loss = 2.974440336227417\n",
      "step = 3817000: loss = 3.0580482482910156\n",
      "step = 3817200: loss = 3.2827391624450684\n",
      "step = 3817400: loss = 5.034753799438477\n",
      "step = 3817600: loss = 2.428180694580078\n",
      "step = 3817800: loss = 2.681898355484009\n",
      "step = 3818000: loss = 4.374638080596924\n",
      "step = 3818200: loss = 2.6869146823883057\n",
      "step = 3818400: loss = 2.806142568588257\n",
      "step = 3818600: loss = 2.631535530090332\n",
      "step = 3818800: loss = 4.290221691131592\n",
      "step = 3819000: loss = 3.8892295360565186\n",
      "step = 3819200: loss = 3.0972416400909424\n",
      "step = 3819400: loss = 2.911729574203491\n",
      "step = 3819600: loss = 4.442554473876953\n",
      "step = 3819800: loss = 3.055957078933716\n",
      "step = 3820000: loss = 3.1281845569610596\n",
      "step = 3820000: Average Return = 2.6500000953674316\n",
      "step = 3820200: loss = 3.876481533050537\n",
      "step = 3820400: loss = 2.7602009773254395\n",
      "step = 3820600: loss = 3.009107828140259\n",
      "step = 3820800: loss = 3.673794984817505\n",
      "step = 3821000: loss = 3.5840108394622803\n",
      "step = 3821200: loss = 3.3133366107940674\n",
      "step = 3821400: loss = 3.7598087787628174\n",
      "step = 3821600: loss = 3.144690990447998\n",
      "step = 3821800: loss = 3.848646402359009\n",
      "step = 3822000: loss = 2.5078842639923096\n",
      "step = 3822200: loss = 3.1711056232452393\n",
      "step = 3822400: loss = 3.508615255355835\n",
      "step = 3822600: loss = 2.8834996223449707\n",
      "step = 3822800: loss = 3.14833927154541\n",
      "step = 3823000: loss = 17.5521183013916\n",
      "step = 3823200: loss = 2.2625784873962402\n",
      "step = 3823400: loss = 3.0538573265075684\n",
      "step = 3823600: loss = 3.0323126316070557\n",
      "step = 3823800: loss = 2.9957079887390137\n",
      "step = 3824000: loss = 3.781277894973755\n",
      "step = 3824200: loss = 4.186059951782227\n",
      "step = 3824400: loss = 3.019794464111328\n",
      "step = 3824600: loss = 3.990987539291382\n",
      "step = 3824800: loss = 2.699265718460083\n",
      "step = 3825000: loss = 2.8934872150421143\n",
      "step = 3825000: Average Return = 5.800000190734863\n",
      "step = 3825200: loss = 3.921706438064575\n",
      "step = 3825400: loss = 2.9884660243988037\n",
      "step = 3825600: loss = 3.6358373165130615\n",
      "step = 3825800: loss = 2.042274236679077\n",
      "step = 3826000: loss = 3.753676652908325\n",
      "step = 3826200: loss = 4.939872741699219\n",
      "step = 3826400: loss = 4.328380584716797\n",
      "step = 3826600: loss = 3.872986316680908\n",
      "step = 3826800: loss = 4.2598419189453125\n",
      "step = 3827000: loss = 3.6546216011047363\n",
      "step = 3827200: loss = 2.8774986267089844\n",
      "step = 3827400: loss = 4.322540760040283\n",
      "step = 3827600: loss = 3.211318254470825\n",
      "step = 3827800: loss = 4.100775718688965\n",
      "step = 3828000: loss = 3.4033491611480713\n",
      "step = 3828200: loss = 4.786032199859619\n",
      "step = 3828400: loss = 3.233762502670288\n",
      "step = 3828600: loss = 2.908196449279785\n",
      "step = 3828800: loss = 4.287075042724609\n",
      "step = 3829000: loss = 3.9261796474456787\n",
      "step = 3829200: loss = 2.674326181411743\n",
      "step = 3829400: loss = 2.071622371673584\n",
      "step = 3829600: loss = 3.438394069671631\n",
      "step = 3829800: loss = 3.8116705417633057\n",
      "step = 3830000: loss = 3.105586290359497\n",
      "step = 3830000: Average Return = 5.050000190734863\n",
      "step = 3830200: loss = 3.6449151039123535\n",
      "step = 3830400: loss = 2.33518385887146\n",
      "step = 3830600: loss = 3.9324750900268555\n",
      "step = 3830800: loss = 4.068309307098389\n",
      "step = 3831000: loss = 2.2007575035095215\n",
      "step = 3831200: loss = 4.196252822875977\n",
      "step = 3831400: loss = 4.051044940948486\n",
      "step = 3831600: loss = 4.035926342010498\n",
      "step = 3831800: loss = 2.976029634475708\n",
      "step = 3832000: loss = 5.871062278747559\n",
      "step = 3832200: loss = 3.469496488571167\n",
      "step = 3832400: loss = 2.8809170722961426\n",
      "step = 3832600: loss = 3.160090684890747\n",
      "step = 3832800: loss = 4.028365612030029\n",
      "step = 3833000: loss = 2.489309310913086\n",
      "step = 3833200: loss = 3.1872267723083496\n",
      "step = 3833400: loss = 2.8095345497131348\n",
      "step = 3833600: loss = 3.598257064819336\n",
      "step = 3833800: loss = 4.6918253898620605\n",
      "step = 3834000: loss = 3.3838348388671875\n",
      "step = 3834200: loss = 3.755948305130005\n",
      "step = 3834400: loss = 3.474994659423828\n",
      "step = 3834600: loss = 3.686283826828003\n",
      "step = 3834800: loss = 3.850881576538086\n",
      "step = 3835000: loss = 4.038824558258057\n",
      "step = 3835000: Average Return = 5.0\n",
      "step = 3835200: loss = 4.907286643981934\n",
      "step = 3835400: loss = 3.5606586933135986\n",
      "step = 3835600: loss = 3.1600823402404785\n",
      "step = 3835800: loss = 4.370982646942139\n",
      "step = 3836000: loss = 3.332711696624756\n",
      "step = 3836200: loss = 4.870578289031982\n",
      "step = 3836400: loss = 3.2536232471466064\n",
      "step = 3836600: loss = 4.235931396484375\n",
      "step = 3836800: loss = 3.0752618312835693\n",
      "step = 3837000: loss = 3.4166088104248047\n",
      "step = 3837200: loss = 4.980902194976807\n",
      "step = 3837400: loss = 3.346592426300049\n",
      "step = 3837600: loss = 2.872454881668091\n",
      "step = 3837800: loss = 3.0867083072662354\n",
      "step = 3838000: loss = 3.1559131145477295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3838200: loss = 4.328580379486084\n",
      "step = 3838400: loss = 2.978532075881958\n",
      "step = 3838600: loss = 3.3221001625061035\n",
      "step = 3838800: loss = 3.931842803955078\n",
      "step = 3839000: loss = 3.707749843597412\n",
      "step = 3839200: loss = 3.888216972351074\n",
      "step = 3839400: loss = 3.5848007202148438\n",
      "step = 3839600: loss = 4.165458679199219\n",
      "step = 3839800: loss = 3.3393919467926025\n",
      "step = 3840000: loss = 3.7048919200897217\n",
      "step = 3840000: Average Return = 4.550000190734863\n",
      "step = 3840200: loss = 2.3524160385131836\n",
      "step = 3840400: loss = 3.1828019618988037\n",
      "step = 3840600: loss = 4.460833549499512\n",
      "step = 3840800: loss = 3.866865873336792\n",
      "step = 3841000: loss = 3.0906574726104736\n",
      "step = 3841200: loss = 4.039029121398926\n",
      "step = 3841400: loss = 2.715959310531616\n",
      "step = 3841600: loss = 4.157931327819824\n",
      "step = 3841800: loss = 3.293135643005371\n",
      "step = 3842000: loss = 3.701974630355835\n",
      "step = 3842200: loss = 4.259430408477783\n",
      "step = 3842400: loss = 3.6171889305114746\n",
      "step = 3842600: loss = 4.297116279602051\n",
      "step = 3842800: loss = 3.741001605987549\n",
      "step = 3843000: loss = 4.295336723327637\n",
      "step = 3843200: loss = 3.6877870559692383\n",
      "step = 3843400: loss = 3.545590400695801\n",
      "step = 3843600: loss = 2.863431930541992\n",
      "step = 3843800: loss = 2.6058990955352783\n",
      "step = 3844000: loss = 3.53295636177063\n",
      "step = 3844200: loss = 3.0811398029327393\n",
      "step = 3844400: loss = 3.4217822551727295\n",
      "step = 3844600: loss = 3.1745758056640625\n",
      "step = 3844800: loss = 3.9205522537231445\n",
      "step = 3845000: loss = 5.353363990783691\n",
      "step = 3845000: Average Return = 5.300000190734863\n",
      "step = 3845200: loss = 2.6282918453216553\n",
      "step = 3845400: loss = 3.7468838691711426\n",
      "step = 3845600: loss = 4.441918849945068\n",
      "step = 3845800: loss = 3.270030975341797\n",
      "step = 3846000: loss = 3.1669273376464844\n",
      "step = 3846200: loss = 2.9601891040802\n",
      "step = 3846400: loss = 2.7153561115264893\n",
      "step = 3846600: loss = 2.6469757556915283\n",
      "step = 3846800: loss = 3.359020948410034\n",
      "step = 3847000: loss = 3.1175615787506104\n",
      "step = 3847200: loss = 2.700958251953125\n",
      "step = 3847400: loss = 3.54244065284729\n",
      "step = 3847600: loss = 3.8227415084838867\n",
      "step = 3847800: loss = 3.7479422092437744\n",
      "step = 3848000: loss = 3.221669912338257\n",
      "step = 3848200: loss = 3.2831926345825195\n",
      "step = 3848400: loss = 7.681446552276611\n",
      "step = 3848600: loss = 3.3152031898498535\n",
      "step = 3848800: loss = 3.4794390201568604\n",
      "step = 3849000: loss = 3.453228712081909\n",
      "step = 3849200: loss = 3.706965208053589\n",
      "step = 3849400: loss = 2.917811870574951\n",
      "step = 3849600: loss = 3.6719613075256348\n",
      "step = 3849800: loss = 2.5898616313934326\n",
      "step = 3850000: loss = 5.01328182220459\n",
      "step = 3850000: Average Return = 6.099999904632568\n",
      "step = 3850200: loss = 2.614777088165283\n",
      "step = 3850400: loss = 3.9056005477905273\n",
      "step = 3850600: loss = 3.074786901473999\n",
      "step = 3850800: loss = 2.391963481903076\n",
      "step = 3851000: loss = 3.6251494884490967\n",
      "step = 3851200: loss = 4.133059501647949\n",
      "step = 3851400: loss = 3.672929286956787\n",
      "step = 3851600: loss = 3.8628060817718506\n",
      "step = 3851800: loss = 2.768627166748047\n",
      "step = 3852000: loss = 2.6859045028686523\n",
      "step = 3852200: loss = 3.2701785564422607\n",
      "step = 3852400: loss = 2.670119047164917\n",
      "step = 3852600: loss = 2.8138558864593506\n",
      "step = 3852800: loss = 2.475815534591675\n",
      "step = 3853000: loss = 6.299535274505615\n",
      "step = 3853200: loss = 3.253929376602173\n",
      "step = 3853400: loss = 3.523845672607422\n",
      "step = 3853600: loss = 4.471430778503418\n",
      "step = 3853800: loss = 4.648960590362549\n",
      "step = 3854000: loss = 3.4127511978149414\n",
      "step = 3854200: loss = 5.198968887329102\n",
      "step = 3854400: loss = 3.554842233657837\n",
      "step = 3854600: loss = 3.5717363357543945\n",
      "step = 3854800: loss = 3.7947075366973877\n",
      "step = 3855000: loss = 4.774710178375244\n",
      "step = 3855000: Average Return = 5.550000190734863\n",
      "step = 3855200: loss = 4.140725135803223\n",
      "step = 3855400: loss = 4.301350116729736\n",
      "step = 3855600: loss = 2.911473274230957\n",
      "step = 3855800: loss = 5.072083473205566\n",
      "step = 3856000: loss = 4.539811134338379\n",
      "step = 3856200: loss = 4.737111568450928\n",
      "step = 3856400: loss = 2.73353910446167\n",
      "step = 3856600: loss = 2.023254871368408\n",
      "step = 3856800: loss = 3.808685302734375\n",
      "step = 3857000: loss = 4.226423740386963\n",
      "step = 3857200: loss = 3.97161602973938\n",
      "step = 3857400: loss = 2.7818145751953125\n",
      "step = 3857600: loss = 2.2247262001037598\n",
      "step = 3857800: loss = 3.779283285140991\n",
      "step = 3858000: loss = 3.9937350749969482\n",
      "step = 3858200: loss = 2.9704387187957764\n",
      "step = 3858400: loss = 2.659360885620117\n",
      "step = 3858600: loss = 4.4102253913879395\n",
      "step = 3858800: loss = 3.8307507038116455\n",
      "step = 3859000: loss = 4.557948589324951\n",
      "step = 3859200: loss = 4.113964557647705\n",
      "step = 3859400: loss = 2.977156400680542\n",
      "step = 3859600: loss = 3.8043901920318604\n",
      "step = 3859800: loss = 3.4074418544769287\n",
      "step = 3860000: loss = 3.40114688873291\n",
      "step = 3860000: Average Return = 4.599999904632568\n",
      "step = 3860200: loss = 4.090157985687256\n",
      "step = 3860400: loss = 2.9444420337677\n",
      "step = 3860600: loss = 2.9578933715820312\n",
      "step = 3860800: loss = 3.5250940322875977\n",
      "step = 3861000: loss = 3.0668020248413086\n",
      "step = 3861200: loss = 3.452895402908325\n",
      "step = 3861400: loss = 3.1951489448547363\n",
      "step = 3861600: loss = 4.727903366088867\n",
      "step = 3861800: loss = 3.275562047958374\n",
      "step = 3862000: loss = 3.717686176300049\n",
      "step = 3862200: loss = 3.807983636856079\n",
      "step = 3862400: loss = 3.1053357124328613\n",
      "step = 3862600: loss = 4.508774280548096\n",
      "step = 3862800: loss = 4.043859481811523\n",
      "step = 3863000: loss = 2.7715933322906494\n",
      "step = 3863200: loss = 5.441262245178223\n",
      "step = 3863400: loss = 2.9068706035614014\n",
      "step = 3863600: loss = 3.861208200454712\n",
      "step = 3863800: loss = 2.9291889667510986\n",
      "step = 3864000: loss = 4.3531365394592285\n",
      "step = 3864200: loss = 2.953930616378784\n",
      "step = 3864400: loss = 4.798825740814209\n",
      "step = 3864600: loss = 3.4511566162109375\n",
      "step = 3864800: loss = 2.7988154888153076\n",
      "step = 3865000: loss = 4.198299407958984\n",
      "step = 3865000: Average Return = 4.900000095367432\n",
      "step = 3865200: loss = 3.070195436477661\n",
      "step = 3865400: loss = 4.148699760437012\n",
      "step = 3865600: loss = 3.7319438457489014\n",
      "step = 3865800: loss = 3.21220326423645\n",
      "step = 3866000: loss = 4.090591907501221\n",
      "step = 3866200: loss = 4.907050609588623\n",
      "step = 3866400: loss = 4.622474670410156\n",
      "step = 3866600: loss = 3.7515106201171875\n",
      "step = 3866800: loss = 3.203852415084839\n",
      "step = 3867000: loss = 2.614269256591797\n",
      "step = 3867200: loss = 2.852297067642212\n",
      "step = 3867400: loss = 3.07301926612854\n",
      "step = 3867600: loss = 3.378221035003662\n",
      "step = 3867800: loss = 3.3717315196990967\n",
      "step = 3868000: loss = 5.152876853942871\n",
      "step = 3868200: loss = 4.167191028594971\n",
      "step = 3868400: loss = 4.228292465209961\n",
      "step = 3868600: loss = 2.9678096771240234\n",
      "step = 3868800: loss = 4.190766334533691\n",
      "step = 3869000: loss = 4.042023658752441\n",
      "step = 3869200: loss = 3.263489246368408\n",
      "step = 3869400: loss = 2.7472481727600098\n",
      "step = 3869600: loss = 3.863236665725708\n",
      "step = 3869800: loss = 2.700101375579834\n",
      "step = 3870000: loss = 3.819283962249756\n",
      "step = 3870000: Average Return = 5.849999904632568\n",
      "step = 3870200: loss = 3.6015865802764893\n",
      "step = 3870400: loss = 3.0902657508850098\n",
      "step = 3870600: loss = 3.611485481262207\n",
      "step = 3870800: loss = 2.786247730255127\n",
      "step = 3871000: loss = 4.023089408874512\n",
      "step = 3871200: loss = 3.8604989051818848\n",
      "step = 3871400: loss = 6.868165016174316\n",
      "step = 3871600: loss = 2.409468412399292\n",
      "step = 3871800: loss = 3.7150707244873047\n",
      "step = 3872000: loss = 3.2202982902526855\n",
      "step = 3872200: loss = 2.8084328174591064\n",
      "step = 3872400: loss = 2.772528886795044\n",
      "step = 3872600: loss = 4.791001796722412\n",
      "step = 3872800: loss = 3.1960625648498535\n",
      "step = 3873000: loss = 4.006364822387695\n",
      "step = 3873200: loss = 4.498998165130615\n",
      "step = 3873400: loss = 3.9974026679992676\n",
      "step = 3873600: loss = 3.9615044593811035\n",
      "step = 3873800: loss = 3.593081474304199\n",
      "step = 3874000: loss = 4.046380043029785\n",
      "step = 3874200: loss = 3.0123753547668457\n",
      "step = 3874400: loss = 4.739220142364502\n",
      "step = 3874600: loss = 4.2887492179870605\n",
      "step = 3874800: loss = 2.9403843879699707\n",
      "step = 3875000: loss = 2.973128080368042\n",
      "step = 3875000: Average Return = 3.799999952316284\n",
      "step = 3875200: loss = 2.566570997238159\n",
      "step = 3875400: loss = 2.718998908996582\n",
      "step = 3875600: loss = 5.365219593048096\n",
      "step = 3875800: loss = 4.627200603485107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3876000: loss = 3.377720832824707\n",
      "step = 3876200: loss = 4.198748588562012\n",
      "step = 3876400: loss = 3.9326934814453125\n",
      "step = 3876600: loss = 2.4661200046539307\n",
      "step = 3876800: loss = 3.1037631034851074\n",
      "step = 3877000: loss = 4.155612468719482\n",
      "step = 3877200: loss = 5.174822807312012\n",
      "step = 3877400: loss = 4.824097633361816\n",
      "step = 3877600: loss = 3.6873650550842285\n",
      "step = 3877800: loss = 2.977609157562256\n",
      "step = 3878000: loss = 4.880309104919434\n",
      "step = 3878200: loss = 3.4487555027008057\n",
      "step = 3878400: loss = 2.723778247833252\n",
      "step = 3878600: loss = 2.4979922771453857\n",
      "step = 3878800: loss = 3.716766357421875\n",
      "step = 3879000: loss = 4.404689311981201\n",
      "step = 3879200: loss = 4.124814987182617\n",
      "step = 3879400: loss = 2.675067663192749\n",
      "step = 3879600: loss = 3.8605751991271973\n",
      "step = 3879800: loss = 3.4222352504730225\n",
      "step = 3880000: loss = 4.146021842956543\n",
      "step = 3880000: Average Return = 5.800000190734863\n",
      "step = 3880200: loss = 3.665090799331665\n",
      "step = 3880400: loss = 6.117380142211914\n",
      "step = 3880600: loss = 2.4996256828308105\n",
      "step = 3880800: loss = 3.490293502807617\n",
      "step = 3881000: loss = 2.937232494354248\n",
      "step = 3881200: loss = 3.1546084880828857\n",
      "step = 3881400: loss = 3.60947322845459\n",
      "step = 3881600: loss = 3.4686501026153564\n",
      "step = 3881800: loss = 4.249260902404785\n",
      "step = 3882000: loss = 4.3414626121521\n",
      "step = 3882200: loss = 3.8871655464172363\n",
      "step = 3882400: loss = 3.9347240924835205\n",
      "step = 3882600: loss = 3.427847146987915\n",
      "step = 3882800: loss = 2.5337307453155518\n",
      "step = 3883000: loss = 6.416595458984375\n",
      "step = 3883200: loss = 3.092679262161255\n",
      "step = 3883400: loss = 3.897050142288208\n",
      "step = 3883600: loss = 4.261972904205322\n",
      "step = 3883800: loss = 3.225335121154785\n",
      "step = 3884000: loss = 5.678504943847656\n",
      "step = 3884200: loss = 3.4969329833984375\n",
      "step = 3884400: loss = 6.303410530090332\n",
      "step = 3884600: loss = 5.3117170333862305\n",
      "step = 3884800: loss = 4.21144437789917\n",
      "step = 3885000: loss = 3.928771495819092\n",
      "step = 3885000: Average Return = 5.800000190734863\n",
      "step = 3885200: loss = 3.4016940593719482\n",
      "step = 3885400: loss = 4.082736015319824\n",
      "step = 3885600: loss = 3.4042537212371826\n",
      "step = 3885800: loss = 3.727277994155884\n",
      "step = 3886000: loss = 3.0574066638946533\n",
      "step = 3886200: loss = 3.889312267303467\n",
      "step = 3886400: loss = 2.8946032524108887\n",
      "step = 3886600: loss = 4.2482805252075195\n",
      "step = 3886800: loss = 2.913527250289917\n",
      "step = 3887000: loss = 3.1533288955688477\n",
      "step = 3887200: loss = 2.9367191791534424\n",
      "step = 3887400: loss = 2.705237865447998\n",
      "step = 3887600: loss = 3.8328857421875\n",
      "step = 3887800: loss = 3.705899953842163\n",
      "step = 3888000: loss = 2.6163265705108643\n",
      "step = 3888200: loss = 4.799283981323242\n",
      "step = 3888400: loss = 3.8094732761383057\n",
      "step = 3888600: loss = 3.7406365871429443\n",
      "step = 3888800: loss = 2.2929961681365967\n",
      "step = 3889000: loss = 3.4939494132995605\n",
      "step = 3889200: loss = 3.110272169113159\n",
      "step = 3889400: loss = 5.172358512878418\n",
      "step = 3889600: loss = 3.811359167098999\n",
      "step = 3889800: loss = 3.9715328216552734\n",
      "step = 3890000: loss = 3.9224724769592285\n",
      "step = 3890000: Average Return = 5.300000190734863\n",
      "step = 3890200: loss = 4.561767101287842\n",
      "step = 3890400: loss = 3.0659074783325195\n",
      "step = 3890600: loss = 3.8945834636688232\n",
      "step = 3890800: loss = 5.176522254943848\n",
      "step = 3891000: loss = 4.576944351196289\n",
      "step = 3891200: loss = 3.1725358963012695\n",
      "step = 3891400: loss = 4.544468402862549\n",
      "step = 3891600: loss = 4.119502067565918\n",
      "step = 3891800: loss = 3.1420648097991943\n",
      "step = 3892000: loss = 3.524596691131592\n",
      "step = 3892200: loss = 4.659430027008057\n",
      "step = 3892400: loss = 2.444659471511841\n",
      "step = 3892600: loss = 4.868219375610352\n",
      "step = 3892800: loss = 3.763840913772583\n",
      "step = 3893000: loss = 5.217321872711182\n",
      "step = 3893200: loss = 3.51379656791687\n",
      "step = 3893400: loss = 3.468038558959961\n",
      "step = 3893600: loss = 3.095933437347412\n",
      "step = 3893800: loss = 1.948799967765808\n",
      "step = 3894000: loss = 5.365350723266602\n",
      "step = 3894200: loss = 4.368563175201416\n",
      "step = 3894400: loss = 4.118236064910889\n",
      "step = 3894600: loss = 3.711721420288086\n",
      "step = 3894800: loss = 3.165881633758545\n",
      "step = 3895000: loss = 3.5645408630371094\n",
      "step = 3895000: Average Return = 3.9000000953674316\n",
      "step = 3895200: loss = 3.4095451831817627\n",
      "step = 3895400: loss = 3.8840997219085693\n",
      "step = 3895600: loss = 4.365240573883057\n",
      "step = 3895800: loss = 4.790036678314209\n",
      "step = 3896000: loss = 4.0151519775390625\n",
      "step = 3896200: loss = 4.087420463562012\n",
      "step = 3896400: loss = 3.3143491744995117\n",
      "step = 3896600: loss = 4.87573766708374\n",
      "step = 3896800: loss = 4.565413951873779\n",
      "step = 3897000: loss = 2.9781813621520996\n",
      "step = 3897200: loss = 3.3695366382598877\n",
      "step = 3897400: loss = 3.589724063873291\n",
      "step = 3897600: loss = 4.401326656341553\n",
      "step = 3897800: loss = 3.1640939712524414\n",
      "step = 3898000: loss = 5.474524021148682\n",
      "step = 3898200: loss = 3.7569661140441895\n",
      "step = 3898400: loss = 5.188507556915283\n",
      "step = 3898600: loss = 3.3030192852020264\n",
      "step = 3898800: loss = 4.060986042022705\n",
      "step = 3899000: loss = 4.118415832519531\n",
      "step = 3899200: loss = 3.160731792449951\n",
      "step = 3899400: loss = 3.9492204189300537\n",
      "step = 3899600: loss = 4.140080451965332\n",
      "step = 3899800: loss = 4.421902656555176\n",
      "step = 3900000: loss = 4.007688522338867\n",
      "step = 3900000: Average Return = 4.550000190734863\n",
      "step = 3900200: loss = 4.519165992736816\n",
      "step = 3900400: loss = 4.775230884552002\n",
      "step = 3900600: loss = 3.03403639793396\n",
      "step = 3900800: loss = 3.793562889099121\n",
      "step = 3901000: loss = 3.4111340045928955\n",
      "step = 3901200: loss = 3.052476167678833\n",
      "step = 3901400: loss = 5.363104343414307\n",
      "step = 3901600: loss = 3.506878614425659\n",
      "step = 3901800: loss = 5.642107009887695\n",
      "step = 3902000: loss = 3.7765514850616455\n",
      "step = 3902200: loss = 2.522531032562256\n",
      "step = 3902400: loss = 3.3997042179107666\n",
      "step = 3902600: loss = 2.373291015625\n",
      "step = 3902800: loss = 2.612131357192993\n",
      "step = 3903000: loss = 4.107551097869873\n",
      "step = 3903200: loss = 5.013155937194824\n",
      "step = 3903400: loss = 3.305662155151367\n",
      "step = 3903600: loss = 2.7340428829193115\n",
      "step = 3903800: loss = 4.371708393096924\n",
      "step = 3904000: loss = 3.46239972114563\n",
      "step = 3904200: loss = 3.4291703701019287\n",
      "step = 3904400: loss = 4.629909038543701\n",
      "step = 3904600: loss = 4.478041172027588\n",
      "step = 3904800: loss = 3.629169464111328\n",
      "step = 3905000: loss = 3.323868989944458\n",
      "step = 3905000: Average Return = 6.849999904632568\n",
      "step = 3905200: loss = 5.851160526275635\n",
      "step = 3905400: loss = 2.028057813644409\n",
      "step = 3905600: loss = 3.740405797958374\n",
      "step = 3905800: loss = 2.9532244205474854\n",
      "step = 3906000: loss = 5.015591621398926\n",
      "step = 3906200: loss = 3.5432000160217285\n",
      "step = 3906400: loss = 3.2100422382354736\n",
      "step = 3906600: loss = 4.503749847412109\n",
      "step = 3906800: loss = 4.5907487869262695\n",
      "step = 3907000: loss = 3.706582546234131\n",
      "step = 3907200: loss = 4.333474159240723\n",
      "step = 3907400: loss = 4.509985446929932\n",
      "step = 3907600: loss = 2.8367810249328613\n",
      "step = 3907800: loss = 3.295950174331665\n",
      "step = 3908000: loss = 2.7413322925567627\n",
      "step = 3908200: loss = 2.4228570461273193\n",
      "step = 3908400: loss = 5.368907451629639\n",
      "step = 3908600: loss = 3.0897185802459717\n",
      "step = 3908800: loss = 4.09334135055542\n",
      "step = 3909000: loss = 3.9972307682037354\n",
      "step = 3909200: loss = 5.224991321563721\n",
      "step = 3909400: loss = 2.6100261211395264\n",
      "step = 3909600: loss = 2.6708083152770996\n",
      "step = 3909800: loss = 3.5084521770477295\n",
      "step = 3910000: loss = 2.7174932956695557\n",
      "step = 3910000: Average Return = 7.199999809265137\n",
      "step = 3910200: loss = 3.277161121368408\n",
      "step = 3910400: loss = 2.9405839443206787\n",
      "step = 3910600: loss = 3.664720058441162\n",
      "step = 3910800: loss = 4.630445957183838\n",
      "step = 3911000: loss = 3.721511125564575\n",
      "step = 3911200: loss = 4.047187328338623\n",
      "step = 3911400: loss = 5.510646343231201\n",
      "step = 3911600: loss = 3.488304376602173\n",
      "step = 3911800: loss = 3.857727289199829\n",
      "step = 3912000: loss = 2.9148919582366943\n",
      "step = 3912200: loss = 2.996080160140991\n",
      "step = 3912400: loss = 3.961642265319824\n",
      "step = 3912600: loss = 3.0101945400238037\n",
      "step = 3912800: loss = 4.6809797286987305\n",
      "step = 3913000: loss = 4.084205627441406\n",
      "step = 3913200: loss = 4.140326499938965\n",
      "step = 3913400: loss = 4.098454475402832\n",
      "step = 3913600: loss = 4.507594108581543\n",
      "step = 3913800: loss = 3.3796706199645996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3914000: loss = 3.9030654430389404\n",
      "step = 3914200: loss = 4.985601902008057\n",
      "step = 3914400: loss = 4.281737327575684\n",
      "step = 3914600: loss = 3.2892651557922363\n",
      "step = 3914800: loss = 2.7037718296051025\n",
      "step = 3915000: loss = 6.154417514801025\n",
      "step = 3915000: Average Return = 6.199999809265137\n",
      "step = 3915200: loss = 2.8147974014282227\n",
      "step = 3915400: loss = 3.4008638858795166\n",
      "step = 3915600: loss = 3.413227081298828\n",
      "step = 3915800: loss = 5.128626823425293\n",
      "step = 3916000: loss = 3.6986515522003174\n",
      "step = 3916200: loss = 4.286086082458496\n",
      "step = 3916400: loss = 4.987081050872803\n",
      "step = 3916600: loss = 2.644049644470215\n",
      "step = 3916800: loss = 3.0951504707336426\n",
      "step = 3917000: loss = 4.588283061981201\n",
      "step = 3917200: loss = 3.4534599781036377\n",
      "step = 3917400: loss = 3.5867345333099365\n",
      "step = 3917600: loss = 3.459689140319824\n",
      "step = 3917800: loss = 4.000692367553711\n",
      "step = 3918000: loss = 3.0302586555480957\n",
      "step = 3918200: loss = 5.169615745544434\n",
      "step = 3918400: loss = 3.72293758392334\n",
      "step = 3918600: loss = 4.784681797027588\n",
      "step = 3918800: loss = 4.117354869842529\n",
      "step = 3919000: loss = 4.5800065994262695\n",
      "step = 3919200: loss = 4.159195899963379\n",
      "step = 3919400: loss = 4.77883243560791\n",
      "step = 3919600: loss = 4.072894096374512\n",
      "step = 3919800: loss = 3.3508708477020264\n",
      "step = 3920000: loss = 4.756872653961182\n",
      "step = 3920000: Average Return = 6.449999809265137\n",
      "step = 3920200: loss = 2.959383249282837\n",
      "step = 3920400: loss = 3.5913310050964355\n",
      "step = 3920600: loss = 3.0831267833709717\n",
      "step = 3920800: loss = 3.5101826190948486\n",
      "step = 3921000: loss = 3.6064388751983643\n",
      "step = 3921200: loss = 3.3901569843292236\n",
      "step = 3921400: loss = 2.1872243881225586\n",
      "step = 3921600: loss = 3.299852132797241\n",
      "step = 3921800: loss = 4.782258033752441\n",
      "step = 3922000: loss = 3.730989694595337\n",
      "step = 3922200: loss = 3.9606611728668213\n",
      "step = 3922400: loss = 3.26385235786438\n",
      "step = 3922600: loss = 4.3342156410217285\n",
      "step = 3922800: loss = 4.165325164794922\n",
      "step = 3923000: loss = 3.608790874481201\n",
      "step = 3923200: loss = 2.548194408416748\n",
      "step = 3923400: loss = 3.8470115661621094\n",
      "step = 3923600: loss = 4.020626544952393\n",
      "step = 3923800: loss = 3.8728585243225098\n",
      "step = 3924000: loss = 5.3184051513671875\n",
      "step = 3924200: loss = 3.9078543186187744\n",
      "step = 3924400: loss = 2.7508561611175537\n",
      "step = 3924600: loss = 2.9652295112609863\n",
      "step = 3924800: loss = 4.7850494384765625\n",
      "step = 3925000: loss = 3.8913209438323975\n",
      "step = 3925000: Average Return = 5.199999809265137\n",
      "step = 3925200: loss = 3.352046012878418\n",
      "step = 3925400: loss = 5.050201416015625\n",
      "step = 3925600: loss = 3.4814388751983643\n",
      "step = 3925800: loss = 3.547112464904785\n",
      "step = 3926000: loss = 2.2305736541748047\n",
      "step = 3926200: loss = 3.6518731117248535\n",
      "step = 3926400: loss = 3.2421562671661377\n",
      "step = 3926600: loss = 3.448178291320801\n",
      "step = 3926800: loss = 3.648231029510498\n",
      "step = 3927000: loss = 2.658869981765747\n",
      "step = 3927200: loss = 3.394712209701538\n",
      "step = 3927400: loss = 2.394975423812866\n",
      "step = 3927600: loss = 3.862539291381836\n",
      "step = 3927800: loss = 3.2562131881713867\n",
      "step = 3928000: loss = 3.6512272357940674\n",
      "step = 3928200: loss = 4.219339370727539\n",
      "step = 3928400: loss = 4.1694722175598145\n",
      "step = 3928600: loss = 3.048630952835083\n",
      "step = 3928800: loss = 3.647892475128174\n",
      "step = 3929000: loss = 4.984362602233887\n",
      "step = 3929200: loss = 2.7919933795928955\n",
      "step = 3929400: loss = 3.8427352905273438\n",
      "step = 3929600: loss = 2.5928521156311035\n",
      "step = 3929800: loss = 2.772077798843384\n",
      "step = 3930000: loss = 2.5891616344451904\n",
      "step = 3930000: Average Return = 6.550000190734863\n",
      "step = 3930200: loss = 2.5837481021881104\n",
      "step = 3930400: loss = 4.040582656860352\n",
      "step = 3930600: loss = 3.598924160003662\n",
      "step = 3930800: loss = 2.4535019397735596\n",
      "step = 3931000: loss = 3.4703245162963867\n",
      "step = 3931200: loss = 2.8976876735687256\n",
      "step = 3931400: loss = 2.894926071166992\n",
      "step = 3931600: loss = 3.303558349609375\n",
      "step = 3931800: loss = 5.314980506896973\n",
      "step = 3932000: loss = 4.11244535446167\n",
      "step = 3932200: loss = 3.7108757495880127\n",
      "step = 3932400: loss = 5.091516494750977\n",
      "step = 3932600: loss = 4.251956462860107\n",
      "step = 3932800: loss = 3.3410778045654297\n",
      "step = 3933000: loss = 3.6484363079071045\n",
      "step = 3933200: loss = 3.027653455734253\n",
      "step = 3933400: loss = 2.9008443355560303\n",
      "step = 3933600: loss = 4.588552951812744\n",
      "step = 3933800: loss = 2.837895631790161\n",
      "step = 3934000: loss = 2.7738847732543945\n",
      "step = 3934200: loss = 2.790584087371826\n",
      "step = 3934400: loss = 3.998300075531006\n",
      "step = 3934600: loss = 4.830286026000977\n",
      "step = 3934800: loss = 4.189896583557129\n",
      "step = 3935000: loss = 3.7958221435546875\n",
      "step = 3935000: Average Return = 5.300000190734863\n",
      "step = 3935200: loss = 3.08746337890625\n",
      "step = 3935400: loss = 3.306028366088867\n",
      "step = 3935600: loss = 3.928570508956909\n",
      "step = 3935800: loss = 3.2548980712890625\n",
      "step = 3936000: loss = 3.000966787338257\n",
      "step = 3936200: loss = 3.490511894226074\n",
      "step = 3936400: loss = 3.781046152114868\n",
      "step = 3936600: loss = 2.5955700874328613\n",
      "step = 3936800: loss = 2.685393810272217\n",
      "step = 3937000: loss = 2.8454668521881104\n",
      "step = 3937200: loss = 4.1799187660217285\n",
      "step = 3937400: loss = 5.045193672180176\n",
      "step = 3937600: loss = 5.56108283996582\n",
      "step = 3937800: loss = 3.5541980266571045\n",
      "step = 3938000: loss = 3.9466564655303955\n",
      "step = 3938200: loss = 3.974990129470825\n",
      "step = 3938400: loss = 5.166719913482666\n",
      "step = 3938600: loss = 5.014197826385498\n",
      "step = 3938800: loss = 4.616466045379639\n",
      "step = 3939000: loss = 4.262903690338135\n",
      "step = 3939200: loss = 3.8446240425109863\n",
      "step = 3939400: loss = 3.645294666290283\n",
      "step = 3939600: loss = 5.0017619132995605\n",
      "step = 3939800: loss = 3.969676971435547\n",
      "step = 3940000: loss = 3.29093074798584\n",
      "step = 3940000: Average Return = 4.5\n",
      "step = 3940200: loss = 2.8707432746887207\n",
      "step = 3940400: loss = 4.722270488739014\n",
      "step = 3940600: loss = 4.021579265594482\n",
      "step = 3940800: loss = 3.185011625289917\n",
      "step = 3941000: loss = 4.066597938537598\n",
      "step = 3941200: loss = 3.8984029293060303\n",
      "step = 3941400: loss = 3.901560068130493\n",
      "step = 3941600: loss = 3.3615968227386475\n",
      "step = 3941800: loss = 3.3282971382141113\n",
      "step = 3942000: loss = 4.24644136428833\n",
      "step = 3942200: loss = 3.824876546859741\n",
      "step = 3942400: loss = 3.3952596187591553\n",
      "step = 3942600: loss = 4.023687839508057\n",
      "step = 3942800: loss = 3.4341142177581787\n",
      "step = 3943000: loss = 3.8376729488372803\n",
      "step = 3943200: loss = 3.7262096405029297\n",
      "step = 3943400: loss = 2.748647928237915\n",
      "step = 3943600: loss = 4.47501802444458\n",
      "step = 3943800: loss = 2.6426732540130615\n",
      "step = 3944000: loss = 3.7992632389068604\n",
      "step = 3944200: loss = 3.179832696914673\n",
      "step = 3944400: loss = 3.676457166671753\n",
      "step = 3944600: loss = 3.4468932151794434\n",
      "step = 3944800: loss = 3.911074161529541\n",
      "step = 3945000: loss = 5.0317583084106445\n",
      "step = 3945000: Average Return = 6.25\n",
      "step = 3945200: loss = 3.572068691253662\n",
      "step = 3945400: loss = 2.3656036853790283\n",
      "step = 3945600: loss = 3.6265175342559814\n",
      "step = 3945800: loss = 4.979782581329346\n",
      "step = 3946000: loss = 3.8844313621520996\n",
      "step = 3946200: loss = 3.5503551959991455\n",
      "step = 3946400: loss = 3.0048882961273193\n",
      "step = 3946600: loss = 5.304709434509277\n",
      "step = 3946800: loss = 2.910802125930786\n",
      "step = 3947000: loss = 4.47368049621582\n",
      "step = 3947200: loss = 3.3302438259124756\n",
      "step = 3947400: loss = 2.680279493331909\n",
      "step = 3947600: loss = 2.332414388656616\n",
      "step = 3947800: loss = 3.558276891708374\n",
      "step = 3948000: loss = 5.054569721221924\n",
      "step = 3948200: loss = 4.513948440551758\n",
      "step = 3948400: loss = 3.2898802757263184\n",
      "step = 3948600: loss = 4.6337151527404785\n",
      "step = 3948800: loss = 2.853663921356201\n",
      "step = 3949000: loss = 4.234750270843506\n",
      "step = 3949200: loss = 3.3982934951782227\n",
      "step = 3949400: loss = 3.3581175804138184\n",
      "step = 3949600: loss = 4.099803447723389\n",
      "step = 3949800: loss = 2.613959312438965\n",
      "step = 3950000: loss = 3.676877737045288\n",
      "step = 3950000: Average Return = 3.8499999046325684\n",
      "step = 3950200: loss = 3.5978431701660156\n",
      "step = 3950400: loss = 3.0065219402313232\n",
      "step = 3950600: loss = 3.151989459991455\n",
      "step = 3950800: loss = 2.042078971862793\n",
      "step = 3951000: loss = 3.192183494567871\n",
      "step = 3951200: loss = 3.1760716438293457\n",
      "step = 3951400: loss = 5.153909206390381\n",
      "step = 3951600: loss = 3.4722955226898193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3951800: loss = 3.8238048553466797\n",
      "step = 3952000: loss = 2.733431100845337\n",
      "step = 3952200: loss = 2.6188085079193115\n",
      "step = 3952400: loss = 3.6595394611358643\n",
      "step = 3952600: loss = 3.42525577545166\n",
      "step = 3952800: loss = 4.293863296508789\n",
      "step = 3953000: loss = 2.467414617538452\n",
      "step = 3953200: loss = 3.226491928100586\n",
      "step = 3953400: loss = 3.5277600288391113\n",
      "step = 3953600: loss = 3.0286922454833984\n",
      "step = 3953800: loss = 5.376009464263916\n",
      "step = 3954000: loss = 3.1829795837402344\n",
      "step = 3954200: loss = 2.5156664848327637\n",
      "step = 3954400: loss = 3.9854564666748047\n",
      "step = 3954600: loss = 2.2004218101501465\n",
      "step = 3954800: loss = 4.02056884765625\n",
      "step = 3955000: loss = 4.063931941986084\n",
      "step = 3955000: Average Return = 4.800000190734863\n",
      "step = 3955200: loss = 2.578580379486084\n",
      "step = 3955400: loss = 2.768812894821167\n",
      "step = 3955600: loss = 3.7087175846099854\n",
      "step = 3955800: loss = 4.293032169342041\n",
      "step = 3956000: loss = 4.478673934936523\n",
      "step = 3956200: loss = 2.3824331760406494\n",
      "step = 3956400: loss = 4.352756023406982\n",
      "step = 3956600: loss = 4.56757116317749\n",
      "step = 3956800: loss = 3.248239278793335\n",
      "step = 3957000: loss = 4.255650997161865\n",
      "step = 3957200: loss = 4.883414268493652\n",
      "step = 3957400: loss = 2.3562674522399902\n",
      "step = 3957600: loss = 2.5802600383758545\n",
      "step = 3957800: loss = 3.005702495574951\n",
      "step = 3958000: loss = 3.7427330017089844\n",
      "step = 3958200: loss = 4.3353471755981445\n",
      "step = 3958400: loss = 3.502119779586792\n",
      "step = 3958600: loss = 2.286080837249756\n",
      "step = 3958800: loss = 3.2315752506256104\n",
      "step = 3959000: loss = 2.573897123336792\n",
      "step = 3959200: loss = 3.156825542449951\n",
      "step = 3959400: loss = 4.125638961791992\n",
      "step = 3959600: loss = 2.7029027938842773\n",
      "step = 3959800: loss = 3.1392977237701416\n",
      "step = 3960000: loss = 2.7916789054870605\n",
      "step = 3960000: Average Return = 5.400000095367432\n",
      "step = 3960200: loss = 3.2062642574310303\n",
      "step = 3960400: loss = 3.511906623840332\n",
      "step = 3960600: loss = 3.2778618335723877\n",
      "step = 3960800: loss = 2.0363657474517822\n",
      "step = 3961000: loss = 2.5867908000946045\n",
      "step = 3961200: loss = 3.3760783672332764\n",
      "step = 3961400: loss = 2.4999966621398926\n",
      "step = 3961600: loss = 2.2523913383483887\n",
      "step = 3961800: loss = 4.064564228057861\n",
      "step = 3962000: loss = 3.376866102218628\n",
      "step = 3962200: loss = 2.892273426055908\n",
      "step = 3962400: loss = 3.606830596923828\n",
      "step = 3962600: loss = 4.300544738769531\n",
      "step = 3962800: loss = 3.2446279525756836\n",
      "step = 3963000: loss = 3.744951009750366\n",
      "step = 3963200: loss = 3.1497695446014404\n",
      "step = 3963400: loss = 3.6602694988250732\n",
      "step = 3963600: loss = 4.030072212219238\n",
      "step = 3963800: loss = 4.042399883270264\n",
      "step = 3964000: loss = 3.2608180046081543\n",
      "step = 3964200: loss = 3.2060210704803467\n",
      "step = 3964400: loss = 3.5525529384613037\n",
      "step = 3964600: loss = 4.072375297546387\n",
      "step = 3964800: loss = 3.480515718460083\n",
      "step = 3965000: loss = 3.050570011138916\n",
      "step = 3965000: Average Return = 3.450000047683716\n",
      "step = 3965200: loss = 4.066308498382568\n",
      "step = 3965400: loss = 2.3566229343414307\n",
      "step = 3965600: loss = 4.208710193634033\n",
      "step = 3965800: loss = 3.493046283721924\n",
      "step = 3966000: loss = 3.4150099754333496\n",
      "step = 3966200: loss = 4.609305381774902\n",
      "step = 3966400: loss = 4.844875812530518\n",
      "step = 3966600: loss = 3.934556245803833\n",
      "step = 3966800: loss = 2.7028045654296875\n",
      "step = 3967000: loss = 2.98478102684021\n",
      "step = 3967200: loss = 2.6468706130981445\n",
      "step = 3967400: loss = 2.907985210418701\n",
      "step = 3967600: loss = 4.424398899078369\n",
      "step = 3967800: loss = 3.0076076984405518\n",
      "step = 3968000: loss = 4.636213779449463\n",
      "step = 3968200: loss = 2.956120729446411\n",
      "step = 3968400: loss = 3.3651323318481445\n",
      "step = 3968600: loss = 2.573594808578491\n",
      "step = 3968800: loss = 4.255349636077881\n",
      "step = 3969000: loss = 3.514451742172241\n",
      "step = 3969200: loss = 3.260352611541748\n",
      "step = 3969400: loss = 4.057265758514404\n",
      "step = 3969600: loss = 2.3412275314331055\n",
      "step = 3969800: loss = 3.3897459506988525\n",
      "step = 3970000: loss = 3.447638988494873\n",
      "step = 3970000: Average Return = 4.900000095367432\n",
      "step = 3970200: loss = 4.103062152862549\n",
      "step = 3970400: loss = 2.514225959777832\n",
      "step = 3970600: loss = 3.9507741928100586\n",
      "step = 3970800: loss = 2.99770188331604\n",
      "step = 3971000: loss = 4.075591087341309\n",
      "step = 3971200: loss = 3.694490909576416\n",
      "step = 3971400: loss = 4.348777770996094\n",
      "step = 3971600: loss = 3.3085882663726807\n",
      "step = 3971800: loss = 2.299506425857544\n",
      "step = 3972000: loss = 4.552780628204346\n",
      "step = 3972200: loss = 2.386143684387207\n",
      "step = 3972400: loss = 3.7114226818084717\n",
      "step = 3972600: loss = 3.815948963165283\n",
      "step = 3972800: loss = 3.6342427730560303\n",
      "step = 3973000: loss = 3.6567764282226562\n",
      "step = 3973200: loss = 3.2152836322784424\n",
      "step = 3973400: loss = 2.448148250579834\n",
      "step = 3973600: loss = 3.624847888946533\n",
      "step = 3973800: loss = 4.465081214904785\n",
      "step = 3974000: loss = 3.419917583465576\n",
      "step = 3974200: loss = 4.1424384117126465\n",
      "step = 3974400: loss = 2.7621781826019287\n",
      "step = 3974600: loss = 3.487274646759033\n",
      "step = 3974800: loss = 2.5189390182495117\n",
      "step = 3975000: loss = 3.1202261447906494\n",
      "step = 3975000: Average Return = 4.699999809265137\n",
      "step = 3975200: loss = 3.3248345851898193\n",
      "step = 3975400: loss = 3.173478841781616\n",
      "step = 3975600: loss = 3.3516600131988525\n",
      "step = 3975800: loss = 4.254826068878174\n",
      "step = 3976000: loss = 3.7112584114074707\n",
      "step = 3976200: loss = 3.9449081420898438\n",
      "step = 3976400: loss = 3.202810525894165\n",
      "step = 3976600: loss = 3.472428321838379\n",
      "step = 3976800: loss = 3.145256519317627\n",
      "step = 3977000: loss = 3.2210371494293213\n",
      "step = 3977200: loss = 3.429851531982422\n",
      "step = 3977400: loss = 3.635209321975708\n",
      "step = 3977600: loss = 4.206589698791504\n",
      "step = 3977800: loss = 2.8867204189300537\n",
      "step = 3978000: loss = 2.3577284812927246\n",
      "step = 3978200: loss = 4.417896270751953\n",
      "step = 3978400: loss = 2.9859025478363037\n",
      "step = 3978600: loss = 3.861772298812866\n",
      "step = 3978800: loss = 2.695850133895874\n",
      "step = 3979000: loss = 4.10687780380249\n",
      "step = 3979200: loss = 3.2292821407318115\n",
      "step = 3979400: loss = 2.861992359161377\n",
      "step = 3979600: loss = 4.42548131942749\n",
      "step = 3979800: loss = 3.5656261444091797\n",
      "step = 3980000: loss = 3.9625003337860107\n",
      "step = 3980000: Average Return = 7.400000095367432\n",
      "step = 3980200: loss = 3.681670665740967\n",
      "step = 3980400: loss = 3.675544023513794\n",
      "step = 3980600: loss = 3.320016384124756\n",
      "step = 3980800: loss = 4.934077739715576\n",
      "step = 3981000: loss = 3.6986563205718994\n",
      "step = 3981200: loss = 3.4759509563446045\n",
      "step = 3981400: loss = 3.884860038757324\n",
      "step = 3981600: loss = 3.2518866062164307\n",
      "step = 3981800: loss = 1.9094308614730835\n",
      "step = 3982000: loss = 3.5973851680755615\n",
      "step = 3982200: loss = 3.0524232387542725\n",
      "step = 3982400: loss = 3.5198936462402344\n",
      "step = 3982600: loss = 3.384276866912842\n",
      "step = 3982800: loss = 4.12011194229126\n",
      "step = 3983000: loss = 3.506098747253418\n",
      "step = 3983200: loss = 4.604318618774414\n",
      "step = 3983400: loss = 3.2977116107940674\n",
      "step = 3983600: loss = 2.6136703491210938\n",
      "step = 3983800: loss = 3.100797653198242\n",
      "step = 3984000: loss = 3.2106306552886963\n",
      "step = 3984200: loss = 4.03685188293457\n",
      "step = 3984400: loss = 3.6571202278137207\n",
      "step = 3984600: loss = 3.5565578937530518\n",
      "step = 3984800: loss = 2.5180118083953857\n",
      "step = 3985000: loss = 3.8876030445098877\n",
      "step = 3985000: Average Return = 4.300000190734863\n",
      "step = 3985200: loss = 4.916950702667236\n",
      "step = 3985400: loss = 4.748892784118652\n",
      "step = 3985600: loss = 4.879711151123047\n",
      "step = 3985800: loss = 4.7584967613220215\n",
      "step = 3986000: loss = 4.827209949493408\n",
      "step = 3986200: loss = 2.5791168212890625\n",
      "step = 3986400: loss = 3.204660654067993\n",
      "step = 3986600: loss = 3.1741349697113037\n",
      "step = 3986800: loss = 3.451890707015991\n",
      "step = 3987000: loss = 3.2513558864593506\n",
      "step = 3987200: loss = 3.9926161766052246\n",
      "step = 3987400: loss = 5.145532131195068\n",
      "step = 3987600: loss = 2.578369140625\n",
      "step = 3987800: loss = 3.3797619342803955\n",
      "step = 3988000: loss = 3.6434426307678223\n",
      "step = 3988200: loss = 2.8081674575805664\n",
      "step = 3988400: loss = 3.1789586544036865\n",
      "step = 3988600: loss = 3.1785192489624023\n",
      "step = 3988800: loss = 2.214402437210083\n",
      "step = 3989000: loss = 3.1969337463378906\n",
      "step = 3989200: loss = 3.3668365478515625\n",
      "step = 3989400: loss = 3.2525851726531982\n",
      "step = 3989600: loss = 3.098536252975464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3989800: loss = 3.1729280948638916\n",
      "step = 3990000: loss = 3.829008102416992\n",
      "step = 3990000: Average Return = 4.0\n",
      "step = 3990200: loss = 3.5761120319366455\n",
      "step = 3990400: loss = 3.058306932449341\n",
      "step = 3990600: loss = 2.76963210105896\n",
      "step = 3990800: loss = 2.2326858043670654\n",
      "step = 3991000: loss = 5.1719889640808105\n",
      "step = 3991200: loss = 3.327901601791382\n",
      "step = 3991400: loss = 3.0529425144195557\n",
      "step = 3991600: loss = 4.228546619415283\n",
      "step = 3991800: loss = 3.072667360305786\n",
      "step = 3992000: loss = 3.0110979080200195\n",
      "step = 3992200: loss = 4.115808486938477\n",
      "step = 3992400: loss = 4.219928741455078\n",
      "step = 3992600: loss = 4.122441291809082\n",
      "step = 3992800: loss = 2.7465875148773193\n",
      "step = 3993000: loss = 3.25348162651062\n",
      "step = 3993200: loss = 3.3156063556671143\n",
      "step = 3993400: loss = 5.782318115234375\n",
      "step = 3993600: loss = 3.940952777862549\n",
      "step = 3993800: loss = 3.2666015625\n",
      "step = 3994000: loss = 3.2106285095214844\n",
      "step = 3994200: loss = 2.996458053588867\n",
      "step = 3994400: loss = 2.813521146774292\n",
      "step = 3994600: loss = 3.147408962249756\n",
      "step = 3994800: loss = 2.751814603805542\n",
      "step = 3995000: loss = 3.4098048210144043\n",
      "step = 3995000: Average Return = 4.449999809265137\n",
      "step = 3995200: loss = 5.474745273590088\n",
      "step = 3995400: loss = 4.198558807373047\n",
      "step = 3995600: loss = 2.6234819889068604\n",
      "step = 3995800: loss = 4.115181922912598\n",
      "step = 3996000: loss = 3.854360818862915\n",
      "step = 3996200: loss = 3.742213726043701\n",
      "step = 3996400: loss = 3.2195546627044678\n",
      "step = 3996600: loss = 3.433760404586792\n",
      "step = 3996800: loss = 4.060779094696045\n",
      "step = 3997000: loss = 3.395275831222534\n",
      "step = 3997200: loss = 3.104487657546997\n",
      "step = 3997400: loss = 5.877827167510986\n",
      "step = 3997600: loss = 5.248868465423584\n",
      "step = 3997800: loss = 2.8593225479125977\n",
      "step = 3998000: loss = 4.024728298187256\n",
      "step = 3998200: loss = 3.8690950870513916\n",
      "step = 3998400: loss = 3.89872670173645\n",
      "step = 3998600: loss = 3.9422552585601807\n",
      "step = 3998800: loss = 2.8941457271575928\n",
      "step = 3999000: loss = 4.444663047790527\n",
      "step = 3999200: loss = 6.174462795257568\n",
      "step = 3999400: loss = 2.2127223014831543\n",
      "step = 3999600: loss = 3.4617302417755127\n",
      "step = 3999800: loss = 3.2512316703796387\n",
      "step = 4000000: loss = 3.6523845195770264\n",
      "step = 4000000: Average Return = 5.650000095367432\n",
      "step = 4000200: loss = 4.007218360900879\n",
      "step = 4000400: loss = 3.312434673309326\n",
      "step = 4000600: loss = 3.8050999641418457\n",
      "step = 4000800: loss = 3.3209543228149414\n",
      "step = 4001000: loss = 3.492324113845825\n",
      "step = 4001200: loss = 3.909871816635132\n",
      "step = 4001400: loss = 3.50984787940979\n",
      "step = 4001600: loss = 3.221297025680542\n",
      "step = 4001800: loss = 1.8186107873916626\n",
      "step = 4002000: loss = 4.256937026977539\n",
      "step = 4002200: loss = 3.1771092414855957\n",
      "step = 4002400: loss = 3.699120283126831\n",
      "step = 4002600: loss = 3.4635913372039795\n",
      "step = 4002800: loss = 3.606311798095703\n",
      "step = 4003000: loss = 3.3532705307006836\n",
      "step = 4003200: loss = 2.910231828689575\n",
      "step = 4003400: loss = 3.5622856616973877\n",
      "step = 4003600: loss = 3.5126523971557617\n",
      "step = 4003800: loss = 4.006946086883545\n",
      "step = 4004000: loss = 3.3463900089263916\n",
      "step = 4004200: loss = 3.8153069019317627\n",
      "step = 4004400: loss = 4.078227996826172\n",
      "step = 4004600: loss = 4.156336784362793\n",
      "step = 4004800: loss = 4.892639636993408\n",
      "step = 4005000: loss = 3.0634357929229736\n",
      "step = 4005000: Average Return = 3.200000047683716\n",
      "step = 4005200: loss = 3.384460687637329\n",
      "step = 4005400: loss = 3.886098623275757\n",
      "step = 4005600: loss = 3.295186758041382\n",
      "step = 4005800: loss = 3.9120771884918213\n",
      "step = 4006000: loss = 4.39255428314209\n",
      "step = 4006200: loss = 3.0954577922821045\n",
      "step = 4006400: loss = 2.788459300994873\n",
      "step = 4006600: loss = 4.617711544036865\n",
      "step = 4006800: loss = 3.9601829051971436\n",
      "step = 4007000: loss = 4.1315131187438965\n",
      "step = 4007200: loss = 4.838981628417969\n",
      "step = 4007400: loss = 3.0875966548919678\n",
      "step = 4007600: loss = 3.91196346282959\n",
      "step = 4007800: loss = 4.942168712615967\n",
      "step = 4008000: loss = 2.8041961193084717\n",
      "step = 4008200: loss = 2.974412441253662\n",
      "step = 4008400: loss = 4.187499046325684\n",
      "step = 4008600: loss = 2.4998230934143066\n",
      "step = 4008800: loss = 3.8990139961242676\n",
      "step = 4009000: loss = 4.288943290710449\n",
      "step = 4009200: loss = 3.392502784729004\n",
      "step = 4009400: loss = 3.7747411727905273\n",
      "step = 4009600: loss = 2.6906609535217285\n",
      "step = 4009800: loss = 3.0613350868225098\n",
      "step = 4010000: loss = 2.787261962890625\n",
      "step = 4010000: Average Return = 3.9000000953674316\n",
      "step = 4010200: loss = 3.240131139755249\n",
      "step = 4010400: loss = 2.2803070545196533\n",
      "step = 4010600: loss = 2.9575178623199463\n",
      "step = 4010800: loss = 4.606055736541748\n",
      "step = 4011000: loss = 3.631434202194214\n",
      "step = 4011200: loss = 3.395514488220215\n",
      "step = 4011400: loss = 2.756150484085083\n",
      "step = 4011600: loss = 3.6872732639312744\n",
      "step = 4011800: loss = 3.3973066806793213\n",
      "step = 4012000: loss = 3.134965181350708\n",
      "step = 4012200: loss = 3.0461459159851074\n",
      "step = 4012400: loss = 5.479820251464844\n",
      "step = 4012600: loss = 2.4958746433258057\n",
      "step = 4012800: loss = 4.289648532867432\n",
      "step = 4013000: loss = 4.039209842681885\n",
      "step = 4013200: loss = 3.552403688430786\n",
      "step = 4013400: loss = 3.38002610206604\n",
      "step = 4013600: loss = 4.169190406799316\n",
      "step = 4013800: loss = 3.476910352706909\n",
      "step = 4014000: loss = 3.246277332305908\n",
      "step = 4014200: loss = 2.508387327194214\n",
      "step = 4014400: loss = 3.550187110900879\n",
      "step = 4014600: loss = 4.56036376953125\n",
      "step = 4014800: loss = 5.037283897399902\n",
      "step = 4015000: loss = 3.210239887237549\n",
      "step = 4015000: Average Return = 2.4000000953674316\n",
      "step = 4015200: loss = 3.010481595993042\n",
      "step = 4015400: loss = 2.9252073764801025\n",
      "step = 4015600: loss = 3.828479528427124\n",
      "step = 4015800: loss = 5.073912620544434\n",
      "step = 4016000: loss = 3.544853448867798\n",
      "step = 4016200: loss = 4.823349475860596\n",
      "step = 4016400: loss = 3.299518346786499\n",
      "step = 4016600: loss = 4.598151206970215\n",
      "step = 4016800: loss = 2.346590042114258\n",
      "step = 4017000: loss = 4.718743324279785\n",
      "step = 4017200: loss = 3.8346471786499023\n",
      "step = 4017400: loss = 4.775963306427002\n",
      "step = 4017600: loss = 3.17555832862854\n",
      "step = 4017800: loss = 3.3868377208709717\n",
      "step = 4018000: loss = 3.561849355697632\n",
      "step = 4018200: loss = 4.142396450042725\n",
      "step = 4018400: loss = 4.454246520996094\n",
      "step = 4018600: loss = 5.058136463165283\n",
      "step = 4018800: loss = 2.288468599319458\n",
      "step = 4019000: loss = 2.2824177742004395\n",
      "step = 4019200: loss = 2.6595067977905273\n",
      "step = 4019400: loss = 4.329798698425293\n",
      "step = 4019600: loss = 3.4097208976745605\n",
      "step = 4019800: loss = 3.412398338317871\n",
      "step = 4020000: loss = 3.183211326599121\n",
      "step = 4020000: Average Return = 4.199999809265137\n",
      "step = 4020200: loss = 4.058499813079834\n",
      "step = 4020400: loss = 3.4186599254608154\n",
      "step = 4020600: loss = 4.229613780975342\n",
      "step = 4020800: loss = 3.5956296920776367\n",
      "step = 4021000: loss = 4.680636882781982\n",
      "step = 4021200: loss = 4.734625816345215\n",
      "step = 4021400: loss = 4.754629611968994\n",
      "step = 4021600: loss = 3.522184133529663\n",
      "step = 4021800: loss = 4.064674377441406\n",
      "step = 4022000: loss = 2.5335397720336914\n",
      "step = 4022200: loss = 3.697495698928833\n",
      "step = 4022400: loss = 4.032320022583008\n",
      "step = 4022600: loss = 3.227292060852051\n",
      "step = 4022800: loss = 4.351839542388916\n",
      "step = 4023000: loss = 2.486034870147705\n",
      "step = 4023200: loss = 2.2732248306274414\n",
      "step = 4023400: loss = 4.5780029296875\n",
      "step = 4023600: loss = 3.941162586212158\n",
      "step = 4023800: loss = 3.727341890335083\n",
      "step = 4024000: loss = 3.387683391571045\n",
      "step = 4024200: loss = 4.493649959564209\n",
      "step = 4024400: loss = 2.977130651473999\n",
      "step = 4024600: loss = 4.447432041168213\n",
      "step = 4024800: loss = 3.3429205417633057\n",
      "step = 4025000: loss = 3.0997157096862793\n",
      "step = 4025000: Average Return = 4.550000190734863\n",
      "step = 4025200: loss = 3.3835058212280273\n",
      "step = 4025400: loss = 3.4224770069122314\n",
      "step = 4025600: loss = 2.956144094467163\n",
      "step = 4025800: loss = 3.622509717941284\n",
      "step = 4026000: loss = 3.070971727371216\n",
      "step = 4026200: loss = 2.953131675720215\n",
      "step = 4026400: loss = 3.492666006088257\n",
      "step = 4026600: loss = 3.0279903411865234\n",
      "step = 4026800: loss = 3.0028839111328125\n",
      "step = 4027000: loss = 3.491680145263672\n",
      "step = 4027200: loss = 3.5419769287109375\n",
      "step = 4027400: loss = 3.6253368854522705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4027600: loss = 4.47185754776001\n",
      "step = 4027800: loss = 2.8184587955474854\n",
      "step = 4028000: loss = 2.6227662563323975\n",
      "step = 4028200: loss = 3.6183769702911377\n",
      "step = 4028400: loss = 2.4797215461730957\n",
      "step = 4028600: loss = 3.2246317863464355\n",
      "step = 4028800: loss = 3.3407180309295654\n",
      "step = 4029000: loss = 3.419066905975342\n",
      "step = 4029200: loss = 2.7314767837524414\n",
      "step = 4029400: loss = 3.617985248565674\n",
      "step = 4029600: loss = 4.435588359832764\n",
      "step = 4029800: loss = 3.527566432952881\n",
      "step = 4030000: loss = 4.044567584991455\n",
      "step = 4030000: Average Return = 4.099999904632568\n",
      "step = 4030200: loss = 2.8331336975097656\n",
      "step = 4030400: loss = 3.3040385246276855\n",
      "step = 4030600: loss = 2.3085110187530518\n",
      "step = 4030800: loss = 3.013932943344116\n",
      "step = 4031000: loss = 3.759343147277832\n",
      "step = 4031200: loss = 2.217691659927368\n",
      "step = 4031400: loss = 2.9683892726898193\n",
      "step = 4031600: loss = 3.1799817085266113\n",
      "step = 4031800: loss = 3.18967342376709\n",
      "step = 4032000: loss = 3.489496946334839\n",
      "step = 4032200: loss = 3.2230234146118164\n",
      "step = 4032400: loss = 3.4870545864105225\n",
      "step = 4032600: loss = 2.808946371078491\n",
      "step = 4032800: loss = 2.8962743282318115\n",
      "step = 4033000: loss = 3.501946449279785\n",
      "step = 4033200: loss = 3.9381566047668457\n",
      "step = 4033400: loss = 3.7382965087890625\n",
      "step = 4033600: loss = 3.8132710456848145\n",
      "step = 4033800: loss = 3.829594373703003\n",
      "step = 4034000: loss = 4.040999889373779\n",
      "step = 4034200: loss = 2.7490224838256836\n",
      "step = 4034400: loss = 3.5125386714935303\n",
      "step = 4034600: loss = 4.443029880523682\n",
      "step = 4034800: loss = 3.4651360511779785\n",
      "step = 4035000: loss = 3.1541197299957275\n",
      "step = 4035000: Average Return = 7.400000095367432\n",
      "step = 4035200: loss = 2.9681689739227295\n",
      "step = 4035400: loss = 2.2552194595336914\n",
      "step = 4035600: loss = 4.313381671905518\n",
      "step = 4035800: loss = 3.3166990280151367\n",
      "step = 4036000: loss = 3.491691827774048\n",
      "step = 4036200: loss = 3.933863639831543\n",
      "step = 4036400: loss = 3.1232845783233643\n",
      "step = 4036600: loss = 2.232335090637207\n",
      "step = 4036800: loss = 2.258103609085083\n",
      "step = 4037000: loss = 5.212975978851318\n",
      "step = 4037200: loss = 3.6196672916412354\n",
      "step = 4037400: loss = 3.948148012161255\n",
      "step = 4037600: loss = 3.0438060760498047\n",
      "step = 4037800: loss = 3.3810009956359863\n",
      "step = 4038000: loss = 2.8341593742370605\n",
      "step = 4038200: loss = 3.3018455505371094\n",
      "step = 4038400: loss = 2.363659620285034\n",
      "step = 4038600: loss = 3.659932851791382\n",
      "step = 4038800: loss = 2.904622793197632\n",
      "step = 4039000: loss = 3.24272084236145\n",
      "step = 4039200: loss = 3.309778928756714\n",
      "step = 4039400: loss = 2.6486425399780273\n",
      "step = 4039600: loss = 3.2100017070770264\n",
      "step = 4039800: loss = 3.3932952880859375\n",
      "step = 4040000: loss = 4.1827616691589355\n",
      "step = 4040000: Average Return = 3.700000047683716\n",
      "step = 4040200: loss = 3.12701153755188\n",
      "step = 4040400: loss = 3.9619979858398438\n",
      "step = 4040600: loss = 3.6228060722351074\n",
      "step = 4040800: loss = 3.574808359146118\n",
      "step = 4041000: loss = 3.8505077362060547\n",
      "step = 4041200: loss = 4.054313659667969\n",
      "step = 4041400: loss = 4.3663506507873535\n",
      "step = 4041600: loss = 2.737394094467163\n",
      "step = 4041800: loss = 5.263803482055664\n",
      "step = 4042000: loss = 4.281555652618408\n",
      "step = 4042200: loss = 3.072662353515625\n",
      "step = 4042400: loss = 4.281740188598633\n",
      "step = 4042600: loss = 3.210707902908325\n",
      "step = 4042800: loss = 4.37869930267334\n",
      "step = 4043000: loss = 2.591878652572632\n",
      "step = 4043200: loss = 2.922168493270874\n",
      "step = 4043400: loss = 4.252840518951416\n",
      "step = 4043600: loss = 3.5743072032928467\n",
      "step = 4043800: loss = 3.9849181175231934\n",
      "step = 4044000: loss = 3.5975725650787354\n",
      "step = 4044200: loss = 2.600797653198242\n",
      "step = 4044400: loss = 4.833125114440918\n",
      "step = 4044600: loss = 4.595322608947754\n",
      "step = 4044800: loss = 3.2014870643615723\n",
      "step = 4045000: loss = 3.84873628616333\n",
      "step = 4045000: Average Return = 5.75\n",
      "step = 4045200: loss = 3.9051513671875\n",
      "step = 4045400: loss = 3.92299485206604\n",
      "step = 4045600: loss = 3.6058692932128906\n",
      "step = 4045800: loss = 3.476357936859131\n",
      "step = 4046000: loss = 3.47772216796875\n",
      "step = 4046200: loss = 3.088176727294922\n",
      "step = 4046400: loss = 4.191280841827393\n",
      "step = 4046600: loss = 3.701510429382324\n",
      "step = 4046800: loss = 4.438743591308594\n",
      "step = 4047000: loss = 5.384129524230957\n",
      "step = 4047200: loss = 4.304938793182373\n",
      "step = 4047400: loss = 3.38779354095459\n",
      "step = 4047600: loss = 3.121922016143799\n",
      "step = 4047800: loss = 3.1371867656707764\n",
      "step = 4048000: loss = 4.173828601837158\n",
      "step = 4048200: loss = 3.880495548248291\n",
      "step = 4048400: loss = 3.661189079284668\n",
      "step = 4048600: loss = 4.156423091888428\n",
      "step = 4048800: loss = 4.032998085021973\n",
      "step = 4049000: loss = 4.363490104675293\n",
      "step = 4049200: loss = 1.8017373085021973\n",
      "step = 4049400: loss = 3.0743868350982666\n",
      "step = 4049600: loss = 3.461395740509033\n",
      "step = 4049800: loss = 3.591002941131592\n",
      "step = 4050000: loss = 2.7417125701904297\n",
      "step = 4050000: Average Return = 5.550000190734863\n",
      "step = 4050200: loss = 3.770280361175537\n",
      "step = 4050400: loss = 4.173943519592285\n",
      "step = 4050600: loss = 2.41274094581604\n",
      "step = 4050800: loss = 2.897742509841919\n",
      "step = 4051000: loss = 3.50642991065979\n",
      "step = 4051200: loss = 5.48884391784668\n",
      "step = 4051400: loss = 2.714315176010132\n",
      "step = 4051600: loss = 4.155354022979736\n",
      "step = 4051800: loss = 3.5936338901519775\n",
      "step = 4052000: loss = 3.157660961151123\n",
      "step = 4052200: loss = 4.621678829193115\n",
      "step = 4052400: loss = 3.5353200435638428\n",
      "step = 4052600: loss = 3.1987788677215576\n",
      "step = 4052800: loss = 2.8128857612609863\n",
      "step = 4053000: loss = 2.9620730876922607\n",
      "step = 4053200: loss = 4.177447319030762\n",
      "step = 4053400: loss = 4.091376781463623\n",
      "step = 4053600: loss = 2.6827232837677\n",
      "step = 4053800: loss = 3.216346263885498\n",
      "step = 4054000: loss = 3.977419853210449\n",
      "step = 4054200: loss = 4.477099895477295\n",
      "step = 4054400: loss = 4.911839008331299\n",
      "step = 4054600: loss = 4.677402973175049\n",
      "step = 4054800: loss = 3.9067442417144775\n",
      "step = 4055000: loss = 3.8261077404022217\n",
      "step = 4055000: Average Return = 5.449999809265137\n",
      "step = 4055200: loss = 3.860752582550049\n",
      "step = 4055400: loss = 3.0444858074188232\n",
      "step = 4055600: loss = 4.410064697265625\n",
      "step = 4055800: loss = 3.0291874408721924\n",
      "step = 4056000: loss = 3.6451568603515625\n",
      "step = 4056200: loss = 2.876291036605835\n",
      "step = 4056400: loss = 4.216678619384766\n",
      "step = 4056600: loss = 2.833207845687866\n",
      "step = 4056800: loss = 3.0188426971435547\n",
      "step = 4057000: loss = 5.910905838012695\n",
      "step = 4057200: loss = 5.375126838684082\n",
      "step = 4057400: loss = 2.6400842666625977\n",
      "step = 4057600: loss = 2.5934460163116455\n",
      "step = 4057800: loss = 4.6881279945373535\n",
      "step = 4058000: loss = 2.6713638305664062\n",
      "step = 4058200: loss = 4.838372707366943\n",
      "step = 4058400: loss = 4.234996318817139\n",
      "step = 4058600: loss = 4.1778717041015625\n",
      "step = 4058800: loss = 3.5804522037506104\n",
      "step = 4059000: loss = 4.01338529586792\n",
      "step = 4059200: loss = 4.796780109405518\n",
      "step = 4059400: loss = 2.9505882263183594\n",
      "step = 4059600: loss = 3.3067128658294678\n",
      "step = 4059800: loss = 4.7349371910095215\n",
      "step = 4060000: loss = 2.7340312004089355\n",
      "step = 4060000: Average Return = 3.5999999046325684\n",
      "step = 4060200: loss = 3.188309669494629\n",
      "step = 4060400: loss = 2.926347494125366\n",
      "step = 4060600: loss = 4.452787399291992\n",
      "step = 4060800: loss = 4.35084342956543\n",
      "step = 4061000: loss = 2.434659719467163\n",
      "step = 4061200: loss = 4.351150035858154\n",
      "step = 4061400: loss = 4.109353065490723\n",
      "step = 4061600: loss = 3.157421112060547\n",
      "step = 4061800: loss = 2.502152442932129\n",
      "step = 4062000: loss = 3.6615850925445557\n",
      "step = 4062200: loss = 4.053044319152832\n",
      "step = 4062400: loss = 2.841644763946533\n",
      "step = 4062600: loss = 4.784541130065918\n",
      "step = 4062800: loss = 3.4070687294006348\n",
      "step = 4063000: loss = 3.146541118621826\n",
      "step = 4063200: loss = 5.318440914154053\n",
      "step = 4063400: loss = 3.928598165512085\n",
      "step = 4063600: loss = 3.78495717048645\n",
      "step = 4063800: loss = 4.559542179107666\n",
      "step = 4064000: loss = 3.5927364826202393\n",
      "step = 4064200: loss = 5.637468338012695\n",
      "step = 4064400: loss = 5.780650615692139\n",
      "step = 4064600: loss = 3.0926272869110107\n",
      "step = 4064800: loss = 3.4623916149139404\n",
      "step = 4065000: loss = 2.2194859981536865\n",
      "step = 4065000: Average Return = 3.9000000953674316\n",
      "step = 4065200: loss = 3.2956817150115967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4065400: loss = 3.4610435962677\n",
      "step = 4065600: loss = 3.8989694118499756\n",
      "step = 4065800: loss = 5.0723443031311035\n",
      "step = 4066000: loss = 2.3477859497070312\n",
      "step = 4066200: loss = 3.748163938522339\n",
      "step = 4066400: loss = 3.768965482711792\n",
      "step = 4066600: loss = 3.280941963195801\n",
      "step = 4066800: loss = 3.137009620666504\n",
      "step = 4067000: loss = 2.8799641132354736\n",
      "step = 4067200: loss = 2.766416072845459\n",
      "step = 4067400: loss = 3.606534004211426\n",
      "step = 4067600: loss = 3.894463062286377\n",
      "step = 4067800: loss = 2.2985429763793945\n",
      "step = 4068000: loss = 2.9491496086120605\n",
      "step = 4068200: loss = 3.3523449897766113\n",
      "step = 4068400: loss = 3.03987979888916\n",
      "step = 4068600: loss = 3.776768922805786\n",
      "step = 4068800: loss = 3.5117945671081543\n",
      "step = 4069000: loss = 3.862800121307373\n",
      "step = 4069200: loss = 2.736389398574829\n",
      "step = 4069400: loss = 3.84871506690979\n",
      "step = 4069600: loss = 2.5441675186157227\n",
      "step = 4069800: loss = 3.282137870788574\n",
      "step = 4070000: loss = 4.253564834594727\n",
      "step = 4070000: Average Return = 5.75\n",
      "step = 4070200: loss = 3.5432262420654297\n",
      "step = 4070400: loss = 4.9282050132751465\n",
      "step = 4070600: loss = 4.044454097747803\n",
      "step = 4070800: loss = 4.672354698181152\n",
      "step = 4071000: loss = 3.589087963104248\n",
      "step = 4071200: loss = 3.7080254554748535\n",
      "step = 4071400: loss = 4.419967174530029\n",
      "step = 4071600: loss = 4.01846170425415\n",
      "step = 4071800: loss = 3.726422071456909\n",
      "step = 4072000: loss = 2.692258834838867\n",
      "step = 4072200: loss = 4.212559700012207\n",
      "step = 4072400: loss = 4.2188849449157715\n",
      "step = 4072600: loss = 2.898554563522339\n",
      "step = 4072800: loss = 2.7981669902801514\n",
      "step = 4073000: loss = 2.9501726627349854\n",
      "step = 4073200: loss = 3.577077865600586\n",
      "step = 4073400: loss = 2.826382637023926\n",
      "step = 4073600: loss = 3.6327292919158936\n",
      "step = 4073800: loss = 3.1805660724639893\n",
      "step = 4074000: loss = 2.3268203735351562\n",
      "step = 4074200: loss = 3.749898910522461\n",
      "step = 4074400: loss = 3.572671413421631\n",
      "step = 4074600: loss = 3.8954200744628906\n",
      "step = 4074800: loss = 3.62548828125\n",
      "step = 4075000: loss = 3.8823630809783936\n",
      "step = 4075000: Average Return = 2.6500000953674316\n",
      "step = 4075200: loss = 5.083886623382568\n",
      "step = 4075400: loss = 3.0977816581726074\n",
      "step = 4075600: loss = 4.201097011566162\n",
      "step = 4075800: loss = 2.5725197792053223\n",
      "step = 4076000: loss = 4.188419818878174\n",
      "step = 4076200: loss = 3.7266714572906494\n",
      "step = 4076400: loss = 2.816972494125366\n",
      "step = 4076600: loss = 3.7458367347717285\n",
      "step = 4076800: loss = 2.602067470550537\n",
      "step = 4077000: loss = 3.449293851852417\n",
      "step = 4077200: loss = 3.899109125137329\n",
      "step = 4077400: loss = 4.8394775390625\n",
      "step = 4077600: loss = 3.9612269401550293\n",
      "step = 4077800: loss = 4.085892677307129\n",
      "step = 4078000: loss = 2.706369161605835\n",
      "step = 4078200: loss = 3.0654289722442627\n",
      "step = 4078400: loss = 3.2756364345550537\n",
      "step = 4078600: loss = 3.6448311805725098\n",
      "step = 4078800: loss = 5.521297454833984\n",
      "step = 4079000: loss = 4.172866344451904\n",
      "step = 4079200: loss = 3.1508493423461914\n",
      "step = 4079400: loss = 3.0194005966186523\n",
      "step = 4079600: loss = 4.142197608947754\n",
      "step = 4079800: loss = 5.767299175262451\n",
      "step = 4080000: loss = 3.8115715980529785\n",
      "step = 4080000: Average Return = 6.150000095367432\n",
      "step = 4080200: loss = 4.465863227844238\n",
      "step = 4080400: loss = 3.634999990463257\n",
      "step = 4080600: loss = 2.4757015705108643\n",
      "step = 4080800: loss = 3.708871364593506\n",
      "step = 4081000: loss = 4.3074445724487305\n",
      "step = 4081200: loss = 3.4140408039093018\n",
      "step = 4081400: loss = 2.7283618450164795\n",
      "step = 4081600: loss = 4.629969596862793\n",
      "step = 4081800: loss = 2.7466728687286377\n",
      "step = 4082000: loss = 2.601146697998047\n",
      "step = 4082200: loss = 2.9046080112457275\n",
      "step = 4082400: loss = 3.9760775566101074\n",
      "step = 4082600: loss = 2.297713041305542\n",
      "step = 4082800: loss = 3.9271721839904785\n",
      "step = 4083000: loss = 2.6652793884277344\n",
      "step = 4083200: loss = 3.635993003845215\n",
      "step = 4083400: loss = 2.7798845767974854\n",
      "step = 4083600: loss = 4.081662654876709\n",
      "step = 4083800: loss = 3.6535937786102295\n",
      "step = 4084000: loss = 4.518593788146973\n",
      "step = 4084200: loss = 3.889362335205078\n",
      "step = 4084400: loss = 3.67523455619812\n",
      "step = 4084600: loss = 4.0498456954956055\n",
      "step = 4084800: loss = 2.703122138977051\n",
      "step = 4085000: loss = 3.6051619052886963\n",
      "step = 4085000: Average Return = 6.25\n",
      "step = 4085200: loss = 4.554158687591553\n",
      "step = 4085400: loss = 2.9536266326904297\n",
      "step = 4085600: loss = 3.0376930236816406\n",
      "step = 4085800: loss = 2.532186508178711\n",
      "step = 4086000: loss = 3.1585607528686523\n",
      "step = 4086200: loss = 3.0995662212371826\n",
      "step = 4086400: loss = 3.614427328109741\n",
      "step = 4086600: loss = 3.444448232650757\n",
      "step = 4086800: loss = 2.789820432662964\n",
      "step = 4087000: loss = 3.6153564453125\n",
      "step = 4087200: loss = 2.9724502563476562\n",
      "step = 4087400: loss = 3.395374059677124\n",
      "step = 4087600: loss = 2.4508769512176514\n",
      "step = 4087800: loss = 3.301424980163574\n",
      "step = 4088000: loss = 2.4567275047302246\n",
      "step = 4088200: loss = 3.994319438934326\n",
      "step = 4088400: loss = 3.130147695541382\n",
      "step = 4088600: loss = 2.6443493366241455\n",
      "step = 4088800: loss = 3.2991514205932617\n",
      "step = 4089000: loss = 2.672917366027832\n",
      "step = 4089200: loss = 4.136812210083008\n",
      "step = 4089400: loss = 2.98630690574646\n",
      "step = 4089600: loss = 4.355745792388916\n",
      "step = 4089800: loss = 3.964160680770874\n",
      "step = 4090000: loss = 4.235424041748047\n",
      "step = 4090000: Average Return = 3.799999952316284\n",
      "step = 4090200: loss = 4.495143890380859\n",
      "step = 4090400: loss = 3.259483575820923\n",
      "step = 4090600: loss = 3.7421679496765137\n",
      "step = 4090800: loss = 2.8459489345550537\n",
      "step = 4091000: loss = 3.34342622756958\n",
      "step = 4091200: loss = 4.43963098526001\n",
      "step = 4091400: loss = 3.4416582584381104\n",
      "step = 4091600: loss = 5.456637859344482\n",
      "step = 4091800: loss = 2.9125359058380127\n",
      "step = 4092000: loss = 3.354112386703491\n",
      "step = 4092200: loss = 4.873291015625\n",
      "step = 4092400: loss = 5.033676624298096\n",
      "step = 4092600: loss = 3.4983346462249756\n",
      "step = 4092800: loss = 2.974994659423828\n",
      "step = 4093000: loss = 2.518167495727539\n",
      "step = 4093200: loss = 1.585417628288269\n",
      "step = 4093400: loss = 4.569403171539307\n",
      "step = 4093600: loss = 2.930894136428833\n",
      "step = 4093800: loss = 4.225194454193115\n",
      "step = 4094000: loss = 3.5640411376953125\n",
      "step = 4094200: loss = 5.392288684844971\n",
      "step = 4094400: loss = 5.840644359588623\n",
      "step = 4094600: loss = 3.8383352756500244\n",
      "step = 4094800: loss = 2.075005054473877\n",
      "step = 4095000: loss = 3.841447353363037\n",
      "step = 4095000: Average Return = 4.650000095367432\n",
      "step = 4095200: loss = 5.119184494018555\n",
      "step = 4095400: loss = 2.7354326248168945\n",
      "step = 4095600: loss = 4.716001510620117\n",
      "step = 4095800: loss = 3.3613128662109375\n",
      "step = 4096000: loss = 3.205801248550415\n",
      "step = 4096200: loss = 3.625406503677368\n",
      "step = 4096400: loss = 4.015438556671143\n",
      "step = 4096600: loss = 2.8149609565734863\n",
      "step = 4096800: loss = 3.943965435028076\n",
      "step = 4097000: loss = 3.069366455078125\n",
      "step = 4097200: loss = 3.7793798446655273\n",
      "step = 4097400: loss = 3.1876211166381836\n",
      "step = 4097600: loss = 2.476579189300537\n",
      "step = 4097800: loss = 3.464381694793701\n",
      "step = 4098000: loss = 3.7776947021484375\n",
      "step = 4098200: loss = 4.7289509773254395\n",
      "step = 4098400: loss = 5.790151119232178\n",
      "step = 4098600: loss = 3.8910140991210938\n",
      "step = 4098800: loss = 3.464139938354492\n",
      "step = 4099000: loss = 4.254022121429443\n",
      "step = 4099200: loss = 5.5717387199401855\n",
      "step = 4099400: loss = 3.722715377807617\n",
      "step = 4099600: loss = 3.1710875034332275\n",
      "step = 4099800: loss = 3.7113945484161377\n",
      "step = 4100000: loss = 3.6645994186401367\n",
      "step = 4100000: Average Return = 4.949999809265137\n",
      "step = 4100200: loss = 4.028609752655029\n",
      "step = 4100400: loss = 4.414567947387695\n",
      "step = 4100600: loss = 2.578890323638916\n",
      "step = 4100800: loss = 3.3869574069976807\n",
      "step = 4101000: loss = 4.042895317077637\n",
      "step = 4101200: loss = 4.2607879638671875\n",
      "step = 4101400: loss = 3.3917076587677\n",
      "step = 4101600: loss = 3.5009605884552\n",
      "step = 4101800: loss = 3.1433827877044678\n",
      "step = 4102000: loss = 4.029430389404297\n",
      "step = 4102200: loss = 4.717360973358154\n",
      "step = 4102400: loss = 3.609083414077759\n",
      "step = 4102600: loss = 4.476128578186035\n",
      "step = 4102800: loss = 1.8043705224990845\n",
      "step = 4103000: loss = 4.508384704589844\n",
      "step = 4103200: loss = 4.348888874053955\n",
      "step = 4103400: loss = 2.691178798675537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4103600: loss = 3.870267868041992\n",
      "step = 4103800: loss = 5.171684265136719\n",
      "step = 4104000: loss = 2.5909831523895264\n",
      "step = 4104200: loss = 3.4005136489868164\n",
      "step = 4104400: loss = 2.942864418029785\n",
      "step = 4104600: loss = 3.9387753009796143\n",
      "step = 4104800: loss = 3.354363441467285\n",
      "step = 4105000: loss = 2.9918572902679443\n",
      "step = 4105000: Average Return = 5.349999904632568\n",
      "step = 4105200: loss = 3.178192615509033\n",
      "step = 4105400: loss = 4.597212314605713\n",
      "step = 4105600: loss = 3.525118827819824\n",
      "step = 4105800: loss = 3.4030964374542236\n",
      "step = 4106000: loss = 2.7970848083496094\n",
      "step = 4106200: loss = 3.4486329555511475\n",
      "step = 4106400: loss = 2.7806200981140137\n",
      "step = 4106600: loss = 2.486103057861328\n",
      "step = 4106800: loss = 4.099996089935303\n",
      "step = 4107000: loss = 2.6392741203308105\n",
      "step = 4107200: loss = 4.094640254974365\n",
      "step = 4107400: loss = 2.9564244747161865\n",
      "step = 4107600: loss = 4.307941436767578\n",
      "step = 4107800: loss = 3.3256072998046875\n",
      "step = 4108000: loss = 4.351325988769531\n",
      "step = 4108200: loss = 3.23687744140625\n",
      "step = 4108400: loss = 4.767873287200928\n",
      "step = 4108600: loss = 2.9576992988586426\n",
      "step = 4108800: loss = 3.4192326068878174\n",
      "step = 4109000: loss = 4.462214469909668\n",
      "step = 4109200: loss = 3.2830967903137207\n",
      "step = 4109400: loss = 3.655147075653076\n",
      "step = 4109600: loss = 3.519310235977173\n",
      "step = 4109800: loss = 2.767864942550659\n",
      "step = 4110000: loss = 3.883472204208374\n",
      "step = 4110000: Average Return = 6.300000190734863\n",
      "step = 4110200: loss = 4.0007734298706055\n",
      "step = 4110400: loss = 3.30826473236084\n",
      "step = 4110600: loss = 2.4619081020355225\n",
      "step = 4110800: loss = 2.7844743728637695\n",
      "step = 4111000: loss = 3.426645517349243\n",
      "step = 4111200: loss = 4.26217794418335\n",
      "step = 4111400: loss = 2.2781288623809814\n",
      "step = 4111600: loss = 2.928814649581909\n",
      "step = 4111800: loss = 3.9804160594940186\n",
      "step = 4112000: loss = 3.2465832233428955\n",
      "step = 4112200: loss = 7.469256401062012\n",
      "step = 4112400: loss = 2.559234857559204\n",
      "step = 4112600: loss = 2.8503947257995605\n",
      "step = 4112800: loss = 3.521524667739868\n",
      "step = 4113000: loss = 4.386673450469971\n",
      "step = 4113200: loss = 3.851526975631714\n",
      "step = 4113400: loss = 3.395725727081299\n",
      "step = 4113600: loss = 3.4914681911468506\n",
      "step = 4113800: loss = 2.6593146324157715\n",
      "step = 4114000: loss = 3.847663402557373\n",
      "step = 4114200: loss = 3.0279862880706787\n",
      "step = 4114400: loss = 3.0310134887695312\n",
      "step = 4114600: loss = 3.7645018100738525\n",
      "step = 4114800: loss = 4.04466438293457\n",
      "step = 4115000: loss = 3.3053624629974365\n",
      "step = 4115000: Average Return = 3.8499999046325684\n",
      "step = 4115200: loss = 4.20851469039917\n",
      "step = 4115400: loss = 2.546552896499634\n",
      "step = 4115600: loss = 3.9188239574432373\n",
      "step = 4115800: loss = 3.1699910163879395\n",
      "step = 4116000: loss = 4.470103740692139\n",
      "step = 4116200: loss = 3.3987340927124023\n",
      "step = 4116400: loss = 3.5420188903808594\n",
      "step = 4116600: loss = 2.295701742172241\n",
      "step = 4116800: loss = 4.045422554016113\n",
      "step = 4117000: loss = 3.966677188873291\n",
      "step = 4117200: loss = 3.372241735458374\n",
      "step = 4117400: loss = 3.9755449295043945\n",
      "step = 4117600: loss = 2.231180191040039\n",
      "step = 4117800: loss = 4.216420650482178\n",
      "step = 4118000: loss = 4.608176231384277\n",
      "step = 4118200: loss = 2.426609992980957\n",
      "step = 4118400: loss = 3.548680305480957\n",
      "step = 4118600: loss = 4.044543743133545\n",
      "step = 4118800: loss = 4.287850856781006\n",
      "step = 4119000: loss = 2.868345260620117\n",
      "step = 4119200: loss = 3.3797662258148193\n",
      "step = 4119400: loss = 3.8615660667419434\n",
      "step = 4119600: loss = 3.325199842453003\n",
      "step = 4119800: loss = 2.934387445449829\n",
      "step = 4120000: loss = 3.1991167068481445\n",
      "step = 4120000: Average Return = 3.549999952316284\n",
      "step = 4120200: loss = 3.1900835037231445\n",
      "step = 4120400: loss = 3.5535528659820557\n",
      "step = 4120600: loss = 3.9270434379577637\n",
      "step = 4120800: loss = 4.929643630981445\n",
      "step = 4121000: loss = 3.7808337211608887\n",
      "step = 4121200: loss = 2.934526205062866\n",
      "step = 4121400: loss = 3.7635905742645264\n",
      "step = 4121600: loss = 3.867870330810547\n",
      "step = 4121800: loss = 2.8377597332000732\n",
      "step = 4122000: loss = 2.842782497406006\n",
      "step = 4122200: loss = 3.7017533779144287\n",
      "step = 4122400: loss = 3.1053080558776855\n",
      "step = 4122600: loss = 4.220794200897217\n",
      "step = 4122800: loss = 4.413253307342529\n",
      "step = 4123000: loss = 3.091449022293091\n",
      "step = 4123200: loss = 4.926836967468262\n",
      "step = 4123400: loss = 2.7947442531585693\n",
      "step = 4123600: loss = 4.252084732055664\n",
      "step = 4123800: loss = 3.398340940475464\n",
      "step = 4124000: loss = 2.7762439250946045\n",
      "step = 4124200: loss = 2.6992573738098145\n",
      "step = 4124400: loss = 3.6631927490234375\n",
      "step = 4124600: loss = 3.375760078430176\n",
      "step = 4124800: loss = 3.6647207736968994\n",
      "step = 4125000: loss = 2.7138431072235107\n",
      "step = 4125000: Average Return = 4.0\n",
      "step = 4125200: loss = 1.981224775314331\n",
      "step = 4125400: loss = 3.2657411098480225\n",
      "step = 4125600: loss = 4.473795413970947\n",
      "step = 4125800: loss = 3.5703752040863037\n",
      "step = 4126000: loss = 2.870673418045044\n",
      "step = 4126200: loss = 3.4284892082214355\n",
      "step = 4126400: loss = 3.2124204635620117\n",
      "step = 4126600: loss = 3.153245210647583\n",
      "step = 4126800: loss = 2.580317258834839\n",
      "step = 4127000: loss = 2.551703691482544\n",
      "step = 4127200: loss = 3.3293707370758057\n",
      "step = 4127400: loss = 4.420724868774414\n",
      "step = 4127600: loss = 2.7306768894195557\n",
      "step = 4127800: loss = 3.5914463996887207\n",
      "step = 4128000: loss = 3.5551350116729736\n",
      "step = 4128200: loss = 2.7367889881134033\n",
      "step = 4128400: loss = 6.239554405212402\n",
      "step = 4128600: loss = 3.069478750228882\n",
      "step = 4128800: loss = 3.33520245552063\n",
      "step = 4129000: loss = 3.4870622158050537\n",
      "step = 4129200: loss = 2.634171724319458\n",
      "step = 4129400: loss = 3.5819091796875\n",
      "step = 4129600: loss = 4.038362503051758\n",
      "step = 4129800: loss = 3.2032716274261475\n",
      "step = 4130000: loss = 3.6283445358276367\n",
      "step = 4130000: Average Return = 5.5\n",
      "step = 4130200: loss = 2.5849695205688477\n",
      "step = 4130400: loss = 5.515832424163818\n",
      "step = 4130600: loss = 1.7882767915725708\n",
      "step = 4130800: loss = 4.628905296325684\n",
      "step = 4131000: loss = 2.9551968574523926\n",
      "step = 4131200: loss = 2.8039214611053467\n",
      "step = 4131400: loss = 3.5522782802581787\n",
      "step = 4131600: loss = 3.835275888442993\n",
      "step = 4131800: loss = 3.1856114864349365\n",
      "step = 4132000: loss = 2.4459738731384277\n",
      "step = 4132200: loss = 4.750094413757324\n",
      "step = 4132400: loss = 4.178404331207275\n",
      "step = 4132600: loss = 3.194547653198242\n",
      "step = 4132800: loss = 2.1173198223114014\n",
      "step = 4133000: loss = 4.098532676696777\n",
      "step = 4133200: loss = 4.152738094329834\n",
      "step = 4133400: loss = 2.7318804264068604\n",
      "step = 4133600: loss = 4.249443054199219\n",
      "step = 4133800: loss = 3.283968687057495\n",
      "step = 4134000: loss = 3.34668231010437\n",
      "step = 4134200: loss = 2.3938727378845215\n",
      "step = 4134400: loss = 2.3413052558898926\n",
      "step = 4134600: loss = 3.235163688659668\n",
      "step = 4134800: loss = 3.923269510269165\n",
      "step = 4135000: loss = 2.5266668796539307\n",
      "step = 4135000: Average Return = 7.349999904632568\n",
      "step = 4135200: loss = 4.084428787231445\n",
      "step = 4135400: loss = 2.77510666847229\n",
      "step = 4135600: loss = 3.3898746967315674\n",
      "step = 4135800: loss = 2.9515786170959473\n",
      "step = 4136000: loss = 3.991168975830078\n",
      "step = 4136200: loss = 2.8590309619903564\n",
      "step = 4136400: loss = 4.206521034240723\n",
      "step = 4136600: loss = 3.879831552505493\n",
      "step = 4136800: loss = 2.4620609283447266\n",
      "step = 4137000: loss = 3.6360912322998047\n",
      "step = 4137200: loss = 4.108692646026611\n",
      "step = 4137400: loss = 2.6133053302764893\n",
      "step = 4137600: loss = 4.497986793518066\n",
      "step = 4137800: loss = 2.8571345806121826\n",
      "step = 4138000: loss = 3.259996175765991\n",
      "step = 4138200: loss = 2.888697385787964\n",
      "step = 4138400: loss = 3.995365619659424\n",
      "step = 4138600: loss = 4.998873710632324\n",
      "step = 4138800: loss = 2.0200014114379883\n",
      "step = 4139000: loss = 4.502490043640137\n",
      "step = 4139200: loss = 2.8456523418426514\n",
      "step = 4139400: loss = 1.9761358499526978\n",
      "step = 4139600: loss = 5.741868019104004\n",
      "step = 4139800: loss = 3.982597827911377\n",
      "step = 4140000: loss = 2.4494497776031494\n",
      "step = 4140000: Average Return = 4.849999904632568\n",
      "step = 4140200: loss = 3.2643542289733887\n",
      "step = 4140400: loss = 3.6952226161956787\n",
      "step = 4140600: loss = 3.2288243770599365\n",
      "step = 4140800: loss = 3.89461350440979\n",
      "step = 4141000: loss = 2.4115850925445557\n",
      "step = 4141200: loss = 2.150456666946411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4141400: loss = 2.4336795806884766\n",
      "step = 4141600: loss = 3.5226848125457764\n",
      "step = 4141800: loss = 2.7955682277679443\n",
      "step = 4142000: loss = 2.3050765991210938\n",
      "step = 4142200: loss = 3.6763458251953125\n",
      "step = 4142400: loss = 2.4395949840545654\n",
      "step = 4142600: loss = 3.364699602127075\n",
      "step = 4142800: loss = 3.2717344760894775\n",
      "step = 4143000: loss = 3.86883282661438\n",
      "step = 4143200: loss = 3.027824878692627\n",
      "step = 4143400: loss = 1.927399754524231\n",
      "step = 4143600: loss = 3.7967183589935303\n",
      "step = 4143800: loss = 4.021268844604492\n",
      "step = 4144000: loss = 5.086511611938477\n",
      "step = 4144200: loss = 3.5168075561523438\n",
      "step = 4144400: loss = 3.435547351837158\n",
      "step = 4144600: loss = 3.5566630363464355\n",
      "step = 4144800: loss = 3.6856956481933594\n",
      "step = 4145000: loss = 3.736922025680542\n",
      "step = 4145000: Average Return = 5.900000095367432\n",
      "step = 4145200: loss = 2.267259120941162\n",
      "step = 4145400: loss = 2.8271713256835938\n",
      "step = 4145600: loss = 4.859813690185547\n",
      "step = 4145800: loss = 4.200439453125\n",
      "step = 4146000: loss = 4.609814167022705\n",
      "step = 4146200: loss = 2.6578447818756104\n",
      "step = 4146400: loss = 2.864600419998169\n",
      "step = 4146600: loss = 3.7162442207336426\n",
      "step = 4146800: loss = 2.9475560188293457\n",
      "step = 4147000: loss = 3.6268553733825684\n",
      "step = 4147200: loss = 2.866856813430786\n",
      "step = 4147400: loss = 4.308230876922607\n",
      "step = 4147600: loss = 3.5423827171325684\n",
      "step = 4147800: loss = 1.845813274383545\n",
      "step = 4148000: loss = 3.9837048053741455\n",
      "step = 4148200: loss = 4.603858947753906\n",
      "step = 4148400: loss = 5.674022197723389\n",
      "step = 4148600: loss = 3.4966893196105957\n",
      "step = 4148800: loss = 3.7950527667999268\n",
      "step = 4149000: loss = 3.3075640201568604\n",
      "step = 4149200: loss = 3.249213218688965\n",
      "step = 4149400: loss = 3.669670820236206\n",
      "step = 4149600: loss = 2.313908100128174\n",
      "step = 4149800: loss = 3.723249673843384\n",
      "step = 4150000: loss = 3.764686107635498\n",
      "step = 4150000: Average Return = 4.25\n",
      "step = 4150200: loss = 2.6326448917388916\n",
      "step = 4150400: loss = 1.9100701808929443\n",
      "step = 4150600: loss = 3.3182342052459717\n",
      "step = 4150800: loss = 3.4109346866607666\n",
      "step = 4151000: loss = 4.273436546325684\n",
      "step = 4151200: loss = 3.8150205612182617\n",
      "step = 4151400: loss = 6.182105541229248\n",
      "step = 4151600: loss = 3.457766056060791\n",
      "step = 4151800: loss = 3.6686363220214844\n",
      "step = 4152000: loss = 3.8135507106781006\n",
      "step = 4152200: loss = 3.2847180366516113\n",
      "step = 4152400: loss = 3.2242088317871094\n",
      "step = 4152600: loss = 3.6478874683380127\n",
      "step = 4152800: loss = 4.0533223152160645\n",
      "step = 4153000: loss = 3.496436595916748\n",
      "step = 4153200: loss = 2.6367416381835938\n",
      "step = 4153400: loss = 2.319706439971924\n",
      "step = 4153600: loss = 2.852318286895752\n",
      "step = 4153800: loss = 3.8828344345092773\n",
      "step = 4154000: loss = 3.249093532562256\n",
      "step = 4154200: loss = 4.2464213371276855\n",
      "step = 4154400: loss = 2.881178140640259\n",
      "step = 4154600: loss = 2.4436984062194824\n",
      "step = 4154800: loss = 2.80956768989563\n",
      "step = 4155000: loss = 4.070209503173828\n",
      "step = 4155000: Average Return = 3.8499999046325684\n",
      "step = 4155200: loss = 3.933138370513916\n",
      "step = 4155400: loss = 3.9970157146453857\n",
      "step = 4155600: loss = 3.4891130924224854\n",
      "step = 4155800: loss = 4.039151191711426\n",
      "step = 4156000: loss = 3.5640532970428467\n",
      "step = 4156200: loss = 3.2305707931518555\n",
      "step = 4156400: loss = 2.7649943828582764\n",
      "step = 4156600: loss = 3.213937997817993\n",
      "step = 4156800: loss = 3.3793153762817383\n",
      "step = 4157000: loss = 3.3657286167144775\n",
      "step = 4157200: loss = 3.3156747817993164\n",
      "step = 4157400: loss = 3.317579507827759\n",
      "step = 4157600: loss = 3.2421538829803467\n",
      "step = 4157800: loss = 2.149980068206787\n",
      "step = 4158000: loss = 2.30825138092041\n",
      "step = 4158200: loss = 2.933655261993408\n",
      "step = 4158400: loss = 3.066629648208618\n",
      "step = 4158600: loss = 2.2044677734375\n",
      "step = 4158800: loss = 3.636537790298462\n",
      "step = 4159000: loss = 3.2836153507232666\n",
      "step = 4159200: loss = 2.8853819370269775\n",
      "step = 4159400: loss = 4.883723258972168\n",
      "step = 4159600: loss = 2.975886106491089\n",
      "step = 4159800: loss = 2.9486496448516846\n",
      "step = 4160000: loss = 3.539262056350708\n",
      "step = 4160000: Average Return = 4.900000095367432\n",
      "step = 4160200: loss = 3.8399510383605957\n",
      "step = 4160400: loss = 4.158361434936523\n",
      "step = 4160600: loss = 4.8855743408203125\n",
      "step = 4160800: loss = 2.201226234436035\n",
      "step = 4161000: loss = 1.899896502494812\n",
      "step = 4161200: loss = 2.589646577835083\n",
      "step = 4161400: loss = 4.273900032043457\n",
      "step = 4161600: loss = 2.8805699348449707\n",
      "step = 4161800: loss = 3.4275338649749756\n",
      "step = 4162000: loss = 4.627186298370361\n",
      "step = 4162200: loss = 3.393557548522949\n",
      "step = 4162400: loss = 2.515873432159424\n",
      "step = 4162600: loss = 2.9961769580841064\n",
      "step = 4162800: loss = 4.043208122253418\n",
      "step = 4163000: loss = 3.060473680496216\n",
      "step = 4163200: loss = 2.531522750854492\n",
      "step = 4163400: loss = 2.8607990741729736\n",
      "step = 4163600: loss = 2.351762056350708\n",
      "step = 4163800: loss = 4.027803421020508\n",
      "step = 4164000: loss = 2.510875701904297\n",
      "step = 4164200: loss = 3.67362904548645\n",
      "step = 4164400: loss = 3.2929439544677734\n",
      "step = 4164600: loss = 3.728614330291748\n",
      "step = 4164800: loss = 2.784153699874878\n",
      "step = 4165000: loss = 3.56372332572937\n",
      "step = 4165000: Average Return = 4.050000190734863\n",
      "step = 4165200: loss = 2.145463466644287\n",
      "step = 4165400: loss = 4.163015842437744\n",
      "step = 4165600: loss = 3.422524929046631\n",
      "step = 4165800: loss = 2.423938035964966\n",
      "step = 4166000: loss = 3.905820846557617\n",
      "step = 4166200: loss = 3.5187339782714844\n",
      "step = 4166400: loss = 3.832188367843628\n",
      "step = 4166600: loss = 4.332266330718994\n",
      "step = 4166800: loss = 3.7552428245544434\n",
      "step = 4167000: loss = 2.4801688194274902\n",
      "step = 4167200: loss = 3.866624116897583\n",
      "step = 4167400: loss = 2.8914036750793457\n",
      "step = 4167600: loss = 3.650233745574951\n",
      "step = 4167800: loss = 2.6335911750793457\n",
      "step = 4168000: loss = 4.4598822593688965\n",
      "step = 4168200: loss = 2.1563875675201416\n",
      "step = 4168400: loss = 4.345437526702881\n",
      "step = 4168600: loss = 2.740428924560547\n",
      "step = 4168800: loss = 3.5174169540405273\n",
      "step = 4169000: loss = 3.503737688064575\n",
      "step = 4169200: loss = 4.066686153411865\n",
      "step = 4169400: loss = 3.3062193393707275\n",
      "step = 4169600: loss = 3.232452392578125\n",
      "step = 4169800: loss = 2.1441478729248047\n",
      "step = 4170000: loss = 3.084146022796631\n",
      "step = 4170000: Average Return = 5.349999904632568\n",
      "step = 4170200: loss = 4.36305570602417\n",
      "step = 4170400: loss = 4.227538108825684\n",
      "step = 4170600: loss = 3.328219413757324\n",
      "step = 4170800: loss = 2.8660638332366943\n",
      "step = 4171000: loss = 2.915430784225464\n",
      "step = 4171200: loss = 2.9250218868255615\n",
      "step = 4171400: loss = 2.8094234466552734\n",
      "step = 4171600: loss = 3.051569700241089\n",
      "step = 4171800: loss = 4.832831382751465\n",
      "step = 4172000: loss = 1.7042917013168335\n",
      "step = 4172200: loss = 2.871713161468506\n",
      "step = 4172400: loss = 2.9100069999694824\n",
      "step = 4172600: loss = 3.155802011489868\n",
      "step = 4172800: loss = 2.87907338142395\n",
      "step = 4173000: loss = 3.0281898975372314\n",
      "step = 4173200: loss = 3.427781105041504\n",
      "step = 4173400: loss = 4.205510139465332\n",
      "step = 4173600: loss = 3.2698116302490234\n",
      "step = 4173800: loss = 1.6755621433258057\n",
      "step = 4174000: loss = 3.2769384384155273\n",
      "step = 4174200: loss = 3.3305914402008057\n",
      "step = 4174400: loss = 3.8595359325408936\n",
      "step = 4174600: loss = 3.193964719772339\n",
      "step = 4174800: loss = 3.121128559112549\n",
      "step = 4175000: loss = 4.502634048461914\n",
      "step = 4175000: Average Return = 4.949999809265137\n",
      "step = 4175200: loss = 2.537855625152588\n",
      "step = 4175400: loss = 3.337902545928955\n",
      "step = 4175600: loss = 2.3024063110351562\n",
      "step = 4175800: loss = 3.251725435256958\n",
      "step = 4176000: loss = 3.2220780849456787\n",
      "step = 4176200: loss = 2.805516481399536\n",
      "step = 4176400: loss = 3.481645345687866\n",
      "step = 4176600: loss = 2.5903539657592773\n",
      "step = 4176800: loss = 2.992051362991333\n",
      "step = 4177000: loss = 4.0042724609375\n",
      "step = 4177200: loss = 4.017092227935791\n",
      "step = 4177400: loss = 2.4483652114868164\n",
      "step = 4177600: loss = 3.1524465084075928\n",
      "step = 4177800: loss = 3.6357786655426025\n",
      "step = 4178000: loss = 2.658465623855591\n",
      "step = 4178200: loss = 3.7448644638061523\n",
      "step = 4178400: loss = 4.242905616760254\n",
      "step = 4178600: loss = 3.219778299331665\n",
      "step = 4178800: loss = 2.951345205307007\n",
      "step = 4179000: loss = 3.761030912399292\n",
      "step = 4179200: loss = 4.045823097229004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4179400: loss = 3.322748899459839\n",
      "step = 4179600: loss = 4.917337417602539\n",
      "step = 4179800: loss = 2.952127695083618\n",
      "step = 4180000: loss = 2.7232308387756348\n",
      "step = 4180000: Average Return = 4.349999904632568\n",
      "step = 4180200: loss = 4.188788414001465\n",
      "step = 4180400: loss = 3.9766223430633545\n",
      "step = 4180600: loss = 3.809743642807007\n",
      "step = 4180800: loss = 2.6655209064483643\n",
      "step = 4181000: loss = 4.221396446228027\n",
      "step = 4181200: loss = 4.0198211669921875\n",
      "step = 4181400: loss = 3.662403106689453\n",
      "step = 4181600: loss = 4.918447971343994\n",
      "step = 4181800: loss = 4.208930969238281\n",
      "step = 4182000: loss = 4.735883712768555\n",
      "step = 4182200: loss = 2.9289016723632812\n",
      "step = 4182400: loss = 3.553103446960449\n",
      "step = 4182600: loss = 3.503092050552368\n",
      "step = 4182800: loss = 3.2739171981811523\n",
      "step = 4183000: loss = 3.839062452316284\n",
      "step = 4183200: loss = 3.468567371368408\n",
      "step = 4183400: loss = 2.9386889934539795\n",
      "step = 4183600: loss = 1.9250611066818237\n",
      "step = 4183800: loss = 5.07777738571167\n",
      "step = 4184000: loss = 3.0859405994415283\n",
      "step = 4184200: loss = 2.852102041244507\n",
      "step = 4184400: loss = 2.75582218170166\n",
      "step = 4184600: loss = 3.960153341293335\n",
      "step = 4184800: loss = 3.577989339828491\n",
      "step = 4185000: loss = 3.7183752059936523\n",
      "step = 4185000: Average Return = 6.099999904632568\n",
      "step = 4185200: loss = 2.193732500076294\n",
      "step = 4185400: loss = 3.737034320831299\n",
      "step = 4185600: loss = 4.478541374206543\n",
      "step = 4185800: loss = 2.712825298309326\n",
      "step = 4186000: loss = 2.5092554092407227\n",
      "step = 4186200: loss = 4.565890789031982\n",
      "step = 4186400: loss = 4.246486663818359\n",
      "step = 4186600: loss = 2.8029286861419678\n",
      "step = 4186800: loss = 3.6380882263183594\n",
      "step = 4187000: loss = 3.914536714553833\n",
      "step = 4187200: loss = 3.9607133865356445\n",
      "step = 4187400: loss = 4.024990081787109\n",
      "step = 4187600: loss = 3.602088451385498\n",
      "step = 4187800: loss = 2.0168089866638184\n",
      "step = 4188000: loss = 3.3360798358917236\n",
      "step = 4188200: loss = 3.581864356994629\n",
      "step = 4188400: loss = 3.5343618392944336\n",
      "step = 4188600: loss = 2.8425517082214355\n",
      "step = 4188800: loss = 3.077788829803467\n",
      "step = 4189000: loss = 2.685234546661377\n",
      "step = 4189200: loss = 3.5773305892944336\n",
      "step = 4189400: loss = 2.7646210193634033\n",
      "step = 4189600: loss = 4.580861568450928\n",
      "step = 4189800: loss = 4.209560871124268\n",
      "step = 4190000: loss = 3.366616725921631\n",
      "step = 4190000: Average Return = 4.0\n",
      "step = 4190200: loss = 2.6751034259796143\n",
      "step = 4190400: loss = 2.865997314453125\n",
      "step = 4190600: loss = 3.2167961597442627\n",
      "step = 4190800: loss = 3.476780891418457\n",
      "step = 4191000: loss = 2.231580972671509\n",
      "step = 4191200: loss = 3.669750928878784\n",
      "step = 4191400: loss = 2.647582530975342\n",
      "step = 4191600: loss = 4.988982200622559\n",
      "step = 4191800: loss = 2.4523308277130127\n",
      "step = 4192000: loss = 3.5704259872436523\n",
      "step = 4192200: loss = 3.6413755416870117\n",
      "step = 4192400: loss = 2.756589889526367\n",
      "step = 4192600: loss = 1.8729921579360962\n",
      "step = 4192800: loss = 3.6498942375183105\n",
      "step = 4193000: loss = 3.162252187728882\n",
      "step = 4193200: loss = 2.5182945728302\n",
      "step = 4193400: loss = 2.258004903793335\n",
      "step = 4193600: loss = 2.7450854778289795\n",
      "step = 4193800: loss = 4.614842414855957\n",
      "step = 4194000: loss = 3.8682854175567627\n",
      "step = 4194200: loss = 3.4717304706573486\n",
      "step = 4194400: loss = 3.239309549331665\n",
      "step = 4194600: loss = 2.9705617427825928\n",
      "step = 4194800: loss = 3.5792346000671387\n",
      "step = 4195000: loss = 3.3992724418640137\n",
      "step = 4195000: Average Return = 2.9000000953674316\n",
      "step = 4195200: loss = 3.2091426849365234\n",
      "step = 4195400: loss = 3.192265272140503\n",
      "step = 4195600: loss = 2.527827024459839\n",
      "step = 4195800: loss = 4.444673538208008\n",
      "step = 4196000: loss = 2.1797664165496826\n",
      "step = 4196200: loss = 3.320735454559326\n",
      "step = 4196400: loss = 2.8617451190948486\n",
      "step = 4196600: loss = 2.9086315631866455\n",
      "step = 4196800: loss = 4.422598361968994\n",
      "step = 4197000: loss = 3.4244935512542725\n",
      "step = 4197200: loss = 2.8935916423797607\n",
      "step = 4197400: loss = 4.407200813293457\n",
      "step = 4197600: loss = 2.8608431816101074\n",
      "step = 4197800: loss = 3.4037060737609863\n",
      "step = 4198000: loss = 3.098360538482666\n",
      "step = 4198200: loss = 3.3573265075683594\n",
      "step = 4198400: loss = 3.2183330059051514\n",
      "step = 4198600: loss = 3.4337141513824463\n",
      "step = 4198800: loss = 2.542961359024048\n",
      "step = 4199000: loss = 3.0438313484191895\n",
      "step = 4199200: loss = 3.6436924934387207\n",
      "step = 4199400: loss = 4.789304256439209\n",
      "step = 4199600: loss = 3.667694091796875\n",
      "step = 4199800: loss = 3.4177441596984863\n",
      "step = 4200000: loss = 4.368206024169922\n",
      "step = 4200000: Average Return = 5.25\n",
      "step = 4200200: loss = 2.7208340167999268\n",
      "step = 4200400: loss = 3.0297045707702637\n",
      "step = 4200600: loss = 2.9581825733184814\n",
      "step = 4200800: loss = 3.270413398742676\n",
      "step = 4201000: loss = 4.406669616699219\n",
      "step = 4201200: loss = 4.624645233154297\n",
      "step = 4201400: loss = 4.022521495819092\n",
      "step = 4201600: loss = 3.64734148979187\n",
      "step = 4201800: loss = 2.9231557846069336\n",
      "step = 4202000: loss = 3.2311558723449707\n",
      "step = 4202200: loss = 2.4842846393585205\n",
      "step = 4202400: loss = 3.409630060195923\n",
      "step = 4202600: loss = 3.0862720012664795\n",
      "step = 4202800: loss = 2.8867030143737793\n",
      "step = 4203000: loss = 4.01947546005249\n",
      "step = 4203200: loss = 3.525840997695923\n",
      "step = 4203400: loss = 3.509458541870117\n",
      "step = 4203600: loss = 2.654980182647705\n",
      "step = 4203800: loss = 3.8137118816375732\n",
      "step = 4204000: loss = 2.6543614864349365\n",
      "step = 4204200: loss = 4.364953994750977\n",
      "step = 4204400: loss = 3.3171353340148926\n",
      "step = 4204600: loss = 2.867912769317627\n",
      "step = 4204800: loss = 3.385343551635742\n",
      "step = 4205000: loss = 3.6837728023529053\n",
      "step = 4205000: Average Return = 2.549999952316284\n",
      "step = 4205200: loss = 5.087618827819824\n",
      "step = 4205400: loss = 3.820432662963867\n",
      "step = 4205600: loss = 3.5312702655792236\n",
      "step = 4205800: loss = 3.135465621948242\n",
      "step = 4206000: loss = 2.712611436843872\n",
      "step = 4206200: loss = 4.162456512451172\n",
      "step = 4206400: loss = 3.0330910682678223\n",
      "step = 4206600: loss = 2.6616692543029785\n",
      "step = 4206800: loss = 2.667098045349121\n",
      "step = 4207000: loss = 3.8757176399230957\n",
      "step = 4207200: loss = 2.3827600479125977\n",
      "step = 4207400: loss = 3.7832469940185547\n",
      "step = 4207600: loss = 4.358009338378906\n",
      "step = 4207800: loss = 3.3969030380249023\n",
      "step = 4208000: loss = 3.8457396030426025\n",
      "step = 4208200: loss = 4.555469036102295\n",
      "step = 4208400: loss = 3.3470752239227295\n",
      "step = 4208600: loss = 3.128819704055786\n",
      "step = 4208800: loss = 3.063915967941284\n",
      "step = 4209000: loss = 3.8603782653808594\n",
      "step = 4209200: loss = 3.9475605487823486\n",
      "step = 4209400: loss = 3.0510177612304688\n",
      "step = 4209600: loss = 2.353040933609009\n",
      "step = 4209800: loss = 2.397958278656006\n",
      "step = 4210000: loss = 3.4826316833496094\n",
      "step = 4210000: Average Return = 5.75\n",
      "step = 4210200: loss = 2.5425376892089844\n",
      "step = 4210400: loss = 3.883112668991089\n",
      "step = 4210600: loss = 2.538007974624634\n",
      "step = 4210800: loss = 4.304038047790527\n",
      "step = 4211000: loss = 3.1164441108703613\n",
      "step = 4211200: loss = 4.193709850311279\n",
      "step = 4211400: loss = 4.444766521453857\n",
      "step = 4211600: loss = 2.136979579925537\n",
      "step = 4211800: loss = 3.330933094024658\n",
      "step = 4212000: loss = 2.9070048332214355\n",
      "step = 4212200: loss = 5.589831829071045\n",
      "step = 4212400: loss = 3.0906875133514404\n",
      "step = 4212600: loss = 4.252728462219238\n",
      "step = 4212800: loss = 5.155510902404785\n",
      "step = 4213000: loss = 4.274816513061523\n",
      "step = 4213200: loss = 4.297255039215088\n",
      "step = 4213400: loss = 3.9000399112701416\n",
      "step = 4213600: loss = 2.0320796966552734\n",
      "step = 4213800: loss = 2.536349296569824\n",
      "step = 4214000: loss = 3.29331636428833\n",
      "step = 4214200: loss = 3.2572481632232666\n",
      "step = 4214400: loss = 3.9108903408050537\n",
      "step = 4214600: loss = 4.323057651519775\n",
      "step = 4214800: loss = 3.2008721828460693\n",
      "step = 4215000: loss = 4.160752773284912\n",
      "step = 4215000: Average Return = 5.699999809265137\n",
      "step = 4215200: loss = 3.925194025039673\n",
      "step = 4215400: loss = 3.5795702934265137\n",
      "step = 4215600: loss = 3.310384511947632\n",
      "step = 4215800: loss = 6.113014221191406\n",
      "step = 4216000: loss = 3.2270469665527344\n",
      "step = 4216200: loss = 3.0975944995880127\n",
      "step = 4216400: loss = 4.073767185211182\n",
      "step = 4216600: loss = 5.03906774520874\n",
      "step = 4216800: loss = 2.6499948501586914\n",
      "step = 4217000: loss = 3.1565709114074707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4217200: loss = 2.2348291873931885\n",
      "step = 4217400: loss = 3.872868061065674\n",
      "step = 4217600: loss = 4.161835670471191\n",
      "step = 4217800: loss = 2.636888265609741\n",
      "step = 4218000: loss = 3.3521728515625\n",
      "step = 4218200: loss = 3.245577812194824\n",
      "step = 4218400: loss = 3.831821918487549\n",
      "step = 4218600: loss = 3.813389778137207\n",
      "step = 4218800: loss = 3.6266674995422363\n",
      "step = 4219000: loss = 2.314932107925415\n",
      "step = 4219200: loss = 1.815571904182434\n",
      "step = 4219400: loss = 2.709522247314453\n",
      "step = 4219600: loss = 2.9735167026519775\n",
      "step = 4219800: loss = 3.198558807373047\n",
      "step = 4220000: loss = 2.550999641418457\n",
      "step = 4220000: Average Return = 2.4000000953674316\n",
      "step = 4220200: loss = 4.282350063323975\n",
      "step = 4220400: loss = 3.0075390338897705\n",
      "step = 4220600: loss = 3.9319982528686523\n",
      "step = 4220800: loss = 3.362826347351074\n",
      "step = 4221000: loss = 3.7353503704071045\n",
      "step = 4221200: loss = 4.937968730926514\n",
      "step = 4221400: loss = 4.202285289764404\n",
      "step = 4221600: loss = 3.556910514831543\n",
      "step = 4221800: loss = 2.954989194869995\n",
      "step = 4222000: loss = 4.878581523895264\n",
      "step = 4222200: loss = 3.8771655559539795\n",
      "step = 4222400: loss = 3.8601505756378174\n",
      "step = 4222600: loss = 4.076528549194336\n",
      "step = 4222800: loss = 2.8503758907318115\n",
      "step = 4223000: loss = 2.8188230991363525\n",
      "step = 4223200: loss = 3.561636209487915\n",
      "step = 4223400: loss = 4.324760437011719\n",
      "step = 4223600: loss = 4.95744514465332\n",
      "step = 4223800: loss = 2.795025587081909\n",
      "step = 4224000: loss = 3.4504623413085938\n",
      "step = 4224200: loss = 4.208683490753174\n",
      "step = 4224400: loss = 3.2267675399780273\n",
      "step = 4224600: loss = 4.180453777313232\n",
      "step = 4224800: loss = 3.0549256801605225\n",
      "step = 4225000: loss = 3.5553407669067383\n",
      "step = 4225000: Average Return = 5.75\n",
      "step = 4225200: loss = 2.926231861114502\n",
      "step = 4225400: loss = 4.021583557128906\n",
      "step = 4225600: loss = 3.5620791912078857\n",
      "step = 4225800: loss = 3.7432708740234375\n",
      "step = 4226000: loss = 5.115548133850098\n",
      "step = 4226200: loss = 3.2695305347442627\n",
      "step = 4226400: loss = 2.6820452213287354\n",
      "step = 4226600: loss = 3.848536968231201\n",
      "step = 4226800: loss = 4.500980377197266\n",
      "step = 4227000: loss = 4.21538782119751\n",
      "step = 4227200: loss = 3.8294994831085205\n",
      "step = 4227400: loss = 2.1568543910980225\n",
      "step = 4227600: loss = 2.822861909866333\n",
      "step = 4227800: loss = 3.381730556488037\n",
      "step = 4228000: loss = 2.9388678073883057\n",
      "step = 4228200: loss = 5.45286226272583\n",
      "step = 4228400: loss = 3.1029140949249268\n",
      "step = 4228600: loss = 4.502910614013672\n",
      "step = 4228800: loss = 3.3224716186523438\n",
      "step = 4229000: loss = 2.078686475753784\n",
      "step = 4229200: loss = 3.596308469772339\n",
      "step = 4229400: loss = 3.423145055770874\n",
      "step = 4229600: loss = 3.4801394939422607\n",
      "step = 4229800: loss = 2.9198760986328125\n",
      "step = 4230000: loss = 3.5197935104370117\n",
      "step = 4230000: Average Return = 3.3499999046325684\n",
      "step = 4230200: loss = 4.154314041137695\n",
      "step = 4230400: loss = 3.143156051635742\n",
      "step = 4230600: loss = 2.2790634632110596\n",
      "step = 4230800: loss = 4.410176753997803\n",
      "step = 4231000: loss = 3.678828239440918\n",
      "step = 4231200: loss = 3.3885698318481445\n",
      "step = 4231400: loss = 2.9784746170043945\n",
      "step = 4231600: loss = 2.6098103523254395\n",
      "step = 4231800: loss = 3.0674731731414795\n",
      "step = 4232000: loss = 2.6413979530334473\n",
      "step = 4232200: loss = 3.1494929790496826\n",
      "step = 4232400: loss = 2.8359272480010986\n",
      "step = 4232600: loss = 4.501643657684326\n",
      "step = 4232800: loss = 2.6061596870422363\n",
      "step = 4233000: loss = 4.6029839515686035\n",
      "step = 4233200: loss = 4.52531099319458\n",
      "step = 4233400: loss = 3.401127815246582\n",
      "step = 4233600: loss = 4.486690521240234\n",
      "step = 4233800: loss = 4.48263692855835\n",
      "step = 4234000: loss = 6.010586261749268\n",
      "step = 4234200: loss = 3.4300105571746826\n",
      "step = 4234400: loss = 3.362363815307617\n",
      "step = 4234600: loss = 2.9434754848480225\n",
      "step = 4234800: loss = 3.6615352630615234\n",
      "step = 4235000: loss = 4.798981666564941\n",
      "step = 4235000: Average Return = 5.949999809265137\n",
      "step = 4235200: loss = 3.305479049682617\n",
      "step = 4235400: loss = 4.557215690612793\n",
      "step = 4235600: loss = 3.844299793243408\n",
      "step = 4235800: loss = 5.505552291870117\n",
      "step = 4236000: loss = 3.3260693550109863\n",
      "step = 4236200: loss = 3.0357601642608643\n",
      "step = 4236400: loss = 3.153017520904541\n",
      "step = 4236600: loss = 4.723372936248779\n",
      "step = 4236800: loss = 3.096057176589966\n",
      "step = 4237000: loss = 2.7123866081237793\n",
      "step = 4237200: loss = 3.725332260131836\n",
      "step = 4237400: loss = 4.2125983238220215\n",
      "step = 4237600: loss = 3.162431478500366\n",
      "step = 4237800: loss = 4.052765846252441\n",
      "step = 4238000: loss = 4.106861591339111\n",
      "step = 4238200: loss = 5.301987648010254\n",
      "step = 4238400: loss = 2.919915199279785\n",
      "step = 4238600: loss = 1.8829855918884277\n",
      "step = 4238800: loss = 3.7184977531433105\n",
      "step = 4239000: loss = 4.337146282196045\n",
      "step = 4239200: loss = 2.5176689624786377\n",
      "step = 4239400: loss = 3.326772928237915\n",
      "step = 4239600: loss = 3.70116925239563\n",
      "step = 4239800: loss = 3.297402858734131\n",
      "step = 4240000: loss = 3.740281820297241\n",
      "step = 4240000: Average Return = 5.300000190734863\n",
      "step = 4240200: loss = 5.424785137176514\n",
      "step = 4240400: loss = 3.235199451446533\n",
      "step = 4240600: loss = 2.7979936599731445\n",
      "step = 4240800: loss = 3.4646060466766357\n",
      "step = 4241000: loss = 3.970978021621704\n",
      "step = 4241200: loss = 2.289510488510132\n",
      "step = 4241400: loss = 3.4470629692077637\n",
      "step = 4241600: loss = 4.76585578918457\n",
      "step = 4241800: loss = 4.066425800323486\n",
      "step = 4242000: loss = 4.016434192657471\n",
      "step = 4242200: loss = 2.8679635524749756\n",
      "step = 4242400: loss = 5.540639400482178\n",
      "step = 4242600: loss = 3.7266323566436768\n",
      "step = 4242800: loss = 2.695291042327881\n",
      "step = 4243000: loss = 2.877192735671997\n",
      "step = 4243200: loss = 1.8204506635665894\n",
      "step = 4243400: loss = 3.1616814136505127\n",
      "step = 4243600: loss = 4.136081695556641\n",
      "step = 4243800: loss = 2.646932363510132\n",
      "step = 4244000: loss = 3.4991981983184814\n",
      "step = 4244200: loss = 3.9087414741516113\n",
      "step = 4244400: loss = 1.7501778602600098\n",
      "step = 4244600: loss = 3.447709321975708\n",
      "step = 4244800: loss = 3.7553811073303223\n",
      "step = 4245000: loss = 3.28938627243042\n",
      "step = 4245000: Average Return = 4.099999904632568\n",
      "step = 4245200: loss = 4.588481426239014\n",
      "step = 4245400: loss = 3.8173828125\n",
      "step = 4245600: loss = 3.2234511375427246\n",
      "step = 4245800: loss = 3.398493528366089\n",
      "step = 4246000: loss = 3.4318645000457764\n",
      "step = 4246200: loss = 3.636979818344116\n",
      "step = 4246400: loss = 2.8386266231536865\n",
      "step = 4246600: loss = 4.0627875328063965\n",
      "step = 4246800: loss = 2.4835705757141113\n",
      "step = 4247000: loss = 3.6578307151794434\n",
      "step = 4247200: loss = 3.012883424758911\n",
      "step = 4247400: loss = 3.7750043869018555\n",
      "step = 4247600: loss = 3.5236005783081055\n",
      "step = 4247800: loss = 3.2472217082977295\n",
      "step = 4248000: loss = 3.681617259979248\n",
      "step = 4248200: loss = 4.099440574645996\n",
      "step = 4248400: loss = 4.108512878417969\n",
      "step = 4248600: loss = 3.678797483444214\n",
      "step = 4248800: loss = 2.906003713607788\n",
      "step = 4249000: loss = 2.6776812076568604\n",
      "step = 4249200: loss = 5.966406345367432\n",
      "step = 4249400: loss = 3.3613533973693848\n",
      "step = 4249600: loss = 4.208198547363281\n",
      "step = 4249800: loss = 3.215911388397217\n",
      "step = 4250000: loss = 2.7210516929626465\n",
      "step = 4250000: Average Return = 5.800000190734863\n",
      "step = 4250200: loss = 2.608107328414917\n",
      "step = 4250400: loss = 3.3978450298309326\n",
      "step = 4250600: loss = 3.655712127685547\n",
      "step = 4250800: loss = 2.894160747528076\n",
      "step = 4251000: loss = 3.0836822986602783\n",
      "step = 4251200: loss = 3.515416145324707\n",
      "step = 4251400: loss = 3.653620481491089\n",
      "step = 4251600: loss = 3.8576529026031494\n",
      "step = 4251800: loss = 3.426579475402832\n",
      "step = 4252000: loss = 2.753282308578491\n",
      "step = 4252200: loss = 3.4116628170013428\n",
      "step = 4252400: loss = 3.518494129180908\n",
      "step = 4252600: loss = 3.492589235305786\n",
      "step = 4252800: loss = 4.613571643829346\n",
      "step = 4253000: loss = 3.442927837371826\n",
      "step = 4253200: loss = 2.583028793334961\n",
      "step = 4253400: loss = 3.115103006362915\n",
      "step = 4253600: loss = 3.897700071334839\n",
      "step = 4253800: loss = 3.349107027053833\n",
      "step = 4254000: loss = 4.420886039733887\n",
      "step = 4254200: loss = 3.536048412322998\n",
      "step = 4254400: loss = 3.9647459983825684\n",
      "step = 4254600: loss = 2.8090574741363525\n",
      "step = 4254800: loss = 4.427870750427246\n",
      "step = 4255000: loss = 2.522355556488037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4255000: Average Return = 6.5\n",
      "step = 4255200: loss = 3.7729902267456055\n",
      "step = 4255400: loss = 3.67734432220459\n",
      "step = 4255600: loss = 5.14283561706543\n",
      "step = 4255800: loss = 3.8410160541534424\n",
      "step = 4256000: loss = 3.248856782913208\n",
      "step = 4256200: loss = 3.7127745151519775\n",
      "step = 4256400: loss = 4.956227779388428\n",
      "step = 4256600: loss = 3.5966145992279053\n",
      "step = 4256800: loss = 2.1763501167297363\n",
      "step = 4257000: loss = 3.120980978012085\n",
      "step = 4257200: loss = 3.7524261474609375\n",
      "step = 4257400: loss = 2.119936227798462\n",
      "step = 4257600: loss = 3.6253228187561035\n",
      "step = 4257800: loss = 3.5542819499969482\n",
      "step = 4258000: loss = 3.40714693069458\n",
      "step = 4258200: loss = 4.280774116516113\n",
      "step = 4258400: loss = 2.8497490882873535\n",
      "step = 4258600: loss = 4.111578464508057\n",
      "step = 4258800: loss = 2.8565592765808105\n",
      "step = 4259000: loss = 3.4894495010375977\n",
      "step = 4259200: loss = 3.879741668701172\n",
      "step = 4259400: loss = 5.310750484466553\n",
      "step = 4259600: loss = 4.8581647872924805\n",
      "step = 4259800: loss = 3.2531707286834717\n",
      "step = 4260000: loss = 2.7253997325897217\n",
      "step = 4260000: Average Return = 4.300000190734863\n",
      "step = 4260200: loss = 2.7119364738464355\n",
      "step = 4260400: loss = 2.948671817779541\n",
      "step = 4260600: loss = 3.9983482360839844\n",
      "step = 4260800: loss = 3.1649856567382812\n",
      "step = 4261000: loss = 3.589266777038574\n",
      "step = 4261200: loss = 3.7994773387908936\n",
      "step = 4261400: loss = 2.881291389465332\n",
      "step = 4261600: loss = 2.7478368282318115\n",
      "step = 4261800: loss = 3.0010039806365967\n",
      "step = 4262000: loss = 3.6497645378112793\n",
      "step = 4262200: loss = 3.552666187286377\n",
      "step = 4262400: loss = 4.328186988830566\n",
      "step = 4262600: loss = 5.206094741821289\n",
      "step = 4262800: loss = 3.6591713428497314\n",
      "step = 4263000: loss = 4.293349742889404\n",
      "step = 4263200: loss = 2.7179815769195557\n",
      "step = 4263400: loss = 3.4263646602630615\n",
      "step = 4263600: loss = 3.9575982093811035\n",
      "step = 4263800: loss = 4.012777805328369\n",
      "step = 4264000: loss = 4.156557083129883\n",
      "step = 4264200: loss = 3.313300132751465\n",
      "step = 4264400: loss = 3.18259334564209\n",
      "step = 4264600: loss = 1.9373759031295776\n",
      "step = 4264800: loss = 3.3458337783813477\n",
      "step = 4265000: loss = 3.5087263584136963\n",
      "step = 4265000: Average Return = 5.75\n",
      "step = 4265200: loss = 3.9466195106506348\n",
      "step = 4265400: loss = 2.6939821243286133\n",
      "step = 4265600: loss = 3.01110577583313\n",
      "step = 4265800: loss = 3.6003198623657227\n",
      "step = 4266000: loss = 3.194927930831909\n",
      "step = 4266200: loss = 2.5065836906433105\n",
      "step = 4266400: loss = 4.4757080078125\n",
      "step = 4266600: loss = 3.83272385597229\n",
      "step = 4266800: loss = 2.669726848602295\n",
      "step = 4267000: loss = 2.4519271850585938\n",
      "step = 4267200: loss = 2.8327364921569824\n",
      "step = 4267400: loss = 3.337810516357422\n",
      "step = 4267600: loss = 3.7277941703796387\n",
      "step = 4267800: loss = 3.6714284420013428\n",
      "step = 4268000: loss = 2.9793224334716797\n",
      "step = 4268200: loss = 3.0550191402435303\n",
      "step = 4268400: loss = 3.976787567138672\n",
      "step = 4268600: loss = 2.966831684112549\n",
      "step = 4268800: loss = 2.9531190395355225\n",
      "step = 4269000: loss = 2.3939225673675537\n",
      "step = 4269200: loss = 3.6390883922576904\n",
      "step = 4269400: loss = 3.874812602996826\n",
      "step = 4269600: loss = 2.82460880279541\n",
      "step = 4269800: loss = 2.0808470249176025\n",
      "step = 4270000: loss = 4.0938286781311035\n",
      "step = 4270000: Average Return = 5.099999904632568\n",
      "step = 4270200: loss = 3.3215651512145996\n",
      "step = 4270400: loss = 3.4083614349365234\n",
      "step = 4270600: loss = 4.2462592124938965\n",
      "step = 4270800: loss = 4.915596961975098\n",
      "step = 4271000: loss = 2.8150506019592285\n",
      "step = 4271200: loss = 2.4505503177642822\n",
      "step = 4271400: loss = 3.9858551025390625\n",
      "step = 4271600: loss = 3.466616153717041\n",
      "step = 4271800: loss = 3.9714345932006836\n",
      "step = 4272000: loss = 4.271040439605713\n",
      "step = 4272200: loss = 3.938124656677246\n",
      "step = 4272400: loss = 4.162846565246582\n",
      "step = 4272600: loss = 3.2428457736968994\n",
      "step = 4272800: loss = 2.263056516647339\n",
      "step = 4273000: loss = 2.9369096755981445\n",
      "step = 4273200: loss = 2.572061777114868\n",
      "step = 4273400: loss = 3.7658398151397705\n",
      "step = 4273600: loss = 3.12798810005188\n",
      "step = 4273800: loss = 3.5739452838897705\n",
      "step = 4274000: loss = 2.334388017654419\n",
      "step = 4274200: loss = 2.949463367462158\n",
      "step = 4274400: loss = 2.5908360481262207\n",
      "step = 4274600: loss = 3.6221816539764404\n",
      "step = 4274800: loss = 3.084993839263916\n",
      "step = 4275000: loss = 3.643545150756836\n",
      "step = 4275000: Average Return = 4.449999809265137\n",
      "step = 4275200: loss = 3.035823345184326\n",
      "step = 4275400: loss = 3.3837242126464844\n",
      "step = 4275600: loss = 3.308022975921631\n",
      "step = 4275800: loss = 3.3328826427459717\n",
      "step = 4276000: loss = 3.792741298675537\n",
      "step = 4276200: loss = 3.3861799240112305\n",
      "step = 4276400: loss = 3.7925784587860107\n",
      "step = 4276600: loss = 4.326973915100098\n",
      "step = 4276800: loss = 3.6074907779693604\n",
      "step = 4277000: loss = 4.0419206619262695\n",
      "step = 4277200: loss = 2.9579522609710693\n",
      "step = 4277400: loss = 5.640592098236084\n",
      "step = 4277600: loss = 3.4341516494750977\n",
      "step = 4277800: loss = 4.157484531402588\n",
      "step = 4278000: loss = 4.008438587188721\n",
      "step = 4278200: loss = 4.240358352661133\n",
      "step = 4278400: loss = 2.6926610469818115\n",
      "step = 4278600: loss = 2.18961501121521\n",
      "step = 4278800: loss = 3.664116144180298\n",
      "step = 4279000: loss = 3.1077702045440674\n",
      "step = 4279200: loss = 3.8173582553863525\n",
      "step = 4279400: loss = 3.3111326694488525\n",
      "step = 4279600: loss = 2.41005802154541\n",
      "step = 4279800: loss = 3.58982253074646\n",
      "step = 4280000: loss = 1.9703114032745361\n",
      "step = 4280000: Average Return = 4.449999809265137\n",
      "step = 4280200: loss = 3.5515530109405518\n",
      "step = 4280400: loss = 3.3600316047668457\n",
      "step = 4280600: loss = 3.197566509246826\n",
      "step = 4280800: loss = 2.795085906982422\n",
      "step = 4281000: loss = 3.079188823699951\n",
      "step = 4281200: loss = 2.989788770675659\n",
      "step = 4281400: loss = 2.8641250133514404\n",
      "step = 4281600: loss = 3.148556709289551\n",
      "step = 4281800: loss = 3.8292369842529297\n",
      "step = 4282000: loss = 2.0764904022216797\n",
      "step = 4282200: loss = 3.939800262451172\n",
      "step = 4282400: loss = 4.903026580810547\n",
      "step = 4282600: loss = 3.796633720397949\n",
      "step = 4282800: loss = 3.688274383544922\n",
      "step = 4283000: loss = 2.8494083881378174\n",
      "step = 4283200: loss = 2.5850319862365723\n",
      "step = 4283400: loss = 4.106167316436768\n",
      "step = 4283600: loss = 3.181079149246216\n",
      "step = 4283800: loss = 4.0092644691467285\n",
      "step = 4284000: loss = 4.08317756652832\n",
      "step = 4284200: loss = 3.5957117080688477\n",
      "step = 4284400: loss = 4.247556686401367\n",
      "step = 4284600: loss = 4.113226413726807\n",
      "step = 4284800: loss = 3.1344141960144043\n",
      "step = 4285000: loss = 3.4527735710144043\n",
      "step = 4285000: Average Return = 4.75\n",
      "step = 4285200: loss = 3.5991289615631104\n",
      "step = 4285400: loss = 3.7190001010894775\n",
      "step = 4285600: loss = 2.708031177520752\n",
      "step = 4285800: loss = 2.9857211112976074\n",
      "step = 4286000: loss = 3.3033416271209717\n",
      "step = 4286200: loss = 4.966464519500732\n",
      "step = 4286400: loss = 4.188443660736084\n",
      "step = 4286600: loss = 3.357008218765259\n",
      "step = 4286800: loss = 3.324756383895874\n",
      "step = 4287000: loss = 4.260350704193115\n",
      "step = 4287200: loss = 3.815387725830078\n",
      "step = 4287400: loss = 3.4709863662719727\n",
      "step = 4287600: loss = 4.965391635894775\n",
      "step = 4287800: loss = 4.305200099945068\n",
      "step = 4288000: loss = 3.0709328651428223\n",
      "step = 4288200: loss = 3.1709301471710205\n",
      "step = 4288400: loss = 3.1792404651641846\n",
      "step = 4288600: loss = 2.8552610874176025\n",
      "step = 4288800: loss = 3.359572649002075\n",
      "step = 4289000: loss = 3.8331854343414307\n",
      "step = 4289200: loss = 4.116284370422363\n",
      "step = 4289400: loss = 3.5247485637664795\n",
      "step = 4289600: loss = 3.112203359603882\n",
      "step = 4289800: loss = 2.93990421295166\n",
      "step = 4290000: loss = 2.773836612701416\n",
      "step = 4290000: Average Return = 4.5\n",
      "step = 4290200: loss = 2.6750171184539795\n",
      "step = 4290400: loss = 4.575746059417725\n",
      "step = 4290600: loss = 2.8135712146759033\n",
      "step = 4290800: loss = 3.8386311531066895\n",
      "step = 4291000: loss = 5.25873327255249\n",
      "step = 4291200: loss = 2.978485107421875\n",
      "step = 4291400: loss = 4.013890266418457\n",
      "step = 4291600: loss = 3.945101261138916\n",
      "step = 4291800: loss = 3.6426568031311035\n",
      "step = 4292000: loss = 3.4379611015319824\n",
      "step = 4292200: loss = 2.835063934326172\n",
      "step = 4292400: loss = 2.606856107711792\n",
      "step = 4292600: loss = 2.184420347213745\n",
      "step = 4292800: loss = 2.3635566234588623\n",
      "step = 4293000: loss = 4.341934680938721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4293200: loss = 3.163083791732788\n",
      "step = 4293400: loss = 3.054044723510742\n",
      "step = 4293600: loss = 2.089160442352295\n",
      "step = 4293800: loss = 3.3940212726593018\n",
      "step = 4294000: loss = 3.2285118103027344\n",
      "step = 4294200: loss = 3.7295079231262207\n",
      "step = 4294400: loss = 3.401662588119507\n",
      "step = 4294600: loss = 2.8850340843200684\n",
      "step = 4294800: loss = 3.1225318908691406\n",
      "step = 4295000: loss = 3.4120664596557617\n",
      "step = 4295000: Average Return = 6.099999904632568\n",
      "step = 4295200: loss = 3.9580209255218506\n",
      "step = 4295400: loss = 2.367039442062378\n",
      "step = 4295600: loss = 3.525617837905884\n",
      "step = 4295800: loss = 3.884695291519165\n",
      "step = 4296000: loss = 3.0958101749420166\n",
      "step = 4296200: loss = 2.8805177211761475\n",
      "step = 4296400: loss = 2.536785840988159\n",
      "step = 4296600: loss = 2.7130513191223145\n",
      "step = 4296800: loss = 3.684236764907837\n",
      "step = 4297000: loss = 3.233868360519409\n",
      "step = 4297200: loss = 3.0604069232940674\n",
      "step = 4297400: loss = 5.223099708557129\n",
      "step = 4297600: loss = 3.3978984355926514\n",
      "step = 4297800: loss = 3.993818998336792\n",
      "step = 4298000: loss = 3.2963576316833496\n",
      "step = 4298200: loss = 3.1064531803131104\n",
      "step = 4298400: loss = 3.2309558391571045\n",
      "step = 4298600: loss = 2.767106056213379\n",
      "step = 4298800: loss = 4.278573036193848\n",
      "step = 4299000: loss = 3.756929636001587\n",
      "step = 4299200: loss = 3.721668243408203\n",
      "step = 4299400: loss = 3.898311138153076\n",
      "step = 4299600: loss = 4.195809364318848\n",
      "step = 4299800: loss = 3.214064359664917\n",
      "step = 4300000: loss = 5.182672023773193\n",
      "step = 4300000: Average Return = 4.949999809265137\n",
      "step = 4300200: loss = 4.776491165161133\n",
      "step = 4300400: loss = 4.227903842926025\n",
      "step = 4300600: loss = 3.033024311065674\n",
      "step = 4300800: loss = 3.727698802947998\n",
      "step = 4301000: loss = 3.4522714614868164\n",
      "step = 4301200: loss = 4.292819499969482\n",
      "step = 4301400: loss = 3.9302642345428467\n",
      "step = 4301600: loss = 2.5189430713653564\n",
      "step = 4301800: loss = 2.6261377334594727\n",
      "step = 4302000: loss = 4.171041965484619\n",
      "step = 4302200: loss = 3.0382957458496094\n",
      "step = 4302400: loss = 3.045386791229248\n",
      "step = 4302600: loss = 4.130519390106201\n",
      "step = 4302800: loss = 4.16316556930542\n",
      "step = 4303000: loss = 3.462869167327881\n",
      "step = 4303200: loss = 2.254202127456665\n",
      "step = 4303400: loss = 2.715485095977783\n",
      "step = 4303600: loss = 2.9478368759155273\n",
      "step = 4303800: loss = 3.732184410095215\n",
      "step = 4304000: loss = 2.303215503692627\n",
      "step = 4304200: loss = 2.955312490463257\n",
      "step = 4304400: loss = 3.2355926036834717\n",
      "step = 4304600: loss = 2.2473154067993164\n",
      "step = 4304800: loss = 2.3424901962280273\n",
      "step = 4305000: loss = 2.2306785583496094\n",
      "step = 4305000: Average Return = 2.799999952316284\n",
      "step = 4305200: loss = 3.446493148803711\n",
      "step = 4305400: loss = 2.319455862045288\n",
      "step = 4305600: loss = 1.7113326787948608\n",
      "step = 4305800: loss = 3.4123003482818604\n",
      "step = 4306000: loss = 3.3194167613983154\n",
      "step = 4306200: loss = 3.157595157623291\n",
      "step = 4306400: loss = 2.5095717906951904\n",
      "step = 4306600: loss = 3.492687940597534\n",
      "step = 4306800: loss = 3.731745481491089\n",
      "step = 4307000: loss = 2.066819190979004\n",
      "step = 4307200: loss = 3.166811466217041\n",
      "step = 4307400: loss = 3.001035690307617\n",
      "step = 4307600: loss = 2.732778310775757\n",
      "step = 4307800: loss = 3.32480525970459\n",
      "step = 4308000: loss = 2.918408155441284\n",
      "step = 4308200: loss = 3.105170249938965\n",
      "step = 4308400: loss = 3.5880093574523926\n",
      "step = 4308600: loss = 2.630512237548828\n",
      "step = 4308800: loss = 3.615935802459717\n",
      "step = 4309000: loss = 2.8271877765655518\n",
      "step = 4309200: loss = 2.807434558868408\n",
      "step = 4309400: loss = 3.143052816390991\n",
      "step = 4309600: loss = 2.015627145767212\n",
      "step = 4309800: loss = 2.9280941486358643\n",
      "step = 4310000: loss = 2.793682813644409\n",
      "step = 4310000: Average Return = 5.400000095367432\n",
      "step = 4310200: loss = 3.1517302989959717\n",
      "step = 4310400: loss = 3.7254321575164795\n",
      "step = 4310600: loss = 3.155088186264038\n",
      "step = 4310800: loss = 2.5171704292297363\n",
      "step = 4311000: loss = 3.9728448390960693\n",
      "step = 4311200: loss = 2.6545445919036865\n",
      "step = 4311400: loss = 4.037557125091553\n",
      "step = 4311600: loss = 4.100525379180908\n",
      "step = 4311800: loss = 2.570814847946167\n",
      "step = 4312000: loss = 2.2789182662963867\n",
      "step = 4312200: loss = 3.8167567253112793\n",
      "step = 4312400: loss = 2.4478726387023926\n",
      "step = 4312600: loss = 3.1382501125335693\n",
      "step = 4312800: loss = 2.622821569442749\n",
      "step = 4313000: loss = 3.7157745361328125\n",
      "step = 4313200: loss = 3.422646999359131\n",
      "step = 4313400: loss = 3.340662717819214\n",
      "step = 4313600: loss = 4.020294189453125\n",
      "step = 4313800: loss = 4.247206211090088\n",
      "step = 4314000: loss = 3.270385980606079\n",
      "step = 4314200: loss = 3.876420736312866\n",
      "step = 4314400: loss = 2.6858325004577637\n",
      "step = 4314600: loss = 4.399056434631348\n",
      "step = 4314800: loss = 2.576538562774658\n",
      "step = 4315000: loss = 3.1210525035858154\n",
      "step = 4315000: Average Return = 3.450000047683716\n",
      "step = 4315200: loss = 3.354461908340454\n",
      "step = 4315400: loss = 2.0096170902252197\n",
      "step = 4315600: loss = 3.0628252029418945\n",
      "step = 4315800: loss = 4.0751776695251465\n",
      "step = 4316000: loss = 3.5415408611297607\n",
      "step = 4316200: loss = 3.429307222366333\n",
      "step = 4316400: loss = 3.2617058753967285\n",
      "step = 4316600: loss = 3.2347052097320557\n",
      "step = 4316800: loss = 1.7722822427749634\n",
      "step = 4317000: loss = 3.302964448928833\n",
      "step = 4317200: loss = 2.236222743988037\n",
      "step = 4317400: loss = 2.7009825706481934\n",
      "step = 4317600: loss = 3.609123468399048\n",
      "step = 4317800: loss = 2.5372321605682373\n",
      "step = 4318000: loss = 1.8971437215805054\n",
      "step = 4318200: loss = 3.4633026123046875\n",
      "step = 4318400: loss = 2.8850860595703125\n",
      "step = 4318600: loss = 3.6794188022613525\n",
      "step = 4318800: loss = 4.274555683135986\n",
      "step = 4319000: loss = 2.2809910774230957\n",
      "step = 4319200: loss = 2.957245349884033\n",
      "step = 4319400: loss = 3.6657803058624268\n",
      "step = 4319600: loss = 4.603490352630615\n",
      "step = 4319800: loss = 3.202103853225708\n",
      "step = 4320000: loss = 2.688371419906616\n",
      "step = 4320000: Average Return = 3.6500000953674316\n",
      "step = 4320200: loss = 3.9940593242645264\n",
      "step = 4320400: loss = 4.3991169929504395\n",
      "step = 4320600: loss = 3.5572264194488525\n",
      "step = 4320800: loss = 3.0025157928466797\n",
      "step = 4321000: loss = 2.654975414276123\n",
      "step = 4321200: loss = 2.6828131675720215\n",
      "step = 4321400: loss = 3.546861410140991\n",
      "step = 4321600: loss = 2.5534555912017822\n",
      "step = 4321800: loss = 3.4416463375091553\n",
      "step = 4322000: loss = 3.2829222679138184\n",
      "step = 4322200: loss = 3.42697811126709\n",
      "step = 4322400: loss = 2.8770055770874023\n",
      "step = 4322600: loss = 3.242324113845825\n",
      "step = 4322800: loss = 2.749530553817749\n",
      "step = 4323000: loss = 3.5150043964385986\n",
      "step = 4323200: loss = 3.575035572052002\n",
      "step = 4323400: loss = 3.880350351333618\n",
      "step = 4323600: loss = 2.9580090045928955\n",
      "step = 4323800: loss = 3.362044095993042\n",
      "step = 4324000: loss = 2.212223529815674\n",
      "step = 4324200: loss = 3.530679941177368\n",
      "step = 4324400: loss = 4.205653667449951\n",
      "step = 4324600: loss = 3.3141703605651855\n",
      "step = 4324800: loss = 4.311861515045166\n",
      "step = 4325000: loss = 2.616452693939209\n",
      "step = 4325000: Average Return = 3.3499999046325684\n",
      "step = 4325200: loss = 3.4675168991088867\n",
      "step = 4325400: loss = 4.858930587768555\n",
      "step = 4325600: loss = 2.707277774810791\n",
      "step = 4325800: loss = 2.5270957946777344\n",
      "step = 4326000: loss = 3.9098751544952393\n",
      "step = 4326200: loss = 3.1644198894500732\n",
      "step = 4326400: loss = 2.577747344970703\n",
      "step = 4326600: loss = 3.4800736904144287\n",
      "step = 4326800: loss = 3.1078994274139404\n",
      "step = 4327000: loss = 3.567047595977783\n",
      "step = 4327200: loss = 2.949943780899048\n",
      "step = 4327400: loss = 3.884373188018799\n",
      "step = 4327600: loss = 3.656674385070801\n",
      "step = 4327800: loss = 3.202664613723755\n",
      "step = 4328000: loss = 3.5981743335723877\n",
      "step = 4328200: loss = 2.8722620010375977\n",
      "step = 4328400: loss = 2.6036219596862793\n",
      "step = 4328600: loss = 4.373621940612793\n",
      "step = 4328800: loss = 4.407047271728516\n",
      "step = 4329000: loss = 3.401766300201416\n",
      "step = 4329200: loss = 3.6184909343719482\n",
      "step = 4329400: loss = 3.3154821395874023\n",
      "step = 4329600: loss = 5.2270917892456055\n",
      "step = 4329800: loss = 3.603910446166992\n",
      "step = 4330000: loss = 3.0496773719787598\n",
      "step = 4330000: Average Return = 5.949999809265137\n",
      "step = 4330200: loss = 2.5469460487365723\n",
      "step = 4330400: loss = 3.0667169094085693\n",
      "step = 4330600: loss = 4.197127342224121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4330800: loss = 3.89263653755188\n",
      "step = 4331000: loss = 3.378843069076538\n",
      "step = 4331200: loss = 3.240062713623047\n",
      "step = 4331400: loss = 3.712991237640381\n",
      "step = 4331600: loss = 3.1329965591430664\n",
      "step = 4331800: loss = 3.2486233711242676\n",
      "step = 4332000: loss = 3.891543388366699\n",
      "step = 4332200: loss = 3.1842198371887207\n",
      "step = 4332400: loss = 2.7041118144989014\n",
      "step = 4332600: loss = 3.1185946464538574\n",
      "step = 4332800: loss = 2.432717800140381\n",
      "step = 4333000: loss = 2.516223192214966\n",
      "step = 4333200: loss = 3.1986186504364014\n",
      "step = 4333400: loss = 3.509042263031006\n",
      "step = 4333600: loss = 3.1465721130371094\n",
      "step = 4333800: loss = 2.2074649333953857\n",
      "step = 4334000: loss = 3.265299081802368\n",
      "step = 4334200: loss = 2.738298177719116\n",
      "step = 4334400: loss = 2.4586424827575684\n",
      "step = 4334600: loss = 3.67935848236084\n",
      "step = 4334800: loss = 3.05196475982666\n",
      "step = 4335000: loss = 1.8499611616134644\n",
      "step = 4335000: Average Return = 4.75\n",
      "step = 4335200: loss = 2.6915407180786133\n",
      "step = 4335400: loss = 3.330704689025879\n",
      "step = 4335600: loss = 3.867903709411621\n",
      "step = 4335800: loss = 3.808750629425049\n",
      "step = 4336000: loss = 3.7486228942871094\n",
      "step = 4336200: loss = 3.8866991996765137\n",
      "step = 4336400: loss = 2.4568967819213867\n",
      "step = 4336600: loss = 4.147427082061768\n",
      "step = 4336800: loss = 4.348379611968994\n",
      "step = 4337000: loss = 4.198520660400391\n",
      "step = 4337200: loss = 1.9173351526260376\n",
      "step = 4337400: loss = 2.861793041229248\n",
      "step = 4337600: loss = 2.2853448390960693\n",
      "step = 4337800: loss = 3.1647896766662598\n",
      "step = 4338000: loss = 3.00264573097229\n",
      "step = 4338200: loss = 3.661539316177368\n",
      "step = 4338400: loss = 2.9790942668914795\n",
      "step = 4338600: loss = 3.2648119926452637\n",
      "step = 4338800: loss = 2.016286611557007\n",
      "step = 4339000: loss = 2.3958683013916016\n",
      "step = 4339200: loss = 2.6463966369628906\n",
      "step = 4339400: loss = 3.88093638420105\n",
      "step = 4339600: loss = 2.381561279296875\n",
      "step = 4339800: loss = 4.011902332305908\n",
      "step = 4340000: loss = 3.6935231685638428\n",
      "step = 4340000: Average Return = 5.550000190734863\n",
      "step = 4340200: loss = 3.796241044998169\n",
      "step = 4340400: loss = 4.307124137878418\n",
      "step = 4340600: loss = 3.1270415782928467\n",
      "step = 4340800: loss = 3.731778383255005\n",
      "step = 4341000: loss = 3.486051559448242\n",
      "step = 4341200: loss = 3.623579263687134\n",
      "step = 4341400: loss = 3.353267192840576\n",
      "step = 4341600: loss = 2.9885613918304443\n",
      "step = 4341800: loss = 2.742405414581299\n",
      "step = 4342000: loss = 2.3527255058288574\n",
      "step = 4342200: loss = 2.5652098655700684\n",
      "step = 4342400: loss = 2.8273162841796875\n",
      "step = 4342600: loss = 2.7014501094818115\n",
      "step = 4342800: loss = 2.8274953365325928\n",
      "step = 4343000: loss = 4.179349899291992\n",
      "step = 4343200: loss = 2.3882272243499756\n",
      "step = 4343400: loss = 3.6729376316070557\n",
      "step = 4343600: loss = 3.461617946624756\n",
      "step = 4343800: loss = 4.017395973205566\n",
      "step = 4344000: loss = 3.4543139934539795\n",
      "step = 4344200: loss = 3.137352228164673\n",
      "step = 4344400: loss = 3.0492029190063477\n",
      "step = 4344600: loss = 2.028571367263794\n",
      "step = 4344800: loss = 4.286652088165283\n",
      "step = 4345000: loss = 3.0132036209106445\n",
      "step = 4345000: Average Return = 3.450000047683716\n",
      "step = 4345200: loss = 4.602149486541748\n",
      "step = 4345400: loss = 3.092372179031372\n",
      "step = 4345600: loss = 5.159317493438721\n",
      "step = 4345800: loss = 3.556950569152832\n",
      "step = 4346000: loss = 2.7066452503204346\n",
      "step = 4346200: loss = 2.1698715686798096\n",
      "step = 4346400: loss = 3.1441986560821533\n",
      "step = 4346600: loss = 5.30711555480957\n",
      "step = 4346800: loss = 2.2289137840270996\n",
      "step = 4347000: loss = 3.1528823375701904\n",
      "step = 4347200: loss = 1.7113776206970215\n",
      "step = 4347400: loss = 3.1368489265441895\n",
      "step = 4347600: loss = 3.271515369415283\n",
      "step = 4347800: loss = 4.091987609863281\n",
      "step = 4348000: loss = 4.171781063079834\n",
      "step = 4348200: loss = 3.714803695678711\n",
      "step = 4348400: loss = 2.7372450828552246\n",
      "step = 4348600: loss = 3.7999114990234375\n",
      "step = 4348800: loss = 2.524153709411621\n",
      "step = 4349000: loss = 4.068787574768066\n",
      "step = 4349200: loss = 3.569697141647339\n",
      "step = 4349400: loss = 2.4027905464172363\n",
      "step = 4349600: loss = 4.6130170822143555\n",
      "step = 4349800: loss = 3.5488460063934326\n",
      "step = 4350000: loss = 3.2567529678344727\n",
      "step = 4350000: Average Return = 4.400000095367432\n",
      "step = 4350200: loss = 4.123531818389893\n",
      "step = 4350400: loss = 3.658403158187866\n",
      "step = 4350600: loss = 3.162238597869873\n",
      "step = 4350800: loss = 4.55511474609375\n",
      "step = 4351000: loss = 3.827423572540283\n",
      "step = 4351200: loss = 3.3819448947906494\n",
      "step = 4351400: loss = 4.3525214195251465\n",
      "step = 4351600: loss = 3.319208860397339\n",
      "step = 4351800: loss = 2.9644627571105957\n",
      "step = 4352000: loss = 3.505680799484253\n",
      "step = 4352200: loss = 3.0613210201263428\n",
      "step = 4352400: loss = 3.9130918979644775\n",
      "step = 4352600: loss = 3.029683828353882\n",
      "step = 4352800: loss = 2.763739585876465\n",
      "step = 4353000: loss = 3.5483081340789795\n",
      "step = 4353200: loss = 2.2904820442199707\n",
      "step = 4353400: loss = 1.9319180250167847\n",
      "step = 4353600: loss = 3.830646514892578\n",
      "step = 4353800: loss = 3.745633125305176\n",
      "step = 4354000: loss = 3.994241237640381\n",
      "step = 4354200: loss = 3.5621910095214844\n",
      "step = 4354400: loss = 3.533374547958374\n",
      "step = 4354600: loss = 2.737449884414673\n",
      "step = 4354800: loss = 2.9249026775360107\n",
      "step = 4355000: loss = 2.3178651332855225\n",
      "step = 4355000: Average Return = 5.25\n",
      "step = 4355200: loss = 3.076796770095825\n",
      "step = 4355400: loss = 2.884796619415283\n",
      "step = 4355600: loss = 2.7898550033569336\n",
      "step = 4355800: loss = 4.0513529777526855\n",
      "step = 4356000: loss = 4.42337703704834\n",
      "step = 4356200: loss = 2.840559720993042\n",
      "step = 4356400: loss = 3.5963711738586426\n",
      "step = 4356600: loss = 2.6190409660339355\n",
      "step = 4356800: loss = 2.194519519805908\n",
      "step = 4357000: loss = 3.730607748031616\n",
      "step = 4357200: loss = 3.42586350440979\n",
      "step = 4357400: loss = 4.3382792472839355\n",
      "step = 4357600: loss = 3.165886878967285\n",
      "step = 4357800: loss = 4.1226654052734375\n",
      "step = 4358000: loss = 3.244675636291504\n",
      "step = 4358200: loss = 3.729118824005127\n",
      "step = 4358400: loss = 2.1254665851593018\n",
      "step = 4358600: loss = 3.080237865447998\n",
      "step = 4358800: loss = 5.561817646026611\n",
      "step = 4359000: loss = 2.5520107746124268\n",
      "step = 4359200: loss = 2.956386089324951\n",
      "step = 4359400: loss = 3.553271770477295\n",
      "step = 4359600: loss = 2.00360369682312\n",
      "step = 4359800: loss = 3.0343432426452637\n",
      "step = 4360000: loss = 3.101297616958618\n",
      "step = 4360000: Average Return = 6.300000190734863\n",
      "step = 4360200: loss = 4.126486301422119\n",
      "step = 4360400: loss = 2.904726505279541\n",
      "step = 4360600: loss = 3.5670225620269775\n",
      "step = 4360800: loss = 3.7796339988708496\n",
      "step = 4361000: loss = 2.7960076332092285\n",
      "step = 4361200: loss = 3.1328978538513184\n",
      "step = 4361400: loss = 2.8524327278137207\n",
      "step = 4361600: loss = 3.5334575176239014\n",
      "step = 4361800: loss = 3.9049558639526367\n",
      "step = 4362000: loss = 2.778046131134033\n",
      "step = 4362200: loss = 3.665224552154541\n",
      "step = 4362400: loss = 2.764326810836792\n",
      "step = 4362600: loss = 3.420809745788574\n",
      "step = 4362800: loss = 2.7624363899230957\n",
      "step = 4363000: loss = 2.8255059719085693\n",
      "step = 4363200: loss = 4.738933086395264\n",
      "step = 4363400: loss = 3.2460155487060547\n",
      "step = 4363600: loss = 4.479620933532715\n",
      "step = 4363800: loss = 2.9225869178771973\n",
      "step = 4364000: loss = 3.7731809616088867\n",
      "step = 4364200: loss = 3.993298292160034\n",
      "step = 4364400: loss = 3.0614542961120605\n",
      "step = 4364600: loss = 2.5581130981445312\n",
      "step = 4364800: loss = 2.391016721725464\n",
      "step = 4365000: loss = 2.6399900913238525\n",
      "step = 4365000: Average Return = 5.150000095367432\n",
      "step = 4365200: loss = 4.097139358520508\n",
      "step = 4365400: loss = 3.871535301208496\n",
      "step = 4365600: loss = 2.9187991619110107\n",
      "step = 4365800: loss = 3.364715576171875\n",
      "step = 4366000: loss = 3.2678041458129883\n",
      "step = 4366200: loss = 3.7536468505859375\n",
      "step = 4366400: loss = 2.705052375793457\n",
      "step = 4366600: loss = 2.416809558868408\n",
      "step = 4366800: loss = 2.927016019821167\n",
      "step = 4367000: loss = 4.460092067718506\n",
      "step = 4367200: loss = 3.8976311683654785\n",
      "step = 4367400: loss = 4.42672061920166\n",
      "step = 4367600: loss = 2.24829363822937\n",
      "step = 4367800: loss = 2.910062551498413\n",
      "step = 4368000: loss = 2.8068432807922363\n",
      "step = 4368200: loss = 3.174788236618042\n",
      "step = 4368400: loss = 2.365755081176758\n",
      "step = 4368600: loss = 5.0916748046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4368800: loss = 4.021399974822998\n",
      "step = 4369000: loss = 4.342649936676025\n",
      "step = 4369200: loss = 3.1668777465820312\n",
      "step = 4369400: loss = 3.0844924449920654\n",
      "step = 4369600: loss = 3.51242733001709\n",
      "step = 4369800: loss = 2.8909754753112793\n",
      "step = 4370000: loss = 3.2845001220703125\n",
      "step = 4370000: Average Return = 4.599999904632568\n",
      "step = 4370200: loss = 4.798449993133545\n",
      "step = 4370400: loss = 4.843136310577393\n",
      "step = 4370600: loss = 3.394757032394409\n",
      "step = 4370800: loss = 3.0442285537719727\n",
      "step = 4371000: loss = 2.0863664150238037\n",
      "step = 4371200: loss = 3.2388360500335693\n",
      "step = 4371400: loss = 3.316434860229492\n",
      "step = 4371600: loss = 4.1532721519470215\n",
      "step = 4371800: loss = 2.4842092990875244\n",
      "step = 4372000: loss = 3.40916109085083\n",
      "step = 4372200: loss = 4.097041606903076\n",
      "step = 4372400: loss = 3.2319576740264893\n",
      "step = 4372600: loss = 3.0195302963256836\n",
      "step = 4372800: loss = 4.7915239334106445\n",
      "step = 4373000: loss = 3.4953062534332275\n",
      "step = 4373200: loss = 4.679361820220947\n",
      "step = 4373400: loss = 2.7498462200164795\n",
      "step = 4373600: loss = 3.8298964500427246\n",
      "step = 4373800: loss = 2.3867344856262207\n",
      "step = 4374000: loss = 2.601384162902832\n",
      "step = 4374200: loss = 2.250615119934082\n",
      "step = 4374400: loss = 2.6643145084381104\n",
      "step = 4374600: loss = 3.2819132804870605\n",
      "step = 4374800: loss = 3.311856985092163\n",
      "step = 4375000: loss = 4.089721202850342\n",
      "step = 4375000: Average Return = 5.349999904632568\n",
      "step = 4375200: loss = 3.956404447555542\n",
      "step = 4375400: loss = 2.8515613079071045\n",
      "step = 4375600: loss = 2.539346933364868\n",
      "step = 4375800: loss = 2.855485200881958\n",
      "step = 4376000: loss = 3.996168851852417\n",
      "step = 4376200: loss = 2.928497552871704\n",
      "step = 4376400: loss = 3.708899974822998\n",
      "step = 4376600: loss = 3.3283305168151855\n",
      "step = 4376800: loss = 3.4988598823547363\n",
      "step = 4377000: loss = 2.946885824203491\n",
      "step = 4377200: loss = 2.6087646484375\n",
      "step = 4377400: loss = 3.098499059677124\n",
      "step = 4377600: loss = 3.782646417617798\n",
      "step = 4377800: loss = 4.058954238891602\n",
      "step = 4378000: loss = 2.961679220199585\n",
      "step = 4378200: loss = 3.153271436691284\n",
      "step = 4378400: loss = 4.919904708862305\n",
      "step = 4378600: loss = 3.521599769592285\n",
      "step = 4378800: loss = 3.327944040298462\n",
      "step = 4379000: loss = 3.5602853298187256\n",
      "step = 4379200: loss = 3.383038282394409\n",
      "step = 4379400: loss = 2.3217504024505615\n",
      "step = 4379600: loss = 5.173330783843994\n",
      "step = 4379800: loss = 3.151099681854248\n",
      "step = 4380000: loss = 3.014122247695923\n",
      "step = 4380000: Average Return = 5.300000190734863\n",
      "step = 4380200: loss = 3.32486891746521\n",
      "step = 4380400: loss = 2.5819692611694336\n",
      "step = 4380600: loss = 2.990450382232666\n",
      "step = 4380800: loss = 3.9656426906585693\n",
      "step = 4381000: loss = 3.1602771282196045\n",
      "step = 4381200: loss = 2.044481039047241\n",
      "step = 4381400: loss = 2.7001760005950928\n",
      "step = 4381600: loss = 4.2770209312438965\n",
      "step = 4381800: loss = 3.6362664699554443\n",
      "step = 4382000: loss = 3.6497907638549805\n",
      "step = 4382200: loss = 4.081233978271484\n",
      "step = 4382400: loss = 4.7166972160339355\n",
      "step = 4382600: loss = 3.256269693374634\n",
      "step = 4382800: loss = 3.6680572032928467\n",
      "step = 4383000: loss = 4.19097900390625\n",
      "step = 4383200: loss = 2.8979382514953613\n",
      "step = 4383400: loss = 3.600100040435791\n",
      "step = 4383600: loss = 3.4840972423553467\n",
      "step = 4383800: loss = 4.361355304718018\n",
      "step = 4384000: loss = 2.447622537612915\n",
      "step = 4384200: loss = 2.614011764526367\n",
      "step = 4384400: loss = 3.0117974281311035\n",
      "step = 4384600: loss = 4.578380584716797\n",
      "step = 4384800: loss = 4.089122772216797\n",
      "step = 4385000: loss = 4.072381973266602\n",
      "step = 4385000: Average Return = 4.650000095367432\n",
      "step = 4385200: loss = 3.6844398975372314\n",
      "step = 4385400: loss = 4.149747371673584\n",
      "step = 4385600: loss = 3.451693534851074\n",
      "step = 4385800: loss = 2.393603801727295\n",
      "step = 4386000: loss = 3.953904867172241\n",
      "step = 4386200: loss = 2.1060783863067627\n",
      "step = 4386400: loss = 2.6132519245147705\n",
      "step = 4386600: loss = 3.740065574645996\n",
      "step = 4386800: loss = 3.575230121612549\n",
      "step = 4387000: loss = 3.642651081085205\n",
      "step = 4387200: loss = 4.267489910125732\n",
      "step = 4387400: loss = 4.327483654022217\n",
      "step = 4387600: loss = 2.027278184890747\n",
      "step = 4387800: loss = 3.2669661045074463\n",
      "step = 4388000: loss = 2.73869252204895\n",
      "step = 4388200: loss = 4.5829877853393555\n",
      "step = 4388400: loss = 3.329890012741089\n",
      "step = 4388600: loss = 3.7785136699676514\n",
      "step = 4388800: loss = 3.469249725341797\n",
      "step = 4389000: loss = 3.08935546875\n",
      "step = 4389200: loss = 4.783493995666504\n",
      "step = 4389400: loss = 2.27519154548645\n",
      "step = 4389600: loss = 3.8370988368988037\n",
      "step = 4389800: loss = 3.072059154510498\n",
      "step = 4390000: loss = 3.2546322345733643\n",
      "step = 4390000: Average Return = 6.5\n",
      "step = 4390200: loss = 4.0797553062438965\n",
      "step = 4390400: loss = 3.290522336959839\n",
      "step = 4390600: loss = 2.020786762237549\n",
      "step = 4390800: loss = 2.118610382080078\n",
      "step = 4391000: loss = 2.9052107334136963\n",
      "step = 4391200: loss = 3.422264337539673\n",
      "step = 4391400: loss = 2.075575351715088\n",
      "step = 4391600: loss = 3.2116899490356445\n",
      "step = 4391800: loss = 3.5736491680145264\n",
      "step = 4392000: loss = 2.7537450790405273\n",
      "step = 4392200: loss = 2.5214712619781494\n",
      "step = 4392400: loss = 3.5553808212280273\n",
      "step = 4392600: loss = 4.34453010559082\n",
      "step = 4392800: loss = 3.971613883972168\n",
      "step = 4393000: loss = 2.79239821434021\n",
      "step = 4393200: loss = 3.1944100856781006\n",
      "step = 4393400: loss = 3.254439115524292\n",
      "step = 4393600: loss = 2.4038033485412598\n",
      "step = 4393800: loss = 2.8339014053344727\n",
      "step = 4394000: loss = 3.2974510192871094\n",
      "step = 4394200: loss = 3.474501848220825\n",
      "step = 4394400: loss = 3.762575626373291\n",
      "step = 4394600: loss = 2.4900572299957275\n",
      "step = 4394800: loss = 3.041020393371582\n",
      "step = 4395000: loss = 2.981036901473999\n",
      "step = 4395000: Average Return = 6.449999809265137\n",
      "step = 4395200: loss = 2.9257402420043945\n",
      "step = 4395400: loss = 4.19259786605835\n",
      "step = 4395600: loss = 2.409813404083252\n",
      "step = 4395800: loss = 3.957554817199707\n",
      "step = 4396000: loss = 4.193583011627197\n",
      "step = 4396200: loss = 3.168729782104492\n",
      "step = 4396400: loss = 3.5251033306121826\n",
      "step = 4396600: loss = 2.4904110431671143\n",
      "step = 4396800: loss = 3.853105306625366\n",
      "step = 4397000: loss = 2.602632761001587\n",
      "step = 4397200: loss = 3.492990016937256\n",
      "step = 4397400: loss = 2.5780770778656006\n",
      "step = 4397600: loss = 4.788903713226318\n",
      "step = 4397800: loss = 4.106507778167725\n",
      "step = 4398000: loss = 3.202643871307373\n",
      "step = 4398200: loss = 3.1609294414520264\n",
      "step = 4398400: loss = 2.4256317615509033\n",
      "step = 4398600: loss = 3.9693682193756104\n",
      "step = 4398800: loss = 4.861930847167969\n",
      "step = 4399000: loss = 3.2655770778656006\n",
      "step = 4399200: loss = 3.2097442150115967\n",
      "step = 4399400: loss = 3.8507800102233887\n",
      "step = 4399600: loss = 3.61572265625\n",
      "step = 4399800: loss = 3.865705728530884\n",
      "step = 4400000: loss = 3.566356658935547\n",
      "step = 4400000: Average Return = 6.800000190734863\n",
      "step = 4400200: loss = 3.027637243270874\n",
      "step = 4400400: loss = 4.39088249206543\n",
      "step = 4400600: loss = 3.642183542251587\n",
      "step = 4400800: loss = 5.2013959884643555\n",
      "step = 4401000: loss = 2.1947553157806396\n",
      "step = 4401200: loss = 3.5749969482421875\n",
      "step = 4401400: loss = 2.380427598953247\n",
      "step = 4401600: loss = 3.616150379180908\n",
      "step = 4401800: loss = 4.15934944152832\n",
      "step = 4402000: loss = 3.2094268798828125\n",
      "step = 4402200: loss = 3.772095203399658\n",
      "step = 4402400: loss = 3.068848133087158\n",
      "step = 4402600: loss = 3.250903844833374\n",
      "step = 4402800: loss = 3.914501190185547\n",
      "step = 4403000: loss = 3.827901601791382\n",
      "step = 4403200: loss = 2.5978589057922363\n",
      "step = 4403400: loss = 4.436076641082764\n",
      "step = 4403600: loss = 2.4150588512420654\n",
      "step = 4403800: loss = 3.4191558361053467\n",
      "step = 4404000: loss = 2.9402413368225098\n",
      "step = 4404200: loss = 2.6296937465667725\n",
      "step = 4404400: loss = 3.844891309738159\n",
      "step = 4404600: loss = 3.3112902641296387\n",
      "step = 4404800: loss = 2.812368631362915\n",
      "step = 4405000: loss = 2.9518890380859375\n",
      "step = 4405000: Average Return = 5.550000190734863\n",
      "step = 4405200: loss = 4.25016975402832\n",
      "step = 4405400: loss = 2.7931299209594727\n",
      "step = 4405600: loss = 3.80423641204834\n",
      "step = 4405800: loss = 4.052780628204346\n",
      "step = 4406000: loss = 3.3335490226745605\n",
      "step = 4406200: loss = 4.601188659667969\n",
      "step = 4406400: loss = 3.9433555603027344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4406600: loss = 3.126743793487549\n",
      "step = 4406800: loss = 3.9989538192749023\n",
      "step = 4407000: loss = 3.138981342315674\n",
      "step = 4407200: loss = 4.371831893920898\n",
      "step = 4407400: loss = 3.5575008392333984\n",
      "step = 4407600: loss = 3.1158926486968994\n",
      "step = 4407800: loss = 4.569438934326172\n",
      "step = 4408000: loss = 3.814244270324707\n",
      "step = 4408200: loss = 3.272052764892578\n",
      "step = 4408400: loss = 2.179213762283325\n",
      "step = 4408600: loss = 3.587948799133301\n",
      "step = 4408800: loss = 3.4111862182617188\n",
      "step = 4409000: loss = 3.512064218521118\n",
      "step = 4409200: loss = 2.1633598804473877\n",
      "step = 4409400: loss = 4.056997776031494\n",
      "step = 4409600: loss = 4.440374851226807\n",
      "step = 4409800: loss = 2.845592498779297\n",
      "step = 4410000: loss = 3.232306480407715\n",
      "step = 4410000: Average Return = 5.400000095367432\n",
      "step = 4410200: loss = 2.5024936199188232\n",
      "step = 4410400: loss = 5.297601222991943\n",
      "step = 4410600: loss = 2.9839701652526855\n",
      "step = 4410800: loss = 4.265925884246826\n",
      "step = 4411000: loss = 3.2699944972991943\n",
      "step = 4411200: loss = 2.288346767425537\n",
      "step = 4411400: loss = 3.534775972366333\n",
      "step = 4411600: loss = 3.900165557861328\n",
      "step = 4411800: loss = 2.499384641647339\n",
      "step = 4412000: loss = 3.598332405090332\n",
      "step = 4412200: loss = 3.7384111881256104\n",
      "step = 4412400: loss = 1.814755916595459\n",
      "step = 4412600: loss = 4.106665134429932\n",
      "step = 4412800: loss = 3.4297990798950195\n",
      "step = 4413000: loss = 3.8887219429016113\n",
      "step = 4413200: loss = 5.235655307769775\n",
      "step = 4413400: loss = 3.888458728790283\n",
      "step = 4413600: loss = 3.5107390880584717\n",
      "step = 4413800: loss = 2.542961835861206\n",
      "step = 4414000: loss = 3.904230833053589\n",
      "step = 4414200: loss = 2.6632373332977295\n",
      "step = 4414400: loss = 4.3435587882995605\n",
      "step = 4414600: loss = 1.7059847116470337\n",
      "step = 4414800: loss = 3.3975164890289307\n",
      "step = 4415000: loss = 3.9689605236053467\n",
      "step = 4415000: Average Return = 3.950000047683716\n",
      "step = 4415200: loss = 3.9472973346710205\n",
      "step = 4415400: loss = 4.598988056182861\n",
      "step = 4415600: loss = 6.645750999450684\n",
      "step = 4415800: loss = 3.83463716506958\n",
      "step = 4416000: loss = 4.235533237457275\n",
      "step = 4416200: loss = 2.3893678188323975\n",
      "step = 4416400: loss = 3.5935802459716797\n",
      "step = 4416600: loss = 3.642915964126587\n",
      "step = 4416800: loss = 3.827862501144409\n",
      "step = 4417000: loss = 3.58035945892334\n",
      "step = 4417200: loss = 3.3106701374053955\n",
      "step = 4417400: loss = 4.214386940002441\n",
      "step = 4417600: loss = 3.0845024585723877\n",
      "step = 4417800: loss = 1.689282774925232\n",
      "step = 4418000: loss = 3.327113628387451\n",
      "step = 4418200: loss = 2.7464699745178223\n",
      "step = 4418400: loss = 3.8319857120513916\n",
      "step = 4418600: loss = 2.5675933361053467\n",
      "step = 4418800: loss = 4.631917476654053\n",
      "step = 4419000: loss = 5.430585861206055\n",
      "step = 4419200: loss = 2.9505391120910645\n",
      "step = 4419400: loss = 3.507082939147949\n",
      "step = 4419600: loss = 4.149144649505615\n",
      "step = 4419800: loss = 4.051782131195068\n",
      "step = 4420000: loss = 2.256805419921875\n",
      "step = 4420000: Average Return = 4.5\n",
      "step = 4420200: loss = 3.7678585052490234\n",
      "step = 4420400: loss = 2.918879270553589\n",
      "step = 4420600: loss = 3.2288012504577637\n",
      "step = 4420800: loss = 4.700372695922852\n",
      "step = 4421000: loss = 2.6381027698516846\n",
      "step = 4421200: loss = 3.6595797538757324\n",
      "step = 4421400: loss = 4.973692417144775\n",
      "step = 4421600: loss = 3.92204213142395\n",
      "step = 4421800: loss = 4.274013042449951\n",
      "step = 4422000: loss = 3.1918187141418457\n",
      "step = 4422200: loss = 4.013189792633057\n",
      "step = 4422400: loss = 3.0815019607543945\n",
      "step = 4422600: loss = 2.2862279415130615\n",
      "step = 4422800: loss = 3.299722194671631\n",
      "step = 4423000: loss = 2.0513789653778076\n",
      "step = 4423200: loss = 2.3574366569519043\n",
      "step = 4423400: loss = 2.1855854988098145\n",
      "step = 4423600: loss = 3.4009721279144287\n",
      "step = 4423800: loss = 5.274894714355469\n",
      "step = 4424000: loss = 4.554605007171631\n",
      "step = 4424200: loss = 4.020096778869629\n",
      "step = 4424400: loss = 3.985731840133667\n",
      "step = 4424600: loss = 4.677927494049072\n",
      "step = 4424800: loss = 3.880737781524658\n",
      "step = 4425000: loss = 3.785914182662964\n",
      "step = 4425000: Average Return = 5.599999904632568\n",
      "step = 4425200: loss = 4.363877296447754\n",
      "step = 4425400: loss = 4.060089588165283\n",
      "step = 4425600: loss = 3.2430760860443115\n",
      "step = 4425800: loss = 3.681429862976074\n",
      "step = 4426000: loss = 3.9795119762420654\n",
      "step = 4426200: loss = 3.6836180686950684\n",
      "step = 4426400: loss = 3.074233055114746\n",
      "step = 4426600: loss = 2.0460736751556396\n",
      "step = 4426800: loss = 2.6268904209136963\n",
      "step = 4427000: loss = 3.429816484451294\n",
      "step = 4427200: loss = 3.796114921569824\n",
      "step = 4427400: loss = 6.54522705078125\n",
      "step = 4427600: loss = 2.686896562576294\n",
      "step = 4427800: loss = 5.314095497131348\n",
      "step = 4428000: loss = 3.162320137023926\n",
      "step = 4428200: loss = 4.773751735687256\n",
      "step = 4428400: loss = 5.939594745635986\n",
      "step = 4428600: loss = 2.123934507369995\n",
      "step = 4428800: loss = 3.5842535495758057\n",
      "step = 4429000: loss = 7.140028953552246\n",
      "step = 4429200: loss = 6.248266696929932\n",
      "step = 4429400: loss = 6.58833122253418\n",
      "step = 4429600: loss = 3.890418291091919\n",
      "step = 4429800: loss = 2.9947991371154785\n",
      "step = 4430000: loss = 6.598940849304199\n",
      "step = 4430000: Average Return = 2.950000047683716\n",
      "step = 4430200: loss = 11.781550407409668\n",
      "step = 4430400: loss = 10.127018928527832\n",
      "step = 4430600: loss = 8.374833106994629\n",
      "step = 4430800: loss = 4.618579864501953\n",
      "step = 4431000: loss = 5.595551013946533\n",
      "step = 4431200: loss = 3.656663179397583\n",
      "step = 4431400: loss = 4.459324359893799\n",
      "step = 4431600: loss = 3.6618196964263916\n",
      "step = 4431800: loss = 6.534858226776123\n",
      "step = 4432000: loss = 20.687999725341797\n",
      "step = 4432200: loss = 4.682059288024902\n",
      "step = 4432400: loss = 4.676961421966553\n",
      "step = 4432600: loss = 4.663144111633301\n",
      "step = 4432800: loss = 3.906337261199951\n",
      "step = 4433000: loss = 2.692789316177368\n",
      "step = 4433200: loss = 5.526790618896484\n",
      "step = 4433400: loss = 5.088475704193115\n",
      "step = 4433600: loss = 42.09669876098633\n",
      "step = 4433800: loss = 4.001493453979492\n",
      "step = 4434000: loss = 5.206676006317139\n",
      "step = 4434200: loss = 4.577609539031982\n",
      "step = 4434400: loss = 4.502753734588623\n",
      "step = 4434600: loss = 6.50813102722168\n",
      "step = 4434800: loss = 5.204963684082031\n",
      "step = 4435000: loss = 4.669576168060303\n",
      "step = 4435000: Average Return = 2.75\n",
      "step = 4435200: loss = 8.020459175109863\n",
      "step = 4435400: loss = 5.520042419433594\n",
      "step = 4435600: loss = 4.508029460906982\n",
      "step = 4435800: loss = 4.318793773651123\n",
      "step = 4436000: loss = 3.069856882095337\n",
      "step = 4436200: loss = 5.700997352600098\n",
      "step = 4436400: loss = 4.543047904968262\n",
      "step = 4436600: loss = 3.78610897064209\n",
      "step = 4436800: loss = 8.604856491088867\n",
      "step = 4437000: loss = 6.125410079956055\n",
      "step = 4437200: loss = 4.6224188804626465\n",
      "step = 4437400: loss = 4.221186637878418\n",
      "step = 4437600: loss = 2.8364295959472656\n",
      "step = 4437800: loss = 3.8080291748046875\n",
      "step = 4438000: loss = 3.158857583999634\n",
      "step = 4438200: loss = 3.3341751098632812\n",
      "step = 4438400: loss = 3.95036244392395\n",
      "step = 4438600: loss = 31.588693618774414\n",
      "step = 4438800: loss = 35.21949768066406\n",
      "step = 4439000: loss = 3.2706148624420166\n",
      "step = 4439200: loss = 5.07792854309082\n",
      "step = 4439400: loss = 12.680517196655273\n",
      "step = 4439600: loss = 8.431108474731445\n",
      "step = 4439800: loss = 11.789960861206055\n",
      "step = 4440000: loss = 4.620352745056152\n",
      "step = 4440000: Average Return = 4.550000190734863\n",
      "step = 4440200: loss = 8.057470321655273\n",
      "step = 4440400: loss = 48.1591682434082\n",
      "step = 4440600: loss = 97.60034942626953\n",
      "step = 4440800: loss = 40.13359451293945\n",
      "step = 4441000: loss = 67.56893157958984\n",
      "step = 4441200: loss = 22.73296546936035\n",
      "step = 4441400: loss = 13.756916046142578\n",
      "step = 4441600: loss = 10.046303749084473\n",
      "step = 4441800: loss = 13.859980583190918\n",
      "step = 4442000: loss = 13.708402633666992\n",
      "step = 4442200: loss = 17.046672821044922\n",
      "step = 4442400: loss = 15.954445838928223\n",
      "step = 4442600: loss = 17.797191619873047\n",
      "step = 4442800: loss = 12.31918716430664\n",
      "step = 4443000: loss = 13.50511360168457\n",
      "step = 4443200: loss = 24.921049118041992\n",
      "step = 4443400: loss = 22.990156173706055\n",
      "step = 4443600: loss = 33.4805793762207\n",
      "step = 4443800: loss = 19.47084617614746\n",
      "step = 4444000: loss = 15.323744773864746\n",
      "step = 4444200: loss = 24.58826446533203\n",
      "step = 4444400: loss = 14.194311141967773\n",
      "step = 4444600: loss = 11.912934303283691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4444800: loss = 26.90376853942871\n",
      "step = 4445000: loss = 44.54323196411133\n",
      "step = 4445000: Average Return = 3.799999952316284\n",
      "step = 4445200: loss = 12.144824981689453\n",
      "step = 4445400: loss = 76.86917877197266\n",
      "step = 4445600: loss = 12.042255401611328\n",
      "step = 4445800: loss = 75.80670928955078\n",
      "step = 4446000: loss = 64.45103454589844\n",
      "step = 4446200: loss = 30.11455535888672\n",
      "step = 4446400: loss = 17.12204360961914\n",
      "step = 4446600: loss = 106.41504669189453\n",
      "step = 4446800: loss = 8.43262767791748\n",
      "step = 4447000: loss = 256.885498046875\n",
      "step = 4447200: loss = 7.48423433303833\n",
      "step = 4447400: loss = 18.570199966430664\n",
      "step = 4447600: loss = 42.196739196777344\n",
      "step = 4447800: loss = 14.4255952835083\n",
      "step = 4448000: loss = 7.00534725189209\n",
      "step = 4448200: loss = 7.231550693511963\n",
      "step = 4448400: loss = 385.6330871582031\n",
      "step = 4448600: loss = 9.322101593017578\n",
      "step = 4448800: loss = 9.68280029296875\n",
      "step = 4449000: loss = 11.27489948272705\n",
      "step = 4449200: loss = 132.74058532714844\n",
      "step = 4449400: loss = 12.30212116241455\n",
      "step = 4449600: loss = 6.785941123962402\n",
      "step = 4449800: loss = 6.850991725921631\n",
      "step = 4450000: loss = 65.59114837646484\n",
      "step = 4450000: Average Return = 4.699999809265137\n",
      "step = 4450200: loss = 9.675040245056152\n",
      "step = 4450400: loss = 30.749738693237305\n",
      "step = 4450600: loss = 112.79566192626953\n",
      "step = 4450800: loss = 77.04766082763672\n",
      "step = 4451000: loss = 89.3897476196289\n",
      "step = 4451200: loss = 25.043554306030273\n",
      "step = 4451400: loss = 7.110422134399414\n",
      "step = 4451600: loss = 92.81867980957031\n",
      "step = 4451800: loss = 398.5269775390625\n",
      "step = 4452000: loss = 21.504175186157227\n",
      "step = 4452200: loss = 6.524196147918701\n",
      "step = 4452400: loss = 8.948586463928223\n",
      "step = 4452600: loss = 7.376731872558594\n",
      "step = 4452800: loss = 168.78919982910156\n",
      "step = 4453000: loss = 9.449588775634766\n",
      "step = 4453200: loss = 7.288167476654053\n",
      "step = 4453400: loss = 5.7559356689453125\n",
      "step = 4453600: loss = 15.3171968460083\n",
      "step = 4453800: loss = 4.875811576843262\n",
      "step = 4454000: loss = 7.19425630569458\n",
      "step = 4454200: loss = 11.634561538696289\n",
      "step = 4454400: loss = 9.941060066223145\n",
      "step = 4454600: loss = 63.90428161621094\n",
      "step = 4454800: loss = 4.450677394866943\n",
      "step = 4455000: loss = 20.068218231201172\n",
      "step = 4455000: Average Return = 3.049999952316284\n",
      "step = 4455200: loss = 61.84172058105469\n",
      "step = 4455400: loss = 7.22136926651001\n",
      "step = 4455600: loss = 7.771485328674316\n",
      "step = 4455800: loss = 13.029345512390137\n",
      "step = 4456000: loss = 29.790546417236328\n",
      "step = 4456200: loss = 78.69365692138672\n",
      "step = 4456400: loss = 129.05064392089844\n",
      "step = 4456600: loss = 126.58795928955078\n",
      "step = 4456800: loss = 23.577978134155273\n",
      "step = 4457000: loss = 9.721780776977539\n",
      "step = 4457200: loss = 9.051305770874023\n",
      "step = 4457400: loss = 8.29401683807373\n",
      "step = 4457600: loss = 55.98738098144531\n",
      "step = 4457800: loss = 5.269963264465332\n",
      "step = 4458000: loss = 129.64173889160156\n",
      "step = 4458200: loss = 7.9020233154296875\n",
      "step = 4458400: loss = 57.54640579223633\n",
      "step = 4458600: loss = 76.67501831054688\n",
      "step = 4458800: loss = 214.8417510986328\n",
      "step = 4459000: loss = 199.34649658203125\n",
      "step = 4459200: loss = 267.052734375\n",
      "step = 4459400: loss = 21.791393280029297\n",
      "step = 4459600: loss = 7.669400691986084\n",
      "step = 4459800: loss = 34.335296630859375\n",
      "step = 4460000: loss = 51.38251876831055\n",
      "step = 4460000: Average Return = 3.0999999046325684\n",
      "step = 4460200: loss = 3.4847967624664307\n",
      "step = 4460400: loss = 16.50611114501953\n",
      "step = 4460600: loss = 11.082015037536621\n",
      "step = 4460800: loss = 28.292293548583984\n",
      "step = 4461000: loss = 42.80469512939453\n",
      "step = 4461200: loss = 104.2689437866211\n",
      "step = 4461400: loss = 24.325111389160156\n",
      "step = 4461600: loss = 17.409196853637695\n",
      "step = 4461800: loss = 6.6712565422058105\n",
      "step = 4462000: loss = 28.097408294677734\n",
      "step = 4462200: loss = 14.209866523742676\n",
      "step = 4462400: loss = 241.77972412109375\n",
      "step = 4462600: loss = 378.6456604003906\n",
      "step = 4462800: loss = 22.788970947265625\n",
      "step = 4463000: loss = 18.985017776489258\n",
      "step = 4463200: loss = 99.69677734375\n",
      "step = 4463400: loss = 16.495697021484375\n",
      "step = 4463600: loss = 6.940818786621094\n",
      "step = 4463800: loss = 10.907711029052734\n",
      "step = 4464000: loss = 31.58966636657715\n",
      "step = 4464200: loss = 7.382508277893066\n",
      "step = 4464400: loss = 9.317809104919434\n",
      "step = 4464600: loss = 302.92913818359375\n",
      "step = 4464800: loss = 16.216951370239258\n",
      "step = 4465000: loss = 214.49026489257812\n",
      "step = 4465000: Average Return = 2.549999952316284\n",
      "step = 4465200: loss = 103.78218841552734\n",
      "step = 4465400: loss = 48.5273323059082\n",
      "step = 4465600: loss = 135.215576171875\n",
      "step = 4465800: loss = 24.027849197387695\n",
      "step = 4466000: loss = 338.1994934082031\n",
      "step = 4466200: loss = 543.4578857421875\n",
      "step = 4466400: loss = 47.19623947143555\n",
      "step = 4466600: loss = 5.699260711669922\n",
      "step = 4466800: loss = 15.55757999420166\n",
      "step = 4467000: loss = 249.50164794921875\n",
      "step = 4467200: loss = 6.881794452667236\n",
      "step = 4467400: loss = 18.1778507232666\n",
      "step = 4467600: loss = 19.08949089050293\n",
      "step = 4467800: loss = 6.3177056312561035\n",
      "step = 4468000: loss = 7.9958038330078125\n",
      "step = 4468200: loss = 52.29601287841797\n",
      "step = 4468400: loss = 5.901858806610107\n",
      "step = 4468600: loss = 261.2430419921875\n",
      "step = 4468800: loss = 10.407910346984863\n",
      "step = 4469000: loss = 85.2291030883789\n",
      "step = 4469200: loss = 9.261652946472168\n",
      "step = 4469400: loss = 179.217041015625\n",
      "step = 4469600: loss = 2851.072509765625\n",
      "step = 4469800: loss = 74.80488586425781\n",
      "step = 4470000: loss = 8.857966423034668\n",
      "step = 4470000: Average Return = 5.199999809265137\n",
      "step = 4470200: loss = 11.955175399780273\n",
      "step = 4470400: loss = 15.453432083129883\n",
      "step = 4470600: loss = 4.76737642288208\n",
      "step = 4470800: loss = 105.46467590332031\n",
      "step = 4471000: loss = 6.867805004119873\n",
      "step = 4471200: loss = 4.749006271362305\n",
      "step = 4471400: loss = 7.926860332489014\n",
      "step = 4471600: loss = 38.12791061401367\n",
      "step = 4471800: loss = 7.345108509063721\n",
      "step = 4472000: loss = 44.360145568847656\n",
      "step = 4472200: loss = 9.639678955078125\n",
      "step = 4472400: loss = 210.3778839111328\n",
      "step = 4472600: loss = 74.16155242919922\n",
      "step = 4472800: loss = 7.780698299407959\n",
      "step = 4473000: loss = 8.00955867767334\n",
      "step = 4473200: loss = 7.648514747619629\n",
      "step = 4473400: loss = 33.673431396484375\n",
      "step = 4473600: loss = 68.645751953125\n",
      "step = 4473800: loss = 7.695518493652344\n",
      "step = 4474000: loss = 6.395382404327393\n",
      "step = 4474200: loss = 7.197695255279541\n",
      "step = 4474400: loss = 7.231556415557861\n",
      "step = 4474600: loss = 62.576141357421875\n",
      "step = 4474800: loss = 13.368456840515137\n",
      "step = 4475000: loss = 5.073610782623291\n",
      "step = 4475000: Average Return = 5.349999904632568\n",
      "step = 4475200: loss = 7.746868133544922\n",
      "step = 4475400: loss = 6.9369354248046875\n",
      "step = 4475600: loss = 6.930447101593018\n",
      "step = 4475800: loss = 9.305132865905762\n",
      "step = 4476000: loss = 6.770651817321777\n",
      "step = 4476200: loss = 6.005220413208008\n",
      "step = 4476400: loss = 9.533820152282715\n",
      "step = 4476600: loss = 6.312654495239258\n",
      "step = 4476800: loss = 6.848803997039795\n",
      "step = 4477000: loss = 5.608647346496582\n",
      "step = 4477200: loss = 5.558658123016357\n",
      "step = 4477400: loss = 7.281839370727539\n",
      "step = 4477600: loss = 7.563591003417969\n",
      "step = 4477800: loss = 9.790576934814453\n",
      "step = 4478000: loss = 9.659756660461426\n",
      "step = 4478200: loss = 6.492966175079346\n",
      "step = 4478400: loss = 12.9941987991333\n",
      "step = 4478600: loss = 9.705476760864258\n",
      "step = 4478800: loss = 6.778870582580566\n",
      "step = 4479000: loss = 5.015996932983398\n",
      "step = 4479200: loss = 8.816008567810059\n",
      "step = 4479400: loss = 5.881101131439209\n",
      "step = 4479600: loss = 5.22676944732666\n",
      "step = 4479800: loss = 5.966332912445068\n",
      "step = 4480000: loss = 5.590080261230469\n",
      "step = 4480000: Average Return = 5.650000095367432\n",
      "step = 4480200: loss = 5.384517192840576\n",
      "step = 4480400: loss = 6.811728477478027\n",
      "step = 4480600: loss = 4.380403518676758\n",
      "step = 4480800: loss = 7.816390037536621\n",
      "step = 4481000: loss = 6.270174026489258\n",
      "step = 4481200: loss = 6.125977516174316\n",
      "step = 4481400: loss = 7.411439418792725\n",
      "step = 4481600: loss = 6.010633945465088\n",
      "step = 4481800: loss = 6.823073387145996\n",
      "step = 4482000: loss = 8.347824096679688\n",
      "step = 4482200: loss = 5.1326446533203125\n",
      "step = 4482400: loss = 4.494832515716553\n",
      "step = 4482600: loss = 5.837357521057129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4482800: loss = 7.103597164154053\n",
      "step = 4483000: loss = 12.633671760559082\n",
      "step = 4483200: loss = 7.315799236297607\n",
      "step = 4483400: loss = 4.716490745544434\n",
      "step = 4483600: loss = 7.683245658874512\n",
      "step = 4483800: loss = 9.328972816467285\n",
      "step = 4484000: loss = 7.231844902038574\n",
      "step = 4484200: loss = 5.1755218505859375\n",
      "step = 4484400: loss = 6.525271415710449\n",
      "step = 4484600: loss = 5.650983810424805\n",
      "step = 4484800: loss = 5.007051944732666\n",
      "step = 4485000: loss = 6.438690185546875\n",
      "step = 4485000: Average Return = 3.1500000953674316\n",
      "step = 4485200: loss = 6.419111251831055\n",
      "step = 4485400: loss = 7.779850959777832\n",
      "step = 4485600: loss = 7.708266258239746\n",
      "step = 4485800: loss = 6.202518939971924\n",
      "step = 4486000: loss = 6.687247276306152\n",
      "step = 4486200: loss = 5.015265941619873\n",
      "step = 4486400: loss = 8.188870429992676\n",
      "step = 4486600: loss = 5.03704833984375\n",
      "step = 4486800: loss = 5.0223388671875\n",
      "step = 4487000: loss = 5.63381290435791\n",
      "step = 4487200: loss = 7.495603084564209\n",
      "step = 4487400: loss = 6.89695930480957\n",
      "step = 4487600: loss = 7.048192977905273\n",
      "step = 4487800: loss = 6.381990909576416\n",
      "step = 4488000: loss = 4.96190881729126\n",
      "step = 4488200: loss = 5.277120113372803\n",
      "step = 4488400: loss = 6.598899841308594\n",
      "step = 4488600: loss = 3.8965489864349365\n",
      "step = 4488800: loss = 6.271539688110352\n",
      "step = 4489000: loss = 5.293453693389893\n",
      "step = 4489200: loss = 6.928808212280273\n",
      "step = 4489400: loss = 5.953989028930664\n",
      "step = 4489600: loss = 4.801054954528809\n",
      "step = 4489800: loss = 3.8375627994537354\n",
      "step = 4490000: loss = 4.509209632873535\n",
      "step = 4490000: Average Return = 2.9000000953674316\n",
      "step = 4490200: loss = 4.914180755615234\n",
      "step = 4490400: loss = 4.540842056274414\n",
      "step = 4490600: loss = 6.818272590637207\n",
      "step = 4490800: loss = 6.584395408630371\n",
      "step = 4491000: loss = 5.0165557861328125\n",
      "step = 4491200: loss = 5.043124198913574\n",
      "step = 4491400: loss = 3.4869823455810547\n",
      "step = 4491600: loss = 6.0865607261657715\n",
      "step = 4491800: loss = 5.37131404876709\n",
      "step = 4492000: loss = 3.919437885284424\n",
      "step = 4492200: loss = 5.0767502784729\n",
      "step = 4492400: loss = 3.888417959213257\n",
      "step = 4492600: loss = 5.170033931732178\n",
      "step = 4492800: loss = 6.548696041107178\n",
      "step = 4493000: loss = 4.954703330993652\n",
      "step = 4493200: loss = 5.720304012298584\n",
      "step = 4493400: loss = 4.345279216766357\n",
      "step = 4493600: loss = 5.40382194519043\n",
      "step = 4493800: loss = 3.5450243949890137\n",
      "step = 4494000: loss = 4.627853870391846\n",
      "step = 4494200: loss = 7.328280448913574\n",
      "step = 4494400: loss = 3.8501434326171875\n",
      "step = 4494600: loss = 5.335383892059326\n",
      "step = 4494800: loss = 6.414766311645508\n",
      "step = 4495000: loss = 4.986763000488281\n",
      "step = 4495000: Average Return = 2.8499999046325684\n",
      "step = 4495200: loss = 4.836704254150391\n",
      "step = 4495400: loss = 4.708162784576416\n",
      "step = 4495600: loss = 6.326202869415283\n",
      "step = 4495800: loss = 5.495185375213623\n",
      "step = 4496000: loss = 4.904743671417236\n",
      "step = 4496200: loss = 5.2454047203063965\n",
      "step = 4496400: loss = 6.40308952331543\n",
      "step = 4496600: loss = 6.78474760055542\n",
      "step = 4496800: loss = 5.561833381652832\n",
      "step = 4497000: loss = 4.835934162139893\n",
      "step = 4497200: loss = 4.381659984588623\n",
      "step = 4497400: loss = 6.053261756896973\n",
      "step = 4497600: loss = 4.413808345794678\n",
      "step = 4497800: loss = 4.854048728942871\n",
      "step = 4498000: loss = 4.710213661193848\n",
      "step = 4498200: loss = 4.790431022644043\n",
      "step = 4498400: loss = 5.235029220581055\n",
      "step = 4498600: loss = 4.2416558265686035\n",
      "step = 4498800: loss = 4.378172397613525\n",
      "step = 4499000: loss = 4.794354438781738\n",
      "step = 4499200: loss = 3.802260398864746\n",
      "step = 4499400: loss = 4.840427875518799\n",
      "step = 4499600: loss = 5.583725929260254\n",
      "step = 4499800: loss = 3.8780791759490967\n",
      "step = 4500000: loss = 4.592051982879639\n",
      "step = 4500000: Average Return = 2.549999952316284\n",
      "step = 4500200: loss = 3.3896639347076416\n",
      "step = 4500400: loss = 4.648055553436279\n",
      "step = 4500600: loss = 5.711158275604248\n",
      "step = 4500800: loss = 4.228213787078857\n",
      "step = 4501000: loss = 5.371638298034668\n",
      "step = 4501200: loss = 7.099568843841553\n",
      "step = 4501400: loss = 5.545828819274902\n",
      "step = 4501600: loss = 4.163886070251465\n",
      "step = 4501800: loss = 5.244853973388672\n",
      "step = 4502000: loss = 4.100620746612549\n",
      "step = 4502200: loss = 4.760804653167725\n",
      "step = 4502400: loss = 5.371094226837158\n",
      "step = 4502600: loss = 5.398652076721191\n",
      "step = 4502800: loss = 5.0519232749938965\n",
      "step = 4503000: loss = 4.696422100067139\n",
      "step = 4503200: loss = 3.897663116455078\n",
      "step = 4503400: loss = 4.994661331176758\n",
      "step = 4503600: loss = 4.473733901977539\n",
      "step = 4503800: loss = 3.7733280658721924\n",
      "step = 4504000: loss = 4.980601787567139\n",
      "step = 4504200: loss = 5.051445960998535\n",
      "step = 4504400: loss = 5.012904167175293\n",
      "step = 4504600: loss = 4.717782497406006\n",
      "step = 4504800: loss = 3.5669960975646973\n",
      "step = 4505000: loss = 3.809778928756714\n",
      "step = 4505000: Average Return = 3.1500000953674316\n",
      "step = 4505200: loss = 4.035834789276123\n",
      "step = 4505400: loss = 5.226202487945557\n",
      "step = 4505600: loss = 5.170820236206055\n",
      "step = 4505800: loss = 5.132448673248291\n",
      "step = 4506000: loss = 5.560089588165283\n",
      "step = 4506200: loss = 5.3611063957214355\n",
      "step = 4506400: loss = 5.270588397979736\n",
      "step = 4506600: loss = 4.059938907623291\n",
      "step = 4506800: loss = 4.482743740081787\n",
      "step = 4507000: loss = 5.670959949493408\n",
      "step = 4507200: loss = 5.205629825592041\n",
      "step = 4507400: loss = 5.01343297958374\n",
      "step = 4507600: loss = 6.012983322143555\n",
      "step = 4507800: loss = 3.8397703170776367\n",
      "step = 4508000: loss = 4.290765762329102\n",
      "step = 4508200: loss = 4.957006931304932\n",
      "step = 4508400: loss = 3.8327674865722656\n",
      "step = 4508600: loss = 6.165772438049316\n",
      "step = 4508800: loss = 4.8314361572265625\n",
      "step = 4509000: loss = 3.923142194747925\n",
      "step = 4509200: loss = 4.319905757904053\n",
      "step = 4509400: loss = 4.118229389190674\n",
      "step = 4509600: loss = 3.743417501449585\n",
      "step = 4509800: loss = 2.9479942321777344\n",
      "step = 4510000: loss = 5.39893913269043\n",
      "step = 4510000: Average Return = 2.4000000953674316\n",
      "step = 4510200: loss = 3.7836625576019287\n",
      "step = 4510400: loss = 4.0284881591796875\n",
      "step = 4510600: loss = 3.77179217338562\n",
      "step = 4510800: loss = 4.57826042175293\n",
      "step = 4511000: loss = 5.063403129577637\n",
      "step = 4511200: loss = 4.731902599334717\n",
      "step = 4511400: loss = 4.018779754638672\n",
      "step = 4511600: loss = 5.635184288024902\n",
      "step = 4511800: loss = 5.0756120681762695\n",
      "step = 4512000: loss = 4.533344745635986\n",
      "step = 4512200: loss = 4.80018424987793\n",
      "step = 4512400: loss = 6.02807092666626\n",
      "step = 4512600: loss = 3.9094419479370117\n",
      "step = 4512800: loss = 4.380379676818848\n",
      "step = 4513000: loss = 5.634199142456055\n",
      "step = 4513200: loss = 4.05666971206665\n",
      "step = 4513400: loss = 4.207487106323242\n",
      "step = 4513600: loss = 6.645606517791748\n",
      "step = 4513800: loss = 5.534285545349121\n",
      "step = 4514000: loss = 2.3675172328948975\n",
      "step = 4514200: loss = 4.880779266357422\n",
      "step = 4514400: loss = 2.5011651515960693\n",
      "step = 4514600: loss = 4.423353672027588\n",
      "step = 4514800: loss = 3.391193151473999\n",
      "step = 4515000: loss = 3.879883050918579\n",
      "step = 4515000: Average Return = 5.5\n",
      "step = 4515200: loss = 5.6743927001953125\n",
      "step = 4515400: loss = 4.404700756072998\n",
      "step = 4515600: loss = 5.2163987159729\n",
      "step = 4515800: loss = 4.553211688995361\n",
      "step = 4516000: loss = 5.0814337730407715\n",
      "step = 4516200: loss = 3.2205846309661865\n",
      "step = 4516400: loss = 4.273353099822998\n",
      "step = 4516600: loss = 4.863340854644775\n",
      "step = 4516800: loss = 3.0080697536468506\n",
      "step = 4517000: loss = 4.234496593475342\n",
      "step = 4517200: loss = 4.2708516120910645\n",
      "step = 4517400: loss = 4.419258117675781\n",
      "step = 4517600: loss = 5.383469104766846\n",
      "step = 4517800: loss = 4.312745094299316\n",
      "step = 4518000: loss = 3.51999568939209\n",
      "step = 4518200: loss = 4.097985744476318\n",
      "step = 4518400: loss = 5.361057758331299\n",
      "step = 4518600: loss = 3.6307013034820557\n",
      "step = 4518800: loss = 3.33162522315979\n",
      "step = 4519000: loss = 4.712549209594727\n",
      "step = 4519200: loss = 4.087189674377441\n",
      "step = 4519400: loss = 4.640613079071045\n",
      "step = 4519600: loss = 4.103722095489502\n",
      "step = 4519800: loss = 3.7180585861206055\n",
      "step = 4520000: loss = 3.858038902282715\n",
      "step = 4520000: Average Return = 3.5999999046325684\n",
      "step = 4520200: loss = 3.788027286529541\n",
      "step = 4520400: loss = 4.2302961349487305\n",
      "step = 4520600: loss = 3.571324586868286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4520800: loss = 5.054234027862549\n",
      "step = 4521000: loss = 5.2789177894592285\n",
      "step = 4521200: loss = 3.672186851501465\n",
      "step = 4521400: loss = 5.688936710357666\n",
      "step = 4521600: loss = 4.331297874450684\n",
      "step = 4521800: loss = 3.818554639816284\n",
      "step = 4522000: loss = 3.466315269470215\n",
      "step = 4522200: loss = 2.6243109703063965\n",
      "step = 4522400: loss = 5.275137901306152\n",
      "step = 4522600: loss = 3.6646337509155273\n",
      "step = 4522800: loss = 3.943071126937866\n",
      "step = 4523000: loss = 3.6451070308685303\n",
      "step = 4523200: loss = 5.800777435302734\n",
      "step = 4523400: loss = 4.616048336029053\n",
      "step = 4523600: loss = 3.411626100540161\n",
      "step = 4523800: loss = 5.916367053985596\n",
      "step = 4524000: loss = 3.032104969024658\n",
      "step = 4524200: loss = 3.080861806869507\n",
      "step = 4524400: loss = 5.172980785369873\n",
      "step = 4524600: loss = 5.266089916229248\n",
      "step = 4524800: loss = 3.5370397567749023\n",
      "step = 4525000: loss = 5.0055437088012695\n",
      "step = 4525000: Average Return = 4.349999904632568\n",
      "step = 4525200: loss = 4.391387939453125\n",
      "step = 4525400: loss = 3.752190113067627\n",
      "step = 4525600: loss = 3.6014480590820312\n",
      "step = 4525800: loss = 4.215299606323242\n",
      "step = 4526000: loss = 3.79683256149292\n",
      "step = 4526200: loss = 3.944532632827759\n",
      "step = 4526400: loss = 5.182886123657227\n",
      "step = 4526600: loss = 3.6959049701690674\n",
      "step = 4526800: loss = 5.095884323120117\n",
      "step = 4527000: loss = 4.7349419593811035\n",
      "step = 4527200: loss = 5.47598934173584\n",
      "step = 4527400: loss = 5.023782730102539\n",
      "step = 4527600: loss = 3.582139253616333\n",
      "step = 4527800: loss = 3.805145263671875\n",
      "step = 4528000: loss = 4.588067054748535\n",
      "step = 4528200: loss = 4.590799331665039\n",
      "step = 4528400: loss = 3.953281879425049\n",
      "step = 4528600: loss = 4.181206703186035\n",
      "step = 4528800: loss = 4.237619876861572\n",
      "step = 4529000: loss = 5.291661262512207\n",
      "step = 4529200: loss = 4.008594036102295\n",
      "step = 4529400: loss = 4.529898166656494\n",
      "step = 4529600: loss = 3.263991117477417\n",
      "step = 4529800: loss = 3.642113447189331\n",
      "step = 4530000: loss = 6.069963455200195\n",
      "step = 4530000: Average Return = 3.4000000953674316\n",
      "step = 4530200: loss = 4.300626277923584\n",
      "step = 4530400: loss = 4.791849136352539\n",
      "step = 4530600: loss = 5.3193583488464355\n",
      "step = 4530800: loss = 4.386921405792236\n",
      "step = 4531000: loss = 3.704340696334839\n",
      "step = 4531200: loss = 4.869291305541992\n",
      "step = 4531400: loss = 5.341557025909424\n",
      "step = 4531600: loss = 3.545527458190918\n",
      "step = 4531800: loss = 3.869946002960205\n",
      "step = 4532000: loss = 5.8911027908325195\n",
      "step = 4532200: loss = 4.428416728973389\n",
      "step = 4532400: loss = 3.9533305168151855\n",
      "step = 4532600: loss = 4.0412821769714355\n",
      "step = 4532800: loss = 3.7007548809051514\n",
      "step = 4533000: loss = 3.5450897216796875\n",
      "step = 4533200: loss = 4.521217346191406\n",
      "step = 4533400: loss = 4.196952819824219\n",
      "step = 4533600: loss = 4.298318862915039\n",
      "step = 4533800: loss = 5.757961750030518\n",
      "step = 4534000: loss = 4.800069332122803\n",
      "step = 4534200: loss = 2.928227663040161\n",
      "step = 4534400: loss = 4.792776584625244\n",
      "step = 4534600: loss = 4.209987163543701\n",
      "step = 4534800: loss = 2.9102818965911865\n",
      "step = 4535000: loss = 5.144168376922607\n",
      "step = 4535000: Average Return = 3.049999952316284\n",
      "step = 4535200: loss = 3.3969011306762695\n",
      "step = 4535400: loss = 3.2050588130950928\n",
      "step = 4535600: loss = 4.303546905517578\n",
      "step = 4535800: loss = 2.4638938903808594\n",
      "step = 4536000: loss = 2.9204046726226807\n",
      "step = 4536200: loss = 6.478365421295166\n",
      "step = 4536400: loss = 3.279850959777832\n",
      "step = 4536600: loss = 3.302452325820923\n",
      "step = 4536800: loss = 3.2376298904418945\n",
      "step = 4537000: loss = 5.733261585235596\n",
      "step = 4537200: loss = 4.521390914916992\n",
      "step = 4537400: loss = 5.42131233215332\n",
      "step = 4537600: loss = 5.311036109924316\n",
      "step = 4537800: loss = 3.8627190589904785\n",
      "step = 4538000: loss = 3.288088321685791\n",
      "step = 4538200: loss = 3.4179155826568604\n",
      "step = 4538400: loss = 3.8569767475128174\n",
      "step = 4538600: loss = 5.649118423461914\n",
      "step = 4538800: loss = 3.239284038543701\n",
      "step = 4539000: loss = 3.3726396560668945\n",
      "step = 4539200: loss = 3.0104918479919434\n",
      "step = 4539400: loss = 4.593204021453857\n",
      "step = 4539600: loss = 4.1031174659729\n",
      "step = 4539800: loss = 3.3214845657348633\n",
      "step = 4540000: loss = 5.924407958984375\n",
      "step = 4540000: Average Return = 5.050000190734863\n",
      "step = 4540200: loss = 5.694633483886719\n",
      "step = 4540400: loss = 4.865195274353027\n",
      "step = 4540600: loss = 3.608266592025757\n",
      "step = 4540800: loss = 4.71552038192749\n",
      "step = 4541000: loss = 4.725546360015869\n",
      "step = 4541200: loss = 4.727724075317383\n",
      "step = 4541400: loss = 4.494104385375977\n",
      "step = 4541600: loss = 3.2978081703186035\n",
      "step = 4541800: loss = 4.120870590209961\n",
      "step = 4542000: loss = 4.64255428314209\n",
      "step = 4542200: loss = 3.182756185531616\n",
      "step = 4542400: loss = 4.9136643409729\n",
      "step = 4542600: loss = 6.407480239868164\n",
      "step = 4542800: loss = 3.859454870223999\n",
      "step = 4543000: loss = 4.927189826965332\n",
      "step = 4543200: loss = 3.6506407260894775\n",
      "step = 4543400: loss = 3.720567464828491\n",
      "step = 4543600: loss = 5.311927318572998\n",
      "step = 4543800: loss = 5.111255168914795\n",
      "step = 4544000: loss = 3.864720106124878\n",
      "step = 4544200: loss = 3.865432024002075\n",
      "step = 4544400: loss = 3.8163936138153076\n",
      "step = 4544600: loss = 4.176881790161133\n",
      "step = 4544800: loss = 4.484338283538818\n",
      "step = 4545000: loss = 5.165706157684326\n",
      "step = 4545000: Average Return = 2.950000047683716\n",
      "step = 4545200: loss = 4.278418064117432\n",
      "step = 4545400: loss = 4.811858177185059\n",
      "step = 4545600: loss = 4.854806423187256\n",
      "step = 4545800: loss = 4.235108375549316\n",
      "step = 4546000: loss = 2.677147150039673\n",
      "step = 4546200: loss = 6.046688079833984\n",
      "step = 4546400: loss = 4.800088882446289\n",
      "step = 4546600: loss = 2.4158425331115723\n",
      "step = 4546800: loss = 4.0041351318359375\n",
      "step = 4547000: loss = 6.199901103973389\n",
      "step = 4547200: loss = 4.2393107414245605\n",
      "step = 4547400: loss = 3.3149914741516113\n",
      "step = 4547600: loss = 4.271979808807373\n",
      "step = 4547800: loss = 3.8392302989959717\n",
      "step = 4548000: loss = 4.5387067794799805\n",
      "step = 4548200: loss = 4.67300271987915\n",
      "step = 4548400: loss = 2.9808642864227295\n",
      "step = 4548600: loss = 3.946523427963257\n",
      "step = 4548800: loss = 2.318985939025879\n",
      "step = 4549000: loss = 4.601350784301758\n",
      "step = 4549200: loss = 5.444794654846191\n",
      "step = 4549400: loss = 3.938124895095825\n",
      "step = 4549600: loss = 4.402768611907959\n",
      "step = 4549800: loss = 4.192631244659424\n",
      "step = 4550000: loss = 3.4978044033050537\n",
      "step = 4550000: Average Return = 4.0\n",
      "step = 4550200: loss = 4.105623722076416\n",
      "step = 4550400: loss = 2.9030046463012695\n",
      "step = 4550600: loss = 4.62161922454834\n",
      "step = 4550800: loss = 4.09731912612915\n",
      "step = 4551000: loss = 5.7646660804748535\n",
      "step = 4551200: loss = 2.828049659729004\n",
      "step = 4551400: loss = 3.0925188064575195\n",
      "step = 4551600: loss = 4.088433265686035\n",
      "step = 4551800: loss = 3.6641314029693604\n",
      "step = 4552000: loss = 4.511986255645752\n",
      "step = 4552200: loss = 4.4239397048950195\n",
      "step = 4552400: loss = 4.890176296234131\n",
      "step = 4552600: loss = 4.006844520568848\n",
      "step = 4552800: loss = 5.177154541015625\n",
      "step = 4553000: loss = 2.984523296356201\n",
      "step = 4553200: loss = 3.7288475036621094\n",
      "step = 4553400: loss = 3.567538261413574\n",
      "step = 4553600: loss = 4.704503059387207\n",
      "step = 4553800: loss = 3.573073625564575\n",
      "step = 4554000: loss = 3.464755058288574\n",
      "step = 4554200: loss = 3.2142724990844727\n",
      "step = 4554400: loss = 3.624660015106201\n",
      "step = 4554600: loss = 4.156747817993164\n",
      "step = 4554800: loss = 2.5430848598480225\n",
      "step = 4555000: loss = 3.0773837566375732\n",
      "step = 4555000: Average Return = 1.100000023841858\n",
      "step = 4555200: loss = 5.409296035766602\n",
      "step = 4555400: loss = 2.9820163249969482\n",
      "step = 4555600: loss = 3.466679573059082\n",
      "step = 4555800: loss = 6.075660705566406\n",
      "step = 4556000: loss = 3.166516065597534\n",
      "step = 4556200: loss = 3.8285276889801025\n",
      "step = 4556400: loss = 3.1096062660217285\n",
      "step = 4556600: loss = 3.169168710708618\n",
      "step = 4556800: loss = 3.1311452388763428\n",
      "step = 4557000: loss = 3.4352781772613525\n",
      "step = 4557200: loss = 3.8590991497039795\n",
      "step = 4557400: loss = 3.339400053024292\n",
      "step = 4557600: loss = 3.5614781379699707\n",
      "step = 4557800: loss = 4.106070518493652\n",
      "step = 4558000: loss = 4.434127330780029\n",
      "step = 4558200: loss = 5.8531084060668945\n",
      "step = 4558400: loss = 4.264581203460693\n",
      "step = 4558600: loss = 3.825040578842163\n",
      "step = 4558800: loss = 3.10878586769104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4559000: loss = 4.557003021240234\n",
      "step = 4559200: loss = 3.6673381328582764\n",
      "step = 4559400: loss = 4.015908241271973\n",
      "step = 4559600: loss = 3.409987449645996\n",
      "step = 4559800: loss = 4.395645618438721\n",
      "step = 4560000: loss = 3.927002429962158\n",
      "step = 4560000: Average Return = 4.449999809265137\n",
      "step = 4560200: loss = 4.24910306930542\n",
      "step = 4560400: loss = 4.821533203125\n",
      "step = 4560600: loss = 3.345221757888794\n",
      "step = 4560800: loss = 3.7663962841033936\n",
      "step = 4561000: loss = 4.06547212600708\n",
      "step = 4561200: loss = 4.479942798614502\n",
      "step = 4561400: loss = 4.144754886627197\n",
      "step = 4561600: loss = 3.2676327228546143\n",
      "step = 4561800: loss = 3.899078607559204\n",
      "step = 4562000: loss = 2.91433048248291\n",
      "step = 4562200: loss = 4.277568817138672\n",
      "step = 4562400: loss = 3.9649815559387207\n",
      "step = 4562600: loss = 3.5088489055633545\n",
      "step = 4562800: loss = 3.1820285320281982\n",
      "step = 4563000: loss = 3.521230459213257\n",
      "step = 4563200: loss = 3.434105157852173\n",
      "step = 4563400: loss = 3.312351942062378\n",
      "step = 4563600: loss = 5.567767143249512\n",
      "step = 4563800: loss = 3.6930088996887207\n",
      "step = 4564000: loss = 4.8529839515686035\n",
      "step = 4564200: loss = 2.9985146522521973\n",
      "step = 4564400: loss = 2.9516868591308594\n",
      "step = 4564600: loss = 3.636603355407715\n",
      "step = 4564800: loss = 4.308893203735352\n",
      "step = 4565000: loss = 3.685283660888672\n",
      "step = 4565000: Average Return = 3.299999952316284\n",
      "step = 4565200: loss = 3.989872932434082\n",
      "step = 4565400: loss = 3.8567473888397217\n",
      "step = 4565600: loss = 3.191580057144165\n",
      "step = 4565800: loss = 3.340728759765625\n",
      "step = 4566000: loss = 3.5403027534484863\n",
      "step = 4566200: loss = 3.7063958644866943\n",
      "step = 4566400: loss = 3.390833616256714\n",
      "step = 4566600: loss = 2.657219886779785\n",
      "step = 4566800: loss = 4.0419721603393555\n",
      "step = 4567000: loss = 2.328927755355835\n",
      "step = 4567200: loss = 3.7492454051971436\n",
      "step = 4567400: loss = 3.954087972640991\n",
      "step = 4567600: loss = 3.753420352935791\n",
      "step = 4567800: loss = 3.525768995285034\n",
      "step = 4568000: loss = 2.9811854362487793\n",
      "step = 4568200: loss = 4.371750831604004\n",
      "step = 4568400: loss = 3.9314351081848145\n",
      "step = 4568600: loss = 3.531813383102417\n",
      "step = 4568800: loss = 3.9395055770874023\n",
      "step = 4569000: loss = 2.570467472076416\n",
      "step = 4569200: loss = 4.090730667114258\n",
      "step = 4569400: loss = 3.3404815196990967\n",
      "step = 4569600: loss = 3.1863021850585938\n",
      "step = 4569800: loss = 4.201156139373779\n",
      "step = 4570000: loss = 4.635750770568848\n",
      "step = 4570000: Average Return = 4.449999809265137\n",
      "step = 4570200: loss = 3.094578981399536\n",
      "step = 4570400: loss = 3.037454128265381\n",
      "step = 4570600: loss = 3.578096866607666\n",
      "step = 4570800: loss = 4.605205535888672\n",
      "step = 4571000: loss = 3.9056835174560547\n",
      "step = 4571200: loss = 4.953373908996582\n",
      "step = 4571400: loss = 4.601210594177246\n",
      "step = 4571600: loss = 4.530066967010498\n",
      "step = 4571800: loss = 3.8366000652313232\n",
      "step = 4572000: loss = 3.372203826904297\n",
      "step = 4572200: loss = 5.782095909118652\n",
      "step = 4572400: loss = 4.9537272453308105\n",
      "step = 4572600: loss = 2.4092650413513184\n",
      "step = 4572800: loss = 3.532313108444214\n",
      "step = 4573000: loss = 4.778227806091309\n",
      "step = 4573200: loss = 5.125482082366943\n",
      "step = 4573400: loss = 5.5717620849609375\n",
      "step = 4573600: loss = 1.6598953008651733\n",
      "step = 4573800: loss = 2.8073885440826416\n",
      "step = 4574000: loss = 4.309626579284668\n",
      "step = 4574200: loss = 3.6224663257598877\n",
      "step = 4574400: loss = 3.874262571334839\n",
      "step = 4574600: loss = 4.063845634460449\n",
      "step = 4574800: loss = 4.451941013336182\n",
      "step = 4575000: loss = 3.9716625213623047\n",
      "step = 4575000: Average Return = 2.8499999046325684\n",
      "step = 4575200: loss = 5.568045616149902\n",
      "step = 4575400: loss = 4.869572162628174\n",
      "step = 4575600: loss = 3.3452553749084473\n",
      "step = 4575800: loss = 4.318043231964111\n",
      "step = 4576000: loss = 2.4888875484466553\n",
      "step = 4576200: loss = 3.2569639682769775\n",
      "step = 4576400: loss = 3.0242998600006104\n",
      "step = 4576600: loss = 2.8808414936065674\n",
      "step = 4576800: loss = 3.9900290966033936\n",
      "step = 4577000: loss = 3.6410398483276367\n",
      "step = 4577200: loss = 5.223105430603027\n",
      "step = 4577400: loss = 3.7730560302734375\n",
      "step = 4577600: loss = 4.164935111999512\n",
      "step = 4577800: loss = 2.5575194358825684\n",
      "step = 4578000: loss = 3.860038995742798\n",
      "step = 4578200: loss = 4.786371231079102\n",
      "step = 4578400: loss = 3.860131025314331\n",
      "step = 4578600: loss = 3.687784433364868\n",
      "step = 4578800: loss = 4.739100456237793\n",
      "step = 4579000: loss = 6.016409397125244\n",
      "step = 4579200: loss = 5.684963226318359\n",
      "step = 4579400: loss = 3.4473814964294434\n",
      "step = 4579600: loss = 3.2241063117980957\n",
      "step = 4579800: loss = 3.4578568935394287\n",
      "step = 4580000: loss = 2.7679638862609863\n",
      "step = 4580000: Average Return = 1.0499999523162842\n",
      "step = 4580200: loss = 3.2818267345428467\n",
      "step = 4580400: loss = 4.077037811279297\n",
      "step = 4580600: loss = 5.007486343383789\n",
      "step = 4580800: loss = 4.061131477355957\n",
      "step = 4581000: loss = 4.264369487762451\n",
      "step = 4581200: loss = 5.168244361877441\n",
      "step = 4581400: loss = 3.0371713638305664\n",
      "step = 4581600: loss = 3.363629102706909\n",
      "step = 4581800: loss = 3.000340223312378\n",
      "step = 4582000: loss = 2.8234899044036865\n",
      "step = 4582200: loss = 3.3896946907043457\n",
      "step = 4582400: loss = 3.568981885910034\n",
      "step = 4582600: loss = 4.407954216003418\n",
      "step = 4582800: loss = 3.683495283126831\n",
      "step = 4583000: loss = 3.9364113807678223\n",
      "step = 4583200: loss = 4.76840353012085\n",
      "step = 4583400: loss = 3.331231117248535\n",
      "step = 4583600: loss = 3.5279557704925537\n",
      "step = 4583800: loss = 3.7718517780303955\n",
      "step = 4584000: loss = 3.900029182434082\n",
      "step = 4584200: loss = 3.0542209148406982\n",
      "step = 4584400: loss = 4.711030960083008\n",
      "step = 4584600: loss = 4.372424602508545\n",
      "step = 4584800: loss = 3.8294851779937744\n",
      "step = 4585000: loss = 4.478569984436035\n",
      "step = 4585000: Average Return = 3.700000047683716\n",
      "step = 4585200: loss = 3.871894359588623\n",
      "step = 4585400: loss = 3.101787567138672\n",
      "step = 4585600: loss = 4.1324920654296875\n",
      "step = 4585800: loss = 3.9332072734832764\n",
      "step = 4586000: loss = 3.9645159244537354\n",
      "step = 4586200: loss = 3.429004430770874\n",
      "step = 4586400: loss = 4.595090866088867\n",
      "step = 4586600: loss = 4.40104341506958\n",
      "step = 4586800: loss = 3.681209087371826\n",
      "step = 4587000: loss = 4.224355220794678\n",
      "step = 4587200: loss = 3.398648500442505\n",
      "step = 4587400: loss = 4.243978023529053\n",
      "step = 4587600: loss = 4.174666881561279\n",
      "step = 4587800: loss = 3.8875608444213867\n",
      "step = 4588000: loss = 5.147769451141357\n",
      "step = 4588200: loss = 4.433570861816406\n",
      "step = 4588400: loss = 3.554999828338623\n",
      "step = 4588600: loss = 5.0749030113220215\n",
      "step = 4588800: loss = 2.725381374359131\n",
      "step = 4589000: loss = 2.6493284702301025\n",
      "step = 4589200: loss = 4.139725685119629\n",
      "step = 4589400: loss = 4.471097946166992\n",
      "step = 4589600: loss = 3.3891172409057617\n",
      "step = 4589800: loss = 4.733667850494385\n",
      "step = 4590000: loss = 3.1352548599243164\n",
      "step = 4590000: Average Return = 2.799999952316284\n",
      "step = 4590200: loss = 2.6811888217926025\n",
      "step = 4590400: loss = 3.368534564971924\n",
      "step = 4590600: loss = 3.1613118648529053\n",
      "step = 4590800: loss = 3.503398895263672\n",
      "step = 4591000: loss = 4.536880016326904\n",
      "step = 4591200: loss = 4.577851295471191\n",
      "step = 4591400: loss = 5.146129131317139\n",
      "step = 4591600: loss = 3.589670419692993\n",
      "step = 4591800: loss = 4.224337577819824\n",
      "step = 4592000: loss = 3.767218589782715\n",
      "step = 4592200: loss = 3.3257877826690674\n",
      "step = 4592400: loss = 3.324967622756958\n",
      "step = 4592600: loss = 4.109118938446045\n",
      "step = 4592800: loss = 3.645869731903076\n",
      "step = 4593000: loss = 2.871624708175659\n",
      "step = 4593200: loss = 4.615146160125732\n",
      "step = 4593400: loss = 4.784797668457031\n",
      "step = 4593600: loss = 4.145142078399658\n",
      "step = 4593800: loss = 3.2607455253601074\n",
      "step = 4594000: loss = 3.6125199794769287\n",
      "step = 4594200: loss = 3.577458381652832\n",
      "step = 4594400: loss = 3.5498757362365723\n",
      "step = 4594600: loss = 3.9762914180755615\n",
      "step = 4594800: loss = 3.082374095916748\n",
      "step = 4595000: loss = 3.5738742351531982\n",
      "step = 4595000: Average Return = 3.299999952316284\n",
      "step = 4595200: loss = 3.5309228897094727\n",
      "step = 4595400: loss = 3.382377862930298\n",
      "step = 4595600: loss = 2.82131290435791\n",
      "step = 4595800: loss = 5.035003662109375\n",
      "step = 4596000: loss = 4.031328201293945\n",
      "step = 4596200: loss = 3.416550636291504\n",
      "step = 4596400: loss = 3.356104612350464\n",
      "step = 4596600: loss = 3.9103734493255615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4596800: loss = 4.686978816986084\n",
      "step = 4597000: loss = 3.337388753890991\n",
      "step = 4597200: loss = 3.2851154804229736\n",
      "step = 4597400: loss = 6.619265079498291\n",
      "step = 4597600: loss = 4.285264015197754\n",
      "step = 4597800: loss = 5.118706703186035\n",
      "step = 4598000: loss = 3.747889995574951\n",
      "step = 4598200: loss = 5.47901725769043\n",
      "step = 4598400: loss = 3.883924722671509\n",
      "step = 4598600: loss = 4.796302795410156\n",
      "step = 4598800: loss = 2.6924726963043213\n",
      "step = 4599000: loss = 5.339993000030518\n",
      "step = 4599200: loss = 4.171119689941406\n",
      "step = 4599400: loss = 2.8669450283050537\n",
      "step = 4599600: loss = 4.3944993019104\n",
      "step = 4599800: loss = 3.8178515434265137\n",
      "step = 4600000: loss = 4.7104597091674805\n",
      "step = 4600000: Average Return = 5.099999904632568\n",
      "step = 4600200: loss = 3.621159553527832\n",
      "step = 4600400: loss = 3.125614643096924\n",
      "step = 4600600: loss = 2.9567418098449707\n",
      "step = 4600800: loss = 4.523608684539795\n",
      "step = 4601000: loss = 3.039738416671753\n",
      "step = 4601200: loss = 3.0338451862335205\n",
      "step = 4601400: loss = 4.482162952423096\n",
      "step = 4601600: loss = 3.7797954082489014\n",
      "step = 4601800: loss = 4.091413974761963\n",
      "step = 4602000: loss = 3.379744291305542\n",
      "step = 4602200: loss = 5.506880760192871\n",
      "step = 4602400: loss = 4.100314617156982\n",
      "step = 4602600: loss = 3.554908275604248\n",
      "step = 4602800: loss = 3.8262274265289307\n",
      "step = 4603000: loss = 4.303175449371338\n",
      "step = 4603200: loss = 3.527238368988037\n",
      "step = 4603400: loss = 4.397861003875732\n",
      "step = 4603600: loss = 3.5531811714172363\n",
      "step = 4603800: loss = 3.559035062789917\n",
      "step = 4604000: loss = 3.616818428039551\n",
      "step = 4604200: loss = 3.9150702953338623\n",
      "step = 4604400: loss = 4.0476837158203125\n",
      "step = 4604600: loss = 3.997882604598999\n",
      "step = 4604800: loss = 3.1972312927246094\n",
      "step = 4605000: loss = 3.910104274749756\n",
      "step = 4605000: Average Return = 3.049999952316284\n",
      "step = 4605200: loss = 4.218297481536865\n",
      "step = 4605400: loss = 2.30255126953125\n",
      "step = 4605600: loss = 3.9705042839050293\n",
      "step = 4605800: loss = 4.001763820648193\n",
      "step = 4606000: loss = 3.472440719604492\n",
      "step = 4606200: loss = 4.268154144287109\n",
      "step = 4606400: loss = 3.9748692512512207\n",
      "step = 4606600: loss = 5.4668097496032715\n",
      "step = 4606800: loss = 2.601313829421997\n",
      "step = 4607000: loss = 3.538234233856201\n",
      "step = 4607200: loss = 2.194596529006958\n",
      "step = 4607400: loss = 4.298442363739014\n",
      "step = 4607600: loss = 3.9959285259246826\n",
      "step = 4607800: loss = 3.147782802581787\n",
      "step = 4608000: loss = 3.735842227935791\n",
      "step = 4608200: loss = 4.782034397125244\n",
      "step = 4608400: loss = 3.141662120819092\n",
      "step = 4608600: loss = 4.05251932144165\n",
      "step = 4608800: loss = 3.632510185241699\n",
      "step = 4609000: loss = 3.5144877433776855\n",
      "step = 4609200: loss = 3.7978460788726807\n",
      "step = 4609400: loss = 5.699412822723389\n",
      "step = 4609600: loss = 2.9223742485046387\n",
      "step = 4609800: loss = 3.100062847137451\n",
      "step = 4610000: loss = 6.023311138153076\n",
      "step = 4610000: Average Return = 1.7000000476837158\n",
      "step = 4610200: loss = 3.622870683670044\n",
      "step = 4610400: loss = 4.068265914916992\n",
      "step = 4610600: loss = 4.982579231262207\n",
      "step = 4610800: loss = 3.805251121520996\n",
      "step = 4611000: loss = 2.492598533630371\n",
      "step = 4611200: loss = 2.9559953212738037\n",
      "step = 4611400: loss = 4.665531635284424\n",
      "step = 4611600: loss = 2.949650526046753\n",
      "step = 4611800: loss = 3.7594945430755615\n",
      "step = 4612000: loss = 3.6659154891967773\n",
      "step = 4612200: loss = 5.672327041625977\n",
      "step = 4612400: loss = 4.821750164031982\n",
      "step = 4612600: loss = 4.639852046966553\n",
      "step = 4612800: loss = 4.214997291564941\n",
      "step = 4613000: loss = 2.4387521743774414\n",
      "step = 4613200: loss = 3.661710023880005\n",
      "step = 4613400: loss = 3.823103666305542\n",
      "step = 4613600: loss = 2.7853264808654785\n",
      "step = 4613800: loss = 5.024428844451904\n",
      "step = 4614000: loss = 4.619251728057861\n",
      "step = 4614200: loss = 3.633852481842041\n",
      "step = 4614400: loss = 3.5822246074676514\n",
      "step = 4614600: loss = 3.5588858127593994\n",
      "step = 4614800: loss = 3.6820995807647705\n",
      "step = 4615000: loss = 3.5036492347717285\n",
      "step = 4615000: Average Return = 4.199999809265137\n",
      "step = 4615200: loss = 4.240772247314453\n",
      "step = 4615400: loss = 3.0678799152374268\n",
      "step = 4615600: loss = 3.9982597827911377\n",
      "step = 4615800: loss = 3.597550868988037\n",
      "step = 4616000: loss = 4.275265216827393\n",
      "step = 4616200: loss = 3.16300630569458\n",
      "step = 4616400: loss = 4.834354400634766\n",
      "step = 4616600: loss = 4.0078253746032715\n",
      "step = 4616800: loss = 4.07815408706665\n",
      "step = 4617000: loss = 4.019943714141846\n",
      "step = 4617200: loss = 5.174979209899902\n",
      "step = 4617400: loss = 4.563056468963623\n",
      "step = 4617600: loss = 4.001447677612305\n",
      "step = 4617800: loss = 4.181121826171875\n",
      "step = 4618000: loss = 3.700439929962158\n",
      "step = 4618200: loss = 3.3967981338500977\n",
      "step = 4618400: loss = 3.262272596359253\n",
      "step = 4618600: loss = 4.0017523765563965\n",
      "step = 4618800: loss = 4.016820430755615\n",
      "step = 4619000: loss = 3.976393938064575\n",
      "step = 4619200: loss = 4.88612174987793\n",
      "step = 4619400: loss = 4.497973442077637\n",
      "step = 4619600: loss = 3.4876675605773926\n",
      "step = 4619800: loss = 3.404508352279663\n",
      "step = 4620000: loss = 4.680549621582031\n",
      "step = 4620000: Average Return = 4.150000095367432\n",
      "step = 4620200: loss = 4.0373101234436035\n",
      "step = 4620400: loss = 3.2186059951782227\n",
      "step = 4620600: loss = 2.8535568714141846\n",
      "step = 4620800: loss = 4.922664165496826\n",
      "step = 4621000: loss = 3.490206241607666\n",
      "step = 4621200: loss = 3.640751838684082\n",
      "step = 4621400: loss = 4.722708702087402\n",
      "step = 4621600: loss = 4.511383056640625\n",
      "step = 4621800: loss = 3.3513295650482178\n",
      "step = 4622000: loss = 2.9135115146636963\n",
      "step = 4622200: loss = 3.6601433753967285\n",
      "step = 4622400: loss = 2.791766881942749\n",
      "step = 4622600: loss = 3.7795159816741943\n",
      "step = 4622800: loss = 3.581765651702881\n",
      "step = 4623000: loss = 3.0517613887786865\n",
      "step = 4623200: loss = 5.323960304260254\n",
      "step = 4623400: loss = 4.281314373016357\n",
      "step = 4623600: loss = 2.6107726097106934\n",
      "step = 4623800: loss = 3.791999101638794\n",
      "step = 4624000: loss = 3.9197566509246826\n",
      "step = 4624200: loss = 3.471909761428833\n",
      "step = 4624400: loss = 3.494062662124634\n",
      "step = 4624600: loss = 4.978480815887451\n",
      "step = 4624800: loss = 2.4520742893218994\n",
      "step = 4625000: loss = 4.091231822967529\n",
      "step = 4625000: Average Return = 3.5999999046325684\n",
      "step = 4625200: loss = 3.155974864959717\n",
      "step = 4625400: loss = 3.245607852935791\n",
      "step = 4625600: loss = 5.47475528717041\n",
      "step = 4625800: loss = 4.26469612121582\n",
      "step = 4626000: loss = 4.7044501304626465\n",
      "step = 4626200: loss = 4.051267147064209\n",
      "step = 4626400: loss = 3.4940099716186523\n",
      "step = 4626600: loss = 4.931896209716797\n",
      "step = 4626800: loss = 3.427821636199951\n",
      "step = 4627000: loss = 4.660885810852051\n",
      "step = 4627200: loss = 3.8646299839019775\n",
      "step = 4627400: loss = 3.9158732891082764\n",
      "step = 4627600: loss = 4.027981281280518\n",
      "step = 4627800: loss = 2.996995210647583\n",
      "step = 4628000: loss = 4.27692985534668\n",
      "step = 4628200: loss = 4.649661064147949\n",
      "step = 4628400: loss = 3.6685948371887207\n",
      "step = 4628600: loss = 3.2952959537506104\n",
      "step = 4628800: loss = 3.3511874675750732\n",
      "step = 4629000: loss = 3.7991130352020264\n",
      "step = 4629200: loss = 3.59456205368042\n",
      "step = 4629400: loss = 2.8941001892089844\n",
      "step = 4629600: loss = 4.576833248138428\n",
      "step = 4629800: loss = 4.166917324066162\n",
      "step = 4630000: loss = 3.4686388969421387\n",
      "step = 4630000: Average Return = 1.600000023841858\n",
      "step = 4630200: loss = 2.948878049850464\n",
      "step = 4630400: loss = 4.061277866363525\n",
      "step = 4630600: loss = 3.717261791229248\n",
      "step = 4630800: loss = 3.849349021911621\n",
      "step = 4631000: loss = 3.130096435546875\n",
      "step = 4631200: loss = 3.9897143840789795\n",
      "step = 4631400: loss = 3.6248278617858887\n",
      "step = 4631600: loss = 3.057123899459839\n",
      "step = 4631800: loss = 3.551877975463867\n",
      "step = 4632000: loss = 3.602484703063965\n",
      "step = 4632200: loss = 4.280038833618164\n",
      "step = 4632400: loss = 4.366298675537109\n",
      "step = 4632600: loss = 2.9949018955230713\n",
      "step = 4632800: loss = 4.114660739898682\n",
      "step = 4633000: loss = 4.651166915893555\n",
      "step = 4633200: loss = 4.643360614776611\n",
      "step = 4633400: loss = 3.245570659637451\n",
      "step = 4633600: loss = 3.8475747108459473\n",
      "step = 4633800: loss = 3.3297929763793945\n",
      "step = 4634000: loss = 3.956976890563965\n",
      "step = 4634200: loss = 4.63902473449707\n",
      "step = 4634400: loss = 2.855410099029541\n",
      "step = 4634600: loss = 3.4702579975128174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4634800: loss = 3.6912412643432617\n",
      "step = 4635000: loss = 4.034797191619873\n",
      "step = 4635000: Average Return = 4.300000190734863\n",
      "step = 4635200: loss = 4.2365264892578125\n",
      "step = 4635400: loss = 4.173499584197998\n",
      "step = 4635600: loss = 2.279285192489624\n",
      "step = 4635800: loss = 4.549657344818115\n",
      "step = 4636000: loss = 3.3948452472686768\n",
      "step = 4636200: loss = 3.84721302986145\n",
      "step = 4636400: loss = 3.11122465133667\n",
      "step = 4636600: loss = 4.587517738342285\n",
      "step = 4636800: loss = 2.088063955307007\n",
      "step = 4637000: loss = 6.440065860748291\n",
      "step = 4637200: loss = 2.768867015838623\n",
      "step = 4637400: loss = 4.694456100463867\n",
      "step = 4637600: loss = 4.180306911468506\n",
      "step = 4637800: loss = 3.0577948093414307\n",
      "step = 4638000: loss = 3.5662670135498047\n",
      "step = 4638200: loss = 2.8412044048309326\n",
      "step = 4638400: loss = 4.13055419921875\n",
      "step = 4638600: loss = 3.3394365310668945\n",
      "step = 4638800: loss = 4.571659564971924\n",
      "step = 4639000: loss = 4.252264499664307\n",
      "step = 4639200: loss = 3.6152262687683105\n",
      "step = 4639400: loss = 3.15187406539917\n",
      "step = 4639600: loss = 3.526425361633301\n",
      "step = 4639800: loss = 3.483231782913208\n",
      "step = 4640000: loss = 3.865283489227295\n",
      "step = 4640000: Average Return = 0.8500000238418579\n",
      "step = 4640200: loss = 4.536926746368408\n",
      "step = 4640400: loss = 4.367855548858643\n",
      "step = 4640600: loss = 5.01833963394165\n",
      "step = 4640800: loss = 3.4696288108825684\n",
      "step = 4641000: loss = 3.839064836502075\n",
      "step = 4641200: loss = 3.738767623901367\n",
      "step = 4641400: loss = 4.538693904876709\n",
      "step = 4641600: loss = 4.0531744956970215\n",
      "step = 4641800: loss = 4.321609973907471\n",
      "step = 4642000: loss = 4.600722789764404\n",
      "step = 4642200: loss = 3.599368095397949\n",
      "step = 4642400: loss = 3.1666927337646484\n",
      "step = 4642600: loss = 4.928625583648682\n",
      "step = 4642800: loss = 2.5711352825164795\n",
      "step = 4643000: loss = 3.1245341300964355\n",
      "step = 4643200: loss = 3.8092503547668457\n",
      "step = 4643400: loss = 3.4768242835998535\n",
      "step = 4643600: loss = 3.8725924491882324\n",
      "step = 4643800: loss = 3.7754087448120117\n",
      "step = 4644000: loss = 2.9550273418426514\n",
      "step = 4644200: loss = 4.0208821296691895\n",
      "step = 4644400: loss = 3.7776477336883545\n",
      "step = 4644600: loss = 4.232144832611084\n",
      "step = 4644800: loss = 3.7787773609161377\n",
      "step = 4645000: loss = 3.2877800464630127\n",
      "step = 4645000: Average Return = 4.349999904632568\n",
      "step = 4645200: loss = 5.127971172332764\n",
      "step = 4645400: loss = 3.820754289627075\n",
      "step = 4645600: loss = 4.4987711906433105\n",
      "step = 4645800: loss = 3.9345197677612305\n",
      "step = 4646000: loss = 3.2845447063446045\n",
      "step = 4646200: loss = 4.052962779998779\n",
      "step = 4646400: loss = 3.817413330078125\n",
      "step = 4646600: loss = 4.39349365234375\n",
      "step = 4646800: loss = 4.365978717803955\n",
      "step = 4647000: loss = 2.604767322540283\n",
      "step = 4647200: loss = 4.33610725402832\n",
      "step = 4647400: loss = 3.8905863761901855\n",
      "step = 4647600: loss = 3.5657365322113037\n",
      "step = 4647800: loss = 3.5021517276763916\n",
      "step = 4648000: loss = 3.862241744995117\n",
      "step = 4648200: loss = 3.8802523612976074\n",
      "step = 4648400: loss = 3.642011880874634\n",
      "step = 4648600: loss = 3.9846861362457275\n",
      "step = 4648800: loss = 2.939725637435913\n",
      "step = 4649000: loss = 3.758138656616211\n",
      "step = 4649200: loss = 4.457082748413086\n",
      "step = 4649400: loss = 3.804652690887451\n",
      "step = 4649600: loss = 4.993832588195801\n",
      "step = 4649800: loss = 3.370573043823242\n",
      "step = 4650000: loss = 4.111818313598633\n",
      "step = 4650000: Average Return = 4.5\n",
      "step = 4650200: loss = 3.2745895385742188\n",
      "step = 4650400: loss = 4.333906173706055\n",
      "step = 4650600: loss = 3.903270721435547\n",
      "step = 4650800: loss = 3.48189640045166\n",
      "step = 4651000: loss = 4.028639316558838\n",
      "step = 4651200: loss = 3.5865800380706787\n",
      "step = 4651400: loss = 5.805682182312012\n",
      "step = 4651600: loss = 3.530346632003784\n",
      "step = 4651800: loss = 4.606743812561035\n",
      "step = 4652000: loss = 4.548299312591553\n",
      "step = 4652200: loss = 3.7198143005371094\n",
      "step = 4652400: loss = 3.4784512519836426\n",
      "step = 4652600: loss = 3.3419647216796875\n",
      "step = 4652800: loss = 4.137263774871826\n",
      "step = 4653000: loss = 3.173774242401123\n",
      "step = 4653200: loss = 3.4464879035949707\n",
      "step = 4653400: loss = 2.9725308418273926\n",
      "step = 4653600: loss = 3.7322564125061035\n",
      "step = 4653800: loss = 3.9555652141571045\n",
      "step = 4654000: loss = 2.260392665863037\n",
      "step = 4654200: loss = 3.030696392059326\n",
      "step = 4654400: loss = 3.688737630844116\n",
      "step = 4654600: loss = 4.797218322753906\n",
      "step = 4654800: loss = 3.6789934635162354\n",
      "step = 4655000: loss = 4.09415340423584\n",
      "step = 4655000: Average Return = 2.200000047683716\n",
      "step = 4655200: loss = 4.006444454193115\n",
      "step = 4655400: loss = 5.081847190856934\n",
      "step = 4655600: loss = 3.0227389335632324\n",
      "step = 4655800: loss = 3.688699245452881\n",
      "step = 4656000: loss = 2.86081600189209\n",
      "step = 4656200: loss = 4.19069766998291\n",
      "step = 4656400: loss = 4.874108791351318\n",
      "step = 4656600: loss = 4.4018402099609375\n",
      "step = 4656800: loss = 3.3580992221832275\n",
      "step = 4657000: loss = 3.954158306121826\n",
      "step = 4657200: loss = 3.466466188430786\n",
      "step = 4657400: loss = 3.664710521697998\n",
      "step = 4657600: loss = 3.8653981685638428\n",
      "step = 4657800: loss = 4.092742919921875\n",
      "step = 4658000: loss = 4.702193260192871\n",
      "step = 4658200: loss = 4.124022960662842\n",
      "step = 4658400: loss = 5.104884147644043\n",
      "step = 4658600: loss = 3.6914684772491455\n",
      "step = 4658800: loss = 4.708429336547852\n",
      "step = 4659000: loss = 4.030057907104492\n",
      "step = 4659200: loss = 5.15778112411499\n",
      "step = 4659400: loss = 3.3611700534820557\n",
      "step = 4659600: loss = 3.237441301345825\n",
      "step = 4659800: loss = 5.2060546875\n",
      "step = 4660000: loss = 3.4838688373565674\n",
      "step = 4660000: Average Return = 3.5\n",
      "step = 4660200: loss = 4.505654811859131\n",
      "step = 4660400: loss = 3.658090114593506\n",
      "step = 4660600: loss = 3.4090418815612793\n",
      "step = 4660800: loss = 3.09273624420166\n",
      "step = 4661000: loss = 4.394832134246826\n",
      "step = 4661200: loss = 3.18521785736084\n",
      "step = 4661400: loss = 3.2127645015716553\n",
      "step = 4661600: loss = 2.757445812225342\n",
      "step = 4661800: loss = 2.753150463104248\n",
      "step = 4662000: loss = 4.724681377410889\n",
      "step = 4662200: loss = 2.798008441925049\n",
      "step = 4662400: loss = 2.888376474380493\n",
      "step = 4662600: loss = 3.433605909347534\n",
      "step = 4662800: loss = 3.1030802726745605\n",
      "step = 4663000: loss = 4.6148505210876465\n",
      "step = 4663200: loss = 2.701420545578003\n",
      "step = 4663400: loss = 3.916529417037964\n",
      "step = 4663600: loss = 4.249755859375\n",
      "step = 4663800: loss = 3.39437198638916\n",
      "step = 4664000: loss = 2.6356935501098633\n",
      "step = 4664200: loss = 4.521026611328125\n",
      "step = 4664400: loss = 3.0589470863342285\n",
      "step = 4664600: loss = 2.9613184928894043\n",
      "step = 4664800: loss = 4.215744495391846\n",
      "step = 4665000: loss = 4.610177516937256\n",
      "step = 4665000: Average Return = 5.199999809265137\n",
      "step = 4665200: loss = 4.503018856048584\n",
      "step = 4665400: loss = 4.262362957000732\n",
      "step = 4665600: loss = 3.0598864555358887\n",
      "step = 4665800: loss = 3.772048234939575\n",
      "step = 4666000: loss = 3.545459508895874\n",
      "step = 4666200: loss = 2.9748833179473877\n",
      "step = 4666400: loss = 3.4575915336608887\n",
      "step = 4666600: loss = 4.317006587982178\n",
      "step = 4666800: loss = 3.2098445892333984\n",
      "step = 4667000: loss = 5.6092209815979\n",
      "step = 4667200: loss = 4.124690055847168\n",
      "step = 4667400: loss = 4.505136489868164\n",
      "step = 4667600: loss = 3.4368574619293213\n",
      "step = 4667800: loss = 3.4796698093414307\n",
      "step = 4668000: loss = 4.392988204956055\n",
      "step = 4668200: loss = 3.712435007095337\n",
      "step = 4668400: loss = 4.882164478302002\n",
      "step = 4668600: loss = 2.830153703689575\n",
      "step = 4668800: loss = 3.794905662536621\n",
      "step = 4669000: loss = 4.4897332191467285\n",
      "step = 4669200: loss = 4.740725994110107\n",
      "step = 4669400: loss = 5.458159446716309\n",
      "step = 4669600: loss = 4.012903213500977\n",
      "step = 4669800: loss = 3.0443012714385986\n",
      "step = 4670000: loss = 5.262939929962158\n",
      "step = 4670000: Average Return = 4.25\n",
      "step = 4670200: loss = 3.991239547729492\n",
      "step = 4670400: loss = 3.4233763217926025\n",
      "step = 4670600: loss = 2.8902995586395264\n",
      "step = 4670800: loss = 3.824049711227417\n",
      "step = 4671000: loss = 3.2814903259277344\n",
      "step = 4671200: loss = 4.248304843902588\n",
      "step = 4671400: loss = 4.032895565032959\n",
      "step = 4671600: loss = 3.234161615371704\n",
      "step = 4671800: loss = 2.471914052963257\n",
      "step = 4672000: loss = 4.477855205535889\n",
      "step = 4672200: loss = 3.6518261432647705\n",
      "step = 4672400: loss = 3.1975433826446533\n",
      "step = 4672600: loss = 3.780707597732544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4672800: loss = 5.065408706665039\n",
      "step = 4673000: loss = 3.4870824813842773\n",
      "step = 4673200: loss = 4.2001261711120605\n",
      "step = 4673400: loss = 4.64468240737915\n",
      "step = 4673600: loss = 4.9491143226623535\n",
      "step = 4673800: loss = 6.189133167266846\n",
      "step = 4674000: loss = 3.9966514110565186\n",
      "step = 4674200: loss = 2.2661142349243164\n",
      "step = 4674400: loss = 4.524441719055176\n",
      "step = 4674600: loss = 3.2066774368286133\n",
      "step = 4674800: loss = 3.2085046768188477\n",
      "step = 4675000: loss = 3.522585391998291\n",
      "step = 4675000: Average Return = 3.1500000953674316\n",
      "step = 4675200: loss = 4.460026264190674\n",
      "step = 4675400: loss = 3.705869674682617\n",
      "step = 4675600: loss = 3.777378559112549\n",
      "step = 4675800: loss = 3.874614715576172\n",
      "step = 4676000: loss = 3.1645121574401855\n",
      "step = 4676200: loss = 3.1185739040374756\n",
      "step = 4676400: loss = 3.523509979248047\n",
      "step = 4676600: loss = 4.333714962005615\n",
      "step = 4676800: loss = 3.852480411529541\n",
      "step = 4677000: loss = 3.8320164680480957\n",
      "step = 4677200: loss = 3.9903793334960938\n",
      "step = 4677400: loss = 3.264681339263916\n",
      "step = 4677600: loss = 4.141529560089111\n",
      "step = 4677800: loss = 4.2469611167907715\n",
      "step = 4678000: loss = 3.92615008354187\n",
      "step = 4678200: loss = 3.0173425674438477\n",
      "step = 4678400: loss = 3.9413821697235107\n",
      "step = 4678600: loss = 2.6579198837280273\n",
      "step = 4678800: loss = 3.236903667449951\n",
      "step = 4679000: loss = 3.186821937561035\n",
      "step = 4679200: loss = 4.345017910003662\n",
      "step = 4679400: loss = 4.68743896484375\n",
      "step = 4679600: loss = 3.657311201095581\n",
      "step = 4679800: loss = 4.117432594299316\n",
      "step = 4680000: loss = 3.299960136413574\n",
      "step = 4680000: Average Return = 3.950000047683716\n",
      "step = 4680200: loss = 3.3217101097106934\n",
      "step = 4680400: loss = 4.607576370239258\n",
      "step = 4680600: loss = 3.063504934310913\n",
      "step = 4680800: loss = 4.96022891998291\n",
      "step = 4681000: loss = 3.252270460128784\n",
      "step = 4681200: loss = 5.183890342712402\n",
      "step = 4681400: loss = 3.9165139198303223\n",
      "step = 4681600: loss = 3.268747568130493\n",
      "step = 4681800: loss = 4.556851863861084\n",
      "step = 4682000: loss = 2.8247311115264893\n",
      "step = 4682200: loss = 3.6792051792144775\n",
      "step = 4682400: loss = 3.9875171184539795\n",
      "step = 4682600: loss = 4.202254772186279\n",
      "step = 4682800: loss = 5.660830974578857\n",
      "step = 4683000: loss = 3.604681968688965\n",
      "step = 4683200: loss = 3.947084903717041\n",
      "step = 4683400: loss = 3.706875801086426\n",
      "step = 4683600: loss = 4.047036647796631\n",
      "step = 4683800: loss = 4.318476676940918\n",
      "step = 4684000: loss = 5.991607666015625\n",
      "step = 4684200: loss = 3.332897663116455\n",
      "step = 4684400: loss = 2.6058268547058105\n",
      "step = 4684600: loss = 3.441309690475464\n",
      "step = 4684800: loss = 3.447646141052246\n",
      "step = 4685000: loss = 3.902209758758545\n",
      "step = 4685000: Average Return = 2.950000047683716\n",
      "step = 4685200: loss = 3.7001922130584717\n",
      "step = 4685400: loss = 3.801532506942749\n",
      "step = 4685600: loss = 4.68509578704834\n",
      "step = 4685800: loss = 2.856302499771118\n",
      "step = 4686000: loss = 4.466500759124756\n",
      "step = 4686200: loss = 3.995976686477661\n",
      "step = 4686400: loss = 4.030331134796143\n",
      "step = 4686600: loss = 4.0704569816589355\n",
      "step = 4686800: loss = 2.6825156211853027\n",
      "step = 4687000: loss = 3.169665575027466\n",
      "step = 4687200: loss = 4.7295427322387695\n",
      "step = 4687400: loss = 3.214632987976074\n",
      "step = 4687600: loss = 2.4958763122558594\n",
      "step = 4687800: loss = 3.7006099224090576\n",
      "step = 4688000: loss = 3.771240472793579\n",
      "step = 4688200: loss = 3.5689592361450195\n",
      "step = 4688400: loss = 4.801431179046631\n",
      "step = 4688600: loss = 4.553047180175781\n",
      "step = 4688800: loss = 4.096529960632324\n",
      "step = 4689000: loss = 3.120704174041748\n",
      "step = 4689200: loss = 4.837730407714844\n",
      "step = 4689400: loss = 3.3674442768096924\n",
      "step = 4689600: loss = 3.147442579269409\n",
      "step = 4689800: loss = 4.061329364776611\n",
      "step = 4690000: loss = 3.474771738052368\n",
      "step = 4690000: Average Return = 1.4500000476837158\n",
      "step = 4690200: loss = 3.6964056491851807\n",
      "step = 4690400: loss = 3.2982029914855957\n",
      "step = 4690600: loss = 3.4046013355255127\n",
      "step = 4690800: loss = 3.582648515701294\n",
      "step = 4691000: loss = 4.238534450531006\n",
      "step = 4691200: loss = 3.468975067138672\n",
      "step = 4691400: loss = 4.385019302368164\n",
      "step = 4691600: loss = 4.864855766296387\n",
      "step = 4691800: loss = 4.573858737945557\n",
      "step = 4692000: loss = 4.852571487426758\n",
      "step = 4692200: loss = 4.604556083679199\n",
      "step = 4692400: loss = 4.126321315765381\n",
      "step = 4692600: loss = 3.6818127632141113\n",
      "step = 4692800: loss = 3.493966579437256\n",
      "step = 4693000: loss = 3.8113176822662354\n",
      "step = 4693200: loss = 3.4250950813293457\n",
      "step = 4693400: loss = 3.530266761779785\n",
      "step = 4693600: loss = 3.375206470489502\n",
      "step = 4693800: loss = 3.763104200363159\n",
      "step = 4694000: loss = 4.606491565704346\n",
      "step = 4694200: loss = 4.144447326660156\n",
      "step = 4694400: loss = 3.1577470302581787\n",
      "step = 4694600: loss = 3.2905566692352295\n",
      "step = 4694800: loss = 3.4433093070983887\n",
      "step = 4695000: loss = 3.753218650817871\n",
      "step = 4695000: Average Return = 0.8999999761581421\n",
      "step = 4695200: loss = 3.9234557151794434\n",
      "step = 4695400: loss = 3.568546772003174\n",
      "step = 4695600: loss = 3.8577215671539307\n",
      "step = 4695800: loss = 4.3487548828125\n",
      "step = 4696000: loss = 3.0220699310302734\n",
      "step = 4696200: loss = 4.030910968780518\n",
      "step = 4696400: loss = 4.759349346160889\n",
      "step = 4696600: loss = 3.368220806121826\n",
      "step = 4696800: loss = 3.661133289337158\n",
      "step = 4697000: loss = 3.7987492084503174\n",
      "step = 4697200: loss = 4.701690196990967\n",
      "step = 4697400: loss = 3.1209053993225098\n",
      "step = 4697600: loss = 4.351783752441406\n",
      "step = 4697800: loss = 4.276793003082275\n",
      "step = 4698000: loss = 3.1525402069091797\n",
      "step = 4698200: loss = 2.809250831604004\n",
      "step = 4698400: loss = 3.454228401184082\n",
      "step = 4698600: loss = 3.23001766204834\n",
      "step = 4698800: loss = 4.47390079498291\n",
      "step = 4699000: loss = 4.048715114593506\n",
      "step = 4699200: loss = 4.8122148513793945\n",
      "step = 4699400: loss = 3.749030351638794\n",
      "step = 4699600: loss = 3.310434103012085\n",
      "step = 4699800: loss = 3.668769121170044\n",
      "step = 4700000: loss = 4.473849773406982\n",
      "step = 4700000: Average Return = 5.449999809265137\n",
      "step = 4700200: loss = 3.4977104663848877\n",
      "step = 4700400: loss = 3.1577444076538086\n",
      "step = 4700600: loss = 4.315725326538086\n",
      "step = 4700800: loss = 3.22780442237854\n",
      "step = 4701000: loss = 3.9336400032043457\n",
      "step = 4701200: loss = 3.8777732849121094\n",
      "step = 4701400: loss = 3.3057548999786377\n",
      "step = 4701600: loss = 4.166568279266357\n",
      "step = 4701800: loss = 3.742250680923462\n",
      "step = 4702000: loss = 4.095417499542236\n",
      "step = 4702200: loss = 3.6736998558044434\n",
      "step = 4702400: loss = 3.2623138427734375\n",
      "step = 4702600: loss = 3.5934934616088867\n",
      "step = 4702800: loss = 4.042724609375\n",
      "step = 4703000: loss = 3.3687121868133545\n",
      "step = 4703200: loss = 4.263312339782715\n",
      "step = 4703400: loss = 3.7966740131378174\n",
      "step = 4703600: loss = 2.988452196121216\n",
      "step = 4703800: loss = 3.5378334522247314\n",
      "step = 4704000: loss = 3.718078851699829\n",
      "step = 4704200: loss = 4.98576021194458\n",
      "step = 4704400: loss = 2.883352518081665\n",
      "step = 4704600: loss = 4.483030319213867\n",
      "step = 4704800: loss = 3.325140953063965\n",
      "step = 4705000: loss = 3.826206922531128\n",
      "step = 4705000: Average Return = 3.8499999046325684\n",
      "step = 4705200: loss = 4.683172225952148\n",
      "step = 4705400: loss = 4.256720066070557\n",
      "step = 4705600: loss = 3.240462064743042\n",
      "step = 4705800: loss = 3.6292426586151123\n",
      "step = 4706000: loss = 2.784529447555542\n",
      "step = 4706200: loss = 4.8631744384765625\n",
      "step = 4706400: loss = 3.9055235385894775\n",
      "step = 4706600: loss = 4.545436382293701\n",
      "step = 4706800: loss = 2.847357749938965\n",
      "step = 4707000: loss = 3.979501247406006\n",
      "step = 4707200: loss = 2.9886114597320557\n",
      "step = 4707400: loss = 4.381031036376953\n",
      "step = 4707600: loss = 3.997906446456909\n",
      "step = 4707800: loss = 4.794328689575195\n",
      "step = 4708000: loss = 3.867929697036743\n",
      "step = 4708200: loss = 4.925887107849121\n",
      "step = 4708400: loss = 3.172497272491455\n",
      "step = 4708600: loss = 3.8955352306365967\n",
      "step = 4708800: loss = 3.3220036029815674\n",
      "step = 4709000: loss = 3.6544365882873535\n",
      "step = 4709200: loss = 3.941527843475342\n",
      "step = 4709400: loss = 3.4645018577575684\n",
      "step = 4709600: loss = 4.725348472595215\n",
      "step = 4709800: loss = 4.140050411224365\n",
      "step = 4710000: loss = 4.621735095977783\n",
      "step = 4710000: Average Return = 3.8499999046325684\n",
      "step = 4710200: loss = 3.4656498432159424\n",
      "step = 4710400: loss = 4.867409706115723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4710600: loss = 3.277186870574951\n",
      "step = 4710800: loss = 3.5307695865631104\n",
      "step = 4711000: loss = 3.9035484790802\n",
      "step = 4711200: loss = 2.367978811264038\n",
      "step = 4711400: loss = 3.4438836574554443\n",
      "step = 4711600: loss = 4.488869667053223\n",
      "step = 4711800: loss = 4.659480094909668\n",
      "step = 4712000: loss = 3.482985734939575\n",
      "step = 4712200: loss = 3.6590402126312256\n",
      "step = 4712400: loss = 4.365882396697998\n",
      "step = 4712600: loss = 4.130185604095459\n",
      "step = 4712800: loss = 4.20276403427124\n",
      "step = 4713000: loss = 4.344886302947998\n",
      "step = 4713200: loss = 4.302034378051758\n",
      "step = 4713400: loss = 3.7687597274780273\n",
      "step = 4713600: loss = 3.6816296577453613\n",
      "step = 4713800: loss = 3.3501787185668945\n",
      "step = 4714000: loss = 4.499279499053955\n",
      "step = 4714200: loss = 4.453542232513428\n",
      "step = 4714400: loss = 3.054994821548462\n",
      "step = 4714600: loss = 4.234983444213867\n",
      "step = 4714800: loss = 2.157273292541504\n",
      "step = 4715000: loss = 3.8884365558624268\n",
      "step = 4715000: Average Return = 3.0\n",
      "step = 4715200: loss = 3.2107865810394287\n",
      "step = 4715400: loss = 4.728870868682861\n",
      "step = 4715600: loss = 3.0154781341552734\n",
      "step = 4715800: loss = 2.9962117671966553\n",
      "step = 4716000: loss = 3.436401844024658\n",
      "step = 4716200: loss = 4.1053876876831055\n",
      "step = 4716400: loss = 3.0859832763671875\n",
      "step = 4716600: loss = 4.140104293823242\n",
      "step = 4716800: loss = 3.5572729110717773\n",
      "step = 4717000: loss = 5.324450492858887\n",
      "step = 4717200: loss = 3.9603734016418457\n",
      "step = 4717400: loss = 2.6118266582489014\n",
      "step = 4717600: loss = 4.425948619842529\n",
      "step = 4717800: loss = 3.3421130180358887\n",
      "step = 4718000: loss = 4.999205589294434\n",
      "step = 4718200: loss = 3.648160696029663\n",
      "step = 4718400: loss = 3.9510552883148193\n",
      "step = 4718600: loss = 4.096195697784424\n",
      "step = 4718800: loss = 3.1111602783203125\n",
      "step = 4719000: loss = 5.109118461608887\n",
      "step = 4719200: loss = 3.3700032234191895\n",
      "step = 4719400: loss = 3.5741350650787354\n",
      "step = 4719600: loss = 4.709656715393066\n",
      "step = 4719800: loss = 4.233203411102295\n",
      "step = 4720000: loss = 5.767469882965088\n",
      "step = 4720000: Average Return = 3.9000000953674316\n",
      "step = 4720200: loss = 2.949516534805298\n",
      "step = 4720400: loss = 4.468015193939209\n",
      "step = 4720600: loss = 3.684297800064087\n",
      "step = 4720800: loss = 3.71768856048584\n",
      "step = 4721000: loss = 3.73129940032959\n",
      "step = 4721200: loss = 2.6055655479431152\n",
      "step = 4721400: loss = 3.0893375873565674\n",
      "step = 4721600: loss = 4.403199195861816\n",
      "step = 4721800: loss = 2.5282022953033447\n",
      "step = 4722000: loss = 2.6297109127044678\n",
      "step = 4722200: loss = 5.170176982879639\n",
      "step = 4722400: loss = 4.00876522064209\n",
      "step = 4722600: loss = 2.803166389465332\n",
      "step = 4722800: loss = 3.319680690765381\n",
      "step = 4723000: loss = 3.2235302925109863\n",
      "step = 4723200: loss = 3.8903756141662598\n",
      "step = 4723400: loss = 3.184246778488159\n",
      "step = 4723600: loss = 3.39292049407959\n",
      "step = 4723800: loss = 3.8723857402801514\n",
      "step = 4724000: loss = 2.7874958515167236\n",
      "step = 4724200: loss = 3.1459808349609375\n",
      "step = 4724400: loss = 3.815654754638672\n",
      "step = 4724600: loss = 3.7515885829925537\n",
      "step = 4724800: loss = 3.0596108436584473\n",
      "step = 4725000: loss = 4.294713020324707\n",
      "step = 4725000: Average Return = 4.349999904632568\n",
      "step = 4725200: loss = 3.357717275619507\n",
      "step = 4725400: loss = 3.1049611568450928\n",
      "step = 4725600: loss = 4.495081901550293\n",
      "step = 4725800: loss = 2.812159776687622\n",
      "step = 4726000: loss = 5.457777976989746\n",
      "step = 4726200: loss = 6.258035182952881\n",
      "step = 4726400: loss = 3.768386125564575\n",
      "step = 4726600: loss = 2.9119157791137695\n",
      "step = 4726800: loss = 2.894759178161621\n",
      "step = 4727000: loss = 4.644613742828369\n",
      "step = 4727200: loss = 4.442272663116455\n",
      "step = 4727400: loss = 4.279004096984863\n",
      "step = 4727600: loss = 3.186237096786499\n",
      "step = 4727800: loss = 4.277662754058838\n",
      "step = 4728000: loss = 4.7321391105651855\n",
      "step = 4728200: loss = 3.234220266342163\n",
      "step = 4728400: loss = 4.353104114532471\n",
      "step = 4728600: loss = 4.894104480743408\n",
      "step = 4728800: loss = 2.5342793464660645\n",
      "step = 4729000: loss = 4.681272983551025\n",
      "step = 4729200: loss = 5.433296203613281\n",
      "step = 4729400: loss = 4.217447280883789\n",
      "step = 4729600: loss = 2.8436670303344727\n",
      "step = 4729800: loss = 4.016435623168945\n",
      "step = 4730000: loss = 3.7779901027679443\n",
      "step = 4730000: Average Return = 4.150000095367432\n",
      "step = 4730200: loss = 3.956427574157715\n",
      "step = 4730400: loss = 4.608543395996094\n",
      "step = 4730600: loss = 3.183420419692993\n",
      "step = 4730800: loss = 3.7243871688842773\n",
      "step = 4731000: loss = 3.254884958267212\n",
      "step = 4731200: loss = 3.704148769378662\n",
      "step = 4731400: loss = 3.1023778915405273\n",
      "step = 4731600: loss = 5.112070560455322\n",
      "step = 4731800: loss = 4.6958208084106445\n",
      "step = 4732000: loss = 4.127866744995117\n",
      "step = 4732200: loss = 3.276047945022583\n",
      "step = 4732400: loss = 4.033700466156006\n",
      "step = 4732600: loss = 3.84460186958313\n",
      "step = 4732800: loss = 5.075804710388184\n",
      "step = 4733000: loss = 2.8999998569488525\n",
      "step = 4733200: loss = 3.9829812049865723\n",
      "step = 4733400: loss = 2.610604763031006\n",
      "step = 4733600: loss = 4.123717308044434\n",
      "step = 4733800: loss = 3.0971102714538574\n",
      "step = 4734000: loss = 3.39701771736145\n",
      "step = 4734200: loss = 3.657376289367676\n",
      "step = 4734400: loss = 3.328247547149658\n",
      "step = 4734600: loss = 2.786628007888794\n",
      "step = 4734800: loss = 4.919869899749756\n",
      "step = 4735000: loss = 4.389726161956787\n",
      "step = 4735000: Average Return = 4.75\n",
      "step = 4735200: loss = 5.321700096130371\n",
      "step = 4735400: loss = 3.628643751144409\n",
      "step = 4735600: loss = 3.210024118423462\n",
      "step = 4735800: loss = 2.8626155853271484\n",
      "step = 4736000: loss = 3.563316583633423\n",
      "step = 4736200: loss = 5.615341663360596\n",
      "step = 4736400: loss = 3.31538987159729\n",
      "step = 4736600: loss = 4.349514484405518\n",
      "step = 4736800: loss = 4.35114860534668\n",
      "step = 4737000: loss = 2.7436299324035645\n",
      "step = 4737200: loss = 3.3178770542144775\n",
      "step = 4737400: loss = 4.505094051361084\n",
      "step = 4737600: loss = 3.4897124767303467\n",
      "step = 4737800: loss = 4.0924506187438965\n",
      "step = 4738000: loss = 3.40873646736145\n",
      "step = 4738200: loss = 3.005267858505249\n",
      "step = 4738400: loss = 4.164512634277344\n",
      "step = 4738600: loss = 3.673161506652832\n",
      "step = 4738800: loss = 4.41593599319458\n",
      "step = 4739000: loss = 3.4538261890411377\n",
      "step = 4739200: loss = 3.9172630310058594\n",
      "step = 4739400: loss = 2.968248128890991\n",
      "step = 4739600: loss = 3.3013060092926025\n",
      "step = 4739800: loss = 3.10188627243042\n",
      "step = 4740000: loss = 4.806196212768555\n",
      "step = 4740000: Average Return = 3.25\n",
      "step = 4740200: loss = 4.1287431716918945\n",
      "step = 4740400: loss = 4.083862781524658\n",
      "step = 4740600: loss = 5.465116024017334\n",
      "step = 4740800: loss = 4.557490348815918\n",
      "step = 4741000: loss = 3.9068353176116943\n",
      "step = 4741200: loss = 2.165693521499634\n",
      "step = 4741400: loss = 3.4951207637786865\n",
      "step = 4741600: loss = 2.9155309200286865\n",
      "step = 4741800: loss = 4.092804908752441\n",
      "step = 4742000: loss = 3.6237967014312744\n",
      "step = 4742200: loss = 4.734924793243408\n",
      "step = 4742400: loss = 5.0922932624816895\n",
      "step = 4742600: loss = 4.740156650543213\n",
      "step = 4742800: loss = 3.7803149223327637\n",
      "step = 4743000: loss = 5.141383171081543\n",
      "step = 4743200: loss = 3.8239641189575195\n",
      "step = 4743400: loss = 2.712636709213257\n",
      "step = 4743600: loss = 3.3109185695648193\n",
      "step = 4743800: loss = 3.85385799407959\n",
      "step = 4744000: loss = 4.4806718826293945\n",
      "step = 4744200: loss = 3.770670175552368\n",
      "step = 4744400: loss = 3.7820136547088623\n",
      "step = 4744600: loss = 3.1597392559051514\n",
      "step = 4744800: loss = 3.967127561569214\n",
      "step = 4745000: loss = 4.0602827072143555\n",
      "step = 4745000: Average Return = 4.349999904632568\n",
      "step = 4745200: loss = 2.6704626083374023\n",
      "step = 4745400: loss = 3.44458270072937\n",
      "step = 4745600: loss = 2.741729974746704\n",
      "step = 4745800: loss = 3.0955822467803955\n",
      "step = 4746000: loss = 4.111969947814941\n",
      "step = 4746200: loss = 4.398386001586914\n",
      "step = 4746400: loss = 4.7093186378479\n",
      "step = 4746600: loss = 5.600763320922852\n",
      "step = 4746800: loss = 4.200489521026611\n",
      "step = 4747000: loss = 3.818023204803467\n",
      "step = 4747200: loss = 3.0128273963928223\n",
      "step = 4747400: loss = 2.505340814590454\n",
      "step = 4747600: loss = 3.1999924182891846\n",
      "step = 4747800: loss = 3.2099266052246094\n",
      "step = 4748000: loss = 4.099607944488525\n",
      "step = 4748200: loss = 4.013465404510498\n",
      "step = 4748400: loss = 3.9624247550964355\n",
      "step = 4748600: loss = 2.9334161281585693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4748800: loss = 3.6366350650787354\n",
      "step = 4749000: loss = 4.960661888122559\n",
      "step = 4749200: loss = 2.801400661468506\n",
      "step = 4749400: loss = 4.541270732879639\n",
      "step = 4749600: loss = 2.805227518081665\n",
      "step = 4749800: loss = 3.5079240798950195\n",
      "step = 4750000: loss = 4.609871864318848\n",
      "step = 4750000: Average Return = 5.099999904632568\n",
      "step = 4750200: loss = 4.913107872009277\n",
      "step = 4750400: loss = 4.113767147064209\n",
      "step = 4750600: loss = 3.027510404586792\n",
      "step = 4750800: loss = 3.4206578731536865\n",
      "step = 4751000: loss = 3.7032973766326904\n",
      "step = 4751200: loss = 3.230746269226074\n",
      "step = 4751400: loss = 3.534823417663574\n",
      "step = 4751600: loss = 4.104665279388428\n",
      "step = 4751800: loss = 2.8994174003601074\n",
      "step = 4752000: loss = 4.336101531982422\n",
      "step = 4752200: loss = 4.414510250091553\n",
      "step = 4752400: loss = 3.6534698009490967\n",
      "step = 4752600: loss = 3.881962537765503\n",
      "step = 4752800: loss = 4.497720241546631\n",
      "step = 4753000: loss = 2.5690393447875977\n",
      "step = 4753200: loss = 3.549515724182129\n",
      "step = 4753400: loss = 3.0082268714904785\n",
      "step = 4753600: loss = 2.6332173347473145\n",
      "step = 4753800: loss = 3.7312169075012207\n",
      "step = 4754000: loss = 2.8527634143829346\n",
      "step = 4754200: loss = 4.92875337600708\n",
      "step = 4754400: loss = 3.033379554748535\n",
      "step = 4754600: loss = 3.3750627040863037\n",
      "step = 4754800: loss = 4.391574859619141\n",
      "step = 4755000: loss = 3.8178985118865967\n",
      "step = 4755000: Average Return = 3.6500000953674316\n",
      "step = 4755200: loss = 2.951580047607422\n",
      "step = 4755400: loss = 4.267319202423096\n",
      "step = 4755600: loss = 3.628284215927124\n",
      "step = 4755800: loss = 2.6330554485321045\n",
      "step = 4756000: loss = 2.6748363971710205\n",
      "step = 4756200: loss = 4.086973667144775\n",
      "step = 4756400: loss = 3.401611328125\n",
      "step = 4756600: loss = 4.066344261169434\n",
      "step = 4756800: loss = 4.796254634857178\n",
      "step = 4757000: loss = 3.9815783500671387\n",
      "step = 4757200: loss = 1.7123754024505615\n",
      "step = 4757400: loss = 2.837270975112915\n",
      "step = 4757600: loss = 3.836216449737549\n",
      "step = 4757800: loss = 5.044912815093994\n",
      "step = 4758000: loss = 2.4662420749664307\n",
      "step = 4758200: loss = 4.426207065582275\n",
      "step = 4758400: loss = 3.383816719055176\n",
      "step = 4758600: loss = 3.4346752166748047\n",
      "step = 4758800: loss = 3.5371906757354736\n",
      "step = 4759000: loss = 4.350607395172119\n",
      "step = 4759200: loss = 3.1754870414733887\n",
      "step = 4759400: loss = 2.9136648178100586\n",
      "step = 4759600: loss = 3.9164345264434814\n",
      "step = 4759800: loss = 3.763949394226074\n",
      "step = 4760000: loss = 3.84270977973938\n",
      "step = 4760000: Average Return = 2.5999999046325684\n",
      "step = 4760200: loss = 4.1000657081604\n",
      "step = 4760400: loss = 3.995131254196167\n",
      "step = 4760600: loss = 3.9392411708831787\n",
      "step = 4760800: loss = 3.3929030895233154\n",
      "step = 4761000: loss = 3.3908636569976807\n",
      "step = 4761200: loss = 5.34464168548584\n",
      "step = 4761400: loss = 2.8465945720672607\n",
      "step = 4761600: loss = 4.7101731300354\n",
      "step = 4761800: loss = 1.8029372692108154\n",
      "step = 4762000: loss = 3.4128265380859375\n",
      "step = 4762200: loss = 3.6441149711608887\n",
      "step = 4762400: loss = 3.6996524333953857\n",
      "step = 4762600: loss = 3.0938801765441895\n",
      "step = 4762800: loss = 3.593996286392212\n",
      "step = 4763000: loss = 3.522874593734741\n",
      "step = 4763200: loss = 3.502598762512207\n",
      "step = 4763400: loss = 2.65401554107666\n",
      "step = 4763600: loss = 3.5771093368530273\n",
      "step = 4763800: loss = 4.557930946350098\n",
      "step = 4764000: loss = 3.714871406555176\n",
      "step = 4764200: loss = 3.507390022277832\n",
      "step = 4764400: loss = 3.712904691696167\n",
      "step = 4764600: loss = 3.0795748233795166\n",
      "step = 4764800: loss = 3.005652904510498\n",
      "step = 4765000: loss = 3.57350754737854\n",
      "step = 4765000: Average Return = 5.599999904632568\n",
      "step = 4765200: loss = 3.731853485107422\n",
      "step = 4765400: loss = 4.559401988983154\n",
      "step = 4765600: loss = 3.433481454849243\n",
      "step = 4765800: loss = 4.203073501586914\n",
      "step = 4766000: loss = 5.701083660125732\n",
      "step = 4766200: loss = 3.712347984313965\n",
      "step = 4766400: loss = 4.092104434967041\n",
      "step = 4766600: loss = 3.585853099822998\n",
      "step = 4766800: loss = 4.163846015930176\n",
      "step = 4767000: loss = 4.105961799621582\n",
      "step = 4767200: loss = 4.22850227355957\n",
      "step = 4767400: loss = 3.736665964126587\n",
      "step = 4767600: loss = 3.8443305492401123\n",
      "step = 4767800: loss = 3.237818479537964\n",
      "step = 4768000: loss = 3.915902853012085\n",
      "step = 4768200: loss = 4.409887790679932\n",
      "step = 4768400: loss = 2.5303401947021484\n",
      "step = 4768600: loss = 3.860585927963257\n",
      "step = 4768800: loss = 2.3903861045837402\n",
      "step = 4769000: loss = 4.591789245605469\n",
      "step = 4769200: loss = 2.6972804069519043\n",
      "step = 4769400: loss = 3.560913562774658\n",
      "step = 4769600: loss = 3.972929000854492\n",
      "step = 4769800: loss = 3.5081939697265625\n",
      "step = 4770000: loss = 3.7867393493652344\n",
      "step = 4770000: Average Return = 4.599999904632568\n",
      "step = 4770200: loss = 2.471560478210449\n",
      "step = 4770400: loss = 3.787091016769409\n",
      "step = 4770600: loss = 3.1920604705810547\n",
      "step = 4770800: loss = 4.078845024108887\n",
      "step = 4771000: loss = 3.4510278701782227\n",
      "step = 4771200: loss = 4.032221794128418\n",
      "step = 4771400: loss = 3.842146873474121\n",
      "step = 4771600: loss = 3.4850358963012695\n",
      "step = 4771800: loss = 4.763312339782715\n",
      "step = 4772000: loss = 5.222432613372803\n",
      "step = 4772200: loss = 4.782362461090088\n",
      "step = 4772400: loss = 2.99065899848938\n",
      "step = 4772600: loss = 1.798363208770752\n",
      "step = 4772800: loss = 3.080084800720215\n",
      "step = 4773000: loss = 3.7906692028045654\n",
      "step = 4773200: loss = 4.008086681365967\n",
      "step = 4773400: loss = 3.0358619689941406\n",
      "step = 4773600: loss = 3.8933098316192627\n",
      "step = 4773800: loss = 3.2965755462646484\n",
      "step = 4774000: loss = 3.226094961166382\n",
      "step = 4774200: loss = 3.027409791946411\n",
      "step = 4774400: loss = 4.949908256530762\n",
      "step = 4774600: loss = 3.0346100330352783\n",
      "step = 4774800: loss = 4.0577898025512695\n",
      "step = 4775000: loss = 4.059523582458496\n",
      "step = 4775000: Average Return = 2.3499999046325684\n",
      "step = 4775200: loss = 3.369138479232788\n",
      "step = 4775400: loss = 3.0103721618652344\n",
      "step = 4775600: loss = 3.966197967529297\n",
      "step = 4775800: loss = 4.012819766998291\n",
      "step = 4776000: loss = 3.3718676567077637\n",
      "step = 4776200: loss = 3.70074462890625\n",
      "step = 4776400: loss = 4.274406433105469\n",
      "step = 4776600: loss = 4.717093467712402\n",
      "step = 4776800: loss = 3.642559766769409\n",
      "step = 4777000: loss = 4.866520404815674\n",
      "step = 4777200: loss = 2.49344539642334\n",
      "step = 4777400: loss = 3.7614586353302\n",
      "step = 4777600: loss = 3.958707094192505\n",
      "step = 4777800: loss = 4.073173522949219\n",
      "step = 4778000: loss = 3.533512592315674\n",
      "step = 4778200: loss = 4.290153503417969\n",
      "step = 4778400: loss = 3.7641215324401855\n",
      "step = 4778600: loss = 3.3194100856781006\n",
      "step = 4778800: loss = 3.38086199760437\n",
      "step = 4779000: loss = 4.621461391448975\n",
      "step = 4779200: loss = 4.565913200378418\n",
      "step = 4779400: loss = 4.784897804260254\n",
      "step = 4779600: loss = 4.0414299964904785\n",
      "step = 4779800: loss = 3.6868529319763184\n",
      "step = 4780000: loss = 5.390303611755371\n",
      "step = 4780000: Average Return = 4.949999809265137\n",
      "step = 4780200: loss = 2.7418289184570312\n",
      "step = 4780400: loss = 3.919084072113037\n",
      "step = 4780600: loss = 3.0245940685272217\n",
      "step = 4780800: loss = 4.448544979095459\n",
      "step = 4781000: loss = 4.5892863273620605\n",
      "step = 4781200: loss = 3.1928186416625977\n",
      "step = 4781400: loss = 3.271031379699707\n",
      "step = 4781600: loss = 3.7962212562561035\n",
      "step = 4781800: loss = 3.241766929626465\n",
      "step = 4782000: loss = 4.269571781158447\n",
      "step = 4782200: loss = 3.3601810932159424\n",
      "step = 4782400: loss = 3.9315409660339355\n",
      "step = 4782600: loss = 4.202545642852783\n",
      "step = 4782800: loss = 3.491610050201416\n",
      "step = 4783000: loss = 3.781405448913574\n",
      "step = 4783200: loss = 4.53457498550415\n",
      "step = 4783400: loss = 3.7461559772491455\n",
      "step = 4783600: loss = 3.068455219268799\n",
      "step = 4783800: loss = 4.479506015777588\n",
      "step = 4784000: loss = 5.134913921356201\n",
      "step = 4784200: loss = 3.7239859104156494\n",
      "step = 4784400: loss = 4.325605392456055\n",
      "step = 4784600: loss = 4.2709059715271\n",
      "step = 4784800: loss = 3.032865285873413\n",
      "step = 4785000: loss = 2.9660580158233643\n",
      "step = 4785000: Average Return = 2.799999952316284\n",
      "step = 4785200: loss = 3.0500926971435547\n",
      "step = 4785400: loss = 4.033969402313232\n",
      "step = 4785600: loss = 3.382561683654785\n",
      "step = 4785800: loss = 1.566541075706482\n",
      "step = 4786000: loss = 4.323318958282471\n",
      "step = 4786200: loss = 3.5053036212921143\n",
      "step = 4786400: loss = 3.7758283615112305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4786600: loss = 5.099932670593262\n",
      "step = 4786800: loss = 3.2240703105926514\n",
      "step = 4787000: loss = 2.9394476413726807\n",
      "step = 4787200: loss = 4.123031139373779\n",
      "step = 4787400: loss = 4.6554856300354\n",
      "step = 4787600: loss = 2.818345308303833\n",
      "step = 4787800: loss = 4.299124717712402\n",
      "step = 4788000: loss = 2.9278404712677\n",
      "step = 4788200: loss = 2.9028046131134033\n",
      "step = 4788400: loss = 3.5743768215179443\n",
      "step = 4788600: loss = 4.688699722290039\n",
      "step = 4788800: loss = 3.0225110054016113\n",
      "step = 4789000: loss = 3.410813570022583\n",
      "step = 4789200: loss = 3.90275502204895\n",
      "step = 4789400: loss = 3.230295419692993\n",
      "step = 4789600: loss = 3.795604705810547\n",
      "step = 4789800: loss = 4.728387832641602\n",
      "step = 4790000: loss = 3.4181408882141113\n",
      "step = 4790000: Average Return = 4.949999809265137\n",
      "step = 4790200: loss = 4.520244121551514\n",
      "step = 4790400: loss = 4.4749579429626465\n",
      "step = 4790600: loss = 5.007937908172607\n",
      "step = 4790800: loss = 4.310333251953125\n",
      "step = 4791000: loss = 4.6646599769592285\n",
      "step = 4791200: loss = 3.2830798625946045\n",
      "step = 4791400: loss = 3.9740545749664307\n",
      "step = 4791600: loss = 4.352092266082764\n",
      "step = 4791800: loss = 3.3909034729003906\n",
      "step = 4792000: loss = 3.212831974029541\n",
      "step = 4792200: loss = 4.6385498046875\n",
      "step = 4792400: loss = 3.505086898803711\n",
      "step = 4792600: loss = 4.110198974609375\n",
      "step = 4792800: loss = 3.077681541442871\n",
      "step = 4793000: loss = 3.174625873565674\n",
      "step = 4793200: loss = 4.31411075592041\n",
      "step = 4793400: loss = 4.501575946807861\n",
      "step = 4793600: loss = 3.8548784255981445\n",
      "step = 4793800: loss = 3.8354830741882324\n",
      "step = 4794000: loss = 4.412106990814209\n",
      "step = 4794200: loss = 4.493487358093262\n",
      "step = 4794400: loss = 3.761417865753174\n",
      "step = 4794600: loss = 4.664761066436768\n",
      "step = 4794800: loss = 3.0295989513397217\n",
      "step = 4795000: loss = 3.5181169509887695\n",
      "step = 4795000: Average Return = 4.050000190734863\n",
      "step = 4795200: loss = 3.9622817039489746\n",
      "step = 4795400: loss = 4.86941385269165\n",
      "step = 4795600: loss = 4.6318840980529785\n",
      "step = 4795800: loss = 4.490447998046875\n",
      "step = 4796000: loss = 2.5946457386016846\n",
      "step = 4796200: loss = 3.0795998573303223\n",
      "step = 4796400: loss = 3.5462584495544434\n",
      "step = 4796600: loss = 3.9389564990997314\n",
      "step = 4796800: loss = 3.204618453979492\n",
      "step = 4797000: loss = 3.6201064586639404\n",
      "step = 4797200: loss = 4.264012336730957\n",
      "step = 4797400: loss = 3.6345152854919434\n",
      "step = 4797600: loss = 3.955343246459961\n",
      "step = 4797800: loss = 4.12676477432251\n",
      "step = 4798000: loss = 4.3851318359375\n",
      "step = 4798200: loss = 4.266107559204102\n",
      "step = 4798400: loss = 5.374507427215576\n",
      "step = 4798600: loss = 4.684582233428955\n",
      "step = 4798800: loss = 5.0836501121521\n",
      "step = 4799000: loss = 3.5981533527374268\n",
      "step = 4799200: loss = 5.241067409515381\n",
      "step = 4799400: loss = 4.116266250610352\n",
      "step = 4799600: loss = 4.375129222869873\n",
      "step = 4799800: loss = 2.987678050994873\n",
      "step = 4800000: loss = 3.26424241065979\n",
      "step = 4800000: Average Return = 2.6500000953674316\n",
      "step = 4800200: loss = 3.5521669387817383\n",
      "step = 4800400: loss = 5.789688587188721\n",
      "step = 4800600: loss = 3.1654272079467773\n",
      "step = 4800800: loss = 3.1828627586364746\n",
      "step = 4801000: loss = 3.004378080368042\n",
      "step = 4801200: loss = 3.820427179336548\n",
      "step = 4801400: loss = 2.9454874992370605\n",
      "step = 4801600: loss = 3.1117777824401855\n",
      "step = 4801800: loss = 4.169685363769531\n",
      "step = 4802000: loss = 3.6788909435272217\n",
      "step = 4802200: loss = 3.565847635269165\n",
      "step = 4802400: loss = 3.397064208984375\n",
      "step = 4802600: loss = 5.359503269195557\n",
      "step = 4802800: loss = 5.314526081085205\n",
      "step = 4803000: loss = 3.917753219604492\n",
      "step = 4803200: loss = 2.6492297649383545\n",
      "step = 4803400: loss = 3.4852070808410645\n",
      "step = 4803600: loss = 3.957582950592041\n",
      "step = 4803800: loss = 4.11752986907959\n",
      "step = 4804000: loss = 4.453219413757324\n",
      "step = 4804200: loss = 3.2706758975982666\n",
      "step = 4804400: loss = 3.636129379272461\n",
      "step = 4804600: loss = 4.750875473022461\n",
      "step = 4804800: loss = 3.5015032291412354\n",
      "step = 4805000: loss = 4.2467217445373535\n",
      "step = 4805000: Average Return = 2.450000047683716\n",
      "step = 4805200: loss = 3.916632890701294\n",
      "step = 4805400: loss = 3.949800968170166\n",
      "step = 4805600: loss = 4.289503574371338\n",
      "step = 4805800: loss = 4.6650261878967285\n",
      "step = 4806000: loss = 2.9906837940216064\n",
      "step = 4806200: loss = 4.3243513107299805\n",
      "step = 4806400: loss = 4.181131362915039\n",
      "step = 4806600: loss = 3.859774589538574\n",
      "step = 4806800: loss = 4.369341850280762\n",
      "step = 4807000: loss = 3.030975341796875\n",
      "step = 4807200: loss = 4.666822910308838\n",
      "step = 4807400: loss = 3.060537576675415\n",
      "step = 4807600: loss = 4.265804290771484\n",
      "step = 4807800: loss = 3.3151462078094482\n",
      "step = 4808000: loss = 3.4588723182678223\n",
      "step = 4808200: loss = 4.525694370269775\n",
      "step = 4808400: loss = 3.779266357421875\n",
      "step = 4808600: loss = 3.8729851245880127\n",
      "step = 4808800: loss = 4.081344127655029\n",
      "step = 4809000: loss = 4.619818687438965\n",
      "step = 4809200: loss = 5.078123569488525\n",
      "step = 4809400: loss = 4.1610612869262695\n",
      "step = 4809600: loss = 4.043333053588867\n",
      "step = 4809800: loss = 3.690955400466919\n",
      "step = 4810000: loss = 3.7606871128082275\n",
      "step = 4810000: Average Return = 3.8499999046325684\n",
      "step = 4810200: loss = 3.0690934658050537\n",
      "step = 4810400: loss = 4.218138217926025\n",
      "step = 4810600: loss = 2.4794247150421143\n",
      "step = 4810800: loss = 4.806723594665527\n",
      "step = 4811000: loss = 4.497267246246338\n",
      "step = 4811200: loss = 4.2832746505737305\n",
      "step = 4811400: loss = 2.9050872325897217\n",
      "step = 4811600: loss = 3.740065813064575\n",
      "step = 4811800: loss = 4.659272193908691\n",
      "step = 4812000: loss = 5.314601898193359\n",
      "step = 4812200: loss = 5.141549110412598\n",
      "step = 4812400: loss = 3.539398431777954\n",
      "step = 4812600: loss = 5.18464469909668\n",
      "step = 4812800: loss = 3.5873937606811523\n",
      "step = 4813000: loss = 3.666144609451294\n",
      "step = 4813200: loss = 3.907479763031006\n",
      "step = 4813400: loss = 3.4822838306427\n",
      "step = 4813600: loss = 3.07659649848938\n",
      "step = 4813800: loss = 4.526986122131348\n",
      "step = 4814000: loss = 4.055212497711182\n",
      "step = 4814200: loss = 4.8343000411987305\n",
      "step = 4814400: loss = 2.9303131103515625\n",
      "step = 4814600: loss = 6.041036128997803\n",
      "step = 4814800: loss = 5.412961959838867\n",
      "step = 4815000: loss = 2.7862231731414795\n",
      "step = 4815000: Average Return = 4.75\n",
      "step = 4815200: loss = 3.004517078399658\n",
      "step = 4815400: loss = 4.942559719085693\n",
      "step = 4815600: loss = 3.3872570991516113\n",
      "step = 4815800: loss = 3.759159564971924\n",
      "step = 4816000: loss = 3.780296564102173\n",
      "step = 4816200: loss = 4.277328014373779\n",
      "step = 4816400: loss = 2.845540761947632\n",
      "step = 4816600: loss = 3.3970985412597656\n",
      "step = 4816800: loss = 4.679553508758545\n",
      "step = 4817000: loss = 4.58278751373291\n",
      "step = 4817200: loss = 3.033930540084839\n",
      "step = 4817400: loss = 3.483433723449707\n",
      "step = 4817600: loss = 3.9534130096435547\n",
      "step = 4817800: loss = 3.331193208694458\n",
      "step = 4818000: loss = 4.329989910125732\n",
      "step = 4818200: loss = 4.006715774536133\n",
      "step = 4818400: loss = 3.474329948425293\n",
      "step = 4818600: loss = 3.9992775917053223\n",
      "step = 4818800: loss = 3.754124641418457\n",
      "step = 4819000: loss = 4.120652198791504\n",
      "step = 4819200: loss = 4.6793532371521\n",
      "step = 4819400: loss = 4.314108848571777\n",
      "step = 4819600: loss = 1.673449993133545\n",
      "step = 4819800: loss = 3.6699647903442383\n",
      "step = 4820000: loss = 4.044839382171631\n",
      "step = 4820000: Average Return = 2.5\n",
      "step = 4820200: loss = 3.876929521560669\n",
      "step = 4820400: loss = 5.063941955566406\n",
      "step = 4820600: loss = 3.9198319911956787\n",
      "step = 4820800: loss = 4.369978427886963\n",
      "step = 4821000: loss = 5.273848056793213\n",
      "step = 4821200: loss = 3.7281508445739746\n",
      "step = 4821400: loss = 5.245043754577637\n",
      "step = 4821600: loss = 4.95125150680542\n",
      "step = 4821800: loss = 2.8936498165130615\n",
      "step = 4822000: loss = 4.595194339752197\n",
      "step = 4822200: loss = 3.2569761276245117\n",
      "step = 4822400: loss = 3.6953325271606445\n",
      "step = 4822600: loss = 4.101785659790039\n",
      "step = 4822800: loss = 3.116358518600464\n",
      "step = 4823000: loss = 3.3968112468719482\n",
      "step = 4823200: loss = 3.3481128215789795\n",
      "step = 4823400: loss = 3.7694952487945557\n",
      "step = 4823600: loss = 4.3448405265808105\n",
      "step = 4823800: loss = 4.1795573234558105\n",
      "step = 4824000: loss = 2.3848090171813965\n",
      "step = 4824200: loss = 4.143303394317627\n",
      "step = 4824400: loss = 3.39534330368042\n",
      "step = 4824600: loss = 4.50425910949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4824800: loss = 3.885179042816162\n",
      "step = 4825000: loss = 4.569312572479248\n",
      "step = 4825000: Average Return = 4.550000190734863\n",
      "step = 4825200: loss = 4.512287616729736\n",
      "step = 4825400: loss = 3.4589550495147705\n",
      "step = 4825600: loss = 3.313377618789673\n",
      "step = 4825800: loss = 4.117034912109375\n",
      "step = 4826000: loss = 3.510366439819336\n",
      "step = 4826200: loss = 5.11534309387207\n",
      "step = 4826400: loss = 6.163208484649658\n",
      "step = 4826600: loss = 2.7789306640625\n",
      "step = 4826800: loss = 2.4251065254211426\n",
      "step = 4827000: loss = 4.383665084838867\n",
      "step = 4827200: loss = 5.299273490905762\n",
      "step = 4827400: loss = 2.785140037536621\n",
      "step = 4827600: loss = 4.285029411315918\n",
      "step = 4827800: loss = 4.168762683868408\n",
      "step = 4828000: loss = 4.037577152252197\n",
      "step = 4828200: loss = 3.6489665508270264\n",
      "step = 4828400: loss = 4.030587196350098\n",
      "step = 4828600: loss = 4.379154682159424\n",
      "step = 4828800: loss = 3.3363966941833496\n",
      "step = 4829000: loss = 2.1190927028656006\n",
      "step = 4829200: loss = 4.050884246826172\n",
      "step = 4829400: loss = 5.352417945861816\n",
      "step = 4829600: loss = 4.599029064178467\n",
      "step = 4829800: loss = 3.953242778778076\n",
      "step = 4830000: loss = 4.077200412750244\n",
      "step = 4830000: Average Return = 2.299999952316284\n",
      "step = 4830200: loss = 4.2424516677856445\n",
      "step = 4830400: loss = 3.8204057216644287\n",
      "step = 4830600: loss = 3.897064208984375\n",
      "step = 4830800: loss = 3.64324951171875\n",
      "step = 4831000: loss = 4.427351951599121\n",
      "step = 4831200: loss = 4.551640033721924\n",
      "step = 4831400: loss = 5.149127006530762\n",
      "step = 4831600: loss = 3.792469024658203\n",
      "step = 4831800: loss = 3.1221349239349365\n",
      "step = 4832000: loss = 3.7569408416748047\n",
      "step = 4832200: loss = 4.373112678527832\n",
      "step = 4832400: loss = 4.505359172821045\n",
      "step = 4832600: loss = 5.030778884887695\n",
      "step = 4832800: loss = 4.169654846191406\n",
      "step = 4833000: loss = 4.906678676605225\n",
      "step = 4833200: loss = 3.813275098800659\n",
      "step = 4833400: loss = 3.403172016143799\n",
      "step = 4833600: loss = 3.0551605224609375\n",
      "step = 4833800: loss = 3.3968396186828613\n",
      "step = 4834000: loss = 4.846036434173584\n",
      "step = 4834200: loss = 4.472738742828369\n",
      "step = 4834400: loss = 3.7477476596832275\n",
      "step = 4834600: loss = 3.1707746982574463\n",
      "step = 4834800: loss = 4.333193302154541\n",
      "step = 4835000: loss = 4.410723686218262\n",
      "step = 4835000: Average Return = 4.25\n",
      "step = 4835200: loss = 4.221362113952637\n",
      "step = 4835400: loss = 3.2998406887054443\n",
      "step = 4835600: loss = 3.853023052215576\n",
      "step = 4835800: loss = 3.08081316947937\n",
      "step = 4836000: loss = 3.5637614727020264\n",
      "step = 4836200: loss = 4.200964450836182\n",
      "step = 4836400: loss = 4.72450590133667\n",
      "step = 4836600: loss = 4.859048843383789\n",
      "step = 4836800: loss = 5.058077335357666\n",
      "step = 4837000: loss = 3.3454012870788574\n",
      "step = 4837200: loss = 3.933818817138672\n",
      "step = 4837400: loss = 4.11201810836792\n",
      "step = 4837600: loss = 2.5263595581054688\n",
      "step = 4837800: loss = 3.6466569900512695\n",
      "step = 4838000: loss = 4.0065226554870605\n",
      "step = 4838200: loss = 3.6924726963043213\n",
      "step = 4838400: loss = 2.796034574508667\n",
      "step = 4838600: loss = 3.7497079372406006\n",
      "step = 4838800: loss = 4.492056369781494\n",
      "step = 4839000: loss = 4.907837867736816\n",
      "step = 4839200: loss = 4.03220796585083\n",
      "step = 4839400: loss = 3.469883918762207\n",
      "step = 4839600: loss = 3.354403495788574\n",
      "step = 4839800: loss = 2.9540834426879883\n",
      "step = 4840000: loss = 3.7048728466033936\n",
      "step = 4840000: Average Return = 4.5\n",
      "step = 4840200: loss = 3.6604766845703125\n",
      "step = 4840400: loss = 2.9847443103790283\n",
      "step = 4840600: loss = 3.8548424243927\n",
      "step = 4840800: loss = 4.064250469207764\n",
      "step = 4841000: loss = 3.6571245193481445\n",
      "step = 4841200: loss = 3.8554601669311523\n",
      "step = 4841400: loss = 3.965546131134033\n",
      "step = 4841600: loss = 3.8849334716796875\n",
      "step = 4841800: loss = 3.1804826259613037\n",
      "step = 4842000: loss = 3.616248607635498\n",
      "step = 4842200: loss = 3.6994316577911377\n",
      "step = 4842400: loss = 3.9321234226226807\n",
      "step = 4842600: loss = 4.103411674499512\n",
      "step = 4842800: loss = 4.490819454193115\n",
      "step = 4843000: loss = 4.781825542449951\n",
      "step = 4843200: loss = 2.842064142227173\n",
      "step = 4843400: loss = 4.727414608001709\n",
      "step = 4843600: loss = 2.916097402572632\n",
      "step = 4843800: loss = 3.0912959575653076\n",
      "step = 4844000: loss = 4.20073127746582\n",
      "step = 4844200: loss = 5.431796550750732\n",
      "step = 4844400: loss = 3.593158483505249\n",
      "step = 4844600: loss = 3.35295033454895\n",
      "step = 4844800: loss = 5.1028852462768555\n",
      "step = 4845000: loss = 4.421102046966553\n",
      "step = 4845000: Average Return = 3.1500000953674316\n",
      "step = 4845200: loss = 4.356330871582031\n",
      "step = 4845400: loss = 2.9498486518859863\n",
      "step = 4845600: loss = 4.087886333465576\n",
      "step = 4845800: loss = 4.386949062347412\n",
      "step = 4846000: loss = 3.7422099113464355\n",
      "step = 4846200: loss = 3.885235548019409\n",
      "step = 4846400: loss = 3.9259002208709717\n",
      "step = 4846600: loss = 4.937012672424316\n",
      "step = 4846800: loss = 3.806084394454956\n",
      "step = 4847000: loss = 4.437071323394775\n",
      "step = 4847200: loss = 2.3700358867645264\n",
      "step = 4847400: loss = 4.723775386810303\n",
      "step = 4847600: loss = 4.204318523406982\n",
      "step = 4847800: loss = 4.867741107940674\n",
      "step = 4848000: loss = 3.834690570831299\n",
      "step = 4848200: loss = 3.83659029006958\n",
      "step = 4848400: loss = 4.222571849822998\n",
      "step = 4848600: loss = 3.0348193645477295\n",
      "step = 4848800: loss = 3.538269519805908\n",
      "step = 4849000: loss = 3.6076366901397705\n",
      "step = 4849200: loss = 2.887482166290283\n",
      "step = 4849400: loss = 3.916940212249756\n",
      "step = 4849600: loss = 4.118965148925781\n",
      "step = 4849800: loss = 5.287212371826172\n",
      "step = 4850000: loss = 3.4000420570373535\n",
      "step = 4850000: Average Return = 3.75\n",
      "step = 4850200: loss = 5.20028018951416\n",
      "step = 4850400: loss = 4.3534088134765625\n",
      "step = 4850600: loss = 3.2217814922332764\n",
      "step = 4850800: loss = 3.7404134273529053\n",
      "step = 4851000: loss = 3.834585428237915\n",
      "step = 4851200: loss = 3.019883871078491\n",
      "step = 4851400: loss = 2.0218331813812256\n",
      "step = 4851600: loss = 3.067187547683716\n",
      "step = 4851800: loss = 4.201905250549316\n",
      "step = 4852000: loss = 5.173518180847168\n",
      "step = 4852200: loss = 3.1095499992370605\n",
      "step = 4852400: loss = 3.5906927585601807\n",
      "step = 4852600: loss = 3.4451122283935547\n",
      "step = 4852800: loss = 3.73345685005188\n",
      "step = 4853000: loss = 3.582998037338257\n",
      "step = 4853200: loss = 3.778822660446167\n",
      "step = 4853400: loss = 3.973734140396118\n",
      "step = 4853600: loss = 4.025625228881836\n",
      "step = 4853800: loss = 4.290394306182861\n",
      "step = 4854000: loss = 3.845364809036255\n",
      "step = 4854200: loss = 4.742691993713379\n",
      "step = 4854400: loss = 3.3346951007843018\n",
      "step = 4854600: loss = 3.1660923957824707\n",
      "step = 4854800: loss = 2.916248083114624\n",
      "step = 4855000: loss = 3.6920433044433594\n",
      "step = 4855000: Average Return = 3.8499999046325684\n",
      "step = 4855200: loss = 3.378899097442627\n",
      "step = 4855400: loss = 3.331413745880127\n",
      "step = 4855600: loss = 3.9992623329162598\n",
      "step = 4855800: loss = 3.1459856033325195\n",
      "step = 4856000: loss = 3.7569375038146973\n",
      "step = 4856200: loss = 4.9894537925720215\n",
      "step = 4856400: loss = 4.309469223022461\n",
      "step = 4856600: loss = 3.947269916534424\n",
      "step = 4856800: loss = 3.463289499282837\n",
      "step = 4857000: loss = 3.2043848037719727\n",
      "step = 4857200: loss = 3.998897075653076\n",
      "step = 4857400: loss = 3.617689847946167\n",
      "step = 4857600: loss = 3.1643340587615967\n",
      "step = 4857800: loss = 3.8703866004943848\n",
      "step = 4858000: loss = 3.11647629737854\n",
      "step = 4858200: loss = 4.153704643249512\n",
      "step = 4858400: loss = 4.563880443572998\n",
      "step = 4858600: loss = 4.311318874359131\n",
      "step = 4858800: loss = 3.738743782043457\n",
      "step = 4859000: loss = 4.667555332183838\n",
      "step = 4859200: loss = 4.74336576461792\n",
      "step = 4859400: loss = 3.625960111618042\n",
      "step = 4859600: loss = 4.939653396606445\n",
      "step = 4859800: loss = 3.686509847640991\n",
      "step = 4860000: loss = 4.050070285797119\n",
      "step = 4860000: Average Return = 4.550000190734863\n",
      "step = 4860200: loss = 3.9217171669006348\n",
      "step = 4860400: loss = 2.9496169090270996\n",
      "step = 4860600: loss = 3.1720409393310547\n",
      "step = 4860800: loss = 4.704225540161133\n",
      "step = 4861000: loss = 4.396234035491943\n",
      "step = 4861200: loss = 3.2784271240234375\n",
      "step = 4861400: loss = 4.708585739135742\n",
      "step = 4861600: loss = 4.778998851776123\n",
      "step = 4861800: loss = 4.123775005340576\n",
      "step = 4862000: loss = 2.883685827255249\n",
      "step = 4862200: loss = 3.2133684158325195\n",
      "step = 4862400: loss = 5.203097820281982\n",
      "step = 4862600: loss = 4.548712253570557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4862800: loss = 4.510283946990967\n",
      "step = 4863000: loss = 3.973421335220337\n",
      "step = 4863200: loss = 2.773737668991089\n",
      "step = 4863400: loss = 3.44999623298645\n",
      "step = 4863600: loss = 3.474224090576172\n",
      "step = 4863800: loss = 3.718522310256958\n",
      "step = 4864000: loss = 4.0111823081970215\n",
      "step = 4864200: loss = 3.6924281120300293\n",
      "step = 4864400: loss = 3.117398500442505\n",
      "step = 4864600: loss = 3.0402727127075195\n",
      "step = 4864800: loss = 3.0725085735321045\n",
      "step = 4865000: loss = 2.974907875061035\n",
      "step = 4865000: Average Return = 2.5999999046325684\n",
      "step = 4865200: loss = 4.704928398132324\n",
      "step = 4865400: loss = 3.464566469192505\n",
      "step = 4865600: loss = 3.205562114715576\n",
      "step = 4865800: loss = 3.9457290172576904\n",
      "step = 4866000: loss = 3.98659086227417\n",
      "step = 4866200: loss = 4.285115718841553\n",
      "step = 4866400: loss = 3.2017436027526855\n",
      "step = 4866600: loss = 4.214263439178467\n",
      "step = 4866800: loss = 5.098939895629883\n",
      "step = 4867000: loss = 3.9035966396331787\n",
      "step = 4867200: loss = 3.029841899871826\n",
      "step = 4867400: loss = 4.381198406219482\n",
      "step = 4867600: loss = 4.359250545501709\n",
      "step = 4867800: loss = 4.460570812225342\n",
      "step = 4868000: loss = 4.124967575073242\n",
      "step = 4868200: loss = 3.705267906188965\n",
      "step = 4868400: loss = 4.569030284881592\n",
      "step = 4868600: loss = 3.9173169136047363\n",
      "step = 4868800: loss = 3.044440269470215\n",
      "step = 4869000: loss = 4.035722732543945\n",
      "step = 4869200: loss = 4.224228382110596\n",
      "step = 4869400: loss = 4.163191318511963\n",
      "step = 4869600: loss = 3.6280534267425537\n",
      "step = 4869800: loss = 3.9147284030914307\n",
      "step = 4870000: loss = 3.545240640640259\n",
      "step = 4870000: Average Return = 3.450000047683716\n",
      "step = 4870200: loss = 3.7843198776245117\n",
      "step = 4870400: loss = 2.554375648498535\n",
      "step = 4870600: loss = 3.3133785724639893\n",
      "step = 4870800: loss = 3.789198637008667\n",
      "step = 4871000: loss = 4.116691589355469\n",
      "step = 4871200: loss = 3.2990150451660156\n",
      "step = 4871400: loss = 4.219289302825928\n",
      "step = 4871600: loss = 4.138757705688477\n",
      "step = 4871800: loss = 3.4180455207824707\n",
      "step = 4872000: loss = 4.599660396575928\n",
      "step = 4872200: loss = 4.086997985839844\n",
      "step = 4872400: loss = 3.912079095840454\n",
      "step = 4872600: loss = 4.478518486022949\n",
      "step = 4872800: loss = 4.234147071838379\n",
      "step = 4873000: loss = 5.083775043487549\n",
      "step = 4873200: loss = 3.0826449394226074\n",
      "step = 4873400: loss = 3.6474852561950684\n",
      "step = 4873600: loss = 3.7911899089813232\n",
      "step = 4873800: loss = 2.983478307723999\n",
      "step = 4874000: loss = 3.8392322063446045\n",
      "step = 4874200: loss = 3.607074737548828\n",
      "step = 4874400: loss = 3.970806837081909\n",
      "step = 4874600: loss = 3.58914852142334\n",
      "step = 4874800: loss = 4.211706638336182\n",
      "step = 4875000: loss = 4.797520160675049\n",
      "step = 4875000: Average Return = 3.950000047683716\n",
      "step = 4875200: loss = 4.4504899978637695\n",
      "step = 4875400: loss = 3.965153455734253\n",
      "step = 4875600: loss = 3.730855703353882\n",
      "step = 4875800: loss = 2.6745927333831787\n",
      "step = 4876000: loss = 4.748326778411865\n",
      "step = 4876200: loss = 3.8348586559295654\n",
      "step = 4876400: loss = 3.5315656661987305\n",
      "step = 4876600: loss = 3.0982506275177\n",
      "step = 4876800: loss = 3.0957422256469727\n",
      "step = 4877000: loss = 2.9014854431152344\n",
      "step = 4877200: loss = 4.149843215942383\n",
      "step = 4877400: loss = 3.7800447940826416\n",
      "step = 4877600: loss = 3.820075035095215\n",
      "step = 4877800: loss = 4.245724678039551\n",
      "step = 4878000: loss = 4.682009696960449\n",
      "step = 4878200: loss = 3.176201343536377\n",
      "step = 4878400: loss = 4.126227855682373\n",
      "step = 4878600: loss = 4.038932800292969\n",
      "step = 4878800: loss = 3.495042085647583\n",
      "step = 4879000: loss = 2.9354379177093506\n",
      "step = 4879200: loss = 4.216827392578125\n",
      "step = 4879400: loss = 3.1602118015289307\n",
      "step = 4879600: loss = 3.297435760498047\n",
      "step = 4879800: loss = 3.056065082550049\n",
      "step = 4880000: loss = 3.92883038520813\n",
      "step = 4880000: Average Return = 2.950000047683716\n",
      "step = 4880200: loss = 3.0578320026397705\n",
      "step = 4880400: loss = 4.392952919006348\n",
      "step = 4880600: loss = 4.426539421081543\n",
      "step = 4880800: loss = 4.112621307373047\n",
      "step = 4881000: loss = 4.001581192016602\n",
      "step = 4881200: loss = 4.056456089019775\n",
      "step = 4881400: loss = 2.832315683364868\n",
      "step = 4881600: loss = 4.364421844482422\n",
      "step = 4881800: loss = 3.9551169872283936\n",
      "step = 4882000: loss = 4.2271881103515625\n",
      "step = 4882200: loss = 4.312367916107178\n",
      "step = 4882400: loss = 2.9477007389068604\n",
      "step = 4882600: loss = 4.618202209472656\n",
      "step = 4882800: loss = 5.318299293518066\n",
      "step = 4883000: loss = 2.6808629035949707\n",
      "step = 4883200: loss = 3.916707992553711\n",
      "step = 4883400: loss = 4.181440353393555\n",
      "step = 4883600: loss = 3.822774648666382\n",
      "step = 4883800: loss = 4.84955358505249\n",
      "step = 4884000: loss = 4.321498394012451\n",
      "step = 4884200: loss = 3.483402729034424\n",
      "step = 4884400: loss = 3.91701340675354\n",
      "step = 4884600: loss = 3.849773406982422\n",
      "step = 4884800: loss = 3.7846031188964844\n",
      "step = 4885000: loss = 2.999969482421875\n",
      "step = 4885000: Average Return = 3.5\n",
      "step = 4885200: loss = 3.715113401412964\n",
      "step = 4885400: loss = 3.6611602306365967\n",
      "step = 4885600: loss = 3.011537790298462\n",
      "step = 4885800: loss = 4.611227989196777\n",
      "step = 4886000: loss = 3.7377002239227295\n",
      "step = 4886200: loss = 3.4873838424682617\n",
      "step = 4886400: loss = 3.8172903060913086\n",
      "step = 4886600: loss = 4.821486473083496\n",
      "step = 4886800: loss = 2.6790435314178467\n",
      "step = 4887000: loss = 3.6845545768737793\n",
      "step = 4887200: loss = 3.77736234664917\n",
      "step = 4887400: loss = 3.8860952854156494\n",
      "step = 4887600: loss = 3.019728899002075\n",
      "step = 4887800: loss = 4.70874547958374\n",
      "step = 4888000: loss = 4.970519065856934\n",
      "step = 4888200: loss = 3.9793150424957275\n",
      "step = 4888400: loss = 3.472867965698242\n",
      "step = 4888600: loss = 3.533735990524292\n",
      "step = 4888800: loss = 3.6705782413482666\n",
      "step = 4889000: loss = 2.610182046890259\n",
      "step = 4889200: loss = 3.3833537101745605\n",
      "step = 4889400: loss = 3.7742576599121094\n",
      "step = 4889600: loss = 2.912811756134033\n",
      "step = 4889800: loss = 3.1353683471679688\n",
      "step = 4890000: loss = 3.138841390609741\n",
      "step = 4890000: Average Return = 3.25\n",
      "step = 4890200: loss = 4.587118625640869\n",
      "step = 4890400: loss = 4.261391639709473\n",
      "step = 4890600: loss = 3.5431089401245117\n",
      "step = 4890800: loss = 3.503422975540161\n",
      "step = 4891000: loss = 3.83756947517395\n",
      "step = 4891200: loss = 4.013805389404297\n",
      "step = 4891400: loss = 4.494748592376709\n",
      "step = 4891600: loss = 5.506158351898193\n",
      "step = 4891800: loss = 3.658672332763672\n",
      "step = 4892000: loss = 4.073514461517334\n",
      "step = 4892200: loss = 3.8400700092315674\n",
      "step = 4892400: loss = 3.7353515625\n",
      "step = 4892600: loss = 3.380937337875366\n",
      "step = 4892800: loss = 3.7541728019714355\n",
      "step = 4893000: loss = 3.6000146865844727\n",
      "step = 4893200: loss = 3.6820132732391357\n",
      "step = 4893400: loss = 4.817732334136963\n",
      "step = 4893600: loss = 4.57589864730835\n",
      "step = 4893800: loss = 5.2983269691467285\n",
      "step = 4894000: loss = 3.3275959491729736\n",
      "step = 4894200: loss = 4.596376895904541\n",
      "step = 4894400: loss = 4.678526401519775\n",
      "step = 4894600: loss = 3.591785192489624\n",
      "step = 4894800: loss = 3.7147347927093506\n",
      "step = 4895000: loss = 3.122392416000366\n",
      "step = 4895000: Average Return = 3.0\n",
      "step = 4895200: loss = 3.1695568561553955\n",
      "step = 4895400: loss = 4.781810760498047\n",
      "step = 4895600: loss = 3.22794246673584\n",
      "step = 4895800: loss = 2.6202943325042725\n",
      "step = 4896000: loss = 3.8967983722686768\n",
      "step = 4896200: loss = 2.385145425796509\n",
      "step = 4896400: loss = 2.8785459995269775\n",
      "step = 4896600: loss = 4.480871677398682\n",
      "step = 4896800: loss = 2.28849720954895\n",
      "step = 4897000: loss = 3.840203046798706\n",
      "step = 4897200: loss = 3.685394763946533\n",
      "step = 4897400: loss = 3.385662794113159\n",
      "step = 4897600: loss = 4.6639862060546875\n",
      "step = 4897800: loss = 5.337450981140137\n",
      "step = 4898000: loss = 3.9413998126983643\n",
      "step = 4898200: loss = 4.632028102874756\n",
      "step = 4898400: loss = 4.553102970123291\n",
      "step = 4898600: loss = 4.2648749351501465\n",
      "step = 4898800: loss = 3.590837240219116\n",
      "step = 4899000: loss = 4.150602340698242\n",
      "step = 4899200: loss = 3.68373441696167\n",
      "step = 4899400: loss = 3.840648651123047\n",
      "step = 4899600: loss = 4.164376258850098\n",
      "step = 4899800: loss = 4.650650978088379\n",
      "step = 4900000: loss = 3.985354423522949\n",
      "step = 4900000: Average Return = 3.75\n",
      "step = 4900200: loss = 4.017145156860352\n",
      "step = 4900400: loss = 3.586108684539795\n",
      "step = 4900600: loss = 4.405234336853027\n",
      "step = 4900800: loss = 3.707209348678589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4901000: loss = 5.005040645599365\n",
      "step = 4901200: loss = 3.4805641174316406\n",
      "step = 4901400: loss = 3.8922064304351807\n",
      "step = 4901600: loss = 4.6795220375061035\n",
      "step = 4901800: loss = 3.232187509536743\n",
      "step = 4902000: loss = 4.159107208251953\n",
      "step = 4902200: loss = 3.8858630657196045\n",
      "step = 4902400: loss = 3.916754961013794\n",
      "step = 4902600: loss = 3.801443099975586\n",
      "step = 4902800: loss = 4.311994552612305\n",
      "step = 4903000: loss = 4.3949198722839355\n",
      "step = 4903200: loss = 4.481251239776611\n",
      "step = 4903400: loss = 3.5799810886383057\n",
      "step = 4903600: loss = 4.631397724151611\n",
      "step = 4903800: loss = 3.1498382091522217\n",
      "step = 4904000: loss = 3.4361021518707275\n",
      "step = 4904200: loss = 3.47125506401062\n",
      "step = 4904400: loss = 3.536207914352417\n",
      "step = 4904600: loss = 4.628226280212402\n",
      "step = 4904800: loss = 3.573469400405884\n",
      "step = 4905000: loss = 4.3287787437438965\n",
      "step = 4905000: Average Return = 3.5999999046325684\n",
      "step = 4905200: loss = 2.5602684020996094\n",
      "step = 4905400: loss = 4.415924549102783\n",
      "step = 4905600: loss = 5.361468315124512\n",
      "step = 4905800: loss = 3.618788957595825\n",
      "step = 4906000: loss = 4.461385250091553\n",
      "step = 4906200: loss = 3.0403361320495605\n",
      "step = 4906400: loss = 3.537973165512085\n",
      "step = 4906600: loss = 5.488115310668945\n",
      "step = 4906800: loss = 3.303403615951538\n",
      "step = 4907000: loss = 4.8722662925720215\n",
      "step = 4907200: loss = 3.4201622009277344\n",
      "step = 4907400: loss = 3.3667027950286865\n",
      "step = 4907600: loss = 4.678706645965576\n",
      "step = 4907800: loss = 3.7385144233703613\n",
      "step = 4908000: loss = 4.228794574737549\n",
      "step = 4908200: loss = 2.482943058013916\n",
      "step = 4908400: loss = 4.31057596206665\n",
      "step = 4908600: loss = 4.2880377769470215\n",
      "step = 4908800: loss = 3.9524176120758057\n",
      "step = 4909000: loss = 3.7544896602630615\n",
      "step = 4909200: loss = 3.789043426513672\n",
      "step = 4909400: loss = 3.677748441696167\n",
      "step = 4909600: loss = 5.058745861053467\n",
      "step = 4909800: loss = 4.029094696044922\n",
      "step = 4910000: loss = 2.5136303901672363\n",
      "step = 4910000: Average Return = 3.950000047683716\n",
      "step = 4910200: loss = 3.029315710067749\n",
      "step = 4910400: loss = 4.152737617492676\n",
      "step = 4910600: loss = 3.555345296859741\n",
      "step = 4910800: loss = 4.769837379455566\n",
      "step = 4911000: loss = 2.9070327281951904\n",
      "step = 4911200: loss = 3.613892078399658\n",
      "step = 4911400: loss = 3.5086028575897217\n",
      "step = 4911600: loss = 3.321683883666992\n",
      "step = 4911800: loss = 3.359369993209839\n",
      "step = 4912000: loss = 2.9574944972991943\n",
      "step = 4912200: loss = 4.306979179382324\n",
      "step = 4912400: loss = 3.3657948970794678\n",
      "step = 4912600: loss = 4.082545280456543\n",
      "step = 4912800: loss = 5.012693881988525\n",
      "step = 4913000: loss = 3.700181722640991\n",
      "step = 4913200: loss = 3.710300922393799\n",
      "step = 4913400: loss = 3.3724467754364014\n",
      "step = 4913600: loss = 3.4281795024871826\n",
      "step = 4913800: loss = 2.9278461933135986\n",
      "step = 4914000: loss = 2.6055455207824707\n",
      "step = 4914200: loss = 4.32432222366333\n",
      "step = 4914400: loss = 3.3518974781036377\n",
      "step = 4914600: loss = 2.9109532833099365\n",
      "step = 4914800: loss = 3.823216676712036\n",
      "step = 4915000: loss = 4.639195442199707\n",
      "step = 4915000: Average Return = 1.649999976158142\n",
      "step = 4915200: loss = 3.7992749214172363\n",
      "step = 4915400: loss = 3.29959774017334\n",
      "step = 4915600: loss = 3.503227472305298\n",
      "step = 4915800: loss = 3.8609492778778076\n",
      "step = 4916000: loss = 4.09706449508667\n",
      "step = 4916200: loss = 5.237218379974365\n",
      "step = 4916400: loss = 4.886215686798096\n",
      "step = 4916600: loss = 4.82236385345459\n",
      "step = 4916800: loss = 3.560826301574707\n",
      "step = 4917000: loss = 3.353687286376953\n",
      "step = 4917200: loss = 2.8172357082366943\n",
      "step = 4917400: loss = 3.7881503105163574\n",
      "step = 4917600: loss = 4.084272384643555\n",
      "step = 4917800: loss = 3.7220189571380615\n",
      "step = 4918000: loss = 4.835165500640869\n",
      "step = 4918200: loss = 5.164870262145996\n",
      "step = 4918400: loss = 4.13128662109375\n",
      "step = 4918600: loss = 3.643502712249756\n",
      "step = 4918800: loss = 2.727753162384033\n",
      "step = 4919000: loss = 3.761094331741333\n",
      "step = 4919200: loss = 3.602651357650757\n",
      "step = 4919400: loss = 4.121264457702637\n",
      "step = 4919600: loss = 5.076994895935059\n",
      "step = 4919800: loss = 2.112884521484375\n",
      "step = 4920000: loss = 4.845485687255859\n",
      "step = 4920000: Average Return = 2.6500000953674316\n",
      "step = 4920200: loss = 2.781975746154785\n",
      "step = 4920400: loss = 4.361180305480957\n",
      "step = 4920600: loss = 2.3439877033233643\n",
      "step = 4920800: loss = 2.988288164138794\n",
      "step = 4921000: loss = 4.406203746795654\n",
      "step = 4921200: loss = 3.9502220153808594\n",
      "step = 4921400: loss = 4.083173751831055\n",
      "step = 4921600: loss = 3.605351448059082\n",
      "step = 4921800: loss = 3.3782386779785156\n",
      "step = 4922000: loss = 3.826214551925659\n",
      "step = 4922200: loss = 5.121370315551758\n",
      "step = 4922400: loss = 3.30706787109375\n",
      "step = 4922600: loss = 4.5662078857421875\n",
      "step = 4922800: loss = 4.319611072540283\n",
      "step = 4923000: loss = 4.921905994415283\n",
      "step = 4923200: loss = 4.618441104888916\n",
      "step = 4923400: loss = 4.754683494567871\n",
      "step = 4923600: loss = 4.280125141143799\n",
      "step = 4923800: loss = 5.016887187957764\n",
      "step = 4924000: loss = 2.3271584510803223\n",
      "step = 4924200: loss = 4.303985595703125\n",
      "step = 4924400: loss = 3.4895739555358887\n",
      "step = 4924600: loss = 3.4805195331573486\n",
      "step = 4924800: loss = 3.384145975112915\n",
      "step = 4925000: loss = 2.6871423721313477\n",
      "step = 4925000: Average Return = 4.099999904632568\n",
      "step = 4925200: loss = 4.1983489990234375\n",
      "step = 4925400: loss = 3.966522693634033\n",
      "step = 4925600: loss = 2.8143327236175537\n",
      "step = 4925800: loss = 3.6068968772888184\n",
      "step = 4926000: loss = 3.7792863845825195\n",
      "step = 4926200: loss = 3.354238271713257\n",
      "step = 4926400: loss = 4.7778143882751465\n",
      "step = 4926600: loss = 3.726555347442627\n",
      "step = 4926800: loss = 1.592183232307434\n",
      "step = 4927000: loss = 3.254368543624878\n",
      "step = 4927200: loss = 3.326744318008423\n",
      "step = 4927400: loss = 2.651015520095825\n",
      "step = 4927600: loss = 3.430039644241333\n",
      "step = 4927800: loss = 4.630728244781494\n",
      "step = 4928000: loss = 3.803837776184082\n",
      "step = 4928200: loss = 3.6773879528045654\n",
      "step = 4928400: loss = 3.9783787727355957\n",
      "step = 4928600: loss = 4.042327880859375\n",
      "step = 4928800: loss = 4.787261009216309\n",
      "step = 4929000: loss = 3.422614336013794\n",
      "step = 4929200: loss = 4.0048322677612305\n",
      "step = 4929400: loss = 3.827965259552002\n",
      "step = 4929600: loss = 4.512606620788574\n",
      "step = 4929800: loss = 5.056746959686279\n",
      "step = 4930000: loss = 3.560260534286499\n",
      "step = 4930000: Average Return = 3.200000047683716\n",
      "step = 4930200: loss = 4.012200355529785\n",
      "step = 4930400: loss = 4.4554362297058105\n",
      "step = 4930600: loss = 4.387237071990967\n",
      "step = 4930800: loss = 4.165026664733887\n",
      "step = 4931000: loss = 4.6325578689575195\n",
      "step = 4931200: loss = 3.8197898864746094\n",
      "step = 4931400: loss = 3.207123279571533\n",
      "step = 4931600: loss = 3.795970916748047\n",
      "step = 4931800: loss = 3.2508490085601807\n",
      "step = 4932000: loss = 3.8319265842437744\n",
      "step = 4932200: loss = 4.208261013031006\n",
      "step = 4932400: loss = 4.088164806365967\n",
      "step = 4932600: loss = 4.524203777313232\n",
      "step = 4932800: loss = 4.270191192626953\n",
      "step = 4933000: loss = 4.288360595703125\n",
      "step = 4933200: loss = 3.396378755569458\n",
      "step = 4933400: loss = 4.638493061065674\n",
      "step = 4933600: loss = 3.4072515964508057\n",
      "step = 4933800: loss = 4.906714916229248\n",
      "step = 4934000: loss = 4.8671674728393555\n",
      "step = 4934200: loss = 5.17789888381958\n",
      "step = 4934400: loss = 4.353395462036133\n",
      "step = 4934600: loss = 3.9539904594421387\n",
      "step = 4934800: loss = 4.416469573974609\n",
      "step = 4935000: loss = 3.5687553882598877\n",
      "step = 4935000: Average Return = 2.450000047683716\n",
      "step = 4935200: loss = 4.409571647644043\n",
      "step = 4935400: loss = 4.263925552368164\n",
      "step = 4935600: loss = 3.1906497478485107\n",
      "step = 4935800: loss = 4.563803672790527\n",
      "step = 4936000: loss = 4.258093357086182\n",
      "step = 4936200: loss = 4.75293493270874\n",
      "step = 4936400: loss = 4.809869766235352\n",
      "step = 4936600: loss = 4.47711181640625\n",
      "step = 4936800: loss = 3.7111785411834717\n",
      "step = 4937000: loss = 3.5651817321777344\n",
      "step = 4937200: loss = 3.3284010887145996\n",
      "step = 4937400: loss = 3.0206499099731445\n",
      "step = 4937600: loss = 5.04197883605957\n",
      "step = 4937800: loss = 4.049698352813721\n",
      "step = 4938000: loss = 3.01084041595459\n",
      "step = 4938200: loss = 3.4085605144500732\n",
      "step = 4938400: loss = 4.261013984680176\n",
      "step = 4938600: loss = 3.5451266765594482\n",
      "step = 4938800: loss = 3.2118871212005615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4939000: loss = 4.1065473556518555\n",
      "step = 4939200: loss = 4.487715244293213\n",
      "step = 4939400: loss = 3.6476097106933594\n",
      "step = 4939600: loss = 2.458246946334839\n",
      "step = 4939800: loss = 4.279272556304932\n",
      "step = 4940000: loss = 3.7134783267974854\n",
      "step = 4940000: Average Return = 3.9000000953674316\n",
      "step = 4940200: loss = 3.250917911529541\n",
      "step = 4940400: loss = 4.336641788482666\n",
      "step = 4940600: loss = 2.6075353622436523\n",
      "step = 4940800: loss = 3.9619665145874023\n",
      "step = 4941000: loss = 4.241351127624512\n",
      "step = 4941200: loss = 3.190023183822632\n",
      "step = 4941400: loss = 4.791402816772461\n",
      "step = 4941600: loss = 3.873453378677368\n",
      "step = 4941800: loss = 3.7237935066223145\n",
      "step = 4942000: loss = 4.811193943023682\n",
      "step = 4942200: loss = 2.5129573345184326\n",
      "step = 4942400: loss = 6.067357063293457\n",
      "step = 4942600: loss = 3.454561710357666\n",
      "step = 4942800: loss = 4.019234657287598\n",
      "step = 4943000: loss = 2.55019474029541\n",
      "step = 4943200: loss = 3.076876163482666\n",
      "step = 4943400: loss = 3.4213621616363525\n",
      "step = 4943600: loss = 5.233674049377441\n",
      "step = 4943800: loss = 3.6045138835906982\n",
      "step = 4944000: loss = 3.952747106552124\n",
      "step = 4944200: loss = 3.810779094696045\n",
      "step = 4944400: loss = 3.3339974880218506\n",
      "step = 4944600: loss = 2.940945863723755\n",
      "step = 4944800: loss = 4.144644260406494\n",
      "step = 4945000: loss = 4.043704509735107\n",
      "step = 4945000: Average Return = 2.25\n",
      "step = 4945200: loss = 1.8976916074752808\n",
      "step = 4945400: loss = 3.2754766941070557\n",
      "step = 4945600: loss = 5.004495620727539\n",
      "step = 4945800: loss = 3.702899694442749\n",
      "step = 4946000: loss = 4.0580291748046875\n",
      "step = 4946200: loss = 4.097996711730957\n",
      "step = 4946400: loss = 3.2581138610839844\n",
      "step = 4946600: loss = 2.844564199447632\n",
      "step = 4946800: loss = 3.6331984996795654\n",
      "step = 4947000: loss = 2.840411424636841\n",
      "step = 4947200: loss = 4.91904878616333\n",
      "step = 4947400: loss = 4.358225345611572\n",
      "step = 4947600: loss = 2.994853973388672\n",
      "step = 4947800: loss = 2.760207414627075\n",
      "step = 4948000: loss = 2.7204513549804688\n",
      "step = 4948200: loss = 4.228364944458008\n",
      "step = 4948400: loss = 3.8387181758880615\n",
      "step = 4948600: loss = 3.560621976852417\n",
      "step = 4948800: loss = 4.550145626068115\n",
      "step = 4949000: loss = 5.745677947998047\n",
      "step = 4949200: loss = 4.653973579406738\n",
      "step = 4949400: loss = 3.396152973175049\n",
      "step = 4949600: loss = 2.995622158050537\n",
      "step = 4949800: loss = 4.388397693634033\n",
      "step = 4950000: loss = 3.072437047958374\n",
      "step = 4950000: Average Return = 4.199999809265137\n",
      "step = 4950200: loss = 4.18043851852417\n",
      "step = 4950400: loss = 4.503628253936768\n",
      "step = 4950600: loss = 3.998210906982422\n",
      "step = 4950800: loss = 4.055301189422607\n",
      "step = 4951000: loss = 3.842471122741699\n",
      "step = 4951200: loss = 3.1224400997161865\n",
      "step = 4951400: loss = 3.604498863220215\n",
      "step = 4951600: loss = 4.234790325164795\n",
      "step = 4951800: loss = 3.7727034091949463\n",
      "step = 4952000: loss = 3.9539685249328613\n",
      "step = 4952200: loss = 3.144984722137451\n",
      "step = 4952400: loss = 3.1376988887786865\n",
      "step = 4952600: loss = 4.17820930480957\n",
      "step = 4952800: loss = 3.696309804916382\n",
      "step = 4953000: loss = 4.100437641143799\n",
      "step = 4953200: loss = 3.325502872467041\n",
      "step = 4953400: loss = 3.304412841796875\n",
      "step = 4953600: loss = 3.6874632835388184\n",
      "step = 4953800: loss = 3.255708932876587\n",
      "step = 4954000: loss = 3.9000260829925537\n",
      "step = 4954200: loss = 3.2531192302703857\n",
      "step = 4954400: loss = 3.937868356704712\n",
      "step = 4954600: loss = 4.121583938598633\n",
      "step = 4954800: loss = 2.2883121967315674\n",
      "step = 4955000: loss = 4.1384196281433105\n",
      "step = 4955000: Average Return = 4.300000190734863\n",
      "step = 4955200: loss = 3.7793774604797363\n",
      "step = 4955400: loss = 4.333011150360107\n",
      "step = 4955600: loss = 3.5086400508880615\n",
      "step = 4955800: loss = 3.684440851211548\n",
      "step = 4956000: loss = 2.7259011268615723\n",
      "step = 4956200: loss = 3.6041650772094727\n",
      "step = 4956400: loss = 4.780239105224609\n",
      "step = 4956600: loss = 3.3740434646606445\n",
      "step = 4956800: loss = 3.7350645065307617\n",
      "step = 4957000: loss = 3.892930746078491\n",
      "step = 4957200: loss = 2.9918670654296875\n",
      "step = 4957400: loss = 3.7330386638641357\n",
      "step = 4957600: loss = 4.63774299621582\n",
      "step = 4957800: loss = 3.214081048965454\n",
      "step = 4958000: loss = 4.835050106048584\n",
      "step = 4958200: loss = 4.873457908630371\n",
      "step = 4958400: loss = 3.713731527328491\n",
      "step = 4958600: loss = 3.1203343868255615\n",
      "step = 4958800: loss = 4.472047328948975\n",
      "step = 4959000: loss = 3.997166395187378\n",
      "step = 4959200: loss = 3.88855242729187\n",
      "step = 4959400: loss = 3.286119222640991\n",
      "step = 4959600: loss = 3.4496448040008545\n",
      "step = 4959800: loss = 4.135961532592773\n",
      "step = 4960000: loss = 4.035957336425781\n",
      "step = 4960000: Average Return = 2.799999952316284\n",
      "step = 4960200: loss = 2.941354274749756\n",
      "step = 4960400: loss = 3.892479419708252\n",
      "step = 4960600: loss = 3.3998403549194336\n",
      "step = 4960800: loss = 4.062922954559326\n",
      "step = 4961000: loss = 4.0114569664001465\n",
      "step = 4961200: loss = 3.237287759780884\n",
      "step = 4961400: loss = 3.859105110168457\n",
      "step = 4961600: loss = 4.010317802429199\n",
      "step = 4961800: loss = 3.7212400436401367\n",
      "step = 4962000: loss = 4.526053428649902\n",
      "step = 4962200: loss = 4.01240873336792\n",
      "step = 4962400: loss = 4.210218906402588\n",
      "step = 4962600: loss = 4.362534523010254\n",
      "step = 4962800: loss = 3.6411726474761963\n",
      "step = 4963000: loss = 5.505794525146484\n",
      "step = 4963200: loss = 3.516866445541382\n",
      "step = 4963400: loss = 4.8134446144104\n",
      "step = 4963600: loss = 3.669384717941284\n",
      "step = 4963800: loss = 3.281116247177124\n",
      "step = 4964000: loss = 3.6303725242614746\n",
      "step = 4964200: loss = 4.10628080368042\n",
      "step = 4964400: loss = 4.315569877624512\n",
      "step = 4964600: loss = 4.4015421867370605\n",
      "step = 4964800: loss = 2.988774299621582\n",
      "step = 4965000: loss = 3.859924554824829\n",
      "step = 4965000: Average Return = 2.9000000953674316\n",
      "step = 4965200: loss = 3.4699511528015137\n",
      "step = 4965400: loss = 3.255347728729248\n",
      "step = 4965600: loss = 3.8242270946502686\n",
      "step = 4965800: loss = 4.829150199890137\n",
      "step = 4966000: loss = 2.919410228729248\n",
      "step = 4966200: loss = 3.1219825744628906\n",
      "step = 4966400: loss = 4.1319708824157715\n",
      "step = 4966600: loss = 2.916490316390991\n",
      "step = 4966800: loss = 4.815862655639648\n",
      "step = 4967000: loss = 3.3605782985687256\n",
      "step = 4967200: loss = 4.197283744812012\n",
      "step = 4967400: loss = 4.077281951904297\n",
      "step = 4967600: loss = 4.662307262420654\n",
      "step = 4967800: loss = 4.702057361602783\n",
      "step = 4968000: loss = 2.8765835762023926\n",
      "step = 4968200: loss = 4.224284648895264\n",
      "step = 4968400: loss = 3.1566646099090576\n",
      "step = 4968600: loss = 3.9977333545684814\n",
      "step = 4968800: loss = 4.004542350769043\n",
      "step = 4969000: loss = 4.404921054840088\n",
      "step = 4969200: loss = 2.336860418319702\n",
      "step = 4969400: loss = 2.9859933853149414\n",
      "step = 4969600: loss = 2.9414401054382324\n",
      "step = 4969800: loss = 3.6642448902130127\n",
      "step = 4970000: loss = 4.688093662261963\n",
      "step = 4970000: Average Return = 1.9500000476837158\n",
      "step = 4970200: loss = 3.1116294860839844\n",
      "step = 4970400: loss = 3.73271107673645\n",
      "step = 4970600: loss = 4.552331447601318\n",
      "step = 4970800: loss = 3.2176127433776855\n",
      "step = 4971000: loss = 4.05413818359375\n",
      "step = 4971200: loss = 2.7109007835388184\n",
      "step = 4971400: loss = 3.8616902828216553\n",
      "step = 4971600: loss = 3.440384864807129\n",
      "step = 4971800: loss = 4.697426795959473\n",
      "step = 4972000: loss = 4.189640522003174\n",
      "step = 4972200: loss = 4.0894269943237305\n",
      "step = 4972400: loss = 4.0125579833984375\n",
      "step = 4972600: loss = 4.716038703918457\n",
      "step = 4972800: loss = 3.8445615768432617\n",
      "step = 4973000: loss = 4.037746429443359\n",
      "step = 4973200: loss = 3.725276470184326\n",
      "step = 4973400: loss = 4.230512619018555\n",
      "step = 4973600: loss = 3.015683174133301\n",
      "step = 4973800: loss = 2.862607002258301\n",
      "step = 4974000: loss = 3.3442752361297607\n",
      "step = 4974200: loss = 2.961590528488159\n",
      "step = 4974400: loss = 4.311840534210205\n",
      "step = 4974600: loss = 4.7250657081604\n",
      "step = 4974800: loss = 2.7880492210388184\n",
      "step = 4975000: loss = 3.711745023727417\n",
      "step = 4975000: Average Return = 3.6500000953674316\n",
      "step = 4975200: loss = 3.901388168334961\n",
      "step = 4975400: loss = 2.3800036907196045\n",
      "step = 4975600: loss = 4.855053424835205\n",
      "step = 4975800: loss = 4.812573432922363\n",
      "step = 4976000: loss = 4.723766803741455\n",
      "step = 4976200: loss = 3.0671889781951904\n",
      "step = 4976400: loss = 5.601990699768066\n",
      "step = 4976600: loss = 4.679601669311523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 4976800: loss = 3.3579647541046143\n",
      "step = 4977000: loss = 3.761935234069824\n",
      "step = 4977200: loss = 4.875665664672852\n",
      "step = 4977400: loss = 2.7849605083465576\n",
      "step = 4977600: loss = 3.109919309616089\n",
      "step = 4977800: loss = 4.205591201782227\n",
      "step = 4978000: loss = 3.5439157485961914\n",
      "step = 4978200: loss = 4.093756198883057\n",
      "step = 4978400: loss = 2.810349702835083\n",
      "step = 4978600: loss = 4.142789363861084\n",
      "step = 4978800: loss = 4.60150146484375\n",
      "step = 4979000: loss = 3.4476194381713867\n",
      "step = 4979200: loss = 4.469531059265137\n",
      "step = 4979400: loss = 3.642747163772583\n",
      "step = 4979600: loss = 4.138186931610107\n",
      "step = 4979800: loss = 3.2409069538116455\n",
      "step = 4980000: loss = 4.408384323120117\n",
      "step = 4980000: Average Return = 5.5\n",
      "step = 4980200: loss = 3.584911346435547\n",
      "step = 4980400: loss = 3.5249876976013184\n",
      "step = 4980600: loss = 5.6776299476623535\n",
      "step = 4980800: loss = 4.55318546295166\n",
      "step = 4981000: loss = 4.615659236907959\n",
      "step = 4981200: loss = 3.2131853103637695\n",
      "step = 4981400: loss = 3.4260613918304443\n",
      "step = 4981600: loss = 3.7269811630249023\n",
      "step = 4981800: loss = 3.087369680404663\n",
      "step = 4982000: loss = 3.7409536838531494\n",
      "step = 4982200: loss = 4.707636833190918\n",
      "step = 4982400: loss = 4.317963123321533\n",
      "step = 4982600: loss = 4.1137471199035645\n",
      "step = 4982800: loss = 4.260922908782959\n",
      "step = 4983000: loss = 3.059967041015625\n",
      "step = 4983200: loss = 4.3154168128967285\n",
      "step = 4983400: loss = 3.4931609630584717\n",
      "step = 4983600: loss = 4.259557247161865\n",
      "step = 4983800: loss = 3.768738269805908\n",
      "step = 4984000: loss = 4.2319135665893555\n",
      "step = 4984200: loss = 3.7606494426727295\n",
      "step = 4984400: loss = 2.5524795055389404\n",
      "step = 4984600: loss = 3.3857738971710205\n",
      "step = 4984800: loss = 3.9981582164764404\n",
      "step = 4985000: loss = 3.434110641479492\n",
      "step = 4985000: Average Return = 2.0999999046325684\n",
      "step = 4985200: loss = 3.7626326084136963\n",
      "step = 4985400: loss = 3.136306047439575\n",
      "step = 4985600: loss = 4.3421430587768555\n",
      "step = 4985800: loss = 3.196553945541382\n",
      "step = 4986000: loss = 3.5278422832489014\n",
      "step = 4986200: loss = 3.914588212966919\n",
      "step = 4986400: loss = 3.952043056488037\n",
      "step = 4986600: loss = 4.27162504196167\n",
      "step = 4986800: loss = 3.729437828063965\n",
      "step = 4987000: loss = 3.749113082885742\n",
      "step = 4987200: loss = 4.539154052734375\n",
      "step = 4987400: loss = 3.5817511081695557\n",
      "step = 4987600: loss = 3.6954357624053955\n",
      "step = 4987800: loss = 4.02528190612793\n",
      "step = 4988000: loss = 4.590022087097168\n",
      "step = 4988200: loss = 4.124830722808838\n",
      "step = 4988400: loss = 3.9281229972839355\n",
      "step = 4988600: loss = 3.889467716217041\n",
      "step = 4988800: loss = 3.552978992462158\n",
      "step = 4989000: loss = 3.7681591510772705\n",
      "step = 4989200: loss = 4.171632766723633\n",
      "step = 4989400: loss = 3.0277671813964844\n",
      "step = 4989600: loss = 3.0502960681915283\n",
      "step = 4989800: loss = 3.303088903427124\n",
      "step = 4990000: loss = 5.051947116851807\n",
      "step = 4990000: Average Return = 3.950000047683716\n",
      "step = 4990200: loss = 3.545198917388916\n",
      "step = 4990400: loss = 3.589853048324585\n",
      "step = 4990600: loss = 3.451526641845703\n",
      "step = 4990800: loss = 4.034358024597168\n",
      "step = 4991000: loss = 4.3719482421875\n",
      "step = 4991200: loss = 4.510805606842041\n",
      "step = 4991400: loss = 3.8465685844421387\n",
      "step = 4991600: loss = 4.061065196990967\n",
      "step = 4991800: loss = 4.136501789093018\n",
      "step = 4992000: loss = 3.74407958984375\n",
      "step = 4992200: loss = 4.889155864715576\n",
      "step = 4992400: loss = 4.3312225341796875\n",
      "step = 4992600: loss = 3.7477738857269287\n",
      "step = 4992800: loss = 4.446964740753174\n",
      "step = 4993000: loss = 3.8131866455078125\n",
      "step = 4993200: loss = 4.179837226867676\n",
      "step = 4993400: loss = 3.3226723670959473\n",
      "step = 4993600: loss = 3.0384342670440674\n",
      "step = 4993800: loss = 3.679344654083252\n",
      "step = 4994000: loss = 3.6992175579071045\n",
      "step = 4994200: loss = 4.37532901763916\n",
      "step = 4994400: loss = 3.8954601287841797\n",
      "step = 4994600: loss = 4.683183193206787\n",
      "step = 4994800: loss = 3.1927645206451416\n",
      "step = 4995000: loss = 3.948431968688965\n",
      "step = 4995000: Average Return = 3.25\n",
      "step = 4995200: loss = 4.652045249938965\n",
      "step = 4995400: loss = 4.21489143371582\n",
      "step = 4995600: loss = 3.364938259124756\n",
      "step = 4995800: loss = 3.796192169189453\n",
      "step = 4996000: loss = 3.032719373703003\n",
      "step = 4996200: loss = 4.047736644744873\n",
      "step = 4996400: loss = 3.747659206390381\n",
      "step = 4996600: loss = 5.271712303161621\n",
      "step = 4996800: loss = 4.032424449920654\n",
      "step = 4997000: loss = 3.7458579540252686\n",
      "step = 4997200: loss = 3.042783737182617\n",
      "step = 4997400: loss = 4.489637851715088\n",
      "step = 4997600: loss = 4.069399356842041\n",
      "step = 4997800: loss = 4.058137893676758\n",
      "step = 4998000: loss = 3.8777012825012207\n",
      "step = 4998200: loss = 4.210346698760986\n",
      "step = 4998400: loss = 4.501868724822998\n",
      "step = 4998600: loss = 4.321313858032227\n",
      "step = 4998800: loss = 2.851902484893799\n",
      "step = 4999000: loss = 3.0752999782562256\n",
      "step = 4999200: loss = 4.132821559906006\n",
      "step = 4999400: loss = 3.9995553493499756\n",
      "step = 4999600: loss = 4.932471752166748\n",
      "step = 4999800: loss = 3.7625210285186768\n",
      "step = 5000000: loss = 3.241309642791748\n",
      "step = 5000000: Average Return = 5.849999904632568\n",
      "step = 5000200: loss = 3.3434033393859863\n",
      "step = 5000400: loss = 3.7165892124176025\n",
      "step = 5000600: loss = 3.722872257232666\n",
      "step = 5000800: loss = 3.5646331310272217\n",
      "step = 5001000: loss = 3.5140602588653564\n",
      "step = 5001200: loss = 3.816689968109131\n",
      "step = 5001400: loss = 3.6019484996795654\n",
      "step = 5001600: loss = 3.6625685691833496\n",
      "step = 5001800: loss = 4.312441825866699\n",
      "step = 5002000: loss = 5.643069267272949\n",
      "step = 5002200: loss = 3.3867416381835938\n",
      "step = 5002400: loss = 2.9489238262176514\n",
      "step = 5002600: loss = 2.7291741371154785\n",
      "step = 5002800: loss = 3.7943501472473145\n",
      "step = 5003000: loss = 3.92205810546875\n",
      "step = 5003200: loss = 2.807427406311035\n",
      "step = 5003400: loss = 4.226377010345459\n",
      "step = 5003600: loss = 4.960261821746826\n",
      "step = 5003800: loss = 3.336087465286255\n",
      "step = 5004000: loss = 4.625830173492432\n",
      "step = 5004200: loss = 4.62759256362915\n",
      "step = 5004400: loss = 4.723683834075928\n",
      "step = 5004600: loss = 3.8573174476623535\n",
      "step = 5004800: loss = 3.489792823791504\n",
      "step = 5005000: loss = 3.235382318496704\n",
      "step = 5005000: Average Return = 5.050000190734863\n",
      "step = 5005200: loss = 4.053924083709717\n",
      "step = 5005400: loss = 3.687288761138916\n",
      "step = 5005600: loss = 4.011312007904053\n",
      "step = 5005800: loss = 3.0884315967559814\n",
      "step = 5006000: loss = 4.3900604248046875\n",
      "step = 5006200: loss = 3.646103620529175\n",
      "step = 5006400: loss = 3.738659620285034\n",
      "step = 5006600: loss = 3.8856709003448486\n",
      "step = 5006800: loss = 4.323089599609375\n",
      "step = 5007000: loss = 3.8919568061828613\n",
      "step = 5007200: loss = 3.332315683364868\n",
      "step = 5007400: loss = 4.684963226318359\n",
      "step = 5007600: loss = 3.82498836517334\n",
      "step = 5007800: loss = 4.376257419586182\n",
      "step = 5008000: loss = 3.9605963230133057\n",
      "step = 5008200: loss = 4.192959308624268\n",
      "step = 5008400: loss = 4.768542766571045\n",
      "step = 5008600: loss = 5.486956596374512\n",
      "step = 5008800: loss = 3.718041181564331\n",
      "step = 5009000: loss = 3.9569926261901855\n",
      "step = 5009200: loss = 3.833505153656006\n",
      "step = 5009400: loss = 2.446143627166748\n",
      "step = 5009600: loss = 4.90275239944458\n",
      "step = 5009800: loss = 3.3756914138793945\n",
      "step = 5010000: loss = 4.103477954864502\n",
      "step = 5010000: Average Return = 3.200000047683716\n",
      "step = 5010200: loss = 4.758276462554932\n",
      "step = 5010400: loss = 3.9940783977508545\n",
      "step = 5010600: loss = 4.227229118347168\n",
      "step = 5010800: loss = 3.875333070755005\n",
      "step = 5011000: loss = 3.3989176750183105\n",
      "step = 5011200: loss = 2.705430746078491\n",
      "step = 5011400: loss = 5.142698287963867\n",
      "step = 5011600: loss = 4.512329578399658\n",
      "step = 5011800: loss = 4.048313617706299\n",
      "step = 5012000: loss = 4.628472328186035\n",
      "step = 5012200: loss = 3.7476677894592285\n",
      "step = 5012400: loss = 3.3587920665740967\n",
      "step = 5012600: loss = 3.429288148880005\n",
      "step = 5012800: loss = 4.131491184234619\n",
      "step = 5013000: loss = 4.380833148956299\n",
      "step = 5013200: loss = 3.3297386169433594\n",
      "step = 5013400: loss = 3.3367631435394287\n",
      "step = 5013600: loss = 4.513462066650391\n",
      "step = 5013800: loss = 3.4526333808898926\n",
      "step = 5014000: loss = 2.733593463897705\n",
      "step = 5014200: loss = 5.375757217407227\n",
      "step = 5014400: loss = 3.7757487297058105\n",
      "step = 5014600: loss = 3.6703062057495117\n",
      "step = 5014800: loss = 4.584756374359131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5015000: loss = 3.810723066329956\n",
      "step = 5015000: Average Return = 5.099999904632568\n",
      "step = 5015200: loss = 3.4523515701293945\n",
      "step = 5015400: loss = 4.203551769256592\n",
      "step = 5015600: loss = 2.5531609058380127\n",
      "step = 5015800: loss = 3.8097758293151855\n",
      "step = 5016000: loss = 4.030889987945557\n",
      "step = 5016200: loss = 4.294570446014404\n",
      "step = 5016400: loss = 2.7063241004943848\n",
      "step = 5016600: loss = 3.548515558242798\n",
      "step = 5016800: loss = 3.8662173748016357\n",
      "step = 5017000: loss = 3.6241908073425293\n",
      "step = 5017200: loss = 3.598620891571045\n",
      "step = 5017400: loss = 5.329859733581543\n",
      "step = 5017600: loss = 4.092231750488281\n",
      "step = 5017800: loss = 3.1809685230255127\n",
      "step = 5018000: loss = 4.215642929077148\n",
      "step = 5018200: loss = 3.576448917388916\n",
      "step = 5018400: loss = 3.5604429244995117\n",
      "step = 5018600: loss = 3.2429089546203613\n",
      "step = 5018800: loss = 3.9642341136932373\n",
      "step = 5019000: loss = 4.394250869750977\n",
      "step = 5019200: loss = 4.394212245941162\n",
      "step = 5019400: loss = 3.6319313049316406\n",
      "step = 5019600: loss = 3.798698902130127\n",
      "step = 5019800: loss = 4.361015319824219\n",
      "step = 5020000: loss = 3.230250120162964\n",
      "step = 5020000: Average Return = 5.849999904632568\n",
      "step = 5020200: loss = 3.22184681892395\n",
      "step = 5020400: loss = 3.5793886184692383\n",
      "step = 5020600: loss = 3.43114972114563\n",
      "step = 5020800: loss = 2.861966371536255\n",
      "step = 5021000: loss = 3.4641478061676025\n",
      "step = 5021200: loss = 4.690880298614502\n",
      "step = 5021400: loss = 3.654771089553833\n",
      "step = 5021600: loss = 3.436485528945923\n",
      "step = 5021800: loss = 4.44436502456665\n",
      "step = 5022000: loss = 4.3968186378479\n",
      "step = 5022200: loss = 3.150136947631836\n",
      "step = 5022400: loss = 3.981248140335083\n",
      "step = 5022600: loss = 3.791168212890625\n",
      "step = 5022800: loss = 3.9581260681152344\n",
      "step = 5023000: loss = 4.065291404724121\n",
      "step = 5023200: loss = 5.463237762451172\n",
      "step = 5023400: loss = 3.9116134643554688\n",
      "step = 5023600: loss = 4.8344831466674805\n",
      "step = 5023800: loss = 3.198042154312134\n",
      "step = 5024000: loss = 4.07816743850708\n",
      "step = 5024200: loss = 3.592369556427002\n",
      "step = 5024400: loss = 3.1841673851013184\n",
      "step = 5024600: loss = 3.816185712814331\n",
      "step = 5024800: loss = 4.160337448120117\n",
      "step = 5025000: loss = 4.657270431518555\n",
      "step = 5025000: Average Return = 4.599999904632568\n",
      "step = 5025200: loss = 4.642647743225098\n",
      "step = 5025400: loss = 4.260679244995117\n",
      "step = 5025600: loss = 3.452220916748047\n",
      "step = 5025800: loss = 4.118937015533447\n",
      "step = 5026000: loss = 4.949969291687012\n",
      "step = 5026200: loss = 4.743642330169678\n",
      "step = 5026400: loss = 3.986729145050049\n",
      "step = 5026600: loss = 2.744965076446533\n",
      "step = 5026800: loss = 3.8357927799224854\n",
      "step = 5027000: loss = 3.2533798217773438\n",
      "step = 5027200: loss = 3.085894823074341\n",
      "step = 5027400: loss = 4.931671142578125\n",
      "step = 5027600: loss = 3.859450101852417\n",
      "step = 5027800: loss = 3.311296224594116\n",
      "step = 5028000: loss = 4.412472724914551\n",
      "step = 5028200: loss = 5.189568996429443\n",
      "step = 5028400: loss = 3.798511266708374\n",
      "step = 5028600: loss = 4.133821487426758\n",
      "step = 5028800: loss = 3.607048511505127\n",
      "step = 5029000: loss = 3.7560839653015137\n",
      "step = 5029200: loss = 3.5506772994995117\n",
      "step = 5029400: loss = 5.497696399688721\n",
      "step = 5029600: loss = 3.44854474067688\n",
      "step = 5029800: loss = 3.4716296195983887\n",
      "step = 5030000: loss = 3.906100034713745\n",
      "step = 5030000: Average Return = 3.4000000953674316\n",
      "step = 5030200: loss = 4.961550235748291\n",
      "step = 5030400: loss = 5.7637553215026855\n",
      "step = 5030600: loss = 3.0864717960357666\n",
      "step = 5030800: loss = 3.9316508769989014\n",
      "step = 5031000: loss = 3.8649721145629883\n",
      "step = 5031200: loss = 4.383298873901367\n",
      "step = 5031400: loss = 3.2123374938964844\n",
      "step = 5031600: loss = 4.639678478240967\n",
      "step = 5031800: loss = 3.7978668212890625\n",
      "step = 5032000: loss = 4.519543647766113\n",
      "step = 5032200: loss = 3.5971529483795166\n",
      "step = 5032400: loss = 3.811755895614624\n",
      "step = 5032600: loss = 3.514254093170166\n",
      "step = 5032800: loss = 4.0172271728515625\n",
      "step = 5033000: loss = 4.936061382293701\n",
      "step = 5033200: loss = 4.710914134979248\n",
      "step = 5033400: loss = 3.6327993869781494\n",
      "step = 5033600: loss = 4.5518670082092285\n",
      "step = 5033800: loss = 2.367547035217285\n",
      "step = 5034000: loss = 3.8136632442474365\n",
      "step = 5034200: loss = 2.7919957637786865\n",
      "step = 5034400: loss = 4.191163063049316\n",
      "step = 5034600: loss = 2.9988436698913574\n",
      "step = 5034800: loss = 4.465790748596191\n",
      "step = 5035000: loss = 3.3132104873657227\n",
      "step = 5035000: Average Return = 4.349999904632568\n",
      "step = 5035200: loss = 4.26651668548584\n",
      "step = 5035400: loss = 3.4014644622802734\n",
      "step = 5035600: loss = 4.276580333709717\n",
      "step = 5035800: loss = 4.421663761138916\n",
      "step = 5036000: loss = 4.0161333084106445\n",
      "step = 5036200: loss = 4.3121137619018555\n",
      "step = 5036400: loss = 4.31340217590332\n",
      "step = 5036600: loss = 3.5453062057495117\n",
      "step = 5036800: loss = 5.468055248260498\n",
      "step = 5037000: loss = 4.389222621917725\n",
      "step = 5037200: loss = 3.809597969055176\n",
      "step = 5037400: loss = 3.6862053871154785\n",
      "step = 5037600: loss = 4.311336517333984\n",
      "step = 5037800: loss = 3.435519218444824\n",
      "step = 5038000: loss = 3.328206777572632\n",
      "step = 5038200: loss = 4.0571513175964355\n",
      "step = 5038400: loss = 4.221778392791748\n",
      "step = 5038600: loss = 4.041654109954834\n",
      "step = 5038800: loss = 2.4830451011657715\n",
      "step = 5039000: loss = 4.0764594078063965\n",
      "step = 5039200: loss = 3.258690118789673\n",
      "step = 5039400: loss = 3.080867290496826\n",
      "step = 5039600: loss = 3.6466143131256104\n",
      "step = 5039800: loss = 3.9087493419647217\n",
      "step = 5040000: loss = 3.831777334213257\n",
      "step = 5040000: Average Return = 4.5\n",
      "step = 5040200: loss = 2.5123085975646973\n",
      "step = 5040400: loss = 3.5379650592803955\n",
      "step = 5040600: loss = 3.8461201190948486\n",
      "step = 5040800: loss = 3.3266830444335938\n",
      "step = 5041000: loss = 3.69827938079834\n",
      "step = 5041200: loss = 4.8276214599609375\n",
      "step = 5041400: loss = 3.186758041381836\n",
      "step = 5041600: loss = 3.726701498031616\n",
      "step = 5041800: loss = 5.017693042755127\n",
      "step = 5042000: loss = 4.084111213684082\n",
      "step = 5042200: loss = 2.9451904296875\n",
      "step = 5042400: loss = 3.7010929584503174\n",
      "step = 5042600: loss = 3.2765438556671143\n",
      "step = 5042800: loss = 3.871013641357422\n",
      "step = 5043000: loss = 4.028834819793701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m collect_data(train_env, agent\u001b[38;5;241m.\u001b[39mcollect_policy, replay_buffer, collect_steps_per_iteration)\n\u001b[0;32m      4\u001b[0m experience, unused_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m----> 5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m      7\u001b[0m step \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_step_counter\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        train_checkpointer.save(train_step_counter)\n",
    "        with open(\"Checkpoints2/returns.txt\", \"w\") as txt:\n",
    "            for item in returns:\n",
    "                txt.write(str(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eeacc26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47000002503395083, 10.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, train_step_counter + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35468749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandomWins = 0\\nagentWins = 0\\nfor i in range(100000):\\n    print(i)\\n    temp = playRandom()\\n    if temp == 1:\\n        randomWins += 1\\n    elif temp == 2:\\n        agentWins += 1\\n\\nprint(\"Random Policy Wins: \" + str(randomWins))\\nprint(\"Agent Policy Wins: \" + str(agentWins))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the agent human vs ai\n",
    "def playHuman():\n",
    "    policy = agent.policy\n",
    "    running = True\n",
    "    env = eval_env\n",
    "    state = env.reset()\n",
    "    pygame.init()\n",
    "\n",
    "    while running:\n",
    "        to_move = True\n",
    "        env.render(\"human\")\n",
    "        mouse = pygame.mouse.get_pos()\n",
    "        \n",
    "        if env._envs[0].current_player == 1:\n",
    "            if 75 + 100 > mouse[0] > 75 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][0]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(0)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "            elif 275 + 100 > mouse[0] > 275 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][1]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(4)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "            elif 475 + 100 > mouse[0] > 475 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][2]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(8)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "        elif env._envs[0].current_player == 2:\n",
    "            time.sleep(2)\n",
    "            action_step = policy.action(state)\n",
    "            env.step(action_step.action)\n",
    "            env.render(\"human\")\n",
    "            to_move = False\n",
    "\n",
    "        elif env._envs[0].current_player == -1 or env._envs[0].game_is_over():\n",
    "            env.render(\"human\")\n",
    "            print(\"Winner: Player \" + str(eval_py_env.current_player))\n",
    "            running = False\n",
    "                    \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                \n",
    "def playRandom():\n",
    "    env = eval_env\n",
    "    random = random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),env.action_spec())\n",
    "    policy = agent.policy\n",
    "    running = True\n",
    "    state = env.reset()\n",
    "    pygame.init()\n",
    "    \n",
    "    while running:\n",
    "        to_move = True\n",
    "        \n",
    "        if env._envs[0].current_player == 1 and not env._envs[0].game_is_over():\n",
    "            action_step = random.action(state)\n",
    "            state = env.step(action_step.action)\n",
    "            to_move = False\n",
    "            \n",
    "        elif env._envs[0].current_player == 2 and not env._envs[0].game_is_over():\n",
    "            action_step = policy.action(state)\n",
    "            state = env.step(action_step.action)\n",
    "            to_move = False\n",
    "\n",
    "        elif env._envs[0].current_player == -1 or env._envs[0].game_is_over():\n",
    "            running = False\n",
    "            pygame.quit()\n",
    "            return env._envs[0].remaining_players[0]\n",
    "        \n",
    "        #uncomment to render\n",
    "        #env.render(\"human\")\n",
    "                    \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                \n",
    "    return 0\n",
    "        \n",
    "playHuman()\n",
    "\n",
    "'''\n",
    "randomWins = 0\n",
    "agentWins = 0\n",
    "for i in range(100000):\n",
    "    print(i)\n",
    "    temp = playRandom()\n",
    "    if temp == 1:\n",
    "        randomWins += 1\n",
    "    elif temp == 2:\n",
    "        agentWins += 1\n",
    "\n",
    "print(\"Random Policy Wins: \" + str(randomWins))\n",
    "print(\"Agent Policy Wins: \" + str(agentWins))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df9111cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot win rate bar chart (manual entry of win %)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel('Win Rate Percentage')\n",
    "ax.set_xlabel('Agent')\n",
    "Agents = ['1', '2', '3', '4']\n",
    "WinRates = [64.777, 64.776, 64.979, 64.695]\n",
    "ax.bar(Agents,WinRates)\n",
    "for i in range(len(Agents)):\n",
    "    plt.text(i, WinRates[i]//2, str(WinRates[i])+\"%\", ha = 'center')\n",
    "plt.ylim((0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be12a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "playHuman()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a9c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
