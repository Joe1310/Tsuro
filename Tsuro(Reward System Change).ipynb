{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3e6a66",
   "metadata": {},
   "source": [
    "The Tile images used in the project were created by Matthias Felleisen and found on his website\n",
    "https://felleisen.org/matthias/4500-f19/tiles.html\n",
    "\n",
    "1. Filename: Tsuro(Reward System Change)\n",
    "2. Version number: 1\n",
    "3. Creation date: 4th October 2022\n",
    "4. Last modification date: 27th April 2023\n",
    "5. Authorâ€™s name: Joseph Henry\n",
    "6. Purpose of the program: Define a reinforcement learning environment based on the board game Tsuro along with an agent to train using the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bcaa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_agents==0.15.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.3.0)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.23.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.19.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (9.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.2.1)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.14.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.16.0)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.5.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym<=0.23.0,>=0.17.0->tf_agents==0.15.0) (0.0.8)\n",
      "Requirement already satisfied: decorator in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.1.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\horridjoe\\opencv\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\horridjoe\\opencv\\lib\\site-packages (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install tf_agents==0.15.0\n",
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4929ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# player piece colors [player1, player2]\n",
    "colors = ['#FF0000', '#0000FF']\n",
    "\n",
    "# paths for each tile 1-35\n",
    "node_combinations = [ \n",
    "    [(0,3), (1,5), (2,6), (4,7)], [(0,4), (1,5), (2,6), (3,7)], [(0,3), (1,6), (2,5), (4,7)], [(0,6), (1,5), (2,4), (3,7)],\n",
    "    [(0,1), (2,3), (4,5), (6,7)], [(0,4), (1,5), (2,3), (6,7)], [(0,6), (1,5), (2,3), (4,7)], [(0,5), (1,4), (2,7), (3,6)],\n",
    "    [(0,5), (1,4), (2,6), (3,7)], [(0,3), (1,4), (2,5), (6,7)], [(0,6), (1,4), (2,5), (3,7)], [(0,5), (1,4), (2,3), (6,7)],\n",
    "    [(0,2), (1,3), (4,6), (5,7)], [(0,2), (1,3), (4,5), (6,7)], [(0,5), (1,3), (2,7), (4,6)], [(0,6), (1,3), (2,7), (4,5)],\n",
    "    [(0,4), (1,3), (2,6), (5,7)], [(0,5), (1,3), (2,6), (4,7)], [(0,4), (1,3), (2,5), (6,7)], [(0,6), (1,3), (2,5), (4,7)],\n",
    "    [(0,5), (1,3), (2,4), (6,7)], [(0,6), (1,3), (2,4), (5,7)], [(0,3), (1,2), (4,7), (5,6)], [(0,3), (1,2), (4,6), (5,7)],\n",
    "    [(0,3), (1,2), (4,5), (6,7)], [(0,4), (1,2), (3,7), (5,6)], [(0,5), (1,2), (3,7), (4,6)], [(0,6), (1,2), (3,7), (4,5)],\n",
    "    [(0,4), (1,2), (3,6), (5,7)], [(0,5), (1,2), (3,6), (4,7)], [(0,4), (1,2), (3,5), (6,7)], [(0,6), (1,2), (3,5), (4,7)],\n",
    "    [(0,5), (1,2), (3,4), (6,7)], [(0,6), (1,2), (3,4), (5,7)], [(0,7), (1,2), (3,4), (5,6)]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7550d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class modelling the tile cards used within Tsuro\n",
    "class Tile():\n",
    "    def __init__(self, tile_num, tile_connections):\n",
    "        self.tile_num = tile_num\n",
    "        self.image = pygame.image.load(\"TsuroImages/\" + str(tile_num) + \".png\")\n",
    "        self.image = pygame.transform.scale(self.image, (100, 100))\n",
    "        self.tile_connections = tile_connections\n",
    "        self.rotation = 1\n",
    "    # decide where the player should move to\n",
    "    def move(self, current_node):\n",
    "        next_node = 0\n",
    "        next_player_tile = 0\n",
    "        for connection in self.tile_connections:\n",
    "            if current_node in connection:\n",
    "                n1, n2 = connection\n",
    "                if n1 == current_node:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n2)\n",
    "                else:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n1)\n",
    "                return next_node, next_player_tile, next_x, next_y\n",
    "        raise Exception(\"Issue in moving players\")\n",
    "    \n",
    "    # update number of times rotation should be applied to connections and image\n",
    "    def rotate_tile(self, rotate):\n",
    "        self.image = pygame.transform.rotate(self.image, rotate * -90)\n",
    "        self.tile_connections = [tuple((element + (2 * rotate)) % 8 for element in couple ) for couple in self.tile_connections]\n",
    "        self.rotation = 1 if (self.rotation + 1 % 4 == 0) else self.rotation + 1\n",
    "    \n",
    "    # get the current rotation value of the tile\n",
    "    def get_rotation(self):\n",
    "        return self.rotation\n",
    "    \n",
    "    # decide which tile and node the player should move to from its current position on this tile\n",
    "    def new_tile_node(self, current_node):\n",
    "        next_node = 0\n",
    "        next_x = 0\n",
    "        next_y = 0\n",
    "        next_player_tile = 0\n",
    "        match current_node:\n",
    "            case 0:\n",
    "                next_node = 3\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case 1:\n",
    "                next_node = 6\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 2:\n",
    "                next_node = 5\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 3:\n",
    "                next_node = 0\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 4:\n",
    "                next_node = 7\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 5:\n",
    "                next_node = 2\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 6:\n",
    "                next_node = 1\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 7:\n",
    "                next_node = 4\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case _:\n",
    "                raise Exception(\"Issue in tile board\")\n",
    "                \n",
    "        return next_node, next_player_tile, next_x, next_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f8391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to model the board game Tsuro as a reinforcement learning environment.\n",
    "class TsuroEnv(gym.Env):\n",
    "    # initialise the environment\n",
    "    def __init__(self):\n",
    "        self.screen = None\n",
    "        self.current_player = 1\n",
    "        self.num_tiles = 35\n",
    "        self.tile_board_size = (6, 6)\n",
    "        self.rotation_board_size = (6, 6)\n",
    "        self.player_board_size = (36,8)\n",
    "        self.num_players = 2\n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "\n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "        \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        self.rotation_board = np.zeros(self.rotation_board_size, dtype = int)\n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "\n",
    "        self.action_space = spaces.Discrete(11)\n",
    "        self.observation_space = spaces.Box(low=-1, high=140, shape=(327,))\n",
    "        \n",
    "    def get_tile_obs(self):\n",
    "        temp = self.rotation_board.flatten()\n",
    "        temp1 = self.tile_board.flatten()\n",
    "        for i in range(35):\n",
    "            temp[i] = ((temp[i] - 1) * 35) + temp1[i]\n",
    "        return temp\n",
    "        \n",
    "    # Resets the environment to default state\n",
    "    def reset(self): \n",
    "        self.current_player = 1\n",
    "        self.rotation_board = np.zeros(self.rotation_board_size, dtype = int)\n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "        \n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "            \n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "            \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        #########################################\n",
    "        #TODO: TESTING STUFF TO BE REMOVED LATER#\n",
    "        #########################################\n",
    "        for i in range(self.num_players):\n",
    "            self.player_board[random.randint(0,5)][i+1] = i+1\n",
    "            \n",
    "        initial_obs = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "\n",
    "        return initial_obs\n",
    "    \n",
    "    # Makes a move in the game based on inputs from player or AI\n",
    "    def step(self, action = -2, move = -1):\n",
    "        if move == -1:\n",
    "            card, rotate = self.get_card(action)\n",
    "        else:\n",
    "            card = move\n",
    "            rotate = 0\n",
    "        \n",
    "        action = self.player_tiles[self.current_player - 1][card]\n",
    "        \n",
    "        if action == -1:\n",
    "            observation = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "            reward = -1\n",
    "            done = 0\n",
    "            return observation, reward, done, {}\n",
    "            \n",
    "        # Removes used tile and adds new tile from deck to hand\n",
    "        self.player_tiles[self.current_player-1].remove(action)\n",
    "        if len(self.remaining_tiles) > 0:\n",
    "            self.player_tiles[self.current_player-1].append(self.remaining_tiles.pop())\n",
    "        else:\n",
    "            self.player_tiles[self.current_player-1].append(-1)\n",
    "        \n",
    "        # Rotates tile (Only used by AI)\n",
    "        self.tiles[action].rotate_tile(rotate)\n",
    "        \n",
    "        reward = 0\n",
    "        self.place_tile(action+1)\n",
    "        self.move_players()\n",
    "        reward = self.reward_function()\n",
    "        if self.game_is_over():\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        self.current_player = self.next_player()\n",
    "        observation = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    # Decides if the game is over\n",
    "    def game_is_over(self):\n",
    "        if len(self.remaining_players) <= 1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Decides the reward (Only used for AI)\n",
    "    def reward_function(self):\n",
    "        if not self.game_is_over():\n",
    "            return 1\n",
    "        if self.game_is_over() and self.remaining_players[0] == self.current_player:\n",
    "            return 5\n",
    "        return -1\n",
    "    \n",
    "    # Places tile in self.tile_board\n",
    "    def place_tile(self, tile):\n",
    "        tile_number, node_number = np.where(self.player_board == self.current_player)\n",
    "        x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "        self.tile_board[x][y] += tile\n",
    "        self.rotation_board[x][y] += self.tiles[tile-1].get_rotation()\n",
    "    \n",
    "    # Moves player piece in self.player_board\n",
    "    def move_players(self):\n",
    "        for player in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == player)\n",
    "            x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "            x = x[0]\n",
    "            y = y[0]\n",
    "            while self.tile_board[x][y] != 0:\n",
    "                tile = self.tiles[(self.tile_board[x][y])-1]\n",
    "                next_node, next_player_tile, next_x, next_y = tile.move(node_number)\n",
    "                self.player_board[tile_number[0]][node_number[0]] = 0\n",
    "                if ((tile_number[0] % 6 == 0) and ((tile_number[0] + next_player_tile) % 6 == 5)) or (tile_number[0] + next_player_tile < 0) or (tile_number[0] + next_player_tile > 35) or ((tile_number[0] % 6 == 5) and ((tile_number[0] + next_player_tile) % 6 == 0)):\n",
    "                    self.remaining_players.remove(player)\n",
    "                    break\n",
    "                else:\n",
    "                    self.player_board[tile_number[0] + next_player_tile][next_node] = player\n",
    "                    x += next_x\n",
    "                    y += next_y\n",
    "                    tile_number, node_number = np.where(self.player_board == player)\n",
    "    \n",
    "    # Quotient and Remainder\n",
    "    def euclidean_division(self, x, y = 6):\n",
    "        return x % y, x // y\n",
    "    \n",
    "    # Action (card, rotation) from input\n",
    "    def get_card(self, x, y = 4):\n",
    "        return  x // y, x % y\n",
    "\n",
    "    # Decide whos turn it is\n",
    "    def next_player(self):\n",
    "        if len(self.remaining_players) == 0:\n",
    "            return -1\n",
    "        if self.current_player not in self.remaining_players:\n",
    "            for player in self.remaining_players:\n",
    "                if player > self.current_player:\n",
    "                    return player\n",
    "                else:\n",
    "                    return self.remaining_players[0]\n",
    "        return self.remaining_players[(self.remaining_players.index(self.current_player) + 1) % len(self.remaining_players)]\n",
    "        \n",
    "    # Render the environment\n",
    "    def render(self, mode):\n",
    "        screen = pygame.display.set_mode((650, 750))\n",
    "        screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the game board\n",
    "        board = pygame.image.load(\"TsuroImages/board.png\")\n",
    "        board = pygame.transform.scale(board, (600, 600))\n",
    "        screen.blit(board, (25,25))\n",
    "        \n",
    "        # Draw current players hand\n",
    "        for i in range (len(self.player_tiles[self.current_player-1])):\n",
    "            tile = self.player_tiles[self.current_player-1][i]\n",
    "            screen.blit(self.tiles[tile].image, (75 + (i * 200), 635))\n",
    "        \n",
    "        # Draw the tiles on the board\n",
    "        for x in range(self.tile_board_size[0]):\n",
    "            for y in range(self.tile_board_size[1]):\n",
    "                val = self.tile_board[x][y]\n",
    "                if val != 0:\n",
    "                    tile = self.tiles[val-1]\n",
    "                    screen.blit(tile.image, (25 + x * 100, 25 + y * 100))\n",
    "                    \n",
    "        # Draw the players' pieces on the board\n",
    "        for i in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == i)\n",
    "            y_add = 0\n",
    "            x_add = 0\n",
    "            y_mult = 0\n",
    "            x_mult = 0\n",
    "            \n",
    "            match node_number[0]:\n",
    "                case 0:\n",
    "                    y_add = 35\n",
    "                case 1:\n",
    "                    x_add = 35\n",
    "                case 2:\n",
    "                    x_add = 70\n",
    "                case 3:\n",
    "                    x_add = 100\n",
    "                    y_add = 35\n",
    "                case 4:\n",
    "                    x_add = 100\n",
    "                    y_add = 70\n",
    "                case 5:\n",
    "                    x_add = 70\n",
    "                    y_add = 100\n",
    "                case 6:\n",
    "                    x_add = 35\n",
    "                    y_add = 100\n",
    "                case 7:\n",
    "                     y_add = 70\n",
    "                case _:\n",
    "                    raise Exception(\"Issue in drawing the player board\")\n",
    "                    \n",
    "            if tile_number[0] != 0:\n",
    "                x_mult, y_mult = TsuroEnv.euclidean_division(self, tile_number[0])\n",
    "            \n",
    "            pygame.draw.circle(screen, colors[i-1], (25 + x_add + (100 * x_mult), 25 + y_add + (100 * y_mult)), 5)\n",
    "            \n",
    "        # Draw text to show who won when game is over\n",
    "        if self.game_is_over() or self.current_player == -1:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            text = font.render('Player ' + str(self.current_player) + ' wins', True, '#00FF00')\n",
    "            textRect = text.get_rect()\n",
    "            textRect.center = (650 // 2, 750 // 2)\n",
    "            screen.blit(text, textRect)\n",
    "            \n",
    "        pygame.display.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8016cc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x25101280b50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training hyper parameters\n",
    "num_iterations = 2500000\n",
    "initial_collect_steps = 10000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "log_interval = 200\n",
    "num_eval_episodes = 500\n",
    "eval_interval = 5000\n",
    "\n",
    "register(\n",
    "    id='TsuroEnv',\n",
    "    entry_point=TsuroEnv,\n",
    ")\n",
    "\n",
    "env_name = \"TsuroEnv\"\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "fc_layer_params = (200, 100)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(num_units, activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(num_actions, activation=None, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03), bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step)\n",
    "    # commenting out render will make training quicker\n",
    "    # environment.render(\"human\")\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "        \n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=\"CheckpointsRC/\",\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step_counter\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34f97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "returns = []\n",
    "\n",
    "with open(\"CheckpointsRC/returns.txt\", \"r\") as txt:\n",
    "    for line in txt:\n",
    "        returns.append(line)\n",
    "        \n",
    "for i in range(len(returns)):\n",
    "    returns[i] = returns[i].strip()\n",
    "    returns[i] = float(returns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a7d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\\nagent.train_step_counter.assign(0)\\n\\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\\nreturns = [avg_return]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Collection for new agent\n",
    "'''\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6085e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5995200: loss = 3.721721649169922\n",
      "step = 5995400: loss = 6.312184810638428\n",
      "step = 5995600: loss = 5.022398948669434\n",
      "step = 5995800: loss = 4.478516578674316\n",
      "step = 5996000: loss = 4.180883884429932\n",
      "step = 5996200: loss = 4.2305521965026855\n",
      "step = 5996400: loss = 5.138334274291992\n",
      "step = 5996600: loss = 3.6939525604248047\n",
      "step = 5996800: loss = 3.6306064128875732\n",
      "step = 5997000: loss = 3.5971412658691406\n",
      "step = 5997200: loss = 4.174731731414795\n",
      "step = 5997400: loss = 4.5114850997924805\n",
      "step = 5997600: loss = 5.810895919799805\n",
      "step = 5997800: loss = 5.222387790679932\n",
      "step = 5998000: loss = 3.465275764465332\n",
      "step = 5998200: loss = 4.844621658325195\n",
      "step = 5998400: loss = 4.796956539154053\n",
      "step = 5998600: loss = 3.3440072536468506\n",
      "step = 5998800: loss = 3.5316414833068848\n",
      "step = 5999000: loss = 3.705691337585449\n",
      "step = 5999200: loss = 4.416993141174316\n",
      "step = 5999400: loss = 3.362119436264038\n",
      "step = 5999600: loss = 4.168577194213867\n",
      "step = 5999800: loss = 2.947946786880493\n",
      "step = 6000000: loss = 3.5882654190063477\n",
      "step = 6000000: Average Return = 4.124000072479248\n",
      "step = 6000200: loss = 4.479039669036865\n",
      "step = 6000400: loss = 3.702998638153076\n",
      "step = 6000600: loss = 3.092644691467285\n",
      "step = 6000800: loss = 3.219590902328491\n",
      "step = 6001000: loss = 5.279023170471191\n",
      "step = 6001200: loss = 4.142203330993652\n",
      "step = 6001400: loss = 4.508594989776611\n",
      "step = 6001600: loss = 3.7247111797332764\n",
      "step = 6001800: loss = 4.435297012329102\n",
      "step = 6002000: loss = 5.288276195526123\n",
      "step = 6002200: loss = 3.5661964416503906\n",
      "step = 6002400: loss = 4.3353986740112305\n",
      "step = 6002600: loss = 4.8773698806762695\n",
      "step = 6002800: loss = 4.32179594039917\n",
      "step = 6003000: loss = 4.552136421203613\n",
      "step = 6003200: loss = 3.386281967163086\n",
      "step = 6003400: loss = 4.445727348327637\n",
      "step = 6003600: loss = 4.5222859382629395\n",
      "step = 6003800: loss = 4.504072666168213\n",
      "step = 6004000: loss = 2.7134807109832764\n",
      "step = 6004200: loss = 3.792647361755371\n",
      "step = 6004400: loss = 4.3230695724487305\n",
      "step = 6004600: loss = 3.885535478591919\n",
      "step = 6004800: loss = 5.081429958343506\n",
      "step = 6005000: loss = 2.3212966918945312\n",
      "step = 6005000: Average Return = 3.8559999465942383\n",
      "step = 6005200: loss = 4.230989456176758\n",
      "step = 6005400: loss = 3.6052916049957275\n",
      "step = 6005600: loss = 3.848741292953491\n",
      "step = 6005800: loss = 3.772353410720825\n",
      "step = 6006000: loss = 5.426619529724121\n",
      "step = 6006200: loss = 4.404087543487549\n",
      "step = 6006400: loss = 5.136555194854736\n",
      "step = 6006600: loss = 4.011773109436035\n",
      "step = 6006800: loss = 3.812046527862549\n",
      "step = 6007000: loss = 5.764872074127197\n",
      "step = 6007200: loss = 5.030521392822266\n",
      "step = 6007400: loss = 4.258721351623535\n",
      "step = 6007600: loss = 4.925552845001221\n",
      "step = 6007800: loss = 4.199567794799805\n",
      "step = 6008000: loss = 4.0884857177734375\n",
      "step = 6008200: loss = 4.593526363372803\n",
      "step = 6008400: loss = 5.0751190185546875\n",
      "step = 6008600: loss = 5.168598651885986\n",
      "step = 6008800: loss = 4.565408706665039\n",
      "step = 6009000: loss = 4.716078758239746\n",
      "step = 6009200: loss = 3.3258681297302246\n",
      "step = 6009400: loss = 3.9747862815856934\n",
      "step = 6009600: loss = 5.139469623565674\n",
      "step = 6009800: loss = 4.173243999481201\n",
      "step = 6010000: loss = 3.799783229827881\n",
      "step = 6010000: Average Return = 3.618000030517578\n",
      "step = 6010200: loss = 5.73980712890625\n",
      "step = 6010400: loss = 4.299532890319824\n",
      "step = 6010600: loss = 3.3554370403289795\n",
      "step = 6010800: loss = 2.2678356170654297\n",
      "step = 6011000: loss = 3.8487133979797363\n",
      "step = 6011200: loss = 2.56925892829895\n",
      "step = 6011400: loss = 5.138986587524414\n",
      "step = 6011600: loss = 3.3719372749328613\n",
      "step = 6011800: loss = 3.5297117233276367\n",
      "step = 6012000: loss = 4.98219108581543\n",
      "step = 6012200: loss = 3.9640684127807617\n",
      "step = 6012400: loss = 3.5590734481811523\n",
      "step = 6012600: loss = 4.271052837371826\n",
      "step = 6012800: loss = 3.351686954498291\n",
      "step = 6013000: loss = 4.43739652633667\n",
      "step = 6013200: loss = 3.412476062774658\n",
      "step = 6013400: loss = 3.33819317817688\n",
      "step = 6013600: loss = 3.7453548908233643\n",
      "step = 6013800: loss = 3.8513247966766357\n",
      "step = 6014000: loss = 4.074743270874023\n",
      "step = 6014200: loss = 3.065655469894409\n",
      "step = 6014400: loss = 3.7992162704467773\n",
      "step = 6014600: loss = 3.267110824584961\n",
      "step = 6014800: loss = 3.607029676437378\n",
      "step = 6015000: loss = 4.316919326782227\n",
      "step = 6015000: Average Return = 4.124000072479248\n",
      "step = 6015200: loss = 5.5138115882873535\n",
      "step = 6015400: loss = 5.381924152374268\n",
      "step = 6015600: loss = 4.876420974731445\n",
      "step = 6015800: loss = 3.289259672164917\n",
      "step = 6016000: loss = 4.42622709274292\n",
      "step = 6016200: loss = 3.5801501274108887\n",
      "step = 6016400: loss = 3.268216133117676\n",
      "step = 6016600: loss = 3.109288454055786\n",
      "step = 6016800: loss = 4.403274059295654\n",
      "step = 6017000: loss = 4.257223606109619\n",
      "step = 6017200: loss = 3.720698118209839\n",
      "step = 6017400: loss = 2.9056103229522705\n",
      "step = 6017600: loss = 4.287697792053223\n",
      "step = 6017800: loss = 4.140213966369629\n",
      "step = 6018000: loss = 4.787228107452393\n",
      "step = 6018200: loss = 3.277432441711426\n",
      "step = 6018400: loss = 3.7112507820129395\n",
      "step = 6018600: loss = 3.9486961364746094\n",
      "step = 6018800: loss = 4.138123035430908\n",
      "step = 6019000: loss = 2.82415771484375\n",
      "step = 6019200: loss = 3.6705639362335205\n",
      "step = 6019400: loss = 4.418027877807617\n",
      "step = 6019600: loss = 6.326732158660889\n",
      "step = 6019800: loss = 4.789801120758057\n",
      "step = 6020000: loss = 3.692223310470581\n",
      "step = 6020000: Average Return = 4.122000217437744\n",
      "step = 6020200: loss = 4.243472099304199\n",
      "step = 6020400: loss = 4.001808166503906\n",
      "step = 6020600: loss = 5.190501689910889\n",
      "step = 6020800: loss = 5.272967338562012\n",
      "step = 6021000: loss = 3.9916679859161377\n",
      "step = 6021200: loss = 4.146705627441406\n",
      "step = 6021400: loss = 3.9630584716796875\n",
      "step = 6021600: loss = 4.463130474090576\n",
      "step = 6021800: loss = 6.085015773773193\n",
      "step = 6022000: loss = 3.416849374771118\n",
      "step = 6022200: loss = 5.254965782165527\n",
      "step = 6022400: loss = 5.406584739685059\n",
      "step = 6022600: loss = 3.695732355117798\n",
      "step = 6022800: loss = 5.3150787353515625\n",
      "step = 6023000: loss = 3.720660924911499\n",
      "step = 6023200: loss = 3.87913179397583\n",
      "step = 6023400: loss = 3.872019052505493\n",
      "step = 6023600: loss = 3.5361289978027344\n",
      "step = 6023800: loss = 4.186768531799316\n",
      "step = 6024000: loss = 3.940943479537964\n",
      "step = 6024200: loss = 5.66083288192749\n",
      "step = 6024400: loss = 3.497220277786255\n",
      "step = 6024600: loss = 4.633595943450928\n",
      "step = 6024800: loss = 4.184139728546143\n",
      "step = 6025000: loss = 4.949593544006348\n",
      "step = 6025000: Average Return = 4.159999847412109\n",
      "step = 6025200: loss = 3.0045113563537598\n",
      "step = 6025400: loss = 5.761260032653809\n",
      "step = 6025600: loss = 3.9754176139831543\n",
      "step = 6025800: loss = 5.011233806610107\n",
      "step = 6026000: loss = 3.981877326965332\n",
      "step = 6026200: loss = 4.318211555480957\n",
      "step = 6026400: loss = 3.724898338317871\n",
      "step = 6026600: loss = 4.088156223297119\n",
      "step = 6026800: loss = 3.7094621658325195\n",
      "step = 6027000: loss = 3.346137285232544\n",
      "step = 6027200: loss = 5.061586380004883\n",
      "step = 6027400: loss = 3.871432304382324\n",
      "step = 6027600: loss = 3.9791014194488525\n",
      "step = 6027800: loss = 2.928093433380127\n",
      "step = 6028000: loss = 4.087208271026611\n",
      "step = 6028200: loss = 4.096441745758057\n",
      "step = 6028400: loss = 5.300591945648193\n",
      "step = 6028600: loss = 4.432258605957031\n",
      "step = 6028800: loss = 4.119323253631592\n",
      "step = 6029000: loss = 3.7002482414245605\n",
      "step = 6029200: loss = 4.168084144592285\n",
      "step = 6029400: loss = 3.6806492805480957\n",
      "step = 6029600: loss = 4.094951152801514\n",
      "step = 6029800: loss = 3.860814094543457\n",
      "step = 6030000: loss = 5.135761737823486\n",
      "step = 6030000: Average Return = 3.7820000648498535\n",
      "step = 6030200: loss = 3.7375683784484863\n",
      "step = 6030400: loss = 4.433553218841553\n",
      "step = 6030600: loss = 3.406733989715576\n",
      "step = 6030800: loss = 5.531233787536621\n",
      "step = 6031000: loss = 4.295596122741699\n",
      "step = 6031200: loss = 4.536710739135742\n",
      "step = 6031400: loss = 4.1566386222839355\n",
      "step = 6031600: loss = 3.5951168537139893\n",
      "step = 6031800: loss = 3.5107367038726807\n",
      "step = 6032000: loss = 3.2816457748413086\n",
      "step = 6032200: loss = 3.056668996810913\n",
      "step = 6032400: loss = 4.0881147384643555\n",
      "step = 6032600: loss = 4.368908405303955\n",
      "step = 6032800: loss = 6.017149448394775\n",
      "step = 6033000: loss = 4.780046463012695\n",
      "step = 6033200: loss = 3.4156417846679688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6033400: loss = 3.0204455852508545\n",
      "step = 6033600: loss = 4.682749271392822\n",
      "step = 6033800: loss = 4.286647319793701\n",
      "step = 6034000: loss = 3.179037094116211\n",
      "step = 6034200: loss = 3.980088472366333\n",
      "step = 6034400: loss = 5.350275993347168\n",
      "step = 6034600: loss = 2.6307873725891113\n",
      "step = 6034800: loss = 4.6301751136779785\n",
      "step = 6035000: loss = 4.230648517608643\n",
      "step = 6035000: Average Return = 3.9519999027252197\n",
      "step = 6035200: loss = 4.478814125061035\n",
      "step = 6035400: loss = 3.7849504947662354\n",
      "step = 6035600: loss = 5.502119541168213\n",
      "step = 6035800: loss = 4.290049076080322\n",
      "step = 6036000: loss = 3.9614055156707764\n",
      "step = 6036200: loss = 6.3939104080200195\n",
      "step = 6036400: loss = 4.757723331451416\n",
      "step = 6036600: loss = 3.402456045150757\n",
      "step = 6036800: loss = 2.862636089324951\n",
      "step = 6037000: loss = 3.852766275405884\n",
      "step = 6037200: loss = 4.232696533203125\n",
      "step = 6037400: loss = 3.723334789276123\n",
      "step = 6037600: loss = 4.095329284667969\n",
      "step = 6037800: loss = 2.4749515056610107\n",
      "step = 6038000: loss = 3.539196729660034\n",
      "step = 6038200: loss = 3.078145742416382\n",
      "step = 6038400: loss = 3.3896493911743164\n",
      "step = 6038600: loss = 4.298269748687744\n",
      "step = 6038800: loss = 4.695703506469727\n",
      "step = 6039000: loss = 4.0200700759887695\n",
      "step = 6039200: loss = 4.012646198272705\n",
      "step = 6039400: loss = 3.5690746307373047\n",
      "step = 6039600: loss = 4.441787242889404\n",
      "step = 6039800: loss = 3.3926455974578857\n",
      "step = 6040000: loss = 3.1606390476226807\n",
      "step = 6040000: Average Return = 4.019999980926514\n",
      "step = 6040200: loss = 4.604713439941406\n",
      "step = 6040400: loss = 4.528561115264893\n",
      "step = 6040600: loss = 5.11653995513916\n",
      "step = 6040800: loss = 3.9138121604919434\n",
      "step = 6041000: loss = 3.7784054279327393\n",
      "step = 6041200: loss = 3.7786436080932617\n",
      "step = 6041400: loss = 3.103991746902466\n",
      "step = 6041600: loss = 3.017606735229492\n",
      "step = 6041800: loss = 3.772362232208252\n",
      "step = 6042000: loss = 4.400317668914795\n",
      "step = 6042200: loss = 3.856903553009033\n",
      "step = 6042400: loss = 5.477377891540527\n",
      "step = 6042600: loss = 2.811628580093384\n",
      "step = 6042800: loss = 3.5931217670440674\n",
      "step = 6043000: loss = 4.318675518035889\n",
      "step = 6043200: loss = 4.0186262130737305\n",
      "step = 6043400: loss = 3.577165365219116\n",
      "step = 6043600: loss = 4.080833435058594\n",
      "step = 6043800: loss = 5.296720027923584\n",
      "step = 6044000: loss = 4.861880779266357\n",
      "step = 6044200: loss = 5.386541843414307\n",
      "step = 6044400: loss = 4.424891948699951\n",
      "step = 6044600: loss = 4.642815589904785\n",
      "step = 6044800: loss = 5.301867485046387\n",
      "step = 6045000: loss = 3.2746336460113525\n",
      "step = 6045000: Average Return = 3.563999891281128\n",
      "step = 6045200: loss = 4.151450157165527\n",
      "step = 6045400: loss = 3.812774896621704\n",
      "step = 6045600: loss = 4.545656681060791\n",
      "step = 6045800: loss = 4.900115489959717\n",
      "step = 6046000: loss = 3.305162191390991\n",
      "step = 6046200: loss = 4.382748126983643\n",
      "step = 6046400: loss = 3.826838254928589\n",
      "step = 6046600: loss = 2.8749020099639893\n",
      "step = 6046800: loss = 4.3323259353637695\n",
      "step = 6047000: loss = 4.375154495239258\n",
      "step = 6047200: loss = 5.299957275390625\n",
      "step = 6047400: loss = 4.427556991577148\n",
      "step = 6047600: loss = 3.5619516372680664\n",
      "step = 6047800: loss = 4.482965469360352\n",
      "step = 6048000: loss = 3.2653398513793945\n",
      "step = 6048200: loss = 4.334411144256592\n",
      "step = 6048400: loss = 3.5164833068847656\n",
      "step = 6048600: loss = 5.139853477478027\n",
      "step = 6048800: loss = 3.2849884033203125\n",
      "step = 6049000: loss = 3.2700183391571045\n",
      "step = 6049200: loss = 4.737969398498535\n",
      "step = 6049400: loss = 3.601090431213379\n",
      "step = 6049600: loss = 4.195886611938477\n",
      "step = 6049800: loss = 3.4092369079589844\n",
      "step = 6050000: loss = 4.27406644821167\n",
      "step = 6050000: Average Return = 3.880000114440918\n",
      "step = 6050200: loss = 4.082788944244385\n",
      "step = 6050400: loss = 4.119329452514648\n",
      "step = 6050600: loss = 2.6520495414733887\n",
      "step = 6050800: loss = 4.710783958435059\n",
      "step = 6051000: loss = 3.5627763271331787\n",
      "step = 6051200: loss = 3.397780656814575\n",
      "step = 6051400: loss = 4.54473876953125\n",
      "step = 6051600: loss = 4.946046352386475\n",
      "step = 6051800: loss = 4.50013542175293\n",
      "step = 6052000: loss = 4.02354097366333\n",
      "step = 6052200: loss = 4.339634418487549\n",
      "step = 6052400: loss = 3.342935085296631\n",
      "step = 6052600: loss = 3.486661672592163\n",
      "step = 6052800: loss = 4.2744364738464355\n",
      "step = 6053000: loss = 4.45095682144165\n",
      "step = 6053200: loss = 3.7965028285980225\n",
      "step = 6053400: loss = 3.188297748565674\n",
      "step = 6053600: loss = 3.6943519115448\n",
      "step = 6053800: loss = 4.186256408691406\n",
      "step = 6054000: loss = 3.9664134979248047\n",
      "step = 6054200: loss = 3.270860195159912\n",
      "step = 6054400: loss = 5.420790195465088\n",
      "step = 6054600: loss = 3.4596080780029297\n",
      "step = 6054800: loss = 4.984402179718018\n",
      "step = 6055000: loss = 4.884114742279053\n",
      "step = 6055000: Average Return = 3.5880000591278076\n",
      "step = 6055200: loss = 4.7111616134643555\n",
      "step = 6055400: loss = 4.254849433898926\n",
      "step = 6055600: loss = 3.3689916133880615\n",
      "step = 6055800: loss = 4.9004340171813965\n",
      "step = 6056000: loss = 5.924136638641357\n",
      "step = 6056200: loss = 5.449123382568359\n",
      "step = 6056400: loss = 4.037815570831299\n",
      "step = 6056600: loss = 3.5103068351745605\n",
      "step = 6056800: loss = 2.5926125049591064\n",
      "step = 6057000: loss = 5.817729473114014\n",
      "step = 6057200: loss = 4.764430046081543\n",
      "step = 6057400: loss = 3.6158180236816406\n",
      "step = 6057600: loss = 3.6816344261169434\n",
      "step = 6057800: loss = 4.847794055938721\n",
      "step = 6058000: loss = 4.297872066497803\n",
      "step = 6058200: loss = 3.9503014087677\n",
      "step = 6058400: loss = 3.3889567852020264\n",
      "step = 6058600: loss = 4.653006553649902\n",
      "step = 6058800: loss = 4.505452632904053\n",
      "step = 6059000: loss = 5.4773850440979\n",
      "step = 6059200: loss = 3.744271755218506\n",
      "step = 6059400: loss = 3.521045446395874\n",
      "step = 6059600: loss = 2.980729341506958\n",
      "step = 6059800: loss = 3.791934013366699\n",
      "step = 6060000: loss = 3.4755563735961914\n",
      "step = 6060000: Average Return = 3.7760000228881836\n",
      "step = 6060200: loss = 4.9246416091918945\n",
      "step = 6060400: loss = 4.275789260864258\n",
      "step = 6060600: loss = 5.256171703338623\n",
      "step = 6060800: loss = 4.3890814781188965\n",
      "step = 6061000: loss = 4.509608268737793\n",
      "step = 6061200: loss = 3.976595878601074\n",
      "step = 6061400: loss = 5.111062049865723\n",
      "step = 6061600: loss = 4.200922966003418\n",
      "step = 6061800: loss = 4.329165458679199\n",
      "step = 6062000: loss = 5.448981761932373\n",
      "step = 6062200: loss = 2.2363269329071045\n",
      "step = 6062400: loss = 3.824286937713623\n",
      "step = 6062600: loss = 4.937558650970459\n",
      "step = 6062800: loss = 4.335740089416504\n",
      "step = 6063000: loss = 4.037076950073242\n",
      "step = 6063200: loss = 3.1769509315490723\n",
      "step = 6063400: loss = 4.04954195022583\n",
      "step = 6063600: loss = 4.456942081451416\n",
      "step = 6063800: loss = 4.5725226402282715\n",
      "step = 6064000: loss = 5.384438514709473\n",
      "step = 6064200: loss = 4.743902683258057\n",
      "step = 6064400: loss = 4.348850727081299\n",
      "step = 6064600: loss = 4.1340508460998535\n",
      "step = 6064800: loss = 5.638558864593506\n",
      "step = 6065000: loss = 5.19724178314209\n",
      "step = 6065000: Average Return = 3.7880001068115234\n",
      "step = 6065200: loss = 3.800502300262451\n",
      "step = 6065400: loss = 4.112738132476807\n",
      "step = 6065600: loss = 4.692193031311035\n",
      "step = 6065800: loss = 4.490504264831543\n",
      "step = 6066000: loss = 4.0171685218811035\n",
      "step = 6066200: loss = 4.802241802215576\n",
      "step = 6066400: loss = 4.487910270690918\n",
      "step = 6066600: loss = 5.095343589782715\n",
      "step = 6066800: loss = 4.713298320770264\n",
      "step = 6067000: loss = 3.941861867904663\n",
      "step = 6067200: loss = 3.722325325012207\n",
      "step = 6067400: loss = 4.032295227050781\n",
      "step = 6067600: loss = 4.096901893615723\n",
      "step = 6067800: loss = 5.0129923820495605\n",
      "step = 6068000: loss = 3.656151294708252\n",
      "step = 6068200: loss = 4.156740665435791\n",
      "step = 6068400: loss = 5.183064937591553\n",
      "step = 6068600: loss = 3.4709532260894775\n",
      "step = 6068800: loss = 3.942230463027954\n",
      "step = 6069000: loss = 4.416337013244629\n",
      "step = 6069200: loss = 3.304938793182373\n",
      "step = 6069400: loss = 3.297122001647949\n",
      "step = 6069600: loss = 3.939427137374878\n",
      "step = 6069800: loss = 4.27787446975708\n",
      "step = 6070000: loss = 4.521084785461426\n",
      "step = 6070000: Average Return = 4.010000228881836\n",
      "step = 6070200: loss = 3.425600528717041\n",
      "step = 6070400: loss = 4.225313186645508\n",
      "step = 6070600: loss = 3.2739739418029785\n",
      "step = 6070800: loss = 3.5343382358551025\n",
      "step = 6071000: loss = 3.0419464111328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6071200: loss = 3.505056381225586\n",
      "step = 6071400: loss = 3.730686664581299\n",
      "step = 6071600: loss = 3.52492618560791\n",
      "step = 6071800: loss = 3.7808141708374023\n",
      "step = 6072000: loss = 3.935399055480957\n",
      "step = 6072200: loss = 3.4706790447235107\n",
      "step = 6072400: loss = 4.522358417510986\n",
      "step = 6072600: loss = 4.097124099731445\n",
      "step = 6072800: loss = 4.8639817237854\n",
      "step = 6073000: loss = 3.0832302570343018\n",
      "step = 6073200: loss = 5.31510066986084\n",
      "step = 6073400: loss = 3.4694161415100098\n",
      "step = 6073600: loss = 3.2751500606536865\n",
      "step = 6073800: loss = 4.318653583526611\n",
      "step = 6074000: loss = 4.803976535797119\n",
      "step = 6074200: loss = 4.369735240936279\n",
      "step = 6074400: loss = 4.61971378326416\n",
      "step = 6074600: loss = 3.47806453704834\n",
      "step = 6074800: loss = 3.6978204250335693\n",
      "step = 6075000: loss = 4.851177215576172\n",
      "step = 6075000: Average Return = 4.326000213623047\n",
      "step = 6075200: loss = 4.689241886138916\n",
      "step = 6075400: loss = 3.8607583045959473\n",
      "step = 6075600: loss = 3.954085111618042\n",
      "step = 6075800: loss = 4.804277420043945\n",
      "step = 6076000: loss = 5.666860103607178\n",
      "step = 6076200: loss = 4.862207412719727\n",
      "step = 6076400: loss = 4.919510841369629\n",
      "step = 6076600: loss = 3.242281436920166\n",
      "step = 6076800: loss = 4.522134304046631\n",
      "step = 6077000: loss = 2.8118133544921875\n",
      "step = 6077200: loss = 4.253646373748779\n",
      "step = 6077400: loss = 3.0964202880859375\n",
      "step = 6077600: loss = 3.600731134414673\n",
      "step = 6077800: loss = 4.097403526306152\n",
      "step = 6078000: loss = 5.422532081604004\n",
      "step = 6078200: loss = 4.687497615814209\n",
      "step = 6078400: loss = 4.372401714324951\n",
      "step = 6078600: loss = 4.5704779624938965\n",
      "step = 6078800: loss = 3.6628024578094482\n",
      "step = 6079000: loss = 4.038967609405518\n",
      "step = 6079200: loss = 5.370657444000244\n",
      "step = 6079400: loss = 4.633222579956055\n",
      "step = 6079600: loss = 4.758619785308838\n",
      "step = 6079800: loss = 3.2042973041534424\n",
      "step = 6080000: loss = 4.634227275848389\n",
      "step = 6080000: Average Return = 4.053999900817871\n",
      "step = 6080200: loss = 3.9475505352020264\n",
      "step = 6080400: loss = 4.15485954284668\n",
      "step = 6080600: loss = 4.220884799957275\n",
      "step = 6080800: loss = 3.6904006004333496\n",
      "step = 6081000: loss = 5.251210689544678\n",
      "step = 6081200: loss = 4.230410575866699\n",
      "step = 6081400: loss = 4.30412483215332\n",
      "step = 6081600: loss = 3.7066240310668945\n",
      "step = 6081800: loss = 5.564211845397949\n",
      "step = 6082000: loss = 5.092918395996094\n",
      "step = 6082200: loss = 5.116963863372803\n",
      "step = 6082400: loss = 4.0043134689331055\n",
      "step = 6082600: loss = 3.772073268890381\n",
      "step = 6082800: loss = 4.942164897918701\n",
      "step = 6083000: loss = 4.600735187530518\n",
      "step = 6083200: loss = 3.6297178268432617\n",
      "step = 6083400: loss = 4.417420387268066\n",
      "step = 6083600: loss = 4.307648658752441\n",
      "step = 6083800: loss = 4.294256687164307\n",
      "step = 6084000: loss = 5.302299499511719\n",
      "step = 6084200: loss = 5.556911945343018\n",
      "step = 6084400: loss = 3.881629228591919\n",
      "step = 6084600: loss = 4.329731464385986\n",
      "step = 6084800: loss = 3.8213512897491455\n",
      "step = 6085000: loss = 5.242430210113525\n",
      "step = 6085000: Average Return = 4.090000152587891\n",
      "step = 6085200: loss = 4.697051048278809\n",
      "step = 6085400: loss = 5.898245811462402\n",
      "step = 6085600: loss = 5.2630228996276855\n",
      "step = 6085800: loss = 4.528065204620361\n",
      "step = 6086000: loss = 4.010679244995117\n",
      "step = 6086200: loss = 3.51082444190979\n",
      "step = 6086400: loss = 3.481374740600586\n",
      "step = 6086600: loss = 2.65639328956604\n",
      "step = 6086800: loss = 4.264521598815918\n",
      "step = 6087000: loss = 5.139225959777832\n",
      "step = 6087200: loss = 4.435249328613281\n",
      "step = 6087400: loss = 4.531224727630615\n",
      "step = 6087600: loss = 3.8983170986175537\n",
      "step = 6087800: loss = 4.193952560424805\n",
      "step = 6088000: loss = 4.5186285972595215\n",
      "step = 6088200: loss = 3.207184314727783\n",
      "step = 6088400: loss = 4.182395935058594\n",
      "step = 6088600: loss = 2.6286697387695312\n",
      "step = 6088800: loss = 4.007199764251709\n",
      "step = 6089000: loss = 3.427367687225342\n",
      "step = 6089200: loss = 4.811631202697754\n",
      "step = 6089400: loss = 3.671215772628784\n",
      "step = 6089600: loss = 3.981577157974243\n",
      "step = 6089800: loss = 3.8427562713623047\n",
      "step = 6090000: loss = 3.3743090629577637\n",
      "step = 6090000: Average Return = 3.74399995803833\n",
      "step = 6090200: loss = 3.618103504180908\n",
      "step = 6090400: loss = 5.915541648864746\n",
      "step = 6090600: loss = 3.33449649810791\n",
      "step = 6090800: loss = 4.542263031005859\n",
      "step = 6091000: loss = 5.189809322357178\n",
      "step = 6091200: loss = 3.0395147800445557\n",
      "step = 6091400: loss = 4.361001491546631\n",
      "step = 6091600: loss = 3.281719207763672\n",
      "step = 6091800: loss = 2.723189115524292\n",
      "step = 6092000: loss = 4.089608192443848\n",
      "step = 6092200: loss = 4.500454425811768\n",
      "step = 6092400: loss = 4.926729202270508\n",
      "step = 6092600: loss = 4.346611022949219\n",
      "step = 6092800: loss = 4.983466148376465\n",
      "step = 6093000: loss = 4.210369110107422\n",
      "step = 6093200: loss = 3.92997145652771\n",
      "step = 6093400: loss = 4.154196739196777\n",
      "step = 6093600: loss = 4.133481025695801\n",
      "step = 6093800: loss = 4.950294017791748\n",
      "step = 6094000: loss = 3.918951988220215\n",
      "step = 6094200: loss = 2.726473331451416\n",
      "step = 6094400: loss = 5.325526237487793\n",
      "step = 6094600: loss = 4.601210594177246\n",
      "step = 6094800: loss = 3.8878822326660156\n",
      "step = 6095000: loss = 4.937338352203369\n",
      "step = 6095000: Average Return = 3.7160000801086426\n",
      "step = 6095200: loss = 4.156320095062256\n",
      "step = 6095400: loss = 4.037777423858643\n",
      "step = 6095600: loss = 3.6266162395477295\n",
      "step = 6095800: loss = 3.5231950283050537\n",
      "step = 6096000: loss = 3.546525478363037\n",
      "step = 6096200: loss = 3.149723529815674\n",
      "step = 6096400: loss = 4.303853511810303\n",
      "step = 6096600: loss = 3.791213274002075\n",
      "step = 6096800: loss = 4.425365924835205\n",
      "step = 6097000: loss = 4.019636154174805\n",
      "step = 6097200: loss = 3.9385719299316406\n",
      "step = 6097400: loss = 3.0298004150390625\n",
      "step = 6097600: loss = 5.830800533294678\n",
      "step = 6097800: loss = 5.205577373504639\n",
      "step = 6098000: loss = 4.8363494873046875\n",
      "step = 6098200: loss = 4.771557331085205\n",
      "step = 6098400: loss = 4.433300971984863\n",
      "step = 6098600: loss = 3.2070350646972656\n",
      "step = 6098800: loss = 4.0014238357543945\n",
      "step = 6099000: loss = 5.193663597106934\n",
      "step = 6099200: loss = 5.218062400817871\n",
      "step = 6099400: loss = 3.921443462371826\n",
      "step = 6099600: loss = 4.011682510375977\n",
      "step = 6099800: loss = 5.662625789642334\n",
      "step = 6100000: loss = 4.0298004150390625\n",
      "step = 6100000: Average Return = 3.681999921798706\n",
      "step = 6100200: loss = 5.16409158706665\n",
      "step = 6100400: loss = 3.959428071975708\n",
      "step = 6100600: loss = 4.202330589294434\n",
      "step = 6100800: loss = 5.1072211265563965\n",
      "step = 6101000: loss = 4.235973358154297\n",
      "step = 6101200: loss = 6.094492435455322\n",
      "step = 6101400: loss = 4.704120635986328\n",
      "step = 6101600: loss = 3.828721761703491\n",
      "step = 6101800: loss = 4.226023197174072\n",
      "step = 6102000: loss = 3.656324625015259\n",
      "step = 6102200: loss = 3.652993679046631\n",
      "step = 6102400: loss = 3.606459379196167\n",
      "step = 6102600: loss = 4.935649394989014\n",
      "step = 6102800: loss = 4.624753475189209\n",
      "step = 6103000: loss = 5.385765075683594\n",
      "step = 6103200: loss = 4.1265339851379395\n",
      "step = 6103400: loss = 3.3253166675567627\n",
      "step = 6103600: loss = 3.8809897899627686\n",
      "step = 6103800: loss = 5.176889419555664\n",
      "step = 6104000: loss = 4.062251567840576\n",
      "step = 6104200: loss = 3.6738991737365723\n",
      "step = 6104400: loss = 3.824965476989746\n",
      "step = 6104600: loss = 3.260648012161255\n",
      "step = 6104800: loss = 4.652724742889404\n",
      "step = 6105000: loss = 5.339184284210205\n",
      "step = 6105000: Average Return = 3.7780001163482666\n",
      "step = 6105200: loss = 5.391661167144775\n",
      "step = 6105400: loss = 4.595326900482178\n",
      "step = 6105600: loss = 3.358675479888916\n",
      "step = 6105800: loss = 4.040066719055176\n",
      "step = 6106000: loss = 4.358903884887695\n",
      "step = 6106200: loss = 3.5845537185668945\n",
      "step = 6106400: loss = 4.385993480682373\n",
      "step = 6106600: loss = 4.157359600067139\n",
      "step = 6106800: loss = 4.364940166473389\n",
      "step = 6107000: loss = 4.197320938110352\n",
      "step = 6107200: loss = 4.220398902893066\n",
      "step = 6107400: loss = 4.6593170166015625\n",
      "step = 6107600: loss = 3.196484327316284\n",
      "step = 6107800: loss = 5.739141941070557\n",
      "step = 6108000: loss = 4.533740997314453\n",
      "step = 6108200: loss = 4.3019938468933105\n",
      "step = 6108400: loss = 3.2871336936950684\n",
      "step = 6108600: loss = 3.6987380981445312\n",
      "step = 6108800: loss = 4.14194917678833\n",
      "step = 6109000: loss = 6.201534271240234\n",
      "step = 6109200: loss = 4.439348220825195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6109400: loss = 6.095999717712402\n",
      "step = 6109600: loss = 3.1630334854125977\n",
      "step = 6109800: loss = 4.015230655670166\n",
      "step = 6110000: loss = 3.9070937633514404\n",
      "step = 6110000: Average Return = 3.9200000762939453\n",
      "step = 6110200: loss = 4.034034252166748\n",
      "step = 6110400: loss = 3.1396946907043457\n",
      "step = 6110600: loss = 4.946160793304443\n",
      "step = 6110800: loss = 3.9266247749328613\n",
      "step = 6111000: loss = 5.046905517578125\n",
      "step = 6111200: loss = 4.766762733459473\n",
      "step = 6111400: loss = 3.555940866470337\n",
      "step = 6111600: loss = 4.475552082061768\n",
      "step = 6111800: loss = 3.146045207977295\n",
      "step = 6112000: loss = 4.535886287689209\n",
      "step = 6112200: loss = 3.8046891689300537\n",
      "step = 6112400: loss = 4.265984058380127\n",
      "step = 6112600: loss = 4.277393817901611\n",
      "step = 6112800: loss = 2.769303321838379\n",
      "step = 6113000: loss = 4.331012725830078\n",
      "step = 6113200: loss = 4.2993927001953125\n",
      "step = 6113400: loss = 4.032430171966553\n",
      "step = 6113600: loss = 3.8072171211242676\n",
      "step = 6113800: loss = 3.6025586128234863\n",
      "step = 6114000: loss = 3.6601035594940186\n",
      "step = 6114200: loss = 5.315438747406006\n",
      "step = 6114400: loss = 3.8864822387695312\n",
      "step = 6114600: loss = 4.657731533050537\n",
      "step = 6114800: loss = 3.930861234664917\n",
      "step = 6115000: loss = 3.5830061435699463\n",
      "step = 6115000: Average Return = 3.9200000762939453\n",
      "step = 6115200: loss = 3.947488307952881\n",
      "step = 6115400: loss = 3.624239206314087\n",
      "step = 6115600: loss = 5.794599533081055\n",
      "step = 6115800: loss = 4.531165599822998\n",
      "step = 6116000: loss = 4.634153842926025\n",
      "step = 6116200: loss = 4.778365135192871\n",
      "step = 6116400: loss = 5.366547584533691\n",
      "step = 6116600: loss = 4.730061054229736\n",
      "step = 6116800: loss = 5.007035255432129\n",
      "step = 6117000: loss = 5.164150238037109\n",
      "step = 6117200: loss = 3.7868447303771973\n",
      "step = 6117400: loss = 5.0519866943359375\n",
      "step = 6117600: loss = 5.324150085449219\n",
      "step = 6117800: loss = 4.905500411987305\n",
      "step = 6118000: loss = 5.574581623077393\n",
      "step = 6118200: loss = 5.305816650390625\n",
      "step = 6118400: loss = 3.6010048389434814\n",
      "step = 6118600: loss = 2.180757999420166\n",
      "step = 6118800: loss = 4.628825664520264\n",
      "step = 6119000: loss = 4.434723854064941\n",
      "step = 6119200: loss = 4.255905628204346\n",
      "step = 6119400: loss = 4.0675458908081055\n",
      "step = 6119600: loss = 4.970455646514893\n",
      "step = 6119800: loss = 4.22180700302124\n",
      "step = 6120000: loss = 4.695815086364746\n",
      "step = 6120000: Average Return = 4.119999885559082\n",
      "step = 6120200: loss = 4.045226573944092\n",
      "step = 6120400: loss = 3.132255792617798\n",
      "step = 6120600: loss = 4.12960958480835\n",
      "step = 6120800: loss = 3.383819580078125\n",
      "step = 6121000: loss = 3.9975249767303467\n",
      "step = 6121200: loss = 3.4453258514404297\n",
      "step = 6121400: loss = 4.029690265655518\n",
      "step = 6121600: loss = 5.0345778465271\n",
      "step = 6121800: loss = 4.48293399810791\n",
      "step = 6122000: loss = 4.603163719177246\n",
      "step = 6122200: loss = 3.8681957721710205\n",
      "step = 6122400: loss = 3.4176440238952637\n",
      "step = 6122600: loss = 3.663666248321533\n",
      "step = 6122800: loss = 3.384087562561035\n",
      "step = 6123000: loss = 3.719489336013794\n",
      "step = 6123200: loss = 4.428889274597168\n",
      "step = 6123400: loss = 4.527747631072998\n",
      "step = 6123600: loss = 4.833262920379639\n",
      "step = 6123800: loss = 4.0682196617126465\n",
      "step = 6124000: loss = 3.523648500442505\n",
      "step = 6124200: loss = 4.090728282928467\n",
      "step = 6124400: loss = 4.004760265350342\n",
      "step = 6124600: loss = 3.12748122215271\n",
      "step = 6124800: loss = 4.559993743896484\n",
      "step = 6125000: loss = 4.647342681884766\n",
      "step = 6125000: Average Return = 3.558000087738037\n",
      "step = 6125200: loss = 4.022587299346924\n",
      "step = 6125400: loss = 3.992020845413208\n",
      "step = 6125600: loss = 4.575778484344482\n",
      "step = 6125800: loss = 3.0032811164855957\n",
      "step = 6126000: loss = 4.671592712402344\n",
      "step = 6126200: loss = 4.580852508544922\n",
      "step = 6126400: loss = 3.9578959941864014\n",
      "step = 6126600: loss = 3.729823589324951\n",
      "step = 6126800: loss = 4.65924596786499\n",
      "step = 6127000: loss = 3.5405895709991455\n",
      "step = 6127200: loss = 3.53800892829895\n",
      "step = 6127400: loss = 4.1218390464782715\n",
      "step = 6127600: loss = 3.016530752182007\n",
      "step = 6127800: loss = 3.428022623062134\n",
      "step = 6128000: loss = 3.6168768405914307\n",
      "step = 6128200: loss = 5.727319240570068\n",
      "step = 6128400: loss = 3.827998638153076\n",
      "step = 6128600: loss = 3.9125468730926514\n",
      "step = 6128800: loss = 4.902671813964844\n",
      "step = 6129000: loss = 3.8826510906219482\n",
      "step = 6129200: loss = 5.578343391418457\n",
      "step = 6129400: loss = 4.91211462020874\n",
      "step = 6129600: loss = 3.6152052879333496\n",
      "step = 6129800: loss = 3.13258957862854\n",
      "step = 6130000: loss = 4.161983966827393\n",
      "step = 6130000: Average Return = 3.7300000190734863\n",
      "step = 6130200: loss = 4.520214557647705\n",
      "step = 6130400: loss = 4.572474956512451\n",
      "step = 6130600: loss = 3.571932554244995\n",
      "step = 6130800: loss = 3.7616093158721924\n",
      "step = 6131000: loss = 4.359444618225098\n",
      "step = 6131200: loss = 5.03532600402832\n",
      "step = 6131400: loss = 5.2662200927734375\n",
      "step = 6131600: loss = 4.850327014923096\n",
      "step = 6131800: loss = 4.2955193519592285\n",
      "step = 6132000: loss = 3.878091335296631\n",
      "step = 6132200: loss = 4.124208450317383\n",
      "step = 6132400: loss = 3.0357744693756104\n",
      "step = 6132600: loss = 3.101264238357544\n",
      "step = 6132800: loss = 4.351845741271973\n",
      "step = 6133000: loss = 4.193624973297119\n",
      "step = 6133200: loss = 3.6268458366394043\n",
      "step = 6133400: loss = 3.3487842082977295\n",
      "step = 6133600: loss = 5.074410915374756\n",
      "step = 6133800: loss = 4.2806267738342285\n",
      "step = 6134000: loss = 3.3006184101104736\n",
      "step = 6134200: loss = 6.092255592346191\n",
      "step = 6134400: loss = 3.6477534770965576\n",
      "step = 6134600: loss = 3.8576133251190186\n",
      "step = 6134800: loss = 3.8078649044036865\n",
      "step = 6135000: loss = 4.41263484954834\n",
      "step = 6135000: Average Return = 3.7799999713897705\n",
      "step = 6135200: loss = 5.2313151359558105\n",
      "step = 6135400: loss = 5.0674567222595215\n",
      "step = 6135600: loss = 5.193366050720215\n",
      "step = 6135800: loss = 3.1130595207214355\n",
      "step = 6136000: loss = 2.8564629554748535\n",
      "step = 6136200: loss = 3.859417676925659\n",
      "step = 6136400: loss = 3.787358045578003\n",
      "step = 6136600: loss = 4.5021257400512695\n",
      "step = 6136800: loss = 5.595389366149902\n",
      "step = 6137000: loss = 3.7367970943450928\n",
      "step = 6137200: loss = 3.8168516159057617\n",
      "step = 6137400: loss = 3.687269926071167\n",
      "step = 6137600: loss = 2.4959683418273926\n",
      "step = 6137800: loss = 4.306090831756592\n",
      "step = 6138000: loss = 5.463083267211914\n",
      "step = 6138200: loss = 4.295410633087158\n",
      "step = 6138400: loss = 5.083024501800537\n",
      "step = 6138600: loss = 3.9746482372283936\n",
      "step = 6138800: loss = 5.61590051651001\n",
      "step = 6139000: loss = 4.428129196166992\n",
      "step = 6139200: loss = 4.141700267791748\n",
      "step = 6139400: loss = 3.3470945358276367\n",
      "step = 6139600: loss = 4.65476655960083\n",
      "step = 6139800: loss = 3.5819878578186035\n",
      "step = 6140000: loss = 5.688315391540527\n",
      "step = 6140000: Average Return = 3.869999885559082\n",
      "step = 6140200: loss = 3.9904704093933105\n",
      "step = 6140400: loss = 3.243729829788208\n",
      "step = 6140600: loss = 4.479483127593994\n",
      "step = 6140800: loss = 3.707545042037964\n",
      "step = 6141000: loss = 3.452880382537842\n",
      "step = 6141200: loss = 5.340359210968018\n",
      "step = 6141400: loss = 4.0593061447143555\n",
      "step = 6141600: loss = 2.1304516792297363\n",
      "step = 6141800: loss = 3.0693821907043457\n",
      "step = 6142000: loss = 4.261645793914795\n",
      "step = 6142200: loss = 3.5126683712005615\n",
      "step = 6142400: loss = 2.945270538330078\n",
      "step = 6142600: loss = 3.4669718742370605\n",
      "step = 6142800: loss = 4.062345504760742\n",
      "step = 6143000: loss = 4.633685111999512\n",
      "step = 6143200: loss = 4.248583793640137\n",
      "step = 6143400: loss = 4.171657562255859\n",
      "step = 6143600: loss = 4.99384880065918\n",
      "step = 6143800: loss = 4.860315322875977\n",
      "step = 6144000: loss = 3.979682445526123\n",
      "step = 6144200: loss = 3.841484785079956\n",
      "step = 6144400: loss = 3.806779146194458\n",
      "step = 6144600: loss = 4.61155891418457\n",
      "step = 6144800: loss = 3.9880104064941406\n",
      "step = 6145000: loss = 3.7362115383148193\n",
      "step = 6145000: Average Return = 3.630000114440918\n",
      "step = 6145200: loss = 4.5421552658081055\n",
      "step = 6145400: loss = 3.4920854568481445\n",
      "step = 6145600: loss = 5.992573261260986\n",
      "step = 6145800: loss = 3.509890079498291\n",
      "step = 6146000: loss = 4.57328462600708\n",
      "step = 6146200: loss = 5.337418079376221\n",
      "step = 6146400: loss = 2.8255839347839355\n",
      "step = 6146600: loss = 6.029263973236084\n",
      "step = 6146800: loss = 3.7486283779144287\n",
      "step = 6147000: loss = 4.262629508972168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6147200: loss = 3.7882308959960938\n",
      "step = 6147400: loss = 3.4762938022613525\n",
      "step = 6147600: loss = 2.851142406463623\n",
      "step = 6147800: loss = 4.737337589263916\n",
      "step = 6148000: loss = 4.691783905029297\n",
      "step = 6148200: loss = 3.54872727394104\n",
      "step = 6148400: loss = 2.616689920425415\n",
      "step = 6148600: loss = 5.075680732727051\n",
      "step = 6148800: loss = 4.230382442474365\n",
      "step = 6149000: loss = 3.9145946502685547\n",
      "step = 6149200: loss = 4.257208824157715\n",
      "step = 6149400: loss = 4.189011096954346\n",
      "step = 6149600: loss = 4.366143226623535\n",
      "step = 6149800: loss = 4.437445163726807\n",
      "step = 6150000: loss = 4.219913005828857\n",
      "step = 6150000: Average Return = 4.064000129699707\n",
      "step = 6150200: loss = 3.6166038513183594\n",
      "step = 6150400: loss = 4.0168609619140625\n",
      "step = 6150600: loss = 4.515202045440674\n",
      "step = 6150800: loss = 2.6233022212982178\n",
      "step = 6151000: loss = 3.86222505569458\n",
      "step = 6151200: loss = 4.410624980926514\n",
      "step = 6151400: loss = 2.8492062091827393\n",
      "step = 6151600: loss = 4.238287448883057\n",
      "step = 6151800: loss = 4.172543048858643\n",
      "step = 6152000: loss = 4.00339412689209\n",
      "step = 6152200: loss = 4.65149450302124\n",
      "step = 6152400: loss = 4.5120320320129395\n",
      "step = 6152600: loss = 4.259645938873291\n",
      "step = 6152800: loss = 3.9967172145843506\n",
      "step = 6153000: loss = 4.025665760040283\n",
      "step = 6153200: loss = 2.865302562713623\n",
      "step = 6153400: loss = 2.740657329559326\n",
      "step = 6153600: loss = 5.200684547424316\n",
      "step = 6153800: loss = 5.290185451507568\n",
      "step = 6154000: loss = 4.413689136505127\n",
      "step = 6154200: loss = 4.262662410736084\n",
      "step = 6154400: loss = 5.562269687652588\n",
      "step = 6154600: loss = 4.607368469238281\n",
      "step = 6154800: loss = 4.633464336395264\n",
      "step = 6155000: loss = 4.319406509399414\n",
      "step = 6155000: Average Return = 4.150000095367432\n",
      "step = 6155200: loss = 4.082345485687256\n",
      "step = 6155400: loss = 4.037544250488281\n",
      "step = 6155600: loss = 3.0873048305511475\n",
      "step = 6155800: loss = 5.107917785644531\n",
      "step = 6156000: loss = 5.2867255210876465\n",
      "step = 6156200: loss = 3.621598243713379\n",
      "step = 6156400: loss = 5.372701168060303\n",
      "step = 6156600: loss = 3.188812732696533\n",
      "step = 6156800: loss = 4.947598934173584\n",
      "step = 6157000: loss = 4.17985200881958\n",
      "step = 6157200: loss = 3.9851841926574707\n",
      "step = 6157400: loss = 3.3150157928466797\n",
      "step = 6157600: loss = 4.3802289962768555\n",
      "step = 6157800: loss = 2.2202911376953125\n",
      "step = 6158000: loss = 4.73729133605957\n",
      "step = 6158200: loss = 4.302131175994873\n",
      "step = 6158400: loss = 4.371088981628418\n",
      "step = 6158600: loss = 5.490880489349365\n",
      "step = 6158800: loss = 5.5898590087890625\n",
      "step = 6159000: loss = 5.256149768829346\n",
      "step = 6159200: loss = 3.931779623031616\n",
      "step = 6159400: loss = 5.057207107543945\n",
      "step = 6159600: loss = 5.526675701141357\n",
      "step = 6159800: loss = 3.7656917572021484\n",
      "step = 6160000: loss = 4.093371868133545\n",
      "step = 6160000: Average Return = 3.936000108718872\n",
      "step = 6160200: loss = 4.047712802886963\n",
      "step = 6160400: loss = 3.5992162227630615\n",
      "step = 6160600: loss = 4.574285507202148\n",
      "step = 6160800: loss = 3.8489928245544434\n",
      "step = 6161000: loss = 4.691808223724365\n",
      "step = 6161200: loss = 3.4147253036499023\n",
      "step = 6161400: loss = 3.5420000553131104\n",
      "step = 6161600: loss = 5.180904865264893\n",
      "step = 6161800: loss = 5.107759475708008\n",
      "step = 6162000: loss = 3.678741931915283\n",
      "step = 6162200: loss = 3.8332011699676514\n",
      "step = 6162400: loss = 3.5019640922546387\n",
      "step = 6162600: loss = 4.973872661590576\n",
      "step = 6162800: loss = 5.360767841339111\n",
      "step = 6163000: loss = 2.98630428314209\n",
      "step = 6163200: loss = 4.591348171234131\n",
      "step = 6163400: loss = 4.352858066558838\n",
      "step = 6163600: loss = 5.214868068695068\n",
      "step = 6163800: loss = 2.6173346042633057\n",
      "step = 6164000: loss = 5.153975486755371\n",
      "step = 6164200: loss = 5.464705467224121\n",
      "step = 6164400: loss = 3.472174644470215\n",
      "step = 6164600: loss = 5.693504333496094\n",
      "step = 6164800: loss = 3.7366323471069336\n",
      "step = 6165000: loss = 3.566845417022705\n",
      "step = 6165000: Average Return = 3.9260001182556152\n",
      "step = 6165200: loss = 4.847070693969727\n",
      "step = 6165400: loss = 3.332426071166992\n",
      "step = 6165600: loss = 4.178037166595459\n",
      "step = 6165800: loss = 4.385239124298096\n",
      "step = 6166000: loss = 5.972000598907471\n",
      "step = 6166200: loss = 3.7672603130340576\n",
      "step = 6166400: loss = 4.314731121063232\n",
      "step = 6166600: loss = 2.546527147293091\n",
      "step = 6166800: loss = 3.187394380569458\n",
      "step = 6167000: loss = 4.3848042488098145\n",
      "step = 6167200: loss = 3.0651988983154297\n",
      "step = 6167400: loss = 3.5175325870513916\n",
      "step = 6167600: loss = 3.4718072414398193\n",
      "step = 6167800: loss = 3.032139539718628\n",
      "step = 6168000: loss = 4.3826727867126465\n",
      "step = 6168200: loss = 5.0752339363098145\n",
      "step = 6168400: loss = 5.340373992919922\n",
      "step = 6168600: loss = 3.6789791584014893\n",
      "step = 6168800: loss = 5.642279148101807\n",
      "step = 6169000: loss = 3.8888752460479736\n",
      "step = 6169200: loss = 4.827012538909912\n",
      "step = 6169400: loss = 3.638434886932373\n",
      "step = 6169600: loss = 4.147409439086914\n",
      "step = 6169800: loss = 3.909569501876831\n",
      "step = 6170000: loss = 3.685325860977173\n",
      "step = 6170000: Average Return = 3.9159998893737793\n",
      "step = 6170200: loss = 3.552460193634033\n",
      "step = 6170400: loss = 4.552408218383789\n",
      "step = 6170600: loss = 6.1898112297058105\n",
      "step = 6170800: loss = 5.146871089935303\n",
      "step = 6171000: loss = 3.8352620601654053\n",
      "step = 6171200: loss = 4.941906452178955\n",
      "step = 6171400: loss = 4.681506156921387\n",
      "step = 6171600: loss = 3.5296270847320557\n",
      "step = 6171800: loss = 4.4053144454956055\n",
      "step = 6172000: loss = 4.444936275482178\n",
      "step = 6172200: loss = 3.3364455699920654\n",
      "step = 6172400: loss = 4.6884965896606445\n",
      "step = 6172600: loss = 3.86034893989563\n",
      "step = 6172800: loss = 3.325594425201416\n",
      "step = 6173000: loss = 4.149083614349365\n",
      "step = 6173200: loss = 3.7338707447052\n",
      "step = 6173400: loss = 4.9073333740234375\n",
      "step = 6173600: loss = 3.5434019565582275\n",
      "step = 6173800: loss = 4.06682014465332\n",
      "step = 6174000: loss = 4.088930130004883\n",
      "step = 6174200: loss = 4.631197929382324\n",
      "step = 6174400: loss = 5.089515209197998\n",
      "step = 6174600: loss = 3.8005974292755127\n",
      "step = 6174800: loss = 2.2591187953948975\n",
      "step = 6175000: loss = 5.477663993835449\n",
      "step = 6175000: Average Return = 3.859999895095825\n",
      "step = 6175200: loss = 4.294095993041992\n",
      "step = 6175400: loss = 4.417271614074707\n",
      "step = 6175600: loss = 4.454321384429932\n",
      "step = 6175800: loss = 4.050395488739014\n",
      "step = 6176000: loss = 2.851097345352173\n",
      "step = 6176200: loss = 3.117732524871826\n",
      "step = 6176400: loss = 5.27851676940918\n",
      "step = 6176600: loss = 3.3791637420654297\n",
      "step = 6176800: loss = 4.901571750640869\n",
      "step = 6177000: loss = 4.024982929229736\n",
      "step = 6177200: loss = 4.4456071853637695\n",
      "step = 6177400: loss = 5.017533302307129\n",
      "step = 6177600: loss = 5.403450012207031\n",
      "step = 6177800: loss = 3.941664934158325\n",
      "step = 6178000: loss = 3.584334135055542\n",
      "step = 6178200: loss = 3.8799889087677\n",
      "step = 6178400: loss = 4.318439483642578\n",
      "step = 6178600: loss = 4.9212422370910645\n",
      "step = 6178800: loss = 4.924960136413574\n",
      "step = 6179000: loss = 4.176180362701416\n",
      "step = 6179200: loss = 4.551391124725342\n",
      "step = 6179400: loss = 3.364084482192993\n",
      "step = 6179600: loss = 4.995034694671631\n",
      "step = 6179800: loss = 2.989650011062622\n",
      "step = 6180000: loss = 3.2192814350128174\n",
      "step = 6180000: Average Return = 3.625999927520752\n",
      "step = 6180200: loss = 4.284512996673584\n",
      "step = 6180400: loss = 3.7049856185913086\n",
      "step = 6180600: loss = 4.773749351501465\n",
      "step = 6180800: loss = 4.6770806312561035\n",
      "step = 6181000: loss = 3.83830189704895\n",
      "step = 6181200: loss = 4.516048908233643\n",
      "step = 6181400: loss = 4.905270099639893\n",
      "step = 6181600: loss = 2.774894952774048\n",
      "step = 6181800: loss = 3.9871299266815186\n",
      "step = 6182000: loss = 4.448988437652588\n",
      "step = 6182200: loss = 3.9516639709472656\n",
      "step = 6182400: loss = 3.643528461456299\n",
      "step = 6182600: loss = 4.169651985168457\n",
      "step = 6182800: loss = 4.250034809112549\n",
      "step = 6183000: loss = 3.8714263439178467\n",
      "step = 6183200: loss = 3.6810805797576904\n",
      "step = 6183400: loss = 4.898629188537598\n",
      "step = 6183600: loss = 3.1337292194366455\n",
      "step = 6183800: loss = 3.7222023010253906\n",
      "step = 6184000: loss = 4.735297203063965\n",
      "step = 6184200: loss = 4.662903785705566\n",
      "step = 6184400: loss = 4.763810157775879\n",
      "step = 6184600: loss = 3.0299694538116455\n",
      "step = 6184800: loss = 3.4803037643432617\n",
      "step = 6185000: loss = 3.6241819858551025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6185000: Average Return = 3.822000026702881\n",
      "step = 6185200: loss = 4.381800651550293\n",
      "step = 6185400: loss = 3.2077722549438477\n",
      "step = 6185600: loss = 4.435975551605225\n",
      "step = 6185800: loss = 4.404715061187744\n",
      "step = 6186000: loss = 4.813523769378662\n",
      "step = 6186200: loss = 4.074595928192139\n",
      "step = 6186400: loss = 3.5639123916625977\n",
      "step = 6186600: loss = 4.3239593505859375\n",
      "step = 6186800: loss = 5.735819339752197\n",
      "step = 6187000: loss = 4.2806243896484375\n",
      "step = 6187200: loss = 5.922889709472656\n",
      "step = 6187400: loss = 3.868511199951172\n",
      "step = 6187600: loss = 3.3857061862945557\n",
      "step = 6187800: loss = 4.715109348297119\n",
      "step = 6188000: loss = 3.9471511840820312\n",
      "step = 6188200: loss = 4.003030776977539\n",
      "step = 6188400: loss = 4.39981746673584\n",
      "step = 6188600: loss = 5.009470462799072\n",
      "step = 6188800: loss = 3.2074944972991943\n",
      "step = 6189000: loss = 5.369207859039307\n",
      "step = 6189200: loss = 3.0961668491363525\n",
      "step = 6189400: loss = 4.126514911651611\n",
      "step = 6189600: loss = 4.979984760284424\n",
      "step = 6189800: loss = 5.916425704956055\n",
      "step = 6190000: loss = 4.162163734436035\n",
      "step = 6190000: Average Return = 3.691999912261963\n",
      "step = 6190200: loss = 2.7981579303741455\n",
      "step = 6190400: loss = 3.8654897212982178\n",
      "step = 6190600: loss = 4.513441562652588\n",
      "step = 6190800: loss = 3.9705984592437744\n",
      "step = 6191000: loss = 4.968493938446045\n",
      "step = 6191200: loss = 4.227619171142578\n",
      "step = 6191400: loss = 5.862289905548096\n",
      "step = 6191600: loss = 4.176912307739258\n",
      "step = 6191800: loss = 4.668365955352783\n",
      "step = 6192000: loss = 3.0820043087005615\n",
      "step = 6192200: loss = 2.8368022441864014\n",
      "step = 6192400: loss = 4.762861728668213\n",
      "step = 6192600: loss = 4.245682716369629\n",
      "step = 6192800: loss = 3.8444340229034424\n",
      "step = 6193000: loss = 4.420928478240967\n",
      "step = 6193200: loss = 3.7052314281463623\n",
      "step = 6193400: loss = 2.8528313636779785\n",
      "step = 6193600: loss = 5.206580638885498\n",
      "step = 6193800: loss = 4.5479278564453125\n",
      "step = 6194000: loss = 4.537909984588623\n",
      "step = 6194200: loss = 3.1250176429748535\n",
      "step = 6194400: loss = 4.661468505859375\n",
      "step = 6194600: loss = 3.992745876312256\n",
      "step = 6194800: loss = 3.5260894298553467\n",
      "step = 6195000: loss = 5.029814720153809\n",
      "step = 6195000: Average Return = 3.619999885559082\n",
      "step = 6195200: loss = 3.858489990234375\n",
      "step = 6195400: loss = 4.438837051391602\n",
      "step = 6195600: loss = 4.784663677215576\n",
      "step = 6195800: loss = 3.7161495685577393\n",
      "step = 6196000: loss = 3.2144901752471924\n",
      "step = 6196200: loss = 4.503834247589111\n",
      "step = 6196400: loss = 3.961421489715576\n",
      "step = 6196600: loss = 5.368295669555664\n",
      "step = 6196800: loss = 3.5688610076904297\n",
      "step = 6197000: loss = 4.486299991607666\n",
      "step = 6197200: loss = 4.892316818237305\n",
      "step = 6197400: loss = 4.27942419052124\n",
      "step = 6197600: loss = 3.3362255096435547\n",
      "step = 6197800: loss = 3.1106343269348145\n",
      "step = 6198000: loss = 3.8632447719573975\n",
      "step = 6198200: loss = 4.6900177001953125\n",
      "step = 6198400: loss = 5.603596210479736\n",
      "step = 6198600: loss = 4.142599582672119\n",
      "step = 6198800: loss = 2.5247464179992676\n",
      "step = 6199000: loss = 4.369433403015137\n",
      "step = 6199200: loss = 3.9500932693481445\n",
      "step = 6199400: loss = 4.8876543045043945\n",
      "step = 6199600: loss = 3.7158567905426025\n",
      "step = 6199800: loss = 4.990184307098389\n",
      "step = 6200000: loss = 3.395559549331665\n",
      "step = 6200000: Average Return = 3.997999906539917\n",
      "step = 6200200: loss = 5.427681922912598\n",
      "step = 6200400: loss = 4.537585735321045\n",
      "step = 6200600: loss = 4.8281683921813965\n",
      "step = 6200800: loss = 4.999082088470459\n",
      "step = 6201000: loss = 3.851100444793701\n",
      "step = 6201200: loss = 4.45179557800293\n",
      "step = 6201400: loss = 3.6682097911834717\n",
      "step = 6201600: loss = 4.296655654907227\n",
      "step = 6201800: loss = 4.515353202819824\n",
      "step = 6202000: loss = 4.1490888595581055\n",
      "step = 6202200: loss = 3.4941844940185547\n",
      "step = 6202400: loss = 4.569671154022217\n",
      "step = 6202600: loss = 3.1247756481170654\n",
      "step = 6202800: loss = 4.009912490844727\n",
      "step = 6203000: loss = 3.9576756954193115\n",
      "step = 6203200: loss = 3.211374521255493\n",
      "step = 6203400: loss = 4.725494384765625\n",
      "step = 6203600: loss = 3.996842861175537\n",
      "step = 6203800: loss = 4.7923150062561035\n",
      "step = 6204000: loss = 3.1830170154571533\n",
      "step = 6204200: loss = 4.955195903778076\n",
      "step = 6204400: loss = 4.589371681213379\n",
      "step = 6204600: loss = 3.3070292472839355\n",
      "step = 6204800: loss = 3.6254985332489014\n",
      "step = 6205000: loss = 4.01352071762085\n",
      "step = 6205000: Average Return = 4.3460001945495605\n",
      "step = 6205200: loss = 2.6560068130493164\n",
      "step = 6205400: loss = 2.141127347946167\n",
      "step = 6205600: loss = 4.66363525390625\n",
      "step = 6205800: loss = 3.9593005180358887\n",
      "step = 6206000: loss = 5.39883279800415\n",
      "step = 6206200: loss = 3.706996440887451\n",
      "step = 6206400: loss = 3.690152406692505\n",
      "step = 6206600: loss = 4.655342102050781\n",
      "step = 6206800: loss = 4.432233810424805\n",
      "step = 6207000: loss = 4.02283239364624\n",
      "step = 6207200: loss = 4.741387367248535\n",
      "step = 6207400: loss = 4.386435508728027\n",
      "step = 6207600: loss = 3.7733993530273438\n",
      "step = 6207800: loss = 3.3458659648895264\n",
      "step = 6208000: loss = 3.3358616828918457\n",
      "step = 6208200: loss = 4.757673740386963\n",
      "step = 6208400: loss = 3.9362220764160156\n",
      "step = 6208600: loss = 4.367173194885254\n",
      "step = 6208800: loss = 3.4537174701690674\n",
      "step = 6209000: loss = 4.167518138885498\n",
      "step = 6209200: loss = 3.475743293762207\n",
      "step = 6209400: loss = 3.66310453414917\n",
      "step = 6209600: loss = 2.9203100204467773\n",
      "step = 6209800: loss = 5.565517902374268\n",
      "step = 6210000: loss = 3.733752965927124\n",
      "step = 6210000: Average Return = 3.819999933242798\n",
      "step = 6210200: loss = 3.477437734603882\n",
      "step = 6210400: loss = 3.7730553150177\n",
      "step = 6210600: loss = 5.790781497955322\n",
      "step = 6210800: loss = 2.3751111030578613\n",
      "step = 6211000: loss = 4.307624340057373\n",
      "step = 6211200: loss = 3.911489725112915\n",
      "step = 6211400: loss = 4.403740882873535\n",
      "step = 6211600: loss = 4.9203104972839355\n",
      "step = 6211800: loss = 5.840594291687012\n",
      "step = 6212000: loss = 4.339771270751953\n",
      "step = 6212200: loss = 3.27421498298645\n",
      "step = 6212400: loss = 4.8470234870910645\n",
      "step = 6212600: loss = 3.9715778827667236\n",
      "step = 6212800: loss = 3.5629677772521973\n",
      "step = 6213000: loss = 3.948604106903076\n",
      "step = 6213200: loss = 3.93009352684021\n",
      "step = 6213400: loss = 3.1968441009521484\n",
      "step = 6213600: loss = 3.9202895164489746\n",
      "step = 6213800: loss = 3.8349273204803467\n",
      "step = 6214000: loss = 4.1498918533325195\n",
      "step = 6214200: loss = 3.7169671058654785\n",
      "step = 6214400: loss = 3.99580979347229\n",
      "step = 6214600: loss = 4.396200656890869\n",
      "step = 6214800: loss = 4.579738140106201\n",
      "step = 6215000: loss = 3.731243848800659\n",
      "step = 6215000: Average Return = 3.7219998836517334\n",
      "step = 6215200: loss = 3.6496386528015137\n",
      "step = 6215400: loss = 4.425942420959473\n",
      "step = 6215600: loss = 4.495029926300049\n",
      "step = 6215800: loss = 3.599987030029297\n",
      "step = 6216000: loss = 3.8298237323760986\n",
      "step = 6216200: loss = 4.006774425506592\n",
      "step = 6216400: loss = 3.801877975463867\n",
      "step = 6216600: loss = 2.694364070892334\n",
      "step = 6216800: loss = 4.409814834594727\n",
      "step = 6217000: loss = 3.432454824447632\n",
      "step = 6217200: loss = 3.262388229370117\n",
      "step = 6217400: loss = 2.615452289581299\n",
      "step = 6217600: loss = 4.02137565612793\n",
      "step = 6217800: loss = 3.915256977081299\n",
      "step = 6218000: loss = 5.068937301635742\n",
      "step = 6218200: loss = 3.1754238605499268\n",
      "step = 6218400: loss = 4.551943302154541\n",
      "step = 6218600: loss = 5.048995494842529\n",
      "step = 6218800: loss = 3.31083083152771\n",
      "step = 6219000: loss = 4.383417129516602\n",
      "step = 6219200: loss = 3.955028533935547\n",
      "step = 6219400: loss = 3.030353307723999\n",
      "step = 6219600: loss = 4.141941547393799\n",
      "step = 6219800: loss = 4.79464054107666\n",
      "step = 6220000: loss = 3.501176118850708\n",
      "step = 6220000: Average Return = 3.828000068664551\n",
      "step = 6220200: loss = 4.407059192657471\n",
      "step = 6220400: loss = 5.819360256195068\n",
      "step = 6220600: loss = 3.943253755569458\n",
      "step = 6220800: loss = 4.258096218109131\n",
      "step = 6221000: loss = 3.33132004737854\n",
      "step = 6221200: loss = 4.230574131011963\n",
      "step = 6221400: loss = 4.536732196807861\n",
      "step = 6221600: loss = 3.1770036220550537\n",
      "step = 6221800: loss = 4.575693130493164\n",
      "step = 6222000: loss = 4.592057704925537\n",
      "step = 6222200: loss = 3.4380009174346924\n",
      "step = 6222400: loss = 3.603147506713867\n",
      "step = 6222600: loss = 4.1922149658203125\n",
      "step = 6222800: loss = 5.040820598602295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6223000: loss = 3.6317787170410156\n",
      "step = 6223200: loss = 3.8003783226013184\n",
      "step = 6223400: loss = 3.2641148567199707\n",
      "step = 6223600: loss = 4.093437194824219\n",
      "step = 6223800: loss = 4.281886577606201\n",
      "step = 6224000: loss = 4.49216890335083\n",
      "step = 6224200: loss = 4.278599739074707\n",
      "step = 6224400: loss = 3.9166688919067383\n",
      "step = 6224600: loss = 5.075018405914307\n",
      "step = 6224800: loss = 4.07886266708374\n",
      "step = 6225000: loss = 4.484333515167236\n",
      "step = 6225000: Average Return = 3.9179999828338623\n",
      "step = 6225200: loss = 4.199531078338623\n",
      "step = 6225400: loss = 4.475429058074951\n",
      "step = 6225600: loss = 4.196878910064697\n",
      "step = 6225800: loss = 4.788882732391357\n",
      "step = 6226000: loss = 3.434483051300049\n",
      "step = 6226200: loss = 4.475122451782227\n",
      "step = 6226400: loss = 5.08296537399292\n",
      "step = 6226600: loss = 2.912235736846924\n",
      "step = 6226800: loss = 4.406641960144043\n",
      "step = 6227000: loss = 3.123901605606079\n",
      "step = 6227200: loss = 4.011288166046143\n",
      "step = 6227400: loss = 3.3588969707489014\n",
      "step = 6227600: loss = 4.625594139099121\n",
      "step = 6227800: loss = 6.426243782043457\n",
      "step = 6228000: loss = 2.6321640014648438\n",
      "step = 6228200: loss = 6.032057762145996\n",
      "step = 6228400: loss = 5.268969535827637\n",
      "step = 6228600: loss = 3.998690605163574\n",
      "step = 6228800: loss = 4.976663112640381\n",
      "step = 6229000: loss = 3.718038558959961\n",
      "step = 6229200: loss = 4.149528503417969\n",
      "step = 6229400: loss = 4.81791877746582\n",
      "step = 6229600: loss = 3.5252842903137207\n",
      "step = 6229800: loss = 6.370302200317383\n",
      "step = 6230000: loss = 4.538327693939209\n",
      "step = 6230000: Average Return = 3.49399995803833\n",
      "step = 6230200: loss = 4.816867351531982\n",
      "step = 6230400: loss = 3.745504140853882\n",
      "step = 6230600: loss = 4.577489376068115\n",
      "step = 6230800: loss = 4.078841686248779\n",
      "step = 6231000: loss = 5.220627307891846\n",
      "step = 6231200: loss = 3.2180323600769043\n",
      "step = 6231400: loss = 3.686542510986328\n",
      "step = 6231600: loss = 3.7676494121551514\n",
      "step = 6231800: loss = 4.143832206726074\n",
      "step = 6232000: loss = 3.9825046062469482\n",
      "step = 6232200: loss = 5.259599685668945\n",
      "step = 6232400: loss = 3.5003762245178223\n",
      "step = 6232600: loss = 4.970179557800293\n",
      "step = 6232800: loss = 5.396765232086182\n",
      "step = 6233000: loss = 2.8210368156433105\n",
      "step = 6233200: loss = 3.281913995742798\n",
      "step = 6233400: loss = 5.0552215576171875\n",
      "step = 6233600: loss = 5.227569580078125\n",
      "step = 6233800: loss = 4.76577091217041\n",
      "step = 6234000: loss = 4.544408321380615\n",
      "step = 6234200: loss = 4.112025260925293\n",
      "step = 6234400: loss = 3.5421435832977295\n",
      "step = 6234600: loss = 4.059668064117432\n",
      "step = 6234800: loss = 6.437398433685303\n",
      "step = 6235000: loss = 4.237183094024658\n",
      "step = 6235000: Average Return = 3.686000108718872\n",
      "step = 6235200: loss = 3.1571857929229736\n",
      "step = 6235400: loss = 4.267772197723389\n",
      "step = 6235600: loss = 3.9608752727508545\n",
      "step = 6235800: loss = 4.54031229019165\n",
      "step = 6236000: loss = 5.615792274475098\n",
      "step = 6236200: loss = 4.23408842086792\n",
      "step = 6236400: loss = 3.8278656005859375\n",
      "step = 6236600: loss = 4.822390079498291\n",
      "step = 6236800: loss = 2.7841479778289795\n",
      "step = 6237000: loss = 3.169713020324707\n",
      "step = 6237200: loss = 4.211699962615967\n",
      "step = 6237400: loss = 4.578727722167969\n",
      "step = 6237600: loss = 3.515566825866699\n",
      "step = 6237800: loss = 4.157751083374023\n",
      "step = 6238000: loss = 4.721844673156738\n",
      "step = 6238200: loss = 3.9265081882476807\n",
      "step = 6238400: loss = 4.215850353240967\n",
      "step = 6238600: loss = 5.171553611755371\n",
      "step = 6238800: loss = 3.3393847942352295\n",
      "step = 6239000: loss = 3.190425395965576\n",
      "step = 6239200: loss = 3.1127405166625977\n",
      "step = 6239400: loss = 2.7874395847320557\n",
      "step = 6239600: loss = 3.7126238346099854\n",
      "step = 6239800: loss = 4.519160747528076\n",
      "step = 6240000: loss = 3.8772103786468506\n",
      "step = 6240000: Average Return = 3.880000114440918\n",
      "step = 6240200: loss = 4.417683124542236\n",
      "step = 6240400: loss = 4.517043590545654\n",
      "step = 6240600: loss = 4.200849533081055\n",
      "step = 6240800: loss = 4.938281536102295\n",
      "step = 6241000: loss = 3.5617785453796387\n",
      "step = 6241200: loss = 4.058172702789307\n",
      "step = 6241400: loss = 4.967682838439941\n",
      "step = 6241600: loss = 5.393823623657227\n",
      "step = 6241800: loss = 3.895270347595215\n",
      "step = 6242000: loss = 4.950098037719727\n",
      "step = 6242200: loss = 4.410675525665283\n",
      "step = 6242400: loss = 3.3296337127685547\n",
      "step = 6242600: loss = 4.404385089874268\n",
      "step = 6242800: loss = 3.3763017654418945\n",
      "step = 6243000: loss = 3.2253124713897705\n",
      "step = 6243200: loss = 3.7373740673065186\n",
      "step = 6243400: loss = 5.364195346832275\n",
      "step = 6243600: loss = 4.059230804443359\n",
      "step = 6243800: loss = 4.444312572479248\n",
      "step = 6244000: loss = 5.258395671844482\n",
      "step = 6244200: loss = 3.0408151149749756\n",
      "step = 6244400: loss = 4.46134090423584\n",
      "step = 6244600: loss = 3.970214366912842\n",
      "step = 6244800: loss = 4.093111991882324\n",
      "step = 6245000: loss = 3.787999153137207\n",
      "step = 6245000: Average Return = 4.026000022888184\n",
      "step = 6245200: loss = 4.176278114318848\n",
      "step = 6245400: loss = 2.7433834075927734\n",
      "step = 6245600: loss = 4.1845316886901855\n",
      "step = 6245800: loss = 4.021649360656738\n",
      "step = 6246000: loss = 4.286044597625732\n",
      "step = 6246200: loss = 3.308011770248413\n",
      "step = 6246400: loss = 5.083704948425293\n",
      "step = 6246600: loss = 3.642789840698242\n",
      "step = 6246800: loss = 4.160160541534424\n",
      "step = 6247000: loss = 4.641823768615723\n",
      "step = 6247200: loss = 5.243096351623535\n",
      "step = 6247400: loss = 4.724559307098389\n",
      "step = 6247600: loss = 4.486551761627197\n",
      "step = 6247800: loss = 3.7514801025390625\n",
      "step = 6248000: loss = 4.042993545532227\n",
      "step = 6248200: loss = 4.1217570304870605\n",
      "step = 6248400: loss = 5.305654048919678\n",
      "step = 6248600: loss = 3.7996294498443604\n",
      "step = 6248800: loss = 3.674532651901245\n",
      "step = 6249000: loss = 5.593563079833984\n",
      "step = 6249200: loss = 4.148561477661133\n",
      "step = 6249400: loss = 4.062228679656982\n",
      "step = 6249600: loss = 4.589046955108643\n",
      "step = 6249800: loss = 2.5825328826904297\n",
      "step = 6250000: loss = 2.9459240436553955\n",
      "step = 6250000: Average Return = 3.7360000610351562\n",
      "step = 6250200: loss = 4.534072399139404\n",
      "step = 6250400: loss = 5.728455543518066\n",
      "step = 6250600: loss = 4.333191871643066\n",
      "step = 6250800: loss = 5.529388427734375\n",
      "step = 6251000: loss = 3.8203532695770264\n",
      "step = 6251200: loss = 4.0807294845581055\n",
      "step = 6251400: loss = 4.175894260406494\n",
      "step = 6251600: loss = 4.545383453369141\n",
      "step = 6251800: loss = 4.8032636642456055\n",
      "step = 6252000: loss = 4.188750267028809\n",
      "step = 6252200: loss = 4.0154709815979\n",
      "step = 6252400: loss = 5.459468841552734\n",
      "step = 6252600: loss = 5.529014587402344\n",
      "step = 6252800: loss = 4.71860933303833\n",
      "step = 6253000: loss = 3.0299198627471924\n",
      "step = 6253200: loss = 4.133907318115234\n",
      "step = 6253400: loss = 3.2550835609436035\n",
      "step = 6253600: loss = 4.358828067779541\n",
      "step = 6253800: loss = 4.715089797973633\n",
      "step = 6254000: loss = 3.843262195587158\n",
      "step = 6254200: loss = 4.174378395080566\n",
      "step = 6254400: loss = 3.097707986831665\n",
      "step = 6254600: loss = 4.23756742477417\n",
      "step = 6254800: loss = 3.2012100219726562\n",
      "step = 6255000: loss = 3.7286558151245117\n",
      "step = 6255000: Average Return = 4.050000190734863\n",
      "step = 6255200: loss = 3.496196746826172\n",
      "step = 6255400: loss = 3.305548667907715\n",
      "step = 6255600: loss = 4.025704860687256\n",
      "step = 6255800: loss = 4.213029861450195\n",
      "step = 6256000: loss = 4.486814022064209\n",
      "step = 6256200: loss = 4.264136791229248\n",
      "step = 6256400: loss = 4.434442520141602\n",
      "step = 6256600: loss = 4.090977668762207\n",
      "step = 6256800: loss = 3.6384124755859375\n",
      "step = 6257000: loss = 3.5516433715820312\n",
      "step = 6257200: loss = 5.057791233062744\n",
      "step = 6257400: loss = 4.90826416015625\n",
      "step = 6257600: loss = 3.6336615085601807\n",
      "step = 6257800: loss = 4.848495960235596\n",
      "step = 6258000: loss = 3.595752477645874\n",
      "step = 6258200: loss = 5.351443767547607\n",
      "step = 6258400: loss = 4.966076374053955\n",
      "step = 6258600: loss = 3.9947314262390137\n",
      "step = 6258800: loss = 4.2087531089782715\n",
      "step = 6259000: loss = 4.305765151977539\n",
      "step = 6259200: loss = 5.92628288269043\n",
      "step = 6259400: loss = 4.0581159591674805\n",
      "step = 6259600: loss = 3.770646810531616\n",
      "step = 6259800: loss = 4.5103302001953125\n",
      "step = 6260000: loss = 4.837775230407715\n",
      "step = 6260000: Average Return = 4.010000228881836\n",
      "step = 6260200: loss = 4.870234966278076\n",
      "step = 6260400: loss = 4.105826377868652\n",
      "step = 6260600: loss = 3.924664258956909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6260800: loss = 4.295735836029053\n",
      "step = 6261000: loss = 4.876925468444824\n",
      "step = 6261200: loss = 3.6209962368011475\n",
      "step = 6261400: loss = 4.658938884735107\n",
      "step = 6261600: loss = 4.164857387542725\n",
      "step = 6261800: loss = 4.733813285827637\n",
      "step = 6262000: loss = 5.123694896697998\n",
      "step = 6262200: loss = 2.715405225753784\n",
      "step = 6262400: loss = 4.9581074714660645\n",
      "step = 6262600: loss = 4.791682243347168\n",
      "step = 6262800: loss = 5.1056928634643555\n",
      "step = 6263000: loss = 5.501023292541504\n",
      "step = 6263200: loss = 4.376328945159912\n",
      "step = 6263400: loss = 3.0468640327453613\n",
      "step = 6263600: loss = 5.301736354827881\n",
      "step = 6263800: loss = 3.3312861919403076\n",
      "step = 6264000: loss = 4.082496166229248\n",
      "step = 6264200: loss = 3.905069589614868\n",
      "step = 6264400: loss = 4.2824296951293945\n",
      "step = 6264600: loss = 5.25587272644043\n",
      "step = 6264800: loss = 4.602818489074707\n",
      "step = 6265000: loss = 3.8704164028167725\n",
      "step = 6265000: Average Return = 3.8399999141693115\n",
      "step = 6265200: loss = 5.042802333831787\n",
      "step = 6265400: loss = 5.249107360839844\n",
      "step = 6265600: loss = 4.949543476104736\n",
      "step = 6265800: loss = 3.8145861625671387\n",
      "step = 6266000: loss = 3.9971585273742676\n",
      "step = 6266200: loss = 2.7952826023101807\n",
      "step = 6266400: loss = 3.470745801925659\n",
      "step = 6266600: loss = 5.406404495239258\n",
      "step = 6266800: loss = 5.234139442443848\n",
      "step = 6267000: loss = 4.70722770690918\n",
      "step = 6267200: loss = 3.973256826400757\n",
      "step = 6267400: loss = 4.906530380249023\n",
      "step = 6267600: loss = 3.5099916458129883\n",
      "step = 6267800: loss = 4.222138404846191\n",
      "step = 6268000: loss = 5.134965896606445\n",
      "step = 6268200: loss = 4.93164587020874\n",
      "step = 6268400: loss = 5.352460861206055\n",
      "step = 6268600: loss = 4.145872592926025\n",
      "step = 6268800: loss = 3.788970470428467\n",
      "step = 6269000: loss = 4.160200119018555\n",
      "step = 6269200: loss = 3.5766875743865967\n",
      "step = 6269400: loss = 3.217470645904541\n",
      "step = 6269600: loss = 3.572451591491699\n",
      "step = 6269800: loss = 3.9782018661499023\n",
      "step = 6270000: loss = 3.445788860321045\n",
      "step = 6270000: Average Return = 3.8440001010894775\n",
      "step = 6270200: loss = 3.5329113006591797\n",
      "step = 6270400: loss = 4.480429649353027\n",
      "step = 6270600: loss = 2.9504940509796143\n",
      "step = 6270800: loss = 5.350258827209473\n",
      "step = 6271000: loss = 3.841108560562134\n",
      "step = 6271200: loss = 3.598559856414795\n",
      "step = 6271400: loss = 4.6401824951171875\n",
      "step = 6271600: loss = 5.259992599487305\n",
      "step = 6271800: loss = 6.139725208282471\n",
      "step = 6272000: loss = 4.023303508758545\n",
      "step = 6272200: loss = 3.3802342414855957\n",
      "step = 6272400: loss = 3.789429187774658\n",
      "step = 6272600: loss = 4.340953826904297\n",
      "step = 6272800: loss = 3.2330944538116455\n",
      "step = 6273000: loss = 2.864755868911743\n",
      "step = 6273200: loss = 4.7540507316589355\n",
      "step = 6273400: loss = 5.250471115112305\n",
      "step = 6273600: loss = 3.278167963027954\n",
      "step = 6273800: loss = 4.34539794921875\n",
      "step = 6274000: loss = 2.66574764251709\n",
      "step = 6274200: loss = 6.102654457092285\n",
      "step = 6274400: loss = 4.9525251388549805\n",
      "step = 6274600: loss = 4.0557990074157715\n",
      "step = 6274800: loss = 4.054192543029785\n",
      "step = 6275000: loss = 4.115732192993164\n",
      "step = 6275000: Average Return = 3.247999906539917\n",
      "step = 6275200: loss = 3.9201149940490723\n",
      "step = 6275400: loss = 3.9304449558258057\n",
      "step = 6275600: loss = 4.3258562088012695\n",
      "step = 6275800: loss = 3.7082836627960205\n",
      "step = 6276000: loss = 3.298128843307495\n",
      "step = 6276200: loss = 3.3643248081207275\n",
      "step = 6276400: loss = 4.837576866149902\n",
      "step = 6276600: loss = 5.0609893798828125\n",
      "step = 6276800: loss = 3.669525384902954\n",
      "step = 6277000: loss = 4.731858730316162\n",
      "step = 6277200: loss = 7.657858848571777\n",
      "step = 6277400: loss = 4.703867435455322\n",
      "step = 6277600: loss = 4.6909098625183105\n",
      "step = 6277800: loss = 5.189064979553223\n",
      "step = 6278000: loss = 4.131582736968994\n",
      "step = 6278200: loss = 3.9158284664154053\n",
      "step = 6278400: loss = 3.5152058601379395\n",
      "step = 6278600: loss = 3.1827409267425537\n",
      "step = 6278800: loss = 5.461110591888428\n",
      "step = 6279000: loss = 3.3812875747680664\n",
      "step = 6279200: loss = 2.7609710693359375\n",
      "step = 6279400: loss = 4.401032447814941\n",
      "step = 6279600: loss = 4.7609968185424805\n",
      "step = 6279800: loss = 4.444930076599121\n",
      "step = 6280000: loss = 4.331567287445068\n",
      "step = 6280000: Average Return = 3.3559999465942383\n",
      "step = 6280200: loss = 5.698138236999512\n",
      "step = 6280400: loss = 3.1590993404388428\n",
      "step = 6280600: loss = 4.1678595542907715\n",
      "step = 6280800: loss = 5.575956344604492\n",
      "step = 6281000: loss = 4.882671356201172\n",
      "step = 6281200: loss = 4.7301483154296875\n",
      "step = 6281400: loss = 4.173111438751221\n",
      "step = 6281600: loss = 4.137606143951416\n",
      "step = 6281800: loss = 3.5652260780334473\n",
      "step = 6282000: loss = 3.443565607070923\n",
      "step = 6282200: loss = 4.379671096801758\n",
      "step = 6282400: loss = 4.249260902404785\n",
      "step = 6282600: loss = 4.705373764038086\n",
      "step = 6282800: loss = 4.320886611938477\n",
      "step = 6283000: loss = 4.173130989074707\n",
      "step = 6283200: loss = 5.439367771148682\n",
      "step = 6283400: loss = 2.722818613052368\n",
      "step = 6283600: loss = 5.112337112426758\n",
      "step = 6283800: loss = 5.59975528717041\n",
      "step = 6284000: loss = 5.084953308105469\n",
      "step = 6284200: loss = 3.19107985496521\n",
      "step = 6284400: loss = 3.8165407180786133\n",
      "step = 6284600: loss = 4.346116065979004\n",
      "step = 6284800: loss = 3.599400520324707\n",
      "step = 6285000: loss = 3.3977208137512207\n",
      "step = 6285000: Average Return = 3.6640000343322754\n",
      "step = 6285200: loss = 4.5544586181640625\n",
      "step = 6285400: loss = 3.357224464416504\n",
      "step = 6285600: loss = 4.21514368057251\n",
      "step = 6285800: loss = 3.3704833984375\n",
      "step = 6286000: loss = 4.067566871643066\n",
      "step = 6286200: loss = 3.133296489715576\n",
      "step = 6286400: loss = 4.35134220123291\n",
      "step = 6286600: loss = 3.2459630966186523\n",
      "step = 6286800: loss = 3.4392449855804443\n",
      "step = 6287000: loss = 3.556650400161743\n",
      "step = 6287200: loss = 3.8540596961975098\n",
      "step = 6287400: loss = 2.8665177822113037\n",
      "step = 6287600: loss = 4.296408176422119\n",
      "step = 6287800: loss = 5.018021583557129\n",
      "step = 6288000: loss = 5.362214088439941\n",
      "step = 6288200: loss = 3.8702375888824463\n",
      "step = 6288400: loss = 4.6413397789001465\n",
      "step = 6288600: loss = 3.4308886528015137\n",
      "step = 6288800: loss = 2.8697497844696045\n",
      "step = 6289000: loss = 5.448864459991455\n",
      "step = 6289200: loss = 2.9362926483154297\n",
      "step = 6289400: loss = 3.569523572921753\n",
      "step = 6289600: loss = 4.0649733543396\n",
      "step = 6289800: loss = 4.016995906829834\n",
      "step = 6290000: loss = 4.990926265716553\n",
      "step = 6290000: Average Return = 4.1579999923706055\n",
      "step = 6290200: loss = 4.991072177886963\n",
      "step = 6290400: loss = 4.607124328613281\n",
      "step = 6290600: loss = 3.580336570739746\n",
      "step = 6290800: loss = 5.357707500457764\n",
      "step = 6291000: loss = 4.513411045074463\n",
      "step = 6291200: loss = 4.606900691986084\n",
      "step = 6291400: loss = 4.552423477172852\n",
      "step = 6291600: loss = 4.220142841339111\n",
      "step = 6291800: loss = 5.728953838348389\n",
      "step = 6292000: loss = 3.175734758377075\n",
      "step = 6292200: loss = 5.122293472290039\n",
      "step = 6292400: loss = 4.1402692794799805\n",
      "step = 6292600: loss = 3.1606836318969727\n",
      "step = 6292800: loss = 4.841750144958496\n",
      "step = 6293000: loss = 4.48947811126709\n",
      "step = 6293200: loss = 2.9049086570739746\n",
      "step = 6293400: loss = 3.547750473022461\n",
      "step = 6293600: loss = 5.435676097869873\n",
      "step = 6293800: loss = 2.95314621925354\n",
      "step = 6294000: loss = 3.822266101837158\n",
      "step = 6294200: loss = 4.137729167938232\n",
      "step = 6294400: loss = 3.590198278427124\n",
      "step = 6294600: loss = 5.794641971588135\n",
      "step = 6294800: loss = 3.921342134475708\n",
      "step = 6295000: loss = 3.9020581245422363\n",
      "step = 6295000: Average Return = 4.019999980926514\n",
      "step = 6295200: loss = 5.418731689453125\n",
      "step = 6295400: loss = 4.09397554397583\n",
      "step = 6295600: loss = 6.203258514404297\n",
      "step = 6295800: loss = 4.725012302398682\n",
      "step = 6296000: loss = 4.217887878417969\n",
      "step = 6296200: loss = 5.581614017486572\n",
      "step = 6296400: loss = 5.390860557556152\n",
      "step = 6296600: loss = 3.477238178253174\n",
      "step = 6296800: loss = 4.242952823638916\n",
      "step = 6297000: loss = 4.758066177368164\n",
      "step = 6297200: loss = 4.971655368804932\n",
      "step = 6297400: loss = 3.0100767612457275\n",
      "step = 6297600: loss = 4.76143217086792\n",
      "step = 6297800: loss = 3.826256275177002\n",
      "step = 6298000: loss = 3.9290647506713867\n",
      "step = 6298200: loss = 5.55174446105957\n",
      "step = 6298400: loss = 4.232034683227539\n",
      "step = 6298600: loss = 3.185713529586792\n",
      "step = 6298800: loss = 4.970585823059082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6299000: loss = 4.275912761688232\n",
      "step = 6299200: loss = 3.5583183765411377\n",
      "step = 6299400: loss = 4.483878135681152\n",
      "step = 6299600: loss = 4.400668621063232\n",
      "step = 6299800: loss = 4.370092391967773\n",
      "step = 6300000: loss = 3.802656888961792\n",
      "step = 6300000: Average Return = 3.6059999465942383\n",
      "step = 6300200: loss = 4.153077602386475\n",
      "step = 6300400: loss = 2.4837868213653564\n",
      "step = 6300600: loss = 3.575190305709839\n",
      "step = 6300800: loss = 4.576803207397461\n",
      "step = 6301000: loss = 4.884060382843018\n",
      "step = 6301200: loss = 5.2165350914001465\n",
      "step = 6301400: loss = 4.259921073913574\n",
      "step = 6301600: loss = 5.683173656463623\n",
      "step = 6301800: loss = 5.029433727264404\n",
      "step = 6302000: loss = 5.082563877105713\n",
      "step = 6302200: loss = 4.365467548370361\n",
      "step = 6302400: loss = 3.893674850463867\n",
      "step = 6302600: loss = 3.710177421569824\n",
      "step = 6302800: loss = 4.432351589202881\n",
      "step = 6303000: loss = 5.351052761077881\n",
      "step = 6303200: loss = 3.7062861919403076\n",
      "step = 6303400: loss = 5.298157691955566\n",
      "step = 6303600: loss = 4.872957229614258\n",
      "step = 6303800: loss = 4.35703706741333\n",
      "step = 6304000: loss = 4.154235363006592\n",
      "step = 6304200: loss = 5.526020526885986\n",
      "step = 6304400: loss = 3.863865852355957\n",
      "step = 6304600: loss = 3.7307207584381104\n",
      "step = 6304800: loss = 5.28629732131958\n",
      "step = 6305000: loss = 4.227351665496826\n",
      "step = 6305000: Average Return = 3.865999937057495\n",
      "step = 6305200: loss = 4.285673141479492\n",
      "step = 6305400: loss = 4.77524995803833\n",
      "step = 6305600: loss = 3.6454594135284424\n",
      "step = 6305800: loss = 3.200331926345825\n",
      "step = 6306000: loss = 3.2779040336608887\n",
      "step = 6306200: loss = 4.639522075653076\n",
      "step = 6306400: loss = 3.416097402572632\n",
      "step = 6306600: loss = 4.5906662940979\n",
      "step = 6306800: loss = 3.209315776824951\n",
      "step = 6307000: loss = 4.560063362121582\n",
      "step = 6307200: loss = 5.587751388549805\n",
      "step = 6307400: loss = 2.736741542816162\n",
      "step = 6307600: loss = 3.8586437702178955\n",
      "step = 6307800: loss = 4.7584991455078125\n",
      "step = 6308000: loss = 5.0631537437438965\n",
      "step = 6308200: loss = 3.918134927749634\n",
      "step = 6308400: loss = 5.327539443969727\n",
      "step = 6308600: loss = 5.598493576049805\n",
      "step = 6308800: loss = 4.540416240692139\n",
      "step = 6309000: loss = 5.564614295959473\n",
      "step = 6309200: loss = 4.005868434906006\n",
      "step = 6309400: loss = 4.07798957824707\n",
      "step = 6309600: loss = 4.562558650970459\n",
      "step = 6309800: loss = 5.143016338348389\n",
      "step = 6310000: loss = 2.8190720081329346\n",
      "step = 6310000: Average Return = 3.8919999599456787\n",
      "step = 6310200: loss = 3.837455987930298\n",
      "step = 6310400: loss = 3.791231393814087\n",
      "step = 6310600: loss = 3.380788564682007\n",
      "step = 6310800: loss = 4.351259231567383\n",
      "step = 6311000: loss = 5.408693790435791\n",
      "step = 6311200: loss = 4.792352676391602\n",
      "step = 6311400: loss = 5.002180576324463\n",
      "step = 6311600: loss = 3.8416576385498047\n",
      "step = 6311800: loss = 4.828484535217285\n",
      "step = 6312000: loss = 4.795097351074219\n",
      "step = 6312200: loss = 4.743706703186035\n",
      "step = 6312400: loss = 3.3812782764434814\n",
      "step = 6312600: loss = 4.509536266326904\n",
      "step = 6312800: loss = 4.483985424041748\n",
      "step = 6313000: loss = 4.709232807159424\n",
      "step = 6313200: loss = 4.484626770019531\n",
      "step = 6313400: loss = 4.647363662719727\n",
      "step = 6313600: loss = 3.758661985397339\n",
      "step = 6313800: loss = 4.367565631866455\n",
      "step = 6314000: loss = 4.409775733947754\n",
      "step = 6314200: loss = 4.647011756896973\n",
      "step = 6314400: loss = 5.332181930541992\n",
      "step = 6314600: loss = 4.227027416229248\n",
      "step = 6314800: loss = 3.7266042232513428\n",
      "step = 6315000: loss = 3.273715019226074\n",
      "step = 6315000: Average Return = 3.941999912261963\n",
      "step = 6315200: loss = 4.472278118133545\n",
      "step = 6315400: loss = 3.2475171089172363\n",
      "step = 6315600: loss = 5.322249412536621\n",
      "step = 6315800: loss = 3.8087642192840576\n",
      "step = 6316000: loss = 3.113440990447998\n",
      "step = 6316200: loss = 4.258014678955078\n",
      "step = 6316400: loss = 4.559059143066406\n",
      "step = 6316600: loss = 2.86633038520813\n",
      "step = 6316800: loss = 5.278702259063721\n",
      "step = 6317000: loss = 5.316915035247803\n",
      "step = 6317200: loss = 3.2368242740631104\n",
      "step = 6317400: loss = 3.395533323287964\n",
      "step = 6317600: loss = 4.536109924316406\n",
      "step = 6317800: loss = 3.206012487411499\n",
      "step = 6318000: loss = 4.065821647644043\n",
      "step = 6318200: loss = 4.014898777008057\n",
      "step = 6318400: loss = 4.796275615692139\n",
      "step = 6318600: loss = 3.4020729064941406\n",
      "step = 6318800: loss = 4.185707092285156\n",
      "step = 6319000: loss = 4.55072546005249\n",
      "step = 6319200: loss = 2.792708158493042\n",
      "step = 6319400: loss = 5.634227275848389\n",
      "step = 6319600: loss = 4.261085510253906\n",
      "step = 6319800: loss = 2.973621129989624\n",
      "step = 6320000: loss = 5.156435489654541\n",
      "step = 6320000: Average Return = 4.0\n",
      "step = 6320200: loss = 4.498185634613037\n",
      "step = 6320400: loss = 4.1748762130737305\n",
      "step = 6320600: loss = 5.569502353668213\n",
      "step = 6320800: loss = 4.076364994049072\n",
      "step = 6321000: loss = 3.618488073348999\n",
      "step = 6321200: loss = 3.0255372524261475\n",
      "step = 6321400: loss = 3.6227614879608154\n",
      "step = 6321600: loss = 2.9404540061950684\n",
      "step = 6321800: loss = 5.740893363952637\n",
      "step = 6322000: loss = 4.5587897300720215\n",
      "step = 6322200: loss = 4.47922945022583\n",
      "step = 6322400: loss = 4.053444862365723\n",
      "step = 6322600: loss = 4.081121921539307\n",
      "step = 6322800: loss = 4.112236022949219\n",
      "step = 6323000: loss = 4.017547607421875\n",
      "step = 6323200: loss = 4.6688032150268555\n",
      "step = 6323400: loss = 4.098216533660889\n",
      "step = 6323600: loss = 4.3307695388793945\n",
      "step = 6323800: loss = 4.866759300231934\n",
      "step = 6324000: loss = 4.257638931274414\n",
      "step = 6324200: loss = 4.028420448303223\n",
      "step = 6324400: loss = 4.687737941741943\n",
      "step = 6324600: loss = 4.391718864440918\n",
      "step = 6324800: loss = 5.162019729614258\n",
      "step = 6325000: loss = 3.5903453826904297\n",
      "step = 6325000: Average Return = 3.7699999809265137\n",
      "step = 6325200: loss = 4.896143913269043\n",
      "step = 6325400: loss = 4.899771690368652\n",
      "step = 6325600: loss = 2.9363911151885986\n",
      "step = 6325800: loss = 3.287367820739746\n",
      "step = 6326000: loss = 3.3604371547698975\n",
      "step = 6326200: loss = 3.818179130554199\n",
      "step = 6326400: loss = 3.403048515319824\n",
      "step = 6326600: loss = 3.246737003326416\n",
      "step = 6326800: loss = 4.95696496963501\n",
      "step = 6327000: loss = 3.425868511199951\n",
      "step = 6327200: loss = 3.178021192550659\n",
      "step = 6327400: loss = 4.825807571411133\n",
      "step = 6327600: loss = 4.2599053382873535\n",
      "step = 6327800: loss = 3.2817182540893555\n",
      "step = 6328000: loss = 3.419055700302124\n",
      "step = 6328200: loss = 3.680471658706665\n",
      "step = 6328400: loss = 5.176990985870361\n",
      "step = 6328600: loss = 2.7342376708984375\n",
      "step = 6328800: loss = 4.589443206787109\n",
      "step = 6329000: loss = 4.113100528717041\n",
      "step = 6329200: loss = 4.252858638763428\n",
      "step = 6329400: loss = 3.861168146133423\n",
      "step = 6329600: loss = 4.119418621063232\n",
      "step = 6329800: loss = 3.7329540252685547\n",
      "step = 6330000: loss = 5.01768684387207\n",
      "step = 6330000: Average Return = 3.7039999961853027\n",
      "step = 6330200: loss = 5.660902976989746\n",
      "step = 6330400: loss = 5.885750770568848\n",
      "step = 6330600: loss = 5.436399459838867\n",
      "step = 6330800: loss = 2.455418348312378\n",
      "step = 6331000: loss = 2.975374937057495\n",
      "step = 6331200: loss = 3.633838415145874\n",
      "step = 6331400: loss = 3.96777081489563\n",
      "step = 6331600: loss = 2.9735918045043945\n",
      "step = 6331800: loss = 2.8068559169769287\n",
      "step = 6332000: loss = 3.4912939071655273\n",
      "step = 6332200: loss = 3.7355358600616455\n",
      "step = 6332400: loss = 2.6238532066345215\n",
      "step = 6332600: loss = 2.7828545570373535\n",
      "step = 6332800: loss = 4.141299247741699\n",
      "step = 6333000: loss = 4.01979398727417\n",
      "step = 6333200: loss = 4.163034439086914\n",
      "step = 6333400: loss = 5.471189022064209\n",
      "step = 6333600: loss = 4.637361526489258\n",
      "step = 6333800: loss = 3.7111668586730957\n",
      "step = 6334000: loss = 3.906315565109253\n",
      "step = 6334200: loss = 3.6825692653656006\n",
      "step = 6334400: loss = 4.338846206665039\n",
      "step = 6334600: loss = 2.8688747882843018\n",
      "step = 6334800: loss = 4.17898416519165\n",
      "step = 6335000: loss = 4.022754192352295\n",
      "step = 6335000: Average Return = 4.349999904632568\n",
      "step = 6335200: loss = 4.44403076171875\n",
      "step = 6335400: loss = 3.36952805519104\n",
      "step = 6335600: loss = 2.240696430206299\n",
      "step = 6335800: loss = 4.284521579742432\n",
      "step = 6336000: loss = 6.919338226318359\n",
      "step = 6336200: loss = 3.9562947750091553\n",
      "step = 6336400: loss = 3.962977409362793\n",
      "step = 6336600: loss = 4.0200324058532715\n",
      "step = 6336800: loss = 5.191293239593506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6337000: loss = 3.5730738639831543\n",
      "step = 6337200: loss = 3.1603190898895264\n",
      "step = 6337400: loss = 4.736087799072266\n",
      "step = 6337600: loss = 4.168814182281494\n",
      "step = 6337800: loss = 3.554689884185791\n",
      "step = 6338000: loss = 5.198655128479004\n",
      "step = 6338200: loss = 4.198616027832031\n",
      "step = 6338400: loss = 4.830876350402832\n",
      "step = 6338600: loss = 4.9605865478515625\n",
      "step = 6338800: loss = 5.005278587341309\n",
      "step = 6339000: loss = 6.41942024230957\n",
      "step = 6339200: loss = 4.447668552398682\n",
      "step = 6339400: loss = 4.434942245483398\n",
      "step = 6339600: loss = 4.0697526931762695\n",
      "step = 6339800: loss = 4.699546813964844\n",
      "step = 6340000: loss = 3.945263624191284\n",
      "step = 6340000: Average Return = 3.8440001010894775\n",
      "step = 6340200: loss = 5.644725322723389\n",
      "step = 6340400: loss = 5.279757499694824\n",
      "step = 6340600: loss = 4.325854778289795\n",
      "step = 6340800: loss = 4.530282974243164\n",
      "step = 6341000: loss = 4.474656581878662\n",
      "step = 6341200: loss = 4.057031631469727\n",
      "step = 6341400: loss = 6.2584710121154785\n",
      "step = 6341600: loss = 5.699129104614258\n",
      "step = 6341800: loss = 4.8257293701171875\n",
      "step = 6342000: loss = 3.63946533203125\n",
      "step = 6342200: loss = 4.218587875366211\n",
      "step = 6342400: loss = 4.484632968902588\n",
      "step = 6342600: loss = 4.45965576171875\n",
      "step = 6342800: loss = 3.5750157833099365\n",
      "step = 6343000: loss = 4.482210159301758\n",
      "step = 6343200: loss = 4.367625713348389\n",
      "step = 6343400: loss = 5.641513347625732\n",
      "step = 6343600: loss = 4.253610610961914\n",
      "step = 6343800: loss = 4.277385711669922\n",
      "step = 6344000: loss = 5.981132507324219\n",
      "step = 6344200: loss = 3.849865674972534\n",
      "step = 6344400: loss = 4.475061893463135\n",
      "step = 6344600: loss = 2.907947301864624\n",
      "step = 6344800: loss = 4.94881010055542\n",
      "step = 6345000: loss = 3.837156295776367\n",
      "step = 6345000: Average Return = 3.691999912261963\n",
      "step = 6345200: loss = 3.5827512741088867\n",
      "step = 6345400: loss = 3.588393211364746\n",
      "step = 6345600: loss = 3.896224021911621\n",
      "step = 6345800: loss = 3.721515417098999\n",
      "step = 6346000: loss = 5.00554084777832\n",
      "step = 6346200: loss = 3.6820068359375\n",
      "step = 6346400: loss = 4.10904598236084\n",
      "step = 6346600: loss = 4.210338115692139\n",
      "step = 6346800: loss = 4.0660200119018555\n",
      "step = 6347000: loss = 4.214195251464844\n",
      "step = 6347200: loss = 3.065703868865967\n",
      "step = 6347400: loss = 3.8224263191223145\n",
      "step = 6347600: loss = 4.0589919090271\n",
      "step = 6347800: loss = 5.125309944152832\n",
      "step = 6348000: loss = 3.731156587600708\n",
      "step = 6348200: loss = 3.262272596359253\n",
      "step = 6348400: loss = 3.947826385498047\n",
      "step = 6348600: loss = 3.638826608657837\n",
      "step = 6348800: loss = 3.9905548095703125\n",
      "step = 6349000: loss = 4.025457859039307\n",
      "step = 6349200: loss = 3.946214437484741\n",
      "step = 6349400: loss = 3.09641695022583\n",
      "step = 6349600: loss = 3.7665090560913086\n",
      "step = 6349800: loss = 3.346513509750366\n",
      "step = 6350000: loss = 4.92847204208374\n",
      "step = 6350000: Average Return = 3.8940000534057617\n",
      "step = 6350200: loss = 4.790548801422119\n",
      "step = 6350400: loss = 2.9724831581115723\n",
      "step = 6350600: loss = 5.168513774871826\n",
      "step = 6350800: loss = 4.234411716461182\n",
      "step = 6351000: loss = 2.717193603515625\n",
      "step = 6351200: loss = 5.113921165466309\n",
      "step = 6351400: loss = 4.9318695068359375\n",
      "step = 6351600: loss = 3.2368004322052\n",
      "step = 6351800: loss = 2.743333101272583\n",
      "step = 6352000: loss = 3.140981912612915\n",
      "step = 6352200: loss = 3.445209264755249\n",
      "step = 6352400: loss = 6.009408950805664\n",
      "step = 6352600: loss = 5.262669563293457\n",
      "step = 6352800: loss = 4.950741767883301\n",
      "step = 6353000: loss = 4.9146552085876465\n",
      "step = 6353200: loss = 4.21793270111084\n",
      "step = 6353400: loss = 3.3697595596313477\n",
      "step = 6353600: loss = 5.1611480712890625\n",
      "step = 6353800: loss = 4.796902179718018\n",
      "step = 6354000: loss = 4.806970596313477\n",
      "step = 6354200: loss = 3.1477246284484863\n",
      "step = 6354400: loss = 2.262598752975464\n",
      "step = 6354600: loss = 4.745360851287842\n",
      "step = 6354800: loss = 3.7454817295074463\n",
      "step = 6355000: loss = 5.612002849578857\n",
      "step = 6355000: Average Return = 3.7279999256134033\n",
      "step = 6355200: loss = 3.800462484359741\n",
      "step = 6355400: loss = 3.61525297164917\n",
      "step = 6355600: loss = 3.5616748332977295\n",
      "step = 6355800: loss = 2.3733577728271484\n",
      "step = 6356000: loss = 4.784035682678223\n",
      "step = 6356200: loss = 5.075902462005615\n",
      "step = 6356400: loss = 4.695438861846924\n",
      "step = 6356600: loss = 3.9092047214508057\n",
      "step = 6356800: loss = 3.630131721496582\n",
      "step = 6357000: loss = 5.201366901397705\n",
      "step = 6357200: loss = 2.8108482360839844\n",
      "step = 6357400: loss = 3.6145079135894775\n",
      "step = 6357600: loss = 3.6399056911468506\n",
      "step = 6357800: loss = 3.5352489948272705\n",
      "step = 6358000: loss = 4.586197376251221\n",
      "step = 6358200: loss = 3.612403392791748\n",
      "step = 6358400: loss = 4.907082557678223\n",
      "step = 6358600: loss = 3.938384532928467\n",
      "step = 6358800: loss = 4.900813102722168\n",
      "step = 6359000: loss = 3.2848150730133057\n",
      "step = 6359200: loss = 5.516144275665283\n",
      "step = 6359400: loss = 3.6419568061828613\n",
      "step = 6359600: loss = 4.539651870727539\n",
      "step = 6359800: loss = 3.2696139812469482\n",
      "step = 6360000: loss = 5.020808696746826\n",
      "step = 6360000: Average Return = 3.941999912261963\n",
      "step = 6360200: loss = 3.403118133544922\n",
      "step = 6360400: loss = 3.3984086513519287\n",
      "step = 6360600: loss = 4.096030235290527\n",
      "step = 6360800: loss = 5.605630397796631\n",
      "step = 6361000: loss = 3.98649001121521\n",
      "step = 6361200: loss = 3.1158082485198975\n",
      "step = 6361400: loss = 3.8172850608825684\n",
      "step = 6361600: loss = 4.356757164001465\n",
      "step = 6361800: loss = 3.8109724521636963\n",
      "step = 6362000: loss = 3.999408483505249\n",
      "step = 6362200: loss = 3.9762816429138184\n",
      "step = 6362400: loss = 3.6449546813964844\n",
      "step = 6362600: loss = 3.144134998321533\n",
      "step = 6362800: loss = 3.7655739784240723\n",
      "step = 6363000: loss = 5.082095146179199\n",
      "step = 6363200: loss = 3.73221755027771\n",
      "step = 6363400: loss = 5.1871256828308105\n",
      "step = 6363600: loss = 4.152562141418457\n",
      "step = 6363800: loss = 4.468127727508545\n",
      "step = 6364000: loss = 3.763378143310547\n",
      "step = 6364200: loss = 3.5732405185699463\n",
      "step = 6364400: loss = 4.2127580642700195\n",
      "step = 6364600: loss = 4.3891448974609375\n",
      "step = 6364800: loss = 3.681730270385742\n",
      "step = 6365000: loss = 3.1343226432800293\n",
      "step = 6365000: Average Return = 3.671999931335449\n",
      "step = 6365200: loss = 3.9641876220703125\n",
      "step = 6365400: loss = 3.7835564613342285\n",
      "step = 6365600: loss = 4.213389873504639\n",
      "step = 6365800: loss = 3.7717034816741943\n",
      "step = 6366000: loss = 3.1065897941589355\n",
      "step = 6366200: loss = 3.184988260269165\n",
      "step = 6366400: loss = 3.652374505996704\n",
      "step = 6366600: loss = 4.765774250030518\n",
      "step = 6366800: loss = 2.543368101119995\n",
      "step = 6367000: loss = 3.9739151000976562\n",
      "step = 6367200: loss = 3.968738317489624\n",
      "step = 6367400: loss = 3.5244393348693848\n",
      "step = 6367600: loss = 4.502344131469727\n",
      "step = 6367800: loss = 3.3204307556152344\n",
      "step = 6368000: loss = 3.2374303340911865\n",
      "step = 6368200: loss = 3.9902665615081787\n",
      "step = 6368400: loss = 3.309539794921875\n",
      "step = 6368600: loss = 5.708378791809082\n",
      "step = 6368800: loss = 3.291160821914673\n",
      "step = 6369000: loss = 5.729176044464111\n",
      "step = 6369200: loss = 2.445375919342041\n",
      "step = 6369400: loss = 3.6940865516662598\n",
      "step = 6369600: loss = 3.63211727142334\n",
      "step = 6369800: loss = 5.788368225097656\n",
      "step = 6370000: loss = 3.3726646900177\n",
      "step = 6370000: Average Return = 3.6740000247955322\n",
      "step = 6370200: loss = 4.622119426727295\n",
      "step = 6370400: loss = 4.298373222351074\n",
      "step = 6370600: loss = 3.345203399658203\n",
      "step = 6370800: loss = 3.345844030380249\n",
      "step = 6371000: loss = 4.043997764587402\n",
      "step = 6371200: loss = 4.015380382537842\n",
      "step = 6371400: loss = 4.338229179382324\n",
      "step = 6371600: loss = 4.998095989227295\n",
      "step = 6371800: loss = 4.971381187438965\n",
      "step = 6372000: loss = 4.906272888183594\n",
      "step = 6372200: loss = 5.068386554718018\n",
      "step = 6372400: loss = 3.393508195877075\n",
      "step = 6372600: loss = 3.7389702796936035\n",
      "step = 6372800: loss = 4.8733391761779785\n",
      "step = 6373000: loss = 4.280268669128418\n",
      "step = 6373200: loss = 4.157516002655029\n",
      "step = 6373400: loss = 4.746519088745117\n",
      "step = 6373600: loss = 4.228930950164795\n",
      "step = 6373800: loss = 3.7908473014831543\n",
      "step = 6374000: loss = 4.306589126586914\n",
      "step = 6374200: loss = 5.39955472946167\n",
      "step = 6374400: loss = 3.284212350845337\n",
      "step = 6374600: loss = 3.9177610874176025\n",
      "step = 6374800: loss = 4.645484447479248\n",
      "step = 6375000: loss = 4.1861066818237305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6375000: Average Return = 4.035999774932861\n",
      "step = 6375200: loss = 3.6994705200195312\n",
      "step = 6375400: loss = 3.0986571311950684\n",
      "step = 6375600: loss = 5.169196605682373\n",
      "step = 6375800: loss = 5.197405815124512\n",
      "step = 6376000: loss = 4.263162612915039\n",
      "step = 6376200: loss = 3.5512588024139404\n",
      "step = 6376400: loss = 3.561013698577881\n",
      "step = 6376600: loss = 3.176813840866089\n",
      "step = 6376800: loss = 2.9925954341888428\n",
      "step = 6377000: loss = 5.741820335388184\n",
      "step = 6377200: loss = 4.790419101715088\n",
      "step = 6377400: loss = 4.27808141708374\n",
      "step = 6377600: loss = 5.566009521484375\n",
      "step = 6377800: loss = 3.6396567821502686\n",
      "step = 6378000: loss = 2.978137731552124\n",
      "step = 6378200: loss = 3.1940407752990723\n",
      "step = 6378400: loss = 3.565073251724243\n",
      "step = 6378600: loss = 5.38675594329834\n",
      "step = 6378800: loss = 5.236226558685303\n",
      "step = 6379000: loss = 3.5323503017425537\n",
      "step = 6379200: loss = 3.1611921787261963\n",
      "step = 6379400: loss = 4.754302501678467\n",
      "step = 6379600: loss = 3.8144946098327637\n",
      "step = 6379800: loss = 4.523617267608643\n",
      "step = 6380000: loss = 3.3485710620880127\n",
      "step = 6380000: Average Return = 4.01800012588501\n",
      "step = 6380200: loss = 4.705906391143799\n",
      "step = 6380400: loss = 4.398079872131348\n",
      "step = 6380600: loss = 4.475900650024414\n",
      "step = 6380800: loss = 3.9777145385742188\n",
      "step = 6381000: loss = 4.578134536743164\n",
      "step = 6381200: loss = 3.6769773960113525\n",
      "step = 6381400: loss = 4.087479591369629\n",
      "step = 6381600: loss = 3.563633441925049\n",
      "step = 6381800: loss = 4.328252792358398\n",
      "step = 6382000: loss = 4.1510443687438965\n",
      "step = 6382200: loss = 4.866290092468262\n",
      "step = 6382400: loss = 3.0588133335113525\n",
      "step = 6382600: loss = 3.8059539794921875\n",
      "step = 6382800: loss = 4.2809367179870605\n",
      "step = 6383000: loss = 4.894694805145264\n",
      "step = 6383200: loss = 3.1104142665863037\n",
      "step = 6383400: loss = 4.911646366119385\n",
      "step = 6383600: loss = 4.347666263580322\n",
      "step = 6383800: loss = 6.0499162673950195\n",
      "step = 6384000: loss = 6.437779426574707\n",
      "step = 6384200: loss = 4.100062370300293\n",
      "step = 6384400: loss = 2.9453349113464355\n",
      "step = 6384600: loss = 4.087911128997803\n",
      "step = 6384800: loss = 4.0159173011779785\n",
      "step = 6385000: loss = 4.692404270172119\n",
      "step = 6385000: Average Return = 3.9579999446868896\n",
      "step = 6385200: loss = 4.280150890350342\n",
      "step = 6385400: loss = 4.948098182678223\n",
      "step = 6385600: loss = 3.542048931121826\n",
      "step = 6385800: loss = 4.920483112335205\n",
      "step = 6386000: loss = 4.019303798675537\n",
      "step = 6386200: loss = 5.221359729766846\n",
      "step = 6386400: loss = 3.6549232006073\n",
      "step = 6386600: loss = 3.806145191192627\n",
      "step = 6386800: loss = 4.0838422775268555\n",
      "step = 6387000: loss = 4.6128926277160645\n",
      "step = 6387200: loss = 2.1337413787841797\n",
      "step = 6387400: loss = 4.590207576751709\n",
      "step = 6387600: loss = 4.136455059051514\n",
      "step = 6387800: loss = 2.7155981063842773\n",
      "step = 6388000: loss = 4.322324275970459\n",
      "step = 6388200: loss = 3.319889545440674\n",
      "step = 6388400: loss = 4.801747798919678\n",
      "step = 6388600: loss = 4.860970973968506\n",
      "step = 6388800: loss = 4.926523208618164\n",
      "step = 6389000: loss = 3.364779472351074\n",
      "step = 6389200: loss = 3.233095645904541\n",
      "step = 6389400: loss = 5.122846603393555\n",
      "step = 6389600: loss = 4.4907660484313965\n",
      "step = 6389800: loss = 4.269951343536377\n",
      "step = 6390000: loss = 2.68127703666687\n",
      "step = 6390000: Average Return = 4.026000022888184\n",
      "step = 6390200: loss = 4.977526664733887\n",
      "step = 6390400: loss = 5.599814414978027\n",
      "step = 6390600: loss = 3.5086870193481445\n",
      "step = 6390800: loss = 4.823779582977295\n",
      "step = 6391000: loss = 3.858214855194092\n",
      "step = 6391200: loss = 4.42473030090332\n",
      "step = 6391400: loss = 3.5865111351013184\n",
      "step = 6391600: loss = 5.303159236907959\n",
      "step = 6391800: loss = 5.427534103393555\n",
      "step = 6392000: loss = 4.072360515594482\n",
      "step = 6392200: loss = 3.4277923107147217\n",
      "step = 6392400: loss = 4.549030780792236\n",
      "step = 6392600: loss = 5.341318130493164\n",
      "step = 6392800: loss = 4.526674270629883\n",
      "step = 6393000: loss = 4.560037612915039\n",
      "step = 6393200: loss = 4.8062872886657715\n",
      "step = 6393400: loss = 4.568722248077393\n",
      "step = 6393600: loss = 3.9564602375030518\n",
      "step = 6393800: loss = 4.226817607879639\n",
      "step = 6394000: loss = 3.2056422233581543\n",
      "step = 6394200: loss = 3.364180326461792\n",
      "step = 6394400: loss = 3.9489734172821045\n",
      "step = 6394600: loss = 4.231626033782959\n",
      "step = 6394800: loss = 3.974290370941162\n",
      "step = 6395000: loss = 4.893933296203613\n",
      "step = 6395000: Average Return = 3.818000078201294\n",
      "step = 6395200: loss = 4.093693256378174\n",
      "step = 6395400: loss = 3.452944755554199\n",
      "step = 6395600: loss = 3.5514042377471924\n",
      "step = 6395800: loss = 4.33107328414917\n",
      "step = 6396000: loss = 3.897855043411255\n",
      "step = 6396200: loss = 4.97337007522583\n",
      "step = 6396400: loss = 6.061214447021484\n",
      "step = 6396600: loss = 3.3697614669799805\n",
      "step = 6396800: loss = 3.683232545852661\n",
      "step = 6397000: loss = 5.705130100250244\n",
      "step = 6397200: loss = 3.813232898712158\n",
      "step = 6397400: loss = 3.48110032081604\n",
      "step = 6397600: loss = 5.349989414215088\n",
      "step = 6397800: loss = 3.4093496799468994\n",
      "step = 6398000: loss = 2.8088388442993164\n",
      "step = 6398200: loss = 4.905786037445068\n",
      "step = 6398400: loss = 3.336315155029297\n",
      "step = 6398600: loss = 3.984938859939575\n",
      "step = 6398800: loss = 5.101388454437256\n",
      "step = 6399000: loss = 5.35288667678833\n",
      "step = 6399200: loss = 4.873147487640381\n",
      "step = 6399400: loss = 4.237412452697754\n",
      "step = 6399600: loss = 4.723852634429932\n",
      "step = 6399800: loss = 5.308533668518066\n",
      "step = 6400000: loss = 4.598355770111084\n",
      "step = 6400000: Average Return = 3.625999927520752\n",
      "step = 6400200: loss = 3.9869537353515625\n",
      "step = 6400400: loss = 4.805132865905762\n",
      "step = 6400600: loss = 4.202828407287598\n",
      "step = 6400800: loss = 3.2631304264068604\n",
      "step = 6401000: loss = 4.066442966461182\n",
      "step = 6401200: loss = 3.2238805294036865\n",
      "step = 6401400: loss = 4.356626510620117\n",
      "step = 6401600: loss = 4.331853866577148\n",
      "step = 6401800: loss = 3.7107696533203125\n",
      "step = 6402000: loss = 2.8386998176574707\n",
      "step = 6402200: loss = 3.3697166442871094\n",
      "step = 6402400: loss = 3.914457321166992\n",
      "step = 6402600: loss = 4.050530433654785\n",
      "step = 6402800: loss = 3.0344107151031494\n",
      "step = 6403000: loss = 5.075396537780762\n",
      "step = 6403200: loss = 3.999732494354248\n",
      "step = 6403400: loss = 3.1895525455474854\n",
      "step = 6403600: loss = 4.882349967956543\n",
      "step = 6403800: loss = 4.578386306762695\n",
      "step = 6404000: loss = 3.942751884460449\n",
      "step = 6404200: loss = 4.567472457885742\n",
      "step = 6404400: loss = 4.032280921936035\n",
      "step = 6404600: loss = 3.79050874710083\n",
      "step = 6404800: loss = 4.985622882843018\n",
      "step = 6405000: loss = 2.5226969718933105\n",
      "step = 6405000: Average Return = 3.9860000610351562\n",
      "step = 6405200: loss = 4.086303234100342\n",
      "step = 6405400: loss = 4.525731563568115\n",
      "step = 6405600: loss = 2.50723934173584\n",
      "step = 6405800: loss = 4.639766693115234\n",
      "step = 6406000: loss = 3.8519060611724854\n",
      "step = 6406200: loss = 4.498490333557129\n",
      "step = 6406400: loss = 5.130607604980469\n",
      "step = 6406600: loss = 4.2744035720825195\n",
      "step = 6406800: loss = 3.598001718521118\n",
      "step = 6407000: loss = 4.410598278045654\n",
      "step = 6407200: loss = 3.841973066329956\n",
      "step = 6407400: loss = 3.8491063117980957\n",
      "step = 6407600: loss = 3.712674379348755\n",
      "step = 6407800: loss = 4.632415771484375\n",
      "step = 6408000: loss = 3.1269264221191406\n",
      "step = 6408200: loss = 4.699156761169434\n",
      "step = 6408400: loss = 4.512630939483643\n",
      "step = 6408600: loss = 4.502551555633545\n",
      "step = 6408800: loss = 5.462630271911621\n",
      "step = 6409000: loss = 5.833414077758789\n",
      "step = 6409200: loss = 4.546760082244873\n",
      "step = 6409400: loss = 3.3903491497039795\n",
      "step = 6409600: loss = 5.090574741363525\n",
      "step = 6409800: loss = 3.829861879348755\n",
      "step = 6410000: loss = 4.320019245147705\n",
      "step = 6410000: Average Return = 3.8299999237060547\n",
      "step = 6410200: loss = 3.820247173309326\n",
      "step = 6410400: loss = 4.943721294403076\n",
      "step = 6410600: loss = 4.8590006828308105\n",
      "step = 6410800: loss = 4.2987470626831055\n",
      "step = 6411000: loss = 4.318156719207764\n",
      "step = 6411200: loss = 3.4446403980255127\n",
      "step = 6411400: loss = 3.824885129928589\n",
      "step = 6411600: loss = 4.709611892700195\n",
      "step = 6411800: loss = 4.5449628829956055\n",
      "step = 6412000: loss = 3.2733068466186523\n",
      "step = 6412200: loss = 4.870073318481445\n",
      "step = 6412400: loss = 3.8026435375213623\n",
      "step = 6412600: loss = 4.848444938659668\n",
      "step = 6412800: loss = 4.787656307220459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6413000: loss = 4.193041801452637\n",
      "step = 6413200: loss = 4.85526180267334\n",
      "step = 6413400: loss = 4.43080472946167\n",
      "step = 6413600: loss = 4.883271217346191\n",
      "step = 6413800: loss = 3.9211065769195557\n",
      "step = 6414000: loss = 5.7736663818359375\n",
      "step = 6414200: loss = 5.980485916137695\n",
      "step = 6414400: loss = 4.227238178253174\n",
      "step = 6414600: loss = 2.6330578327178955\n",
      "step = 6414800: loss = 4.045502185821533\n",
      "step = 6415000: loss = 3.573157548904419\n",
      "step = 6415000: Average Return = 3.890000104904175\n",
      "step = 6415200: loss = 4.245012283325195\n",
      "step = 6415400: loss = 4.203125\n",
      "step = 6415600: loss = 4.189751625061035\n",
      "step = 6415800: loss = 5.094022274017334\n",
      "step = 6416000: loss = 4.074767589569092\n",
      "step = 6416200: loss = 3.435704231262207\n",
      "step = 6416400: loss = 5.07464599609375\n",
      "step = 6416600: loss = 6.174068927764893\n",
      "step = 6416800: loss = 4.706142902374268\n",
      "step = 6417000: loss = 3.517591714859009\n",
      "step = 6417200: loss = 3.7838780879974365\n",
      "step = 6417400: loss = 3.175023078918457\n",
      "step = 6417600: loss = 3.6570613384246826\n",
      "step = 6417800: loss = 4.184003829956055\n",
      "step = 6418000: loss = 3.182537078857422\n",
      "step = 6418200: loss = 4.7671990394592285\n",
      "step = 6418400: loss = 4.681369304656982\n",
      "step = 6418600: loss = 5.962062835693359\n",
      "step = 6418800: loss = 4.108213901519775\n",
      "step = 6419000: loss = 4.205561637878418\n",
      "step = 6419200: loss = 3.0963377952575684\n",
      "step = 6419400: loss = 4.7142486572265625\n",
      "step = 6419600: loss = 4.391515254974365\n",
      "step = 6419800: loss = 4.534533500671387\n",
      "step = 6420000: loss = 3.586308240890503\n",
      "step = 6420000: Average Return = 3.8259999752044678\n",
      "step = 6420200: loss = 4.042445182800293\n",
      "step = 6420400: loss = 3.1271257400512695\n",
      "step = 6420600: loss = 4.466373920440674\n",
      "step = 6420800: loss = 4.273660182952881\n",
      "step = 6421000: loss = 4.7857160568237305\n",
      "step = 6421200: loss = 3.9778943061828613\n",
      "step = 6421400: loss = 3.999307870864868\n",
      "step = 6421600: loss = 3.8501904010772705\n",
      "step = 6421800: loss = 5.987276554107666\n",
      "step = 6422000: loss = 2.8343496322631836\n",
      "step = 6422200: loss = 4.993176460266113\n",
      "step = 6422400: loss = 4.290848731994629\n",
      "step = 6422600: loss = 5.324798583984375\n",
      "step = 6422800: loss = 4.900964260101318\n",
      "step = 6423000: loss = 4.0645928382873535\n",
      "step = 6423200: loss = 4.272019386291504\n",
      "step = 6423400: loss = 3.753115177154541\n",
      "step = 6423600: loss = 5.118277072906494\n",
      "step = 6423800: loss = 4.045345306396484\n",
      "step = 6424000: loss = 3.721984386444092\n",
      "step = 6424200: loss = 4.458930492401123\n",
      "step = 6424400: loss = 2.773061990737915\n",
      "step = 6424600: loss = 3.5767266750335693\n",
      "step = 6424800: loss = 5.851851463317871\n",
      "step = 6425000: loss = 5.436683654785156\n",
      "step = 6425000: Average Return = 4.078000068664551\n",
      "step = 6425200: loss = 4.098927974700928\n",
      "step = 6425400: loss = 4.4084014892578125\n",
      "step = 6425600: loss = 4.053305625915527\n",
      "step = 6425800: loss = 5.584957599639893\n",
      "step = 6426000: loss = 4.965297698974609\n",
      "step = 6426200: loss = 3.807114362716675\n",
      "step = 6426400: loss = 6.087853908538818\n",
      "step = 6426600: loss = 4.672208786010742\n",
      "step = 6426800: loss = 4.729872703552246\n",
      "step = 6427000: loss = 4.067111492156982\n",
      "step = 6427200: loss = 4.007447242736816\n",
      "step = 6427400: loss = 3.6127254962921143\n",
      "step = 6427600: loss = 4.554740905761719\n",
      "step = 6427800: loss = 4.522892951965332\n",
      "step = 6428000: loss = 5.817797660827637\n",
      "step = 6428200: loss = 4.189002513885498\n",
      "step = 6428400: loss = 3.6883654594421387\n",
      "step = 6428600: loss = 3.209369421005249\n",
      "step = 6428800: loss = 4.136600971221924\n",
      "step = 6429000: loss = 4.014436721801758\n",
      "step = 6429200: loss = 3.5752675533294678\n",
      "step = 6429400: loss = 4.502889156341553\n",
      "step = 6429600: loss = 5.769581317901611\n",
      "step = 6429800: loss = 5.00716495513916\n",
      "step = 6430000: loss = 2.3922743797302246\n",
      "step = 6430000: Average Return = 3.6540000438690186\n",
      "step = 6430200: loss = 4.780736446380615\n",
      "step = 6430400: loss = 3.435685873031616\n",
      "step = 6430600: loss = 5.187519550323486\n",
      "step = 6430800: loss = 4.233876705169678\n",
      "step = 6431000: loss = 3.254361391067505\n",
      "step = 6431200: loss = 3.5354485511779785\n",
      "step = 6431400: loss = 4.5186262130737305\n",
      "step = 6431600: loss = 5.204889297485352\n",
      "step = 6431800: loss = 4.196033000946045\n",
      "step = 6432000: loss = 3.076329231262207\n",
      "step = 6432200: loss = 2.825824022293091\n",
      "step = 6432400: loss = 4.3801069259643555\n",
      "step = 6432600: loss = 3.1034514904022217\n",
      "step = 6432800: loss = 6.283244609832764\n",
      "step = 6433000: loss = 3.387622594833374\n",
      "step = 6433200: loss = 4.975443363189697\n",
      "step = 6433400: loss = 4.0749616622924805\n",
      "step = 6433600: loss = 5.050259113311768\n",
      "step = 6433800: loss = 3.5921220779418945\n",
      "step = 6434000: loss = 4.498982906341553\n",
      "step = 6434200: loss = 6.10106086730957\n",
      "step = 6434400: loss = 4.052257061004639\n",
      "step = 6434600: loss = 3.1219286918640137\n",
      "step = 6434800: loss = 4.920703411102295\n",
      "step = 6435000: loss = 3.56259822845459\n",
      "step = 6435000: Average Return = 3.868000030517578\n",
      "step = 6435200: loss = 4.053333759307861\n",
      "step = 6435400: loss = 3.4176409244537354\n",
      "step = 6435600: loss = 3.498753070831299\n",
      "step = 6435800: loss = 4.028415679931641\n",
      "step = 6436000: loss = 5.225793838500977\n",
      "step = 6436200: loss = 5.227341175079346\n",
      "step = 6436400: loss = 5.3883585929870605\n",
      "step = 6436600: loss = 2.505772590637207\n",
      "step = 6436800: loss = 3.8315975666046143\n",
      "step = 6437000: loss = 5.413814544677734\n",
      "step = 6437200: loss = 3.253535747528076\n",
      "step = 6437400: loss = 3.9573593139648438\n",
      "step = 6437600: loss = 4.55754280090332\n",
      "step = 6437800: loss = 3.9781267642974854\n",
      "step = 6438000: loss = 4.171630382537842\n",
      "step = 6438200: loss = 4.5242109298706055\n",
      "step = 6438400: loss = 3.702563762664795\n",
      "step = 6438600: loss = 3.570718288421631\n",
      "step = 6438800: loss = 4.485558032989502\n",
      "step = 6439000: loss = 3.8284881114959717\n",
      "step = 6439200: loss = 4.208180904388428\n",
      "step = 6439400: loss = 4.352839946746826\n",
      "step = 6439600: loss = 5.3080010414123535\n",
      "step = 6439800: loss = 5.101721286773682\n",
      "step = 6440000: loss = 3.384765625\n",
      "step = 6440000: Average Return = 3.4260001182556152\n",
      "step = 6440200: loss = 4.251565456390381\n",
      "step = 6440400: loss = 4.463255405426025\n",
      "step = 6440600: loss = 3.578355073928833\n",
      "step = 6440800: loss = 5.375560283660889\n",
      "step = 6441000: loss = 5.716248512268066\n",
      "step = 6441200: loss = 4.882453918457031\n",
      "step = 6441400: loss = 3.4534733295440674\n",
      "step = 6441600: loss = 3.186412811279297\n",
      "step = 6441800: loss = 5.401525974273682\n",
      "step = 6442000: loss = 3.354384183883667\n",
      "step = 6442200: loss = 3.7213616371154785\n",
      "step = 6442400: loss = 3.58477783203125\n",
      "step = 6442600: loss = 4.4965643882751465\n",
      "step = 6442800: loss = 2.8076765537261963\n",
      "step = 6443000: loss = 4.479207515716553\n",
      "step = 6443200: loss = 4.489499568939209\n",
      "step = 6443400: loss = 3.602098226547241\n",
      "step = 6443600: loss = 3.1008551120758057\n",
      "step = 6443800: loss = 3.5186657905578613\n",
      "step = 6444000: loss = 3.834554433822632\n",
      "step = 6444200: loss = 2.6223180294036865\n",
      "step = 6444400: loss = 4.137909412384033\n",
      "step = 6444600: loss = 4.974549293518066\n",
      "step = 6444800: loss = 3.384462833404541\n",
      "step = 6445000: loss = 3.9556450843811035\n",
      "step = 6445000: Average Return = 3.6040000915527344\n",
      "step = 6445200: loss = 3.1940624713897705\n",
      "step = 6445400: loss = 4.0480241775512695\n",
      "step = 6445600: loss = 3.4302098751068115\n",
      "step = 6445800: loss = 5.852935791015625\n",
      "step = 6446000: loss = 3.668276309967041\n",
      "step = 6446200: loss = 4.911821365356445\n",
      "step = 6446400: loss = 5.067548751831055\n",
      "step = 6446600: loss = 4.7070393562316895\n",
      "step = 6446800: loss = 4.988809585571289\n",
      "step = 6447000: loss = 4.288727760314941\n",
      "step = 6447200: loss = 3.9847073554992676\n",
      "step = 6447400: loss = 3.3764617443084717\n",
      "step = 6447600: loss = 4.914155960083008\n",
      "step = 6447800: loss = 4.842055320739746\n",
      "step = 6448000: loss = 3.2770373821258545\n",
      "step = 6448200: loss = 2.881795644760132\n",
      "step = 6448400: loss = 4.554202556610107\n",
      "step = 6448600: loss = 4.715448379516602\n",
      "step = 6448800: loss = 3.997809410095215\n",
      "step = 6449000: loss = 5.016354084014893\n",
      "step = 6449200: loss = 3.6648614406585693\n",
      "step = 6449400: loss = 4.203563213348389\n",
      "step = 6449600: loss = 3.602318286895752\n",
      "step = 6449800: loss = 5.498254299163818\n",
      "step = 6450000: loss = 4.460197448730469\n",
      "step = 6450000: Average Return = 3.6700000762939453\n",
      "step = 6450200: loss = 4.363985538482666\n",
      "step = 6450400: loss = 4.674197673797607\n",
      "step = 6450600: loss = 2.176575183868408\n",
      "step = 6450800: loss = 5.550111770629883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6451000: loss = 4.3178815841674805\n",
      "step = 6451200: loss = 4.137399196624756\n",
      "step = 6451400: loss = 4.352545261383057\n",
      "step = 6451600: loss = 4.068772315979004\n",
      "step = 6451800: loss = 6.293587684631348\n",
      "step = 6452000: loss = 4.320467948913574\n",
      "step = 6452200: loss = 3.635545492172241\n",
      "step = 6452400: loss = 3.222691535949707\n",
      "step = 6452600: loss = 4.6850762367248535\n",
      "step = 6452800: loss = 3.747573137283325\n",
      "step = 6453000: loss = 3.2213759422302246\n",
      "step = 6453200: loss = 4.995697975158691\n",
      "step = 6453400: loss = 2.926067352294922\n",
      "step = 6453600: loss = 3.153981924057007\n",
      "step = 6453800: loss = 3.7204344272613525\n",
      "step = 6454000: loss = 4.086048126220703\n",
      "step = 6454200: loss = 4.28043794631958\n",
      "step = 6454400: loss = 5.707631587982178\n",
      "step = 6454600: loss = 3.892730712890625\n",
      "step = 6454800: loss = 3.8281939029693604\n",
      "step = 6455000: loss = 4.114816665649414\n",
      "step = 6455000: Average Return = 3.7239999771118164\n",
      "step = 6455200: loss = 5.249595642089844\n",
      "step = 6455400: loss = 4.647380352020264\n",
      "step = 6455600: loss = 4.17115592956543\n",
      "step = 6455800: loss = 3.13250732421875\n",
      "step = 6456000: loss = 4.471498966217041\n",
      "step = 6456200: loss = 6.002312660217285\n",
      "step = 6456400: loss = 3.8720693588256836\n",
      "step = 6456600: loss = 3.6417324542999268\n",
      "step = 6456800: loss = 5.2631025314331055\n",
      "step = 6457000: loss = 4.221447467803955\n",
      "step = 6457200: loss = 3.2026610374450684\n",
      "step = 6457400: loss = 2.6474528312683105\n",
      "step = 6457600: loss = 4.416828632354736\n",
      "step = 6457800: loss = 4.027993679046631\n",
      "step = 6458000: loss = 4.289161205291748\n",
      "step = 6458200: loss = 4.538704872131348\n",
      "step = 6458400: loss = 4.136937141418457\n",
      "step = 6458600: loss = 3.2395312786102295\n",
      "step = 6458800: loss = 4.203225612640381\n",
      "step = 6459000: loss = 4.234764099121094\n",
      "step = 6459200: loss = 3.7381248474121094\n",
      "step = 6459400: loss = 4.915619373321533\n",
      "step = 6459600: loss = 4.573540687561035\n",
      "step = 6459800: loss = 4.818758964538574\n",
      "step = 6460000: loss = 3.6303489208221436\n",
      "step = 6460000: Average Return = 3.990000009536743\n",
      "step = 6460200: loss = 3.327836513519287\n",
      "step = 6460400: loss = 3.984692096710205\n",
      "step = 6460600: loss = 3.096306324005127\n",
      "step = 6460800: loss = 5.021111965179443\n",
      "step = 6461000: loss = 2.856595277786255\n",
      "step = 6461200: loss = 4.493019104003906\n",
      "step = 6461400: loss = 3.874080181121826\n",
      "step = 6461600: loss = 4.611155986785889\n",
      "step = 6461800: loss = 4.776376247406006\n",
      "step = 6462000: loss = 3.7644572257995605\n",
      "step = 6462200: loss = 4.857474327087402\n",
      "step = 6462400: loss = 4.24308443069458\n",
      "step = 6462600: loss = 3.604177236557007\n",
      "step = 6462800: loss = 4.587496280670166\n",
      "step = 6463000: loss = 4.652247428894043\n",
      "step = 6463200: loss = 2.433131217956543\n",
      "step = 6463400: loss = 4.923534393310547\n",
      "step = 6463600: loss = 5.223855018615723\n",
      "step = 6463800: loss = 3.270366668701172\n",
      "step = 6464000: loss = 5.279472351074219\n",
      "step = 6464200: loss = 2.375955104827881\n",
      "step = 6464400: loss = 4.715892791748047\n",
      "step = 6464600: loss = 3.7831649780273438\n",
      "step = 6464800: loss = 4.225097179412842\n",
      "step = 6465000: loss = 3.983893394470215\n",
      "step = 6465000: Average Return = 3.936000108718872\n",
      "step = 6465200: loss = 4.21401834487915\n",
      "step = 6465400: loss = 3.401099443435669\n",
      "step = 6465600: loss = 4.732473850250244\n",
      "step = 6465800: loss = 4.818689346313477\n",
      "step = 6466000: loss = 4.801551818847656\n",
      "step = 6466200: loss = 4.956326007843018\n",
      "step = 6466400: loss = 4.067997455596924\n",
      "step = 6466600: loss = 3.9958436489105225\n",
      "step = 6466800: loss = 4.116052627563477\n",
      "step = 6467000: loss = 4.967358112335205\n",
      "step = 6467200: loss = 4.658763408660889\n",
      "step = 6467400: loss = 4.945169925689697\n",
      "step = 6467600: loss = 6.46207857131958\n",
      "step = 6467800: loss = 3.741306781768799\n",
      "step = 6468000: loss = 4.114702224731445\n",
      "step = 6468200: loss = 4.794693946838379\n",
      "step = 6468400: loss = 3.564826011657715\n",
      "step = 6468600: loss = 3.7111175060272217\n",
      "step = 6468800: loss = 6.066158294677734\n",
      "step = 6469000: loss = 4.968398094177246\n",
      "step = 6469200: loss = 4.002674102783203\n",
      "step = 6469400: loss = 3.995565414428711\n",
      "step = 6469600: loss = 5.371503829956055\n",
      "step = 6469800: loss = 5.374771595001221\n",
      "step = 6470000: loss = 4.049655914306641\n",
      "step = 6470000: Average Return = 4.084000110626221\n",
      "step = 6470200: loss = 4.965638160705566\n",
      "step = 6470400: loss = 3.3428611755371094\n",
      "step = 6470600: loss = 4.9141645431518555\n",
      "step = 6470800: loss = 3.503551483154297\n",
      "step = 6471000: loss = 3.809598684310913\n",
      "step = 6471200: loss = 3.068540334701538\n",
      "step = 6471400: loss = 3.679149627685547\n",
      "step = 6471600: loss = 4.135386943817139\n",
      "step = 6471800: loss = 3.4306869506835938\n",
      "step = 6472000: loss = 4.495345115661621\n",
      "step = 6472200: loss = 4.336686134338379\n",
      "step = 6472400: loss = 3.5343425273895264\n",
      "step = 6472600: loss = 3.1842410564422607\n",
      "step = 6472800: loss = 3.1157379150390625\n",
      "step = 6473000: loss = 4.8502936363220215\n",
      "step = 6473200: loss = 3.5509798526763916\n",
      "step = 6473400: loss = 2.9873571395874023\n",
      "step = 6473600: loss = 4.622461318969727\n",
      "step = 6473800: loss = 3.3205180168151855\n",
      "step = 6474000: loss = 3.27190899848938\n",
      "step = 6474200: loss = 4.285401344299316\n",
      "step = 6474400: loss = 4.936028480529785\n",
      "step = 6474600: loss = 5.266534328460693\n",
      "step = 6474800: loss = 4.0787763595581055\n",
      "step = 6475000: loss = 3.8886401653289795\n",
      "step = 6475000: Average Return = 3.6559998989105225\n",
      "step = 6475200: loss = 2.939791202545166\n",
      "step = 6475400: loss = 4.315619945526123\n",
      "step = 6475600: loss = 2.7342751026153564\n",
      "step = 6475800: loss = 4.28218936920166\n",
      "step = 6476000: loss = 5.398056507110596\n",
      "step = 6476200: loss = 3.7778561115264893\n",
      "step = 6476400: loss = 4.362823963165283\n",
      "step = 6476600: loss = 4.82765531539917\n",
      "step = 6476800: loss = 4.684138298034668\n",
      "step = 6477000: loss = 4.120899677276611\n",
      "step = 6477200: loss = 4.102667331695557\n",
      "step = 6477400: loss = 3.343560218811035\n",
      "step = 6477600: loss = 4.40916633605957\n",
      "step = 6477800: loss = 5.598421573638916\n",
      "step = 6478000: loss = 3.339524984359741\n",
      "step = 6478200: loss = 4.341230869293213\n",
      "step = 6478400: loss = 4.6645188331604\n",
      "step = 6478600: loss = 3.8551948070526123\n",
      "step = 6478800: loss = 4.189578056335449\n",
      "step = 6479000: loss = 4.107473850250244\n",
      "step = 6479200: loss = 3.8665359020233154\n",
      "step = 6479400: loss = 4.692376613616943\n",
      "step = 6479600: loss = 3.030332565307617\n",
      "step = 6479800: loss = 5.809205055236816\n",
      "step = 6480000: loss = 4.057304859161377\n",
      "step = 6480000: Average Return = 3.696000099182129\n",
      "step = 6480200: loss = 2.662729024887085\n",
      "step = 6480400: loss = 3.776397228240967\n",
      "step = 6480600: loss = 2.7076098918914795\n",
      "step = 6480800: loss = 3.878416061401367\n",
      "step = 6481000: loss = 4.490349769592285\n",
      "step = 6481200: loss = 3.947352170944214\n",
      "step = 6481400: loss = 3.788156032562256\n",
      "step = 6481600: loss = 3.454780101776123\n",
      "step = 6481800: loss = 3.86185622215271\n",
      "step = 6482000: loss = 2.72645902633667\n",
      "step = 6482200: loss = 3.251715660095215\n",
      "step = 6482400: loss = 4.554765224456787\n",
      "step = 6482600: loss = 4.1634979248046875\n",
      "step = 6482800: loss = 5.927845478057861\n",
      "step = 6483000: loss = 2.0173463821411133\n",
      "step = 6483200: loss = 4.9924163818359375\n",
      "step = 6483400: loss = 4.177652835845947\n",
      "step = 6483600: loss = 6.142459392547607\n",
      "step = 6483800: loss = 4.247981548309326\n",
      "step = 6484000: loss = 7.163251876831055\n",
      "step = 6484200: loss = 3.2441155910491943\n",
      "step = 6484400: loss = 3.9653234481811523\n",
      "step = 6484600: loss = 4.788168907165527\n",
      "step = 6484800: loss = 4.229442596435547\n",
      "step = 6485000: loss = 3.872755765914917\n",
      "step = 6485000: Average Return = 3.7219998836517334\n",
      "step = 6485200: loss = 4.515877723693848\n",
      "step = 6485400: loss = 4.383454322814941\n",
      "step = 6485600: loss = 3.366689920425415\n",
      "step = 6485800: loss = 4.602171897888184\n",
      "step = 6486000: loss = 3.7187130451202393\n",
      "step = 6486200: loss = 3.8257017135620117\n",
      "step = 6486400: loss = 4.8988752365112305\n",
      "step = 6486600: loss = 3.884354829788208\n",
      "step = 6486800: loss = 3.777972936630249\n",
      "step = 6487000: loss = 3.798537254333496\n",
      "step = 6487200: loss = 4.028078079223633\n",
      "step = 6487400: loss = 3.018296718597412\n",
      "step = 6487600: loss = 3.9638354778289795\n",
      "step = 6487800: loss = 3.7433366775512695\n",
      "step = 6488000: loss = 2.8234758377075195\n",
      "step = 6488200: loss = 6.007452487945557\n",
      "step = 6488400: loss = 6.899479866027832\n",
      "step = 6488600: loss = 3.485496997833252\n",
      "step = 6488800: loss = 5.272416114807129\n",
      "step = 6489000: loss = 4.688872814178467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6489200: loss = 4.147768020629883\n",
      "step = 6489400: loss = 3.943176746368408\n",
      "step = 6489600: loss = 3.900334358215332\n",
      "step = 6489800: loss = 4.428408145904541\n",
      "step = 6490000: loss = 3.9332847595214844\n",
      "step = 6490000: Average Return = 3.7360000610351562\n",
      "step = 6490200: loss = 4.7483086585998535\n",
      "step = 6490400: loss = 3.506763219833374\n",
      "step = 6490600: loss = 4.565006732940674\n",
      "step = 6490800: loss = 5.365999698638916\n",
      "step = 6491000: loss = 4.45745849609375\n",
      "step = 6491200: loss = 4.2075982093811035\n",
      "step = 6491400: loss = 5.202545166015625\n",
      "step = 6491600: loss = 4.906794548034668\n",
      "step = 6491800: loss = 3.8281147480010986\n",
      "step = 6492000: loss = 4.770329475402832\n",
      "step = 6492200: loss = 4.967592239379883\n",
      "step = 6492400: loss = 5.36158561706543\n",
      "step = 6492600: loss = 5.00773811340332\n",
      "step = 6492800: loss = 4.397641658782959\n",
      "step = 6493000: loss = 4.266026973724365\n",
      "step = 6493200: loss = 3.478847026824951\n",
      "step = 6493400: loss = 4.888721466064453\n",
      "step = 6493600: loss = 4.338351249694824\n",
      "step = 6493800: loss = 4.462671279907227\n",
      "step = 6494000: loss = 3.104459047317505\n",
      "step = 6494200: loss = 3.7762577533721924\n",
      "step = 6494400: loss = 3.7964863777160645\n",
      "step = 6494600: loss = 4.6325554847717285\n",
      "step = 6494800: loss = 2.955320358276367\n",
      "step = 6495000: loss = 5.382736682891846\n",
      "step = 6495000: Average Return = 4.4120001792907715\n",
      "step = 6495200: loss = 4.331593990325928\n",
      "step = 6495400: loss = 4.992731094360352\n",
      "step = 6495600: loss = 3.62734055519104\n",
      "step = 6495800: loss = 5.805383205413818\n",
      "step = 6496000: loss = 3.379441499710083\n",
      "step = 6496200: loss = 3.599086284637451\n",
      "step = 6496400: loss = 4.2433271408081055\n",
      "step = 6496600: loss = 4.096721649169922\n",
      "step = 6496800: loss = 5.040022850036621\n",
      "step = 6497000: loss = 4.422245502471924\n",
      "step = 6497200: loss = 3.5397145748138428\n",
      "step = 6497400: loss = 4.2751312255859375\n",
      "step = 6497600: loss = 5.579885005950928\n",
      "step = 6497800: loss = 3.4554901123046875\n",
      "step = 6498000: loss = 4.669541835784912\n",
      "step = 6498200: loss = 3.5626745223999023\n",
      "step = 6498400: loss = 2.2077322006225586\n",
      "step = 6498600: loss = 3.702047348022461\n",
      "step = 6498800: loss = 3.1168882846832275\n",
      "step = 6499000: loss = 4.332009792327881\n",
      "step = 6499200: loss = 4.48647403717041\n",
      "step = 6499400: loss = 3.137866258621216\n",
      "step = 6499600: loss = 3.8399658203125\n",
      "step = 6499800: loss = 3.595471143722534\n",
      "step = 6500000: loss = 2.862739086151123\n",
      "step = 6500000: Average Return = 4.070000171661377\n",
      "step = 6500200: loss = 3.715318441390991\n",
      "step = 6500400: loss = 4.227900505065918\n",
      "step = 6500600: loss = 3.777228832244873\n",
      "step = 6500800: loss = 5.875673294067383\n",
      "step = 6501000: loss = 3.426199197769165\n",
      "step = 6501200: loss = 4.048511028289795\n",
      "step = 6501400: loss = 3.409822940826416\n",
      "step = 6501600: loss = 4.019926071166992\n",
      "step = 6501800: loss = 2.418997049331665\n",
      "step = 6502000: loss = 3.9409730434417725\n",
      "step = 6502200: loss = 4.33657169342041\n",
      "step = 6502400: loss = 3.543461799621582\n",
      "step = 6502600: loss = 3.363865852355957\n",
      "step = 6502800: loss = 2.5390517711639404\n",
      "step = 6503000: loss = 3.9901416301727295\n",
      "step = 6503200: loss = 4.535420894622803\n",
      "step = 6503400: loss = 5.587793827056885\n",
      "step = 6503600: loss = 4.857374668121338\n",
      "step = 6503800: loss = 3.5551419258117676\n",
      "step = 6504000: loss = 5.24790096282959\n",
      "step = 6504200: loss = 2.6953837871551514\n",
      "step = 6504400: loss = 3.729534864425659\n",
      "step = 6504600: loss = 4.346421241760254\n",
      "step = 6504800: loss = 3.370046854019165\n",
      "step = 6505000: loss = 3.727529287338257\n",
      "step = 6505000: Average Return = 3.687999963760376\n",
      "step = 6505200: loss = 4.54176664352417\n",
      "step = 6505400: loss = 3.9813616275787354\n",
      "step = 6505600: loss = 4.130990982055664\n",
      "step = 6505800: loss = 5.600217342376709\n",
      "step = 6506000: loss = 3.321712017059326\n",
      "step = 6506200: loss = 4.151559352874756\n",
      "step = 6506400: loss = 2.5113704204559326\n",
      "step = 6506600: loss = 4.9487175941467285\n",
      "step = 6506800: loss = 4.252274513244629\n",
      "step = 6507000: loss = 4.499575138092041\n",
      "step = 6507200: loss = 2.9003612995147705\n",
      "step = 6507400: loss = 4.713706016540527\n",
      "step = 6507600: loss = 4.336431980133057\n",
      "step = 6507800: loss = 3.4935076236724854\n",
      "step = 6508000: loss = 3.9596738815307617\n",
      "step = 6508200: loss = 5.707192420959473\n",
      "step = 6508400: loss = 3.6414554119110107\n",
      "step = 6508600: loss = 4.204230308532715\n",
      "step = 6508800: loss = 4.27045202255249\n",
      "step = 6509000: loss = 4.649654388427734\n",
      "step = 6509200: loss = 4.179095268249512\n",
      "step = 6509400: loss = 4.1980719566345215\n",
      "step = 6509600: loss = 5.205501556396484\n",
      "step = 6509800: loss = 4.3982672691345215\n",
      "step = 6510000: loss = 4.6997175216674805\n",
      "step = 6510000: Average Return = 3.696000099182129\n",
      "step = 6510200: loss = 3.911451578140259\n",
      "step = 6510400: loss = 5.376920223236084\n",
      "step = 6510600: loss = 4.548012733459473\n",
      "step = 6510800: loss = 4.7723188400268555\n",
      "step = 6511000: loss = 3.8602135181427\n",
      "step = 6511200: loss = 5.766005516052246\n",
      "step = 6511400: loss = 4.477877140045166\n",
      "step = 6511600: loss = 3.957627534866333\n",
      "step = 6511800: loss = 4.779289245605469\n",
      "step = 6512000: loss = 4.317196846008301\n",
      "step = 6512200: loss = 4.26206111907959\n",
      "step = 6512400: loss = 2.9175658226013184\n",
      "step = 6512600: loss = 5.359797954559326\n",
      "step = 6512800: loss = 4.5610480308532715\n",
      "step = 6513000: loss = 5.979773998260498\n",
      "step = 6513200: loss = 4.493154048919678\n",
      "step = 6513400: loss = 3.498549699783325\n",
      "step = 6513600: loss = 4.91731071472168\n",
      "step = 6513800: loss = 4.287623882293701\n",
      "step = 6514000: loss = 4.497136116027832\n",
      "step = 6514200: loss = 4.816343784332275\n",
      "step = 6514400: loss = 3.17522931098938\n",
      "step = 6514600: loss = 3.6442129611968994\n",
      "step = 6514800: loss = 4.961935520172119\n",
      "step = 6515000: loss = 4.3157572746276855\n",
      "step = 6515000: Average Return = 4.125999927520752\n",
      "step = 6515200: loss = 4.6972575187683105\n",
      "step = 6515400: loss = 4.293273448944092\n",
      "step = 6515600: loss = 2.640450954437256\n",
      "step = 6515800: loss = 3.0845227241516113\n",
      "step = 6516000: loss = 3.487685441970825\n",
      "step = 6516200: loss = 4.168166637420654\n",
      "step = 6516400: loss = 6.134680271148682\n",
      "step = 6516600: loss = 5.883894920349121\n",
      "step = 6516800: loss = 3.4238970279693604\n",
      "step = 6517000: loss = 4.54843282699585\n",
      "step = 6517200: loss = 4.9559431076049805\n",
      "step = 6517400: loss = 3.9042043685913086\n",
      "step = 6517600: loss = 4.422108173370361\n",
      "step = 6517800: loss = 6.268947601318359\n",
      "step = 6518000: loss = 3.846564769744873\n",
      "step = 6518200: loss = 4.483177185058594\n",
      "step = 6518400: loss = 5.048616409301758\n",
      "step = 6518600: loss = 3.2575063705444336\n",
      "step = 6518800: loss = 5.190412521362305\n",
      "step = 6519000: loss = 3.8909153938293457\n",
      "step = 6519200: loss = 3.8081555366516113\n",
      "step = 6519400: loss = 4.599374771118164\n",
      "step = 6519600: loss = 4.170656204223633\n",
      "step = 6519800: loss = 4.780261993408203\n",
      "step = 6520000: loss = 4.268483638763428\n",
      "step = 6520000: Average Return = 3.7160000801086426\n",
      "step = 6520200: loss = 4.993151664733887\n",
      "step = 6520400: loss = 4.500802040100098\n",
      "step = 6520600: loss = 5.409193515777588\n",
      "step = 6520800: loss = 3.2828991413116455\n",
      "step = 6521000: loss = 4.705374717712402\n",
      "step = 6521200: loss = 3.0828850269317627\n",
      "step = 6521400: loss = 3.207519054412842\n",
      "step = 6521600: loss = 4.50411319732666\n",
      "step = 6521800: loss = 4.65406608581543\n",
      "step = 6522000: loss = 2.5479273796081543\n",
      "step = 6522200: loss = 3.437396764755249\n",
      "step = 6522400: loss = 4.099138259887695\n",
      "step = 6522600: loss = 4.526221752166748\n",
      "step = 6522800: loss = 4.176909923553467\n",
      "step = 6523000: loss = 3.7889955043792725\n",
      "step = 6523200: loss = 5.079253196716309\n",
      "step = 6523400: loss = 4.284565448760986\n",
      "step = 6523600: loss = 3.5730955600738525\n",
      "step = 6523800: loss = 4.231624126434326\n",
      "step = 6524000: loss = 5.073849678039551\n",
      "step = 6524200: loss = 4.5522894859313965\n",
      "step = 6524400: loss = 6.060122489929199\n",
      "step = 6524600: loss = 3.059431791305542\n",
      "step = 6524800: loss = 4.653663158416748\n",
      "step = 6525000: loss = 5.115384578704834\n",
      "step = 6525000: Average Return = 3.7119998931884766\n",
      "step = 6525200: loss = 4.086613655090332\n",
      "step = 6525400: loss = 3.5607919692993164\n",
      "step = 6525600: loss = 4.224638938903809\n",
      "step = 6525800: loss = 3.8458471298217773\n",
      "step = 6526000: loss = 5.178990364074707\n",
      "step = 6526200: loss = 5.174382209777832\n",
      "step = 6526400: loss = 3.911324977874756\n",
      "step = 6526600: loss = 2.9689598083496094\n",
      "step = 6526800: loss = 4.3813652992248535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6527000: loss = 4.8645782470703125\n",
      "step = 6527200: loss = 3.5452258586883545\n",
      "step = 6527400: loss = 4.615321636199951\n",
      "step = 6527600: loss = 4.948537349700928\n",
      "step = 6527800: loss = 4.32551908493042\n",
      "step = 6528000: loss = 4.886528015136719\n",
      "step = 6528200: loss = 4.63851261138916\n",
      "step = 6528400: loss = 3.8299920558929443\n",
      "step = 6528600: loss = 3.597468852996826\n",
      "step = 6528800: loss = 4.0600996017456055\n",
      "step = 6529000: loss = 5.513290882110596\n",
      "step = 6529200: loss = 3.543076753616333\n",
      "step = 6529400: loss = 4.945545673370361\n",
      "step = 6529600: loss = 3.567877769470215\n",
      "step = 6529800: loss = 5.946383476257324\n",
      "step = 6530000: loss = 5.836992263793945\n",
      "step = 6530000: Average Return = 3.4100000858306885\n",
      "step = 6530200: loss = 4.548415660858154\n",
      "step = 6530400: loss = 2.7940917015075684\n",
      "step = 6530600: loss = 2.9052717685699463\n",
      "step = 6530800: loss = 4.980656623840332\n",
      "step = 6531000: loss = 2.001068592071533\n",
      "step = 6531200: loss = 4.186161994934082\n",
      "step = 6531400: loss = 4.664988994598389\n",
      "step = 6531600: loss = 4.3314666748046875\n",
      "step = 6531800: loss = 3.8452978134155273\n",
      "step = 6532000: loss = 4.7925543785095215\n",
      "step = 6532200: loss = 4.932735443115234\n",
      "step = 6532400: loss = 3.992899179458618\n",
      "step = 6532600: loss = 3.6615214347839355\n",
      "step = 6532800: loss = 3.16524600982666\n",
      "step = 6533000: loss = 4.134166717529297\n",
      "step = 6533200: loss = 2.875570297241211\n",
      "step = 6533400: loss = 4.308712482452393\n",
      "step = 6533600: loss = 3.092083692550659\n",
      "step = 6533800: loss = 5.063431739807129\n",
      "step = 6534000: loss = 5.120767593383789\n",
      "step = 6534200: loss = 5.32770299911499\n",
      "step = 6534400: loss = 3.8978168964385986\n",
      "step = 6534600: loss = 3.3977482318878174\n",
      "step = 6534800: loss = 5.259525775909424\n",
      "step = 6535000: loss = 4.390820026397705\n",
      "step = 6535000: Average Return = 3.799999952316284\n",
      "step = 6535200: loss = 3.0612685680389404\n",
      "step = 6535400: loss = 3.168633460998535\n",
      "step = 6535600: loss = 3.734957695007324\n",
      "step = 6535800: loss = 2.9919064044952393\n",
      "step = 6536000: loss = 4.443413734436035\n",
      "step = 6536200: loss = 5.442956447601318\n",
      "step = 6536400: loss = 4.7635931968688965\n",
      "step = 6536600: loss = 3.0599169731140137\n",
      "step = 6536800: loss = 4.60775089263916\n",
      "step = 6537000: loss = 3.96075177192688\n",
      "step = 6537200: loss = 2.839911937713623\n",
      "step = 6537400: loss = 3.6020901203155518\n",
      "step = 6537600: loss = 5.200723648071289\n",
      "step = 6537800: loss = 3.964841842651367\n",
      "step = 6538000: loss = 3.1860105991363525\n",
      "step = 6538200: loss = 4.4429144859313965\n",
      "step = 6538400: loss = 6.0555419921875\n",
      "step = 6538600: loss = 3.399027109146118\n",
      "step = 6538800: loss = 4.027616500854492\n",
      "step = 6539000: loss = 3.5243213176727295\n",
      "step = 6539200: loss = 4.324493408203125\n",
      "step = 6539400: loss = 4.512524127960205\n",
      "step = 6539600: loss = 4.890017986297607\n",
      "step = 6539800: loss = 4.62579870223999\n",
      "step = 6540000: loss = 3.9798638820648193\n",
      "step = 6540000: Average Return = 3.8320000171661377\n",
      "step = 6540200: loss = 3.476890802383423\n",
      "step = 6540400: loss = 4.978591442108154\n",
      "step = 6540600: loss = 4.401933193206787\n",
      "step = 6540800: loss = 5.159440994262695\n",
      "step = 6541000: loss = 5.0128631591796875\n",
      "step = 6541200: loss = 2.9451892375946045\n",
      "step = 6541400: loss = 5.66960334777832\n",
      "step = 6541600: loss = 3.038853883743286\n",
      "step = 6541800: loss = 4.282689094543457\n",
      "step = 6542000: loss = 4.785872459411621\n",
      "step = 6542200: loss = 4.929956436157227\n",
      "step = 6542400: loss = 4.109244346618652\n",
      "step = 6542600: loss = 4.656317234039307\n",
      "step = 6542800: loss = 4.3220367431640625\n",
      "step = 6543000: loss = 4.15692663192749\n",
      "step = 6543200: loss = 3.893982410430908\n",
      "step = 6543400: loss = 3.9426732063293457\n",
      "step = 6543600: loss = 4.308575630187988\n",
      "step = 6543800: loss = 4.1465373039245605\n",
      "step = 6544000: loss = 5.088232517242432\n",
      "step = 6544200: loss = 4.489476680755615\n",
      "step = 6544400: loss = 3.7636353969573975\n",
      "step = 6544600: loss = 4.30558443069458\n",
      "step = 6544800: loss = 4.379055976867676\n",
      "step = 6545000: loss = 3.350287675857544\n",
      "step = 6545000: Average Return = 3.697999954223633\n",
      "step = 6545200: loss = 3.940108060836792\n",
      "step = 6545400: loss = 4.726901054382324\n",
      "step = 6545600: loss = 4.101863384246826\n",
      "step = 6545800: loss = 3.517956495285034\n",
      "step = 6546000: loss = 3.4823806285858154\n",
      "step = 6546200: loss = 5.559103965759277\n",
      "step = 6546400: loss = 4.105647563934326\n",
      "step = 6546600: loss = 4.96671199798584\n",
      "step = 6546800: loss = 4.771243095397949\n",
      "step = 6547000: loss = 3.4490203857421875\n",
      "step = 6547200: loss = 3.840956449508667\n",
      "step = 6547400: loss = 3.1948299407958984\n",
      "step = 6547600: loss = 3.273022174835205\n",
      "step = 6547800: loss = 3.734626293182373\n",
      "step = 6548000: loss = 3.706273317337036\n",
      "step = 6548200: loss = 4.43931770324707\n",
      "step = 6548400: loss = 3.319873571395874\n",
      "step = 6548600: loss = 4.774664402008057\n",
      "step = 6548800: loss = 4.0311102867126465\n",
      "step = 6549000: loss = 5.210134029388428\n",
      "step = 6549200: loss = 5.081595420837402\n",
      "step = 6549400: loss = 4.014892101287842\n",
      "step = 6549600: loss = 4.552348613739014\n",
      "step = 6549800: loss = 3.84479022026062\n",
      "step = 6550000: loss = 4.182401180267334\n",
      "step = 6550000: Average Return = 4.0960001945495605\n",
      "step = 6550200: loss = 4.603522777557373\n",
      "step = 6550400: loss = 5.365010738372803\n",
      "step = 6550600: loss = 4.129727840423584\n",
      "step = 6550800: loss = 3.360804557800293\n",
      "step = 6551000: loss = 3.924943208694458\n",
      "step = 6551200: loss = 4.362475395202637\n",
      "step = 6551400: loss = 5.570662498474121\n",
      "step = 6551600: loss = 4.536073207855225\n",
      "step = 6551800: loss = 2.488891839981079\n",
      "step = 6552000: loss = 4.492310523986816\n",
      "step = 6552200: loss = 3.8130745887756348\n",
      "step = 6552400: loss = 3.6736056804656982\n",
      "step = 6552600: loss = 3.873403787612915\n",
      "step = 6552800: loss = 4.406230449676514\n",
      "step = 6553000: loss = 4.319751262664795\n",
      "step = 6553200: loss = 3.8717381954193115\n",
      "step = 6553400: loss = 4.693897724151611\n",
      "step = 6553600: loss = 4.697009086608887\n",
      "step = 6553800: loss = 4.8802313804626465\n",
      "step = 6554000: loss = 4.027372360229492\n",
      "step = 6554200: loss = 5.363070011138916\n",
      "step = 6554400: loss = 3.8657188415527344\n",
      "step = 6554600: loss = 3.47194504737854\n",
      "step = 6554800: loss = 3.662794828414917\n",
      "step = 6555000: loss = 3.95261287689209\n",
      "step = 6555000: Average Return = 3.7100000381469727\n",
      "step = 6555200: loss = 3.74587082862854\n",
      "step = 6555400: loss = 5.308418273925781\n",
      "step = 6555600: loss = 3.945340394973755\n",
      "step = 6555800: loss = 5.422607421875\n",
      "step = 6556000: loss = 5.1427459716796875\n",
      "step = 6556200: loss = 4.458439350128174\n",
      "step = 6556400: loss = 4.801736831665039\n",
      "step = 6556600: loss = 5.003640651702881\n",
      "step = 6556800: loss = 4.561996936798096\n",
      "step = 6557000: loss = 4.149298191070557\n",
      "step = 6557200: loss = 5.0851664543151855\n",
      "step = 6557400: loss = 4.370688438415527\n",
      "step = 6557600: loss = 5.495921611785889\n",
      "step = 6557800: loss = 5.49778938293457\n",
      "step = 6558000: loss = 5.272183895111084\n",
      "step = 6558200: loss = 4.8422322273254395\n",
      "step = 6558400: loss = 4.412746906280518\n",
      "step = 6558600: loss = 3.951573371887207\n",
      "step = 6558800: loss = 4.389078140258789\n",
      "step = 6559000: loss = 3.4413630962371826\n",
      "step = 6559200: loss = 5.090270042419434\n",
      "step = 6559400: loss = 3.989161968231201\n",
      "step = 6559600: loss = 4.408731460571289\n",
      "step = 6559800: loss = 3.256782054901123\n",
      "step = 6560000: loss = 2.870548725128174\n",
      "step = 6560000: Average Return = 3.6700000762939453\n",
      "step = 6560200: loss = 5.905572414398193\n",
      "step = 6560400: loss = 3.1008191108703613\n",
      "step = 6560600: loss = 4.055298328399658\n",
      "step = 6560800: loss = 3.62996506690979\n",
      "step = 6561000: loss = 4.911863327026367\n",
      "step = 6561200: loss = 4.994174003601074\n",
      "step = 6561400: loss = 5.947457313537598\n",
      "step = 6561600: loss = 4.49721097946167\n",
      "step = 6561800: loss = 4.657843589782715\n",
      "step = 6562000: loss = 3.9919304847717285\n",
      "step = 6562200: loss = 4.459897518157959\n",
      "step = 6562400: loss = 4.49951171875\n",
      "step = 6562600: loss = 5.219354629516602\n",
      "step = 6562800: loss = 3.7868294715881348\n",
      "step = 6563000: loss = 3.381028652191162\n",
      "step = 6563200: loss = 4.587200164794922\n",
      "step = 6563400: loss = 2.749171018600464\n",
      "step = 6563600: loss = 3.683056592941284\n",
      "step = 6563800: loss = 4.975930690765381\n",
      "step = 6564000: loss = 4.333337306976318\n",
      "step = 6564200: loss = 4.154755115509033\n",
      "step = 6564400: loss = 4.010612487792969\n",
      "step = 6564600: loss = 4.945993423461914\n",
      "step = 6564800: loss = 5.770956516265869\n",
      "step = 6565000: loss = 4.894034385681152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6565000: Average Return = 3.4519999027252197\n",
      "step = 6565200: loss = 4.320460796356201\n",
      "step = 6565400: loss = 3.732633590698242\n",
      "step = 6565600: loss = 3.9168803691864014\n",
      "step = 6565800: loss = 2.784533977508545\n",
      "step = 6566000: loss = 4.605435848236084\n",
      "step = 6566200: loss = 4.990578651428223\n",
      "step = 6566400: loss = 3.621657609939575\n",
      "step = 6566600: loss = 3.6860644817352295\n",
      "step = 6566800: loss = 3.861980438232422\n",
      "step = 6567000: loss = 5.158178806304932\n",
      "step = 6567200: loss = 4.892251968383789\n",
      "step = 6567400: loss = 3.9040772914886475\n",
      "step = 6567600: loss = 2.4648537635803223\n",
      "step = 6567800: loss = 2.9448375701904297\n",
      "step = 6568000: loss = 3.9369328022003174\n",
      "step = 6568200: loss = 3.7475101947784424\n",
      "step = 6568400: loss = 4.749974727630615\n",
      "step = 6568600: loss = 4.684539794921875\n",
      "step = 6568800: loss = 4.23379373550415\n",
      "step = 6569000: loss = 4.439396381378174\n",
      "step = 6569200: loss = 4.4115986824035645\n",
      "step = 6569400: loss = 4.5450944900512695\n",
      "step = 6569600: loss = 4.935596466064453\n",
      "step = 6569800: loss = 4.874995231628418\n",
      "step = 6570000: loss = 3.879342555999756\n",
      "step = 6570000: Average Return = 3.990000009536743\n",
      "step = 6570200: loss = 4.82274055480957\n",
      "step = 6570400: loss = 3.543318510055542\n",
      "step = 6570600: loss = 4.809267997741699\n",
      "step = 6570800: loss = 3.5531375408172607\n",
      "step = 6571000: loss = 4.146520137786865\n",
      "step = 6571200: loss = 3.202813148498535\n",
      "step = 6571400: loss = 3.0657854080200195\n",
      "step = 6571600: loss = 4.311931610107422\n",
      "step = 6571800: loss = 4.860813617706299\n",
      "step = 6572000: loss = 3.967977285385132\n",
      "step = 6572200: loss = 4.480740547180176\n",
      "step = 6572400: loss = 4.858249187469482\n",
      "step = 6572600: loss = 3.6475493907928467\n",
      "step = 6572800: loss = 4.159661769866943\n",
      "step = 6573000: loss = 4.922067165374756\n",
      "step = 6573200: loss = 4.5128068923950195\n",
      "step = 6573400: loss = 3.0393736362457275\n",
      "step = 6573600: loss = 4.412578582763672\n",
      "step = 6573800: loss = 5.047011375427246\n",
      "step = 6574000: loss = 1.9361141920089722\n",
      "step = 6574200: loss = 4.686279296875\n",
      "step = 6574400: loss = 4.898557186126709\n",
      "step = 6574600: loss = 3.7979893684387207\n",
      "step = 6574800: loss = 5.012399196624756\n",
      "step = 6575000: loss = 4.89728307723999\n",
      "step = 6575000: Average Return = 3.98799991607666\n",
      "step = 6575200: loss = 4.050887584686279\n",
      "step = 6575400: loss = 4.381234169006348\n",
      "step = 6575600: loss = 4.020367622375488\n",
      "step = 6575800: loss = 5.1637372970581055\n",
      "step = 6576000: loss = 3.944338083267212\n",
      "step = 6576200: loss = 3.924232006072998\n",
      "step = 6576400: loss = 3.839709997177124\n",
      "step = 6576600: loss = 4.142451286315918\n",
      "step = 6576800: loss = 5.016572952270508\n",
      "step = 6577000: loss = 5.29667329788208\n",
      "step = 6577200: loss = 3.7309107780456543\n",
      "step = 6577400: loss = 3.794264554977417\n",
      "step = 6577600: loss = 4.226715564727783\n",
      "step = 6577800: loss = 3.396152973175049\n",
      "step = 6578000: loss = 4.670699596405029\n",
      "step = 6578200: loss = 2.5413055419921875\n",
      "step = 6578400: loss = 4.400428295135498\n",
      "step = 6578600: loss = 3.0703790187835693\n",
      "step = 6578800: loss = 4.239445209503174\n",
      "step = 6579000: loss = 4.917473316192627\n",
      "step = 6579200: loss = 4.513911724090576\n",
      "step = 6579400: loss = 4.683842658996582\n",
      "step = 6579600: loss = 4.6138014793396\n",
      "step = 6579800: loss = 4.504328727722168\n",
      "step = 6580000: loss = 4.861874580383301\n",
      "step = 6580000: Average Return = 3.944000005722046\n",
      "step = 6580200: loss = 3.3682448863983154\n",
      "step = 6580400: loss = 5.421980857849121\n",
      "step = 6580600: loss = 5.8541436195373535\n",
      "step = 6580800: loss = 4.29988431930542\n",
      "step = 6581000: loss = 5.217178821563721\n",
      "step = 6581200: loss = 3.6650874614715576\n",
      "step = 6581400: loss = 3.476722002029419\n",
      "step = 6581600: loss = 3.6216201782226562\n",
      "step = 6581800: loss = 3.9150936603546143\n",
      "step = 6582000: loss = 3.223612070083618\n",
      "step = 6582200: loss = 4.62589693069458\n",
      "step = 6582400: loss = 4.835007190704346\n",
      "step = 6582600: loss = 4.235255241394043\n",
      "step = 6582800: loss = 4.391577243804932\n",
      "step = 6583000: loss = 5.551685810089111\n",
      "step = 6583200: loss = 4.60227108001709\n",
      "step = 6583400: loss = 3.8312642574310303\n",
      "step = 6583600: loss = 3.8106353282928467\n",
      "step = 6583800: loss = 3.956943988800049\n",
      "step = 6584000: loss = 4.819083213806152\n",
      "step = 6584200: loss = 3.7907323837280273\n",
      "step = 6584400: loss = 3.6051878929138184\n",
      "step = 6584600: loss = 2.9115893840789795\n",
      "step = 6584800: loss = 5.671633243560791\n",
      "step = 6585000: loss = 3.448462963104248\n",
      "step = 6585000: Average Return = 3.828000068664551\n",
      "step = 6585200: loss = 4.6133198738098145\n",
      "step = 6585400: loss = 3.7001004219055176\n",
      "step = 6585600: loss = 3.179919719696045\n",
      "step = 6585800: loss = 4.339831829071045\n",
      "step = 6586000: loss = 4.932101249694824\n",
      "step = 6586200: loss = 4.078273296356201\n",
      "step = 6586400: loss = 3.460923671722412\n",
      "step = 6586600: loss = 4.703651428222656\n",
      "step = 6586800: loss = 5.399756908416748\n",
      "step = 6587000: loss = 4.698338508605957\n",
      "step = 6587200: loss = 4.192073822021484\n",
      "step = 6587400: loss = 5.1852707862854\n",
      "step = 6587600: loss = 4.726171016693115\n",
      "step = 6587800: loss = 5.964906692504883\n",
      "step = 6588000: loss = 2.4585611820220947\n",
      "step = 6588200: loss = 4.822825908660889\n",
      "step = 6588400: loss = 3.5925652980804443\n",
      "step = 6588600: loss = 3.7884185314178467\n",
      "step = 6588800: loss = 5.008127689361572\n",
      "step = 6589000: loss = 3.9632575511932373\n",
      "step = 6589200: loss = 4.278438568115234\n",
      "step = 6589400: loss = 5.162730693817139\n",
      "step = 6589600: loss = 4.2009406089782715\n",
      "step = 6589800: loss = 2.929295063018799\n",
      "step = 6590000: loss = 3.6393284797668457\n",
      "step = 6590000: Average Return = 3.555999994277954\n",
      "step = 6590200: loss = 4.907687187194824\n",
      "step = 6590400: loss = 4.22467041015625\n",
      "step = 6590600: loss = 3.794973134994507\n",
      "step = 6590800: loss = 2.6649632453918457\n",
      "step = 6591000: loss = 4.785650730133057\n",
      "step = 6591200: loss = 3.1733622550964355\n",
      "step = 6591400: loss = 4.578273773193359\n",
      "step = 6591600: loss = 4.416379451751709\n",
      "step = 6591800: loss = 3.5339248180389404\n",
      "step = 6592000: loss = 3.700298309326172\n",
      "step = 6592200: loss = 5.300242900848389\n",
      "step = 6592400: loss = 5.176918983459473\n",
      "step = 6592600: loss = 5.855723857879639\n",
      "step = 6592800: loss = 3.469525098800659\n",
      "step = 6593000: loss = 4.4564433097839355\n",
      "step = 6593200: loss = 4.014797210693359\n",
      "step = 6593400: loss = 2.9957656860351562\n",
      "step = 6593600: loss = 2.577000141143799\n",
      "step = 6593800: loss = 4.303287982940674\n",
      "step = 6594000: loss = 4.728549003601074\n",
      "step = 6594200: loss = 4.859946250915527\n",
      "step = 6594400: loss = 3.86025071144104\n",
      "step = 6594600: loss = 3.65813946723938\n",
      "step = 6594800: loss = 4.671243667602539\n",
      "step = 6595000: loss = 5.092750549316406\n",
      "step = 6595000: Average Return = 3.6059999465942383\n",
      "step = 6595200: loss = 4.635859966278076\n",
      "step = 6595400: loss = 3.014474630355835\n",
      "step = 6595600: loss = 3.712440013885498\n",
      "step = 6595800: loss = 3.9482293128967285\n",
      "step = 6596000: loss = 7.25653076171875\n",
      "step = 6596200: loss = 4.126167297363281\n",
      "step = 6596400: loss = 3.731074333190918\n",
      "step = 6596600: loss = 3.2644765377044678\n",
      "step = 6596800: loss = 4.725361347198486\n",
      "step = 6597000: loss = 5.044279098510742\n",
      "step = 6597200: loss = 5.779246807098389\n",
      "step = 6597400: loss = 3.3279638290405273\n",
      "step = 6597600: loss = 3.963742256164551\n",
      "step = 6597800: loss = 4.764765739440918\n",
      "step = 6598000: loss = 2.1881282329559326\n",
      "step = 6598200: loss = 4.528946876525879\n",
      "step = 6598400: loss = 4.2984747886657715\n",
      "step = 6598600: loss = 6.184830665588379\n",
      "step = 6598800: loss = 4.733137130737305\n",
      "step = 6599000: loss = 2.9211909770965576\n",
      "step = 6599200: loss = 3.9031732082366943\n",
      "step = 6599400: loss = 4.562661647796631\n",
      "step = 6599600: loss = 4.007659912109375\n",
      "step = 6599800: loss = 3.115687608718872\n",
      "step = 6600000: loss = 5.691306114196777\n",
      "step = 6600000: Average Return = 3.803999900817871\n",
      "step = 6600200: loss = 3.147350788116455\n",
      "step = 6600400: loss = 3.580070972442627\n",
      "step = 6600600: loss = 5.89886474609375\n",
      "step = 6600800: loss = 3.3785436153411865\n",
      "step = 6601000: loss = 2.7461814880371094\n",
      "step = 6601200: loss = 4.321061134338379\n",
      "step = 6601400: loss = 2.7348074913024902\n",
      "step = 6601600: loss = 4.79749059677124\n",
      "step = 6601800: loss = 5.325878620147705\n",
      "step = 6602000: loss = 3.9027388095855713\n",
      "step = 6602200: loss = 4.806004047393799\n",
      "step = 6602400: loss = 3.824974536895752\n",
      "step = 6602600: loss = 4.620461463928223\n",
      "step = 6602800: loss = 4.016834735870361\n",
      "step = 6603000: loss = 3.585045099258423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6603200: loss = 5.499673843383789\n",
      "step = 6603400: loss = 2.8945956230163574\n",
      "step = 6603600: loss = 3.678081512451172\n",
      "step = 6603800: loss = 5.124454975128174\n",
      "step = 6604000: loss = 3.7913830280303955\n",
      "step = 6604200: loss = 4.123239517211914\n",
      "step = 6604400: loss = 3.227091073989868\n",
      "step = 6604600: loss = 3.9728939533233643\n",
      "step = 6604800: loss = 4.570633411407471\n",
      "step = 6605000: loss = 3.5396313667297363\n",
      "step = 6605000: Average Return = 4.064000129699707\n",
      "step = 6605200: loss = 4.549286842346191\n",
      "step = 6605400: loss = 3.454767942428589\n",
      "step = 6605600: loss = 4.166843414306641\n",
      "step = 6605800: loss = 4.327101230621338\n",
      "step = 6606000: loss = 3.2899038791656494\n",
      "step = 6606200: loss = 4.2456183433532715\n",
      "step = 6606400: loss = 4.738654613494873\n",
      "step = 6606600: loss = 4.13352108001709\n",
      "step = 6606800: loss = 4.513794898986816\n",
      "step = 6607000: loss = 4.896460056304932\n",
      "step = 6607200: loss = 4.314357757568359\n",
      "step = 6607400: loss = 4.519502639770508\n",
      "step = 6607600: loss = 4.341879367828369\n",
      "step = 6607800: loss = 3.7514615058898926\n",
      "step = 6608000: loss = 4.575874328613281\n",
      "step = 6608200: loss = 4.2177534103393555\n",
      "step = 6608400: loss = 3.0333385467529297\n",
      "step = 6608600: loss = 4.6979498863220215\n",
      "step = 6608800: loss = 2.6028871536254883\n",
      "step = 6609000: loss = 5.060812950134277\n",
      "step = 6609200: loss = 3.805914878845215\n",
      "step = 6609400: loss = 5.222397804260254\n",
      "step = 6609600: loss = 3.323108196258545\n",
      "step = 6609800: loss = 4.998228073120117\n",
      "step = 6610000: loss = 4.16844367980957\n",
      "step = 6610000: Average Return = 3.9679999351501465\n",
      "step = 6610200: loss = 4.243659973144531\n",
      "step = 6610400: loss = 4.543760776519775\n",
      "step = 6610600: loss = 4.151353359222412\n",
      "step = 6610800: loss = 4.561731815338135\n",
      "step = 6611000: loss = 3.9284274578094482\n",
      "step = 6611200: loss = 6.305718898773193\n",
      "step = 6611400: loss = 5.068342208862305\n",
      "step = 6611600: loss = 4.497960567474365\n",
      "step = 6611800: loss = 5.493189811706543\n",
      "step = 6612000: loss = 4.263891220092773\n",
      "step = 6612200: loss = 5.057371139526367\n",
      "step = 6612400: loss = 4.053991794586182\n",
      "step = 6612600: loss = 4.134476184844971\n",
      "step = 6612800: loss = 5.103634357452393\n",
      "step = 6613000: loss = 4.061983108520508\n",
      "step = 6613200: loss = 4.4594831466674805\n",
      "step = 6613400: loss = 5.552346706390381\n",
      "step = 6613600: loss = 5.347174644470215\n",
      "step = 6613800: loss = 4.520533561706543\n",
      "step = 6614000: loss = 4.2747297286987305\n",
      "step = 6614200: loss = 3.8645119667053223\n",
      "step = 6614400: loss = 5.685773849487305\n",
      "step = 6614600: loss = 3.7153501510620117\n",
      "step = 6614800: loss = 3.2190096378326416\n",
      "step = 6615000: loss = 4.657992839813232\n",
      "step = 6615000: Average Return = 3.1459999084472656\n",
      "step = 6615200: loss = 3.4219603538513184\n",
      "step = 6615400: loss = 3.9872829914093018\n",
      "step = 6615600: loss = 5.487261772155762\n",
      "step = 6615800: loss = 5.184899806976318\n",
      "step = 6616000: loss = 4.874489784240723\n",
      "step = 6616200: loss = 4.3152546882629395\n",
      "step = 6616400: loss = 3.852595806121826\n",
      "step = 6616600: loss = 4.594236373901367\n",
      "step = 6616800: loss = 3.5066888332366943\n",
      "step = 6617000: loss = 5.77288293838501\n",
      "step = 6617200: loss = 5.358775615692139\n",
      "step = 6617400: loss = 3.11855411529541\n",
      "step = 6617600: loss = 3.8261680603027344\n",
      "step = 6617800: loss = 3.036916971206665\n",
      "step = 6618000: loss = 4.498208045959473\n",
      "step = 6618200: loss = 4.282844066619873\n",
      "step = 6618400: loss = 4.416501522064209\n",
      "step = 6618600: loss = 4.629206657409668\n",
      "step = 6618800: loss = 3.7790825366973877\n",
      "step = 6619000: loss = 4.242416858673096\n",
      "step = 6619200: loss = 4.076195240020752\n",
      "step = 6619400: loss = 3.8942339420318604\n",
      "step = 6619600: loss = 3.4599273204803467\n",
      "step = 6619800: loss = 5.233000755310059\n",
      "step = 6620000: loss = 3.7111213207244873\n",
      "step = 6620000: Average Return = 3.8499999046325684\n",
      "step = 6620200: loss = 4.17112922668457\n",
      "step = 6620400: loss = 5.625451564788818\n",
      "step = 6620600: loss = 3.765808582305908\n",
      "step = 6620800: loss = 3.9636731147766113\n",
      "step = 6621000: loss = 5.384975433349609\n",
      "step = 6621200: loss = 5.410451412200928\n",
      "step = 6621400: loss = 4.6552252769470215\n",
      "step = 6621600: loss = 3.64107346534729\n",
      "step = 6621800: loss = 4.493223667144775\n",
      "step = 6622000: loss = 2.488100290298462\n",
      "step = 6622200: loss = 4.533511161804199\n",
      "step = 6622400: loss = 4.01343297958374\n",
      "step = 6622600: loss = 4.2906575202941895\n",
      "step = 6622800: loss = 5.781504154205322\n",
      "step = 6623000: loss = 2.9522526264190674\n",
      "step = 6623200: loss = 5.417025089263916\n",
      "step = 6623400: loss = 3.882627487182617\n",
      "step = 6623600: loss = 3.6426143646240234\n",
      "step = 6623800: loss = 3.240633487701416\n",
      "step = 6624000: loss = 4.143429756164551\n",
      "step = 6624200: loss = 4.326688289642334\n",
      "step = 6624400: loss = 3.4275200366973877\n",
      "step = 6624600: loss = 5.021733283996582\n",
      "step = 6624800: loss = 5.871082782745361\n",
      "step = 6625000: loss = 3.611309766769409\n",
      "step = 6625000: Average Return = 3.9159998893737793\n",
      "step = 6625200: loss = 5.089542388916016\n",
      "step = 6625400: loss = 5.318803787231445\n",
      "step = 6625600: loss = 4.216217041015625\n",
      "step = 6625800: loss = 5.715299129486084\n",
      "step = 6626000: loss = 5.5908074378967285\n",
      "step = 6626200: loss = 3.0120723247528076\n",
      "step = 6626400: loss = 3.1799252033233643\n",
      "step = 6626600: loss = 4.953151226043701\n",
      "step = 6626800: loss = 5.038546562194824\n",
      "step = 6627000: loss = 5.811273097991943\n",
      "step = 6627200: loss = 4.550390243530273\n",
      "step = 6627400: loss = 3.772235631942749\n",
      "step = 6627600: loss = 5.1458353996276855\n",
      "step = 6627800: loss = 3.564396858215332\n",
      "step = 6628000: loss = 3.5767693519592285\n",
      "step = 6628200: loss = 4.278707981109619\n",
      "step = 6628400: loss = 4.1442413330078125\n",
      "step = 6628600: loss = 4.24324893951416\n",
      "step = 6628800: loss = 4.297028541564941\n",
      "step = 6629000: loss = 4.293642044067383\n",
      "step = 6629200: loss = 4.526031970977783\n",
      "step = 6629400: loss = 3.7818877696990967\n",
      "step = 6629600: loss = 4.1214518547058105\n",
      "step = 6629800: loss = 4.109070777893066\n",
      "step = 6630000: loss = 4.0676116943359375\n",
      "step = 6630000: Average Return = 3.946000099182129\n",
      "step = 6630200: loss = 2.0636003017425537\n",
      "step = 6630400: loss = 3.614089250564575\n",
      "step = 6630600: loss = 3.9472787380218506\n",
      "step = 6630800: loss = 5.260898113250732\n",
      "step = 6631000: loss = 4.696636199951172\n",
      "step = 6631200: loss = 5.119836807250977\n",
      "step = 6631400: loss = 4.809248447418213\n",
      "step = 6631600: loss = 3.5852649211883545\n",
      "step = 6631800: loss = 5.643199920654297\n",
      "step = 6632000: loss = 4.522645473480225\n",
      "step = 6632200: loss = 3.5725648403167725\n",
      "step = 6632400: loss = 6.0958123207092285\n",
      "step = 6632600: loss = 3.8881595134735107\n",
      "step = 6632800: loss = 5.307653903961182\n",
      "step = 6633000: loss = 4.156205177307129\n",
      "step = 6633200: loss = 4.967381000518799\n",
      "step = 6633400: loss = 3.962470531463623\n",
      "step = 6633600: loss = 7.025290489196777\n",
      "step = 6633800: loss = 2.735471248626709\n",
      "step = 6634000: loss = 4.30726432800293\n",
      "step = 6634200: loss = 2.7973434925079346\n",
      "step = 6634400: loss = 3.1163442134857178\n",
      "step = 6634600: loss = 5.081840991973877\n",
      "step = 6634800: loss = 4.011301040649414\n",
      "step = 6635000: loss = 4.687724590301514\n",
      "step = 6635000: Average Return = 3.7139999866485596\n",
      "step = 6635200: loss = 5.58108377456665\n",
      "step = 6635400: loss = 4.471710205078125\n",
      "step = 6635600: loss = 3.1769652366638184\n",
      "step = 6635800: loss = 4.711921691894531\n",
      "step = 6636000: loss = 4.196328639984131\n",
      "step = 6636200: loss = 5.36283016204834\n",
      "step = 6636400: loss = 4.467910289764404\n",
      "step = 6636600: loss = 4.532296657562256\n",
      "step = 6636800: loss = 4.138367652893066\n",
      "step = 6637000: loss = 4.871851444244385\n",
      "step = 6637200: loss = 3.508970260620117\n",
      "step = 6637400: loss = 4.464711666107178\n",
      "step = 6637600: loss = 5.014533996582031\n",
      "step = 6637800: loss = 4.633889198303223\n",
      "step = 6638000: loss = 3.790698528289795\n",
      "step = 6638200: loss = 4.186460494995117\n",
      "step = 6638400: loss = 3.896902322769165\n",
      "step = 6638600: loss = 3.1977527141571045\n",
      "step = 6638800: loss = 4.652307510375977\n",
      "step = 6639000: loss = 3.9365997314453125\n",
      "step = 6639200: loss = 4.318497180938721\n",
      "step = 6639400: loss = 3.612067699432373\n",
      "step = 6639600: loss = 3.7305755615234375\n",
      "step = 6639800: loss = 3.52099347114563\n",
      "step = 6640000: loss = 4.4360809326171875\n",
      "step = 6640000: Average Return = 3.931999921798706\n",
      "step = 6640200: loss = 2.642136812210083\n",
      "step = 6640400: loss = 4.028417110443115\n",
      "step = 6640600: loss = 3.9484057426452637\n",
      "step = 6640800: loss = 4.964284896850586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6641000: loss = 3.3256616592407227\n",
      "step = 6641200: loss = 4.557293891906738\n",
      "step = 6641400: loss = 3.8383610248565674\n",
      "step = 6641600: loss = 3.9477782249450684\n",
      "step = 6641800: loss = 5.310168266296387\n",
      "step = 6642000: loss = 3.6865346431732178\n",
      "step = 6642200: loss = 4.687551975250244\n",
      "step = 6642400: loss = 3.176372528076172\n",
      "step = 6642600: loss = 4.263217449188232\n",
      "step = 6642800: loss = 4.014604568481445\n",
      "step = 6643000: loss = 3.1247735023498535\n",
      "step = 6643200: loss = 4.295222759246826\n",
      "step = 6643400: loss = 3.76226544380188\n",
      "step = 6643600: loss = 3.6649465560913086\n",
      "step = 6643800: loss = 4.533857822418213\n",
      "step = 6644000: loss = 4.314009189605713\n",
      "step = 6644200: loss = 3.53237247467041\n",
      "step = 6644400: loss = 4.417401313781738\n",
      "step = 6644600: loss = 3.296210289001465\n",
      "step = 6644800: loss = 4.364664077758789\n",
      "step = 6645000: loss = 4.037410736083984\n",
      "step = 6645000: Average Return = 3.065999984741211\n",
      "step = 6645200: loss = 3.232034921646118\n",
      "step = 6645400: loss = 4.579688549041748\n",
      "step = 6645600: loss = 4.827925205230713\n",
      "step = 6645800: loss = 4.525968074798584\n",
      "step = 6646000: loss = 4.510680198669434\n",
      "step = 6646200: loss = 3.844487428665161\n",
      "step = 6646400: loss = 3.8521888256073\n",
      "step = 6646600: loss = 3.745349884033203\n",
      "step = 6646800: loss = 5.225914001464844\n",
      "step = 6647000: loss = 6.468998432159424\n",
      "step = 6647200: loss = 5.227084636688232\n",
      "step = 6647400: loss = 4.682651042938232\n",
      "step = 6647600: loss = 5.615167617797852\n",
      "step = 6647800: loss = 4.455117225646973\n",
      "step = 6648000: loss = 4.1950249671936035\n",
      "step = 6648200: loss = 4.5720977783203125\n",
      "step = 6648400: loss = 4.238094329833984\n",
      "step = 6648600: loss = 3.837632894515991\n",
      "step = 6648800: loss = 3.611487865447998\n",
      "step = 6649000: loss = 4.088098526000977\n",
      "step = 6649200: loss = 3.2940661907196045\n",
      "step = 6649400: loss = 4.448338985443115\n",
      "step = 6649600: loss = 4.609523296356201\n",
      "step = 6649800: loss = 3.1323277950286865\n",
      "step = 6650000: loss = 6.013840198516846\n",
      "step = 6650000: Average Return = 3.997999906539917\n",
      "step = 6650200: loss = 4.346697807312012\n",
      "step = 6650400: loss = 3.136845111846924\n",
      "step = 6650600: loss = 4.311748027801514\n",
      "step = 6650800: loss = 3.378762722015381\n",
      "step = 6651000: loss = 4.610934257507324\n",
      "step = 6651200: loss = 3.9452006816864014\n",
      "step = 6651400: loss = 3.2572057247161865\n",
      "step = 6651600: loss = 4.55332612991333\n",
      "step = 6651800: loss = 4.212391376495361\n",
      "step = 6652000: loss = 5.087876319885254\n",
      "step = 6652200: loss = 4.851492881774902\n",
      "step = 6652400: loss = 4.949985980987549\n",
      "step = 6652600: loss = 4.229622840881348\n",
      "step = 6652800: loss = 5.205838680267334\n",
      "step = 6653000: loss = 6.015041828155518\n",
      "step = 6653200: loss = 4.172543048858643\n",
      "step = 6653400: loss = 3.8515443801879883\n",
      "step = 6653600: loss = 3.6207451820373535\n",
      "step = 6653800: loss = 4.486873149871826\n",
      "step = 6654000: loss = 5.011932849884033\n",
      "step = 6654200: loss = 5.100822448730469\n",
      "step = 6654400: loss = 2.868746280670166\n",
      "step = 6654600: loss = 3.8131179809570312\n",
      "step = 6654800: loss = 5.547270774841309\n",
      "step = 6655000: loss = 3.0784895420074463\n",
      "step = 6655000: Average Return = 3.937999963760376\n",
      "step = 6655200: loss = 5.085498809814453\n",
      "step = 6655400: loss = 3.9209835529327393\n",
      "step = 6655600: loss = 5.038254737854004\n",
      "step = 6655800: loss = 5.158266544342041\n",
      "step = 6656000: loss = 4.039808750152588\n",
      "step = 6656200: loss = 4.072239398956299\n",
      "step = 6656400: loss = 5.868556976318359\n",
      "step = 6656600: loss = 4.542924880981445\n",
      "step = 6656800: loss = 3.417346954345703\n",
      "step = 6657000: loss = 4.621987342834473\n",
      "step = 6657200: loss = 3.4247236251831055\n",
      "step = 6657400: loss = 3.1416988372802734\n",
      "step = 6657600: loss = 4.543889045715332\n",
      "step = 6657800: loss = 4.341986179351807\n",
      "step = 6658000: loss = 5.143091201782227\n",
      "step = 6658200: loss = 4.100487232208252\n",
      "step = 6658400: loss = 4.6630048751831055\n",
      "step = 6658600: loss = 4.142470836639404\n",
      "step = 6658800: loss = 3.913609027862549\n",
      "step = 6659000: loss = 1.8444548845291138\n",
      "step = 6659200: loss = 3.4245388507843018\n",
      "step = 6659400: loss = 5.9842352867126465\n",
      "step = 6659600: loss = 4.517749309539795\n",
      "step = 6659800: loss = 4.5806097984313965\n",
      "step = 6660000: loss = 4.188713073730469\n",
      "step = 6660000: Average Return = 3.553999900817871\n",
      "step = 6660200: loss = 3.177009105682373\n",
      "step = 6660400: loss = 4.270461559295654\n",
      "step = 6660600: loss = 5.045702934265137\n",
      "step = 6660800: loss = 3.057809352874756\n",
      "step = 6661000: loss = 5.190996170043945\n",
      "step = 6661200: loss = 3.6130242347717285\n",
      "step = 6661400: loss = 3.553852081298828\n",
      "step = 6661600: loss = 5.110222816467285\n",
      "step = 6661800: loss = 3.5786967277526855\n",
      "step = 6662000: loss = 4.309234142303467\n",
      "step = 6662200: loss = 4.021974563598633\n",
      "step = 6662400: loss = 3.9660916328430176\n",
      "step = 6662600: loss = 5.196391582489014\n",
      "step = 6662800: loss = 4.572720050811768\n",
      "step = 6663000: loss = 4.192878246307373\n",
      "step = 6663200: loss = 5.113650321960449\n",
      "step = 6663400: loss = 3.8319268226623535\n",
      "step = 6663600: loss = 3.193603992462158\n",
      "step = 6663800: loss = 4.879854679107666\n",
      "step = 6664000: loss = 3.842350959777832\n",
      "step = 6664200: loss = 4.010845184326172\n",
      "step = 6664400: loss = 3.4350039958953857\n",
      "step = 6664600: loss = 3.1423559188842773\n",
      "step = 6664800: loss = 3.8936846256256104\n",
      "step = 6665000: loss = 4.539931774139404\n",
      "step = 6665000: Average Return = 3.865999937057495\n",
      "step = 6665200: loss = 5.6339874267578125\n",
      "step = 6665400: loss = 2.7024714946746826\n",
      "step = 6665600: loss = 3.59405517578125\n",
      "step = 6665800: loss = 4.723574638366699\n",
      "step = 6666000: loss = 3.10095477104187\n",
      "step = 6666200: loss = 4.510491371154785\n",
      "step = 6666400: loss = 3.4648349285125732\n",
      "step = 6666600: loss = 3.425785541534424\n",
      "step = 6666800: loss = 3.982131242752075\n",
      "step = 6667000: loss = 5.231165885925293\n",
      "step = 6667200: loss = 4.519540786743164\n",
      "step = 6667400: loss = 3.6731419563293457\n",
      "step = 6667600: loss = 5.730490684509277\n",
      "step = 6667800: loss = 4.648782253265381\n",
      "step = 6668000: loss = 5.009160995483398\n",
      "step = 6668200: loss = 4.795534133911133\n",
      "step = 6668400: loss = 2.9213149547576904\n",
      "step = 6668600: loss = 5.248005390167236\n",
      "step = 6668800: loss = 4.662996768951416\n",
      "step = 6669000: loss = 4.937857151031494\n",
      "step = 6669200: loss = 5.294472694396973\n",
      "step = 6669400: loss = 3.7232959270477295\n",
      "step = 6669600: loss = 3.3501853942871094\n",
      "step = 6669800: loss = 5.458189964294434\n",
      "step = 6670000: loss = 5.467099666595459\n",
      "step = 6670000: Average Return = 4.22599983215332\n",
      "step = 6670200: loss = 4.031518936157227\n",
      "step = 6670400: loss = 5.671950340270996\n",
      "step = 6670600: loss = 4.093740940093994\n",
      "step = 6670800: loss = 3.327946901321411\n",
      "step = 6671000: loss = 3.3123087882995605\n",
      "step = 6671200: loss = 3.5619821548461914\n",
      "step = 6671400: loss = 3.7508955001831055\n",
      "step = 6671600: loss = 4.190945625305176\n",
      "step = 6671800: loss = 3.749654531478882\n",
      "step = 6672000: loss = 2.9381182193756104\n",
      "step = 6672200: loss = 5.049933433532715\n",
      "step = 6672400: loss = 5.76593017578125\n",
      "step = 6672600: loss = 4.528276443481445\n",
      "step = 6672800: loss = 5.089895725250244\n",
      "step = 6673000: loss = 4.813349723815918\n",
      "step = 6673200: loss = 5.4705095291137695\n",
      "step = 6673400: loss = 3.4325315952301025\n",
      "step = 6673600: loss = 4.5944952964782715\n",
      "step = 6673800: loss = 3.4078662395477295\n",
      "step = 6674000: loss = 3.432363748550415\n",
      "step = 6674200: loss = 4.4020676612854\n",
      "step = 6674400: loss = 5.624073505401611\n",
      "step = 6674600: loss = 4.981531143188477\n",
      "step = 6674800: loss = 4.127841949462891\n",
      "step = 6675000: loss = 3.409355640411377\n",
      "step = 6675000: Average Return = 3.9539999961853027\n",
      "step = 6675200: loss = 4.156703472137451\n",
      "step = 6675400: loss = 4.797757148742676\n",
      "step = 6675600: loss = 3.938230276107788\n",
      "step = 6675800: loss = 3.9881794452667236\n",
      "step = 6676000: loss = 3.8352043628692627\n",
      "step = 6676200: loss = 3.8611462116241455\n",
      "step = 6676400: loss = 3.8200504779815674\n",
      "step = 6676600: loss = 4.648674011230469\n",
      "step = 6676800: loss = 4.618874549865723\n",
      "step = 6677000: loss = 5.30607271194458\n",
      "step = 6677200: loss = 4.490260124206543\n",
      "step = 6677400: loss = 4.068759441375732\n",
      "step = 6677600: loss = 4.851837635040283\n",
      "step = 6677800: loss = 2.9352123737335205\n",
      "step = 6678000: loss = 6.715047359466553\n",
      "step = 6678200: loss = 5.801705360412598\n",
      "step = 6678400: loss = 3.6506407260894775\n",
      "step = 6678600: loss = 3.281351327896118\n",
      "step = 6678800: loss = 4.617358684539795\n",
      "step = 6679000: loss = 6.24064826965332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6679200: loss = 4.227924823760986\n",
      "step = 6679400: loss = 4.490689754486084\n",
      "step = 6679600: loss = 4.520518779754639\n",
      "step = 6679800: loss = 3.434213876724243\n",
      "step = 6680000: loss = 3.953568935394287\n",
      "step = 6680000: Average Return = 3.7839999198913574\n",
      "step = 6680200: loss = 4.331204891204834\n",
      "step = 6680400: loss = 4.195681095123291\n",
      "step = 6680600: loss = 4.431516647338867\n",
      "step = 6680800: loss = 3.1728670597076416\n",
      "step = 6681000: loss = 5.0264973640441895\n",
      "step = 6681200: loss = 5.146361351013184\n",
      "step = 6681400: loss = 6.056111812591553\n",
      "step = 6681600: loss = 5.39654541015625\n",
      "step = 6681800: loss = 4.353381633758545\n",
      "step = 6682000: loss = 6.021529197692871\n",
      "step = 6682200: loss = 5.531642913818359\n",
      "step = 6682400: loss = 3.0255656242370605\n",
      "step = 6682600: loss = 3.368089199066162\n",
      "step = 6682800: loss = 4.4760308265686035\n",
      "step = 6683000: loss = 5.224592208862305\n",
      "step = 6683200: loss = 3.1763973236083984\n",
      "step = 6683400: loss = 5.368862152099609\n",
      "step = 6683600: loss = 4.797026634216309\n",
      "step = 6683800: loss = 4.475794792175293\n",
      "step = 6684000: loss = 3.634507894515991\n",
      "step = 6684200: loss = 3.919271230697632\n",
      "step = 6684400: loss = 4.841586589813232\n",
      "step = 6684600: loss = 6.026758670806885\n",
      "step = 6684800: loss = 4.084871768951416\n",
      "step = 6685000: loss = 3.558993339538574\n",
      "step = 6685000: Average Return = 3.759999990463257\n",
      "step = 6685200: loss = 5.785417556762695\n",
      "step = 6685400: loss = 4.953855991363525\n",
      "step = 6685600: loss = 4.314948558807373\n",
      "step = 6685800: loss = 4.563241481781006\n",
      "step = 6686000: loss = 4.269668102264404\n",
      "step = 6686200: loss = 4.204213619232178\n",
      "step = 6686400: loss = 6.172940731048584\n",
      "step = 6686600: loss = 4.142531871795654\n",
      "step = 6686800: loss = 4.2933855056762695\n",
      "step = 6687000: loss = 4.552002906799316\n",
      "step = 6687200: loss = 4.827945232391357\n",
      "step = 6687400: loss = 3.564784526824951\n",
      "step = 6687600: loss = 3.8199827671051025\n",
      "step = 6687800: loss = 4.393476486206055\n",
      "step = 6688000: loss = 3.523000955581665\n",
      "step = 6688200: loss = 4.063886642456055\n",
      "step = 6688400: loss = 4.196832656860352\n",
      "step = 6688600: loss = 4.575573444366455\n",
      "step = 6688800: loss = 4.0965142250061035\n",
      "step = 6689000: loss = 3.787794828414917\n",
      "step = 6689200: loss = 3.4685869216918945\n",
      "step = 6689400: loss = 4.930985450744629\n",
      "step = 6689600: loss = 5.129630088806152\n",
      "step = 6689800: loss = 2.4315919876098633\n",
      "step = 6690000: loss = 3.82631516456604\n",
      "step = 6690000: Average Return = 3.8980000019073486\n",
      "step = 6690200: loss = 5.386106967926025\n",
      "step = 6690400: loss = 5.466287612915039\n",
      "step = 6690600: loss = 5.06227445602417\n",
      "step = 6690800: loss = 4.02801513671875\n",
      "step = 6691000: loss = 5.200775146484375\n",
      "step = 6691200: loss = 3.7586541175842285\n",
      "step = 6691400: loss = 3.0333006381988525\n",
      "step = 6691600: loss = 3.1982598304748535\n",
      "step = 6691800: loss = 3.144428014755249\n",
      "step = 6692000: loss = 4.1441779136657715\n",
      "step = 6692200: loss = 4.49768590927124\n",
      "step = 6692400: loss = 4.3078413009643555\n",
      "step = 6692600: loss = 4.666025161743164\n",
      "step = 6692800: loss = 4.453094482421875\n",
      "step = 6693000: loss = 3.893339157104492\n",
      "step = 6693200: loss = 3.3325352668762207\n",
      "step = 6693400: loss = 4.285618782043457\n",
      "step = 6693600: loss = 5.160531044006348\n",
      "step = 6693800: loss = 3.6472597122192383\n",
      "step = 6694000: loss = 2.1051993370056152\n",
      "step = 6694200: loss = 4.638665199279785\n",
      "step = 6694400: loss = 4.64382266998291\n",
      "step = 6694600: loss = 3.25639271736145\n",
      "step = 6694800: loss = 2.9796557426452637\n",
      "step = 6695000: loss = 2.561741352081299\n",
      "step = 6695000: Average Return = 3.944000005722046\n",
      "step = 6695200: loss = 4.389233112335205\n",
      "step = 6695400: loss = 2.475128650665283\n",
      "step = 6695600: loss = 5.826869964599609\n",
      "step = 6695800: loss = 3.2519242763519287\n",
      "step = 6696000: loss = 4.210416316986084\n",
      "step = 6696200: loss = 4.949873447418213\n",
      "step = 6696400: loss = 4.155586242675781\n",
      "step = 6696600: loss = 5.560404300689697\n",
      "step = 6696800: loss = 2.9735920429229736\n",
      "step = 6697000: loss = 4.522593975067139\n",
      "step = 6697200: loss = 6.117342472076416\n",
      "step = 6697400: loss = 4.260241985321045\n",
      "step = 6697600: loss = 5.472751140594482\n",
      "step = 6697800: loss = 4.092105388641357\n",
      "step = 6698000: loss = 4.446698188781738\n",
      "step = 6698200: loss = 4.4783453941345215\n",
      "step = 6698400: loss = 3.876964569091797\n",
      "step = 6698600: loss = 3.9555461406707764\n",
      "step = 6698800: loss = 3.746112585067749\n",
      "step = 6699000: loss = 4.528018474578857\n",
      "step = 6699200: loss = 3.5455572605133057\n",
      "step = 6699400: loss = 3.9903318881988525\n",
      "step = 6699600: loss = 4.138796806335449\n",
      "step = 6699800: loss = 2.508389711380005\n",
      "step = 6700000: loss = 3.3541927337646484\n",
      "step = 6700000: Average Return = 3.99399995803833\n",
      "step = 6700200: loss = 3.4375076293945312\n",
      "step = 6700400: loss = 3.666062593460083\n",
      "step = 6700600: loss = 4.580240249633789\n",
      "step = 6700800: loss = 3.9572362899780273\n",
      "step = 6701000: loss = 3.8248403072357178\n",
      "step = 6701200: loss = 3.920628070831299\n",
      "step = 6701400: loss = 4.3987555503845215\n",
      "step = 6701600: loss = 4.522162914276123\n",
      "step = 6701800: loss = 5.061206340789795\n",
      "step = 6702000: loss = 3.9195823669433594\n",
      "step = 6702200: loss = 3.6212034225463867\n",
      "step = 6702400: loss = 4.725507736206055\n",
      "step = 6702600: loss = 5.383815765380859\n",
      "step = 6702800: loss = 5.998178958892822\n",
      "step = 6703000: loss = 5.543545246124268\n",
      "step = 6703200: loss = 3.559579372406006\n",
      "step = 6703400: loss = 6.371499061584473\n",
      "step = 6703600: loss = 4.079421043395996\n",
      "step = 6703800: loss = 3.5215578079223633\n",
      "step = 6704000: loss = 4.931010723114014\n",
      "step = 6704200: loss = 3.5868823528289795\n",
      "step = 6704400: loss = 5.822188377380371\n",
      "step = 6704600: loss = 4.032317638397217\n",
      "step = 6704800: loss = 5.313597202301025\n",
      "step = 6705000: loss = 5.319575786590576\n",
      "step = 6705000: Average Return = 3.746000051498413\n",
      "step = 6705200: loss = 4.524262428283691\n",
      "step = 6705400: loss = 3.911048412322998\n",
      "step = 6705600: loss = 3.759101390838623\n",
      "step = 6705800: loss = 3.280937433242798\n",
      "step = 6706000: loss = 3.6486330032348633\n",
      "step = 6706200: loss = 2.6821722984313965\n",
      "step = 6706400: loss = 5.805235385894775\n",
      "step = 6706600: loss = 5.4783759117126465\n",
      "step = 6706800: loss = 3.3193023204803467\n",
      "step = 6707000: loss = 5.515193462371826\n",
      "step = 6707200: loss = 3.903083324432373\n",
      "step = 6707400: loss = 3.000615358352661\n",
      "step = 6707600: loss = 3.0990700721740723\n",
      "step = 6707800: loss = 4.8673014640808105\n",
      "step = 6708000: loss = 4.338796138763428\n",
      "step = 6708200: loss = 4.555224418640137\n",
      "step = 6708400: loss = 4.137073993682861\n",
      "step = 6708600: loss = 4.273655414581299\n",
      "step = 6708800: loss = 5.730992317199707\n",
      "step = 6709000: loss = 4.8940534591674805\n",
      "step = 6709200: loss = 4.4348464012146\n",
      "step = 6709400: loss = 4.02881383895874\n",
      "step = 6709600: loss = 4.430003643035889\n",
      "step = 6709800: loss = 4.455103397369385\n",
      "step = 6710000: loss = 3.646260976791382\n",
      "step = 6710000: Average Return = 4.064000129699707\n",
      "step = 6710200: loss = 2.62778639793396\n",
      "step = 6710400: loss = 4.128413200378418\n",
      "step = 6710600: loss = 5.197304725646973\n",
      "step = 6710800: loss = 3.200399160385132\n",
      "step = 6711000: loss = 4.51447868347168\n",
      "step = 6711200: loss = 4.185746192932129\n",
      "step = 6711400: loss = 3.26187801361084\n",
      "step = 6711600: loss = 2.7671332359313965\n",
      "step = 6711800: loss = 4.050804138183594\n",
      "step = 6712000: loss = 3.0242178440093994\n",
      "step = 6712200: loss = 3.428921937942505\n",
      "step = 6712400: loss = 4.0528244972229\n",
      "step = 6712600: loss = 4.095343589782715\n",
      "step = 6712800: loss = 4.106293678283691\n",
      "step = 6713000: loss = 4.135402202606201\n",
      "step = 6713200: loss = 5.159186840057373\n",
      "step = 6713400: loss = 4.7530035972595215\n",
      "step = 6713600: loss = 4.813943862915039\n",
      "step = 6713800: loss = 3.7393646240234375\n",
      "step = 6714000: loss = 5.111202239990234\n",
      "step = 6714200: loss = 4.433577060699463\n",
      "step = 6714400: loss = 3.982462167739868\n",
      "step = 6714600: loss = 4.324954509735107\n",
      "step = 6714800: loss = 3.4723939895629883\n",
      "step = 6715000: loss = 4.124392509460449\n",
      "step = 6715000: Average Return = 3.950000047683716\n",
      "step = 6715200: loss = 4.108709812164307\n",
      "step = 6715400: loss = 3.6251111030578613\n",
      "step = 6715600: loss = 4.385820388793945\n",
      "step = 6715800: loss = 6.34627103805542\n",
      "step = 6716000: loss = 4.355332851409912\n",
      "step = 6716200: loss = 3.9288740158081055\n",
      "step = 6716400: loss = 3.6530656814575195\n",
      "step = 6716600: loss = 3.1989691257476807\n",
      "step = 6716800: loss = 3.851771116256714\n",
      "step = 6717000: loss = 3.7649004459381104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6717200: loss = 5.60240364074707\n",
      "step = 6717400: loss = 4.490548133850098\n",
      "step = 6717600: loss = 4.165299415588379\n",
      "step = 6717800: loss = 3.7055580615997314\n",
      "step = 6718000: loss = 5.211522579193115\n",
      "step = 6718200: loss = 3.7193069458007812\n",
      "step = 6718400: loss = 5.675292015075684\n",
      "step = 6718600: loss = 4.6113176345825195\n",
      "step = 6718800: loss = 4.389528274536133\n",
      "step = 6719000: loss = 3.4072210788726807\n",
      "step = 6719200: loss = 4.160015106201172\n",
      "step = 6719400: loss = 3.608738899230957\n",
      "step = 6719600: loss = 4.140509128570557\n",
      "step = 6719800: loss = 3.7964513301849365\n",
      "step = 6720000: loss = 4.2476372718811035\n",
      "step = 6720000: Average Return = 3.131999969482422\n",
      "step = 6720200: loss = 3.4993464946746826\n",
      "step = 6720400: loss = 3.1946141719818115\n",
      "step = 6720600: loss = 3.511310338973999\n",
      "step = 6720800: loss = 4.834535121917725\n",
      "step = 6721000: loss = 5.050051689147949\n",
      "step = 6721200: loss = 3.148991107940674\n",
      "step = 6721400: loss = 4.902029037475586\n",
      "step = 6721600: loss = 4.603137493133545\n",
      "step = 6721800: loss = 4.776208400726318\n",
      "step = 6722000: loss = 3.944042921066284\n",
      "step = 6722200: loss = 4.943550109863281\n",
      "step = 6722400: loss = 4.762758731842041\n",
      "step = 6722600: loss = 4.102717876434326\n",
      "step = 6722800: loss = 4.413212299346924\n",
      "step = 6723000: loss = 3.8995091915130615\n",
      "step = 6723200: loss = 3.441287040710449\n",
      "step = 6723400: loss = 5.378920078277588\n",
      "step = 6723600: loss = 5.502439022064209\n",
      "step = 6723800: loss = 5.73866081237793\n",
      "step = 6724000: loss = 3.5130560398101807\n",
      "step = 6724200: loss = 3.8866801261901855\n",
      "step = 6724400: loss = 4.312496662139893\n",
      "step = 6724600: loss = 3.1393895149230957\n",
      "step = 6724800: loss = 4.052662372589111\n",
      "step = 6725000: loss = 4.37360143661499\n",
      "step = 6725000: Average Return = 3.9100000858306885\n",
      "step = 6725200: loss = 4.474954128265381\n",
      "step = 6725400: loss = 4.440783500671387\n",
      "step = 6725600: loss = 5.630359172821045\n",
      "step = 6725800: loss = 4.761838912963867\n",
      "step = 6726000: loss = 5.00167179107666\n",
      "step = 6726200: loss = 4.6772613525390625\n",
      "step = 6726400: loss = 3.8510406017303467\n",
      "step = 6726600: loss = 4.949574947357178\n",
      "step = 6726800: loss = 3.3775248527526855\n",
      "step = 6727000: loss = 4.529955863952637\n",
      "step = 6727200: loss = 3.749668598175049\n",
      "step = 6727400: loss = 3.6278202533721924\n",
      "step = 6727600: loss = 4.23391056060791\n",
      "step = 6727800: loss = 5.111475944519043\n",
      "step = 6728000: loss = 4.254673480987549\n",
      "step = 6728200: loss = 4.17324161529541\n",
      "step = 6728400: loss = 3.90555477142334\n",
      "step = 6728600: loss = 4.795764923095703\n",
      "step = 6728800: loss = 3.521836757659912\n",
      "step = 6729000: loss = 5.087515830993652\n",
      "step = 6729200: loss = 5.320480823516846\n",
      "step = 6729400: loss = 4.648185729980469\n",
      "step = 6729600: loss = 4.547195911407471\n",
      "step = 6729800: loss = 4.164106845855713\n",
      "step = 6730000: loss = 4.165271282196045\n",
      "step = 6730000: Average Return = 3.364000082015991\n",
      "step = 6730200: loss = 3.21073055267334\n",
      "step = 6730400: loss = 4.818232536315918\n",
      "step = 6730600: loss = 3.8658864498138428\n",
      "step = 6730800: loss = 4.6913886070251465\n",
      "step = 6731000: loss = 4.06842565536499\n",
      "step = 6731200: loss = 4.770900249481201\n",
      "step = 6731400: loss = 3.679525136947632\n",
      "step = 6731600: loss = 5.174520015716553\n",
      "step = 6731800: loss = 5.756231784820557\n",
      "step = 6732000: loss = 2.9405691623687744\n",
      "step = 6732200: loss = 3.736002206802368\n",
      "step = 6732400: loss = 2.676783323287964\n",
      "step = 6732600: loss = 4.1752238273620605\n",
      "step = 6732800: loss = 2.9739677906036377\n",
      "step = 6733000: loss = 3.6615242958068848\n",
      "step = 6733200: loss = 4.951632976531982\n",
      "step = 6733400: loss = 4.727686882019043\n",
      "step = 6733600: loss = 6.039396286010742\n",
      "step = 6733800: loss = 3.399139404296875\n",
      "step = 6734000: loss = 4.586105823516846\n",
      "step = 6734200: loss = 2.9798011779785156\n",
      "step = 6734400: loss = 3.6990902423858643\n",
      "step = 6734600: loss = 2.969719171524048\n",
      "step = 6734800: loss = 4.08198881149292\n",
      "step = 6735000: loss = 2.854923725128174\n",
      "step = 6735000: Average Return = 3.431999921798706\n",
      "step = 6735200: loss = 5.467772006988525\n",
      "step = 6735400: loss = 3.9912569522857666\n",
      "step = 6735600: loss = 3.5320963859558105\n",
      "step = 6735800: loss = 3.8515501022338867\n",
      "step = 6736000: loss = 5.168398380279541\n",
      "step = 6736200: loss = 4.107720375061035\n",
      "step = 6736400: loss = 4.128714561462402\n",
      "step = 6736600: loss = 4.443918228149414\n",
      "step = 6736800: loss = 3.7302098274230957\n",
      "step = 6737000: loss = 3.4129478931427\n",
      "step = 6737200: loss = 3.3332951068878174\n",
      "step = 6737400: loss = 3.0952794551849365\n",
      "step = 6737600: loss = 3.5330758094787598\n",
      "step = 6737800: loss = 4.565473556518555\n",
      "step = 6738000: loss = 4.847560405731201\n",
      "step = 6738200: loss = 5.206061363220215\n",
      "step = 6738400: loss = 4.776472091674805\n",
      "step = 6738600: loss = 4.3806071281433105\n",
      "step = 6738800: loss = 5.658497333526611\n",
      "step = 6739000: loss = 6.094295024871826\n",
      "step = 6739200: loss = 4.4764628410339355\n",
      "step = 6739400: loss = 4.880448341369629\n",
      "step = 6739600: loss = 3.770517587661743\n",
      "step = 6739800: loss = 4.011625289916992\n",
      "step = 6740000: loss = 4.626161098480225\n",
      "step = 6740000: Average Return = 3.697999954223633\n",
      "step = 6740200: loss = 4.227090835571289\n",
      "step = 6740400: loss = 2.6827874183654785\n",
      "step = 6740600: loss = 4.382705211639404\n",
      "step = 6740800: loss = 4.935223579406738\n",
      "step = 6741000: loss = 2.9037392139434814\n",
      "step = 6741200: loss = 4.619506359100342\n",
      "step = 6741400: loss = 3.8310441970825195\n",
      "step = 6741600: loss = 3.807933807373047\n",
      "step = 6741800: loss = 3.315727710723877\n",
      "step = 6742000: loss = 4.992964267730713\n",
      "step = 6742200: loss = 3.12493634223938\n",
      "step = 6742400: loss = 4.190092086791992\n",
      "step = 6742600: loss = 3.395420551300049\n",
      "step = 6742800: loss = 3.2389280796051025\n",
      "step = 6743000: loss = 5.074740409851074\n",
      "step = 6743200: loss = 3.863369941711426\n",
      "step = 6743400: loss = 6.175373554229736\n",
      "step = 6743600: loss = 5.496817588806152\n",
      "step = 6743800: loss = 4.0368452072143555\n",
      "step = 6744000: loss = 4.508339881896973\n",
      "step = 6744200: loss = 4.1428937911987305\n",
      "step = 6744400: loss = 5.47648286819458\n",
      "step = 6744600: loss = 3.888122797012329\n",
      "step = 6744800: loss = 4.44635534286499\n",
      "step = 6745000: loss = 3.867948532104492\n",
      "step = 6745000: Average Return = 3.8519999980926514\n",
      "step = 6745200: loss = 3.297924757003784\n",
      "step = 6745400: loss = 4.232952117919922\n",
      "step = 6745600: loss = 3.94862961769104\n",
      "step = 6745800: loss = 4.208307266235352\n",
      "step = 6746000: loss = 3.2226758003234863\n",
      "step = 6746200: loss = 4.110047340393066\n",
      "step = 6746400: loss = 3.5598909854888916\n",
      "step = 6746600: loss = 5.04703426361084\n",
      "step = 6746800: loss = 3.3924617767333984\n",
      "step = 6747000: loss = 5.969354152679443\n",
      "step = 6747200: loss = 3.106374979019165\n",
      "step = 6747400: loss = 4.270806789398193\n",
      "step = 6747600: loss = 4.186327934265137\n",
      "step = 6747800: loss = 4.326925754547119\n",
      "step = 6748000: loss = 2.946063995361328\n",
      "step = 6748200: loss = 4.063921928405762\n",
      "step = 6748400: loss = 3.7402453422546387\n",
      "step = 6748600: loss = 4.51463508605957\n",
      "step = 6748800: loss = 3.295207977294922\n",
      "step = 6749000: loss = 3.3744957447052\n",
      "step = 6749200: loss = 3.5611870288848877\n",
      "step = 6749400: loss = 4.065210342407227\n",
      "step = 6749600: loss = 3.1082637310028076\n",
      "step = 6749800: loss = 4.486734390258789\n",
      "step = 6750000: loss = 2.9176275730133057\n",
      "step = 6750000: Average Return = 3.9019999504089355\n",
      "step = 6750200: loss = 3.184685707092285\n",
      "step = 6750400: loss = 2.9585134983062744\n",
      "step = 6750600: loss = 4.2026166915893555\n",
      "step = 6750800: loss = 5.496460437774658\n",
      "step = 6751000: loss = 3.869354724884033\n",
      "step = 6751200: loss = 3.75530743598938\n",
      "step = 6751400: loss = 4.293787479400635\n",
      "step = 6751600: loss = 5.278436183929443\n",
      "step = 6751800: loss = 4.478172302246094\n",
      "step = 6752000: loss = 4.597391128540039\n",
      "step = 6752200: loss = 5.554407596588135\n",
      "step = 6752400: loss = 4.481992244720459\n",
      "step = 6752600: loss = 4.320407390594482\n",
      "step = 6752800: loss = 5.534570217132568\n",
      "step = 6753000: loss = 3.6041338443756104\n",
      "step = 6753200: loss = 4.60982608795166\n",
      "step = 6753400: loss = 4.901225566864014\n",
      "step = 6753600: loss = 4.059560298919678\n",
      "step = 6753800: loss = 3.27962064743042\n",
      "step = 6754000: loss = 3.934605360031128\n",
      "step = 6754200: loss = 4.838168144226074\n",
      "step = 6754400: loss = 5.114691734313965\n",
      "step = 6754600: loss = 5.402589797973633\n",
      "step = 6754800: loss = 4.9150800704956055\n",
      "step = 6755000: loss = 6.315427303314209\n",
      "step = 6755000: Average Return = 3.7920000553131104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6755200: loss = 4.680232524871826\n",
      "step = 6755400: loss = 4.3280930519104\n",
      "step = 6755600: loss = 4.425404071807861\n",
      "step = 6755800: loss = 4.679055213928223\n",
      "step = 6756000: loss = 4.165393829345703\n",
      "step = 6756200: loss = 3.5718705654144287\n",
      "step = 6756400: loss = 4.49406099319458\n",
      "step = 6756600: loss = 3.5939013957977295\n",
      "step = 6756800: loss = 4.26520299911499\n",
      "step = 6757000: loss = 5.105963706970215\n",
      "step = 6757200: loss = 3.31447434425354\n",
      "step = 6757400: loss = 5.421731472015381\n",
      "step = 6757600: loss = 4.112637996673584\n",
      "step = 6757800: loss = 3.38515305519104\n",
      "step = 6758000: loss = 6.086679458618164\n",
      "step = 6758200: loss = 4.161956787109375\n",
      "step = 6758400: loss = 2.9553654193878174\n",
      "step = 6758600: loss = 4.00858211517334\n",
      "step = 6758800: loss = 4.845098972320557\n",
      "step = 6759000: loss = 4.6995720863342285\n",
      "step = 6759200: loss = 5.639697551727295\n",
      "step = 6759400: loss = 4.255546569824219\n",
      "step = 6759600: loss = 5.493049144744873\n",
      "step = 6759800: loss = 4.327101230621338\n",
      "step = 6760000: loss = 3.7528140544891357\n",
      "step = 6760000: Average Return = 3.9560000896453857\n",
      "step = 6760200: loss = 6.067623138427734\n",
      "step = 6760400: loss = 3.6990232467651367\n",
      "step = 6760600: loss = 4.132627964019775\n",
      "step = 6760800: loss = 3.7463793754577637\n",
      "step = 6761000: loss = 4.961439609527588\n",
      "step = 6761200: loss = 4.570308208465576\n",
      "step = 6761400: loss = 6.020220756530762\n",
      "step = 6761600: loss = 3.458235740661621\n",
      "step = 6761800: loss = 3.814601421356201\n",
      "step = 6762000: loss = 4.344854831695557\n",
      "step = 6762200: loss = 5.11909818649292\n",
      "step = 6762400: loss = 4.268461227416992\n",
      "step = 6762600: loss = 3.5086724758148193\n",
      "step = 6762800: loss = 3.8767964839935303\n",
      "step = 6763000: loss = 4.588457107543945\n",
      "step = 6763200: loss = 3.69596004486084\n",
      "step = 6763400: loss = 4.420201778411865\n",
      "step = 6763600: loss = 5.641410827636719\n",
      "step = 6763800: loss = 5.655055999755859\n",
      "step = 6764000: loss = 4.8815131187438965\n",
      "step = 6764200: loss = 3.6221559047698975\n",
      "step = 6764400: loss = 4.29904842376709\n",
      "step = 6764600: loss = 4.270754814147949\n",
      "step = 6764800: loss = 3.0484514236450195\n",
      "step = 6765000: loss = 3.056931734085083\n",
      "step = 6765000: Average Return = 4.150000095367432\n",
      "step = 6765200: loss = 4.6595683097839355\n",
      "step = 6765400: loss = 5.340841770172119\n",
      "step = 6765600: loss = 6.452574253082275\n",
      "step = 6765800: loss = 4.339117527008057\n",
      "step = 6766000: loss = 5.4086174964904785\n",
      "step = 6766200: loss = 3.739943504333496\n",
      "step = 6766400: loss = 4.661166667938232\n",
      "step = 6766600: loss = 4.190858840942383\n",
      "step = 6766800: loss = 4.837089538574219\n",
      "step = 6767000: loss = 5.558608055114746\n",
      "step = 6767200: loss = 4.108226299285889\n",
      "step = 6767400: loss = 3.9632177352905273\n",
      "step = 6767600: loss = 3.7430968284606934\n",
      "step = 6767800: loss = 5.108200550079346\n",
      "step = 6768000: loss = 5.212421894073486\n",
      "step = 6768200: loss = 5.34765625\n",
      "step = 6768400: loss = 4.302722930908203\n",
      "step = 6768600: loss = 4.455447196960449\n",
      "step = 6768800: loss = 3.1544909477233887\n",
      "step = 6769000: loss = 4.420197486877441\n",
      "step = 6769200: loss = 4.093699932098389\n",
      "step = 6769400: loss = 3.2924513816833496\n",
      "step = 6769600: loss = 4.820971965789795\n",
      "step = 6769800: loss = 3.0692057609558105\n",
      "step = 6770000: loss = 4.52646541595459\n",
      "step = 6770000: Average Return = 4.1519999504089355\n",
      "step = 6770200: loss = 3.575378656387329\n",
      "step = 6770400: loss = 3.76218318939209\n",
      "step = 6770600: loss = 5.101191997528076\n",
      "step = 6770800: loss = 4.067940711975098\n",
      "step = 6771000: loss = 3.6728835105895996\n",
      "step = 6771200: loss = 2.829458475112915\n",
      "step = 6771400: loss = 5.271922588348389\n",
      "step = 6771600: loss = 3.065487861633301\n",
      "step = 6771800: loss = 3.340364933013916\n",
      "step = 6772000: loss = 2.937924385070801\n",
      "step = 6772200: loss = 3.884636163711548\n",
      "step = 6772400: loss = 4.788272380828857\n",
      "step = 6772600: loss = 3.396472454071045\n",
      "step = 6772800: loss = 3.5254485607147217\n",
      "step = 6773000: loss = 4.406926155090332\n",
      "step = 6773200: loss = 3.944831371307373\n",
      "step = 6773400: loss = 2.951678991317749\n",
      "step = 6773600: loss = 4.300638675689697\n",
      "step = 6773800: loss = 3.1201653480529785\n",
      "step = 6774000: loss = 4.463220119476318\n",
      "step = 6774200: loss = 4.1151580810546875\n",
      "step = 6774400: loss = 4.168826103210449\n",
      "step = 6774600: loss = 2.8597841262817383\n",
      "step = 6774800: loss = 4.5948710441589355\n",
      "step = 6775000: loss = 3.9093663692474365\n",
      "step = 6775000: Average Return = 3.9860000610351562\n",
      "step = 6775200: loss = 4.494742393493652\n",
      "step = 6775400: loss = 4.031163215637207\n",
      "step = 6775600: loss = 3.4996135234832764\n",
      "step = 6775800: loss = 4.549038887023926\n",
      "step = 6776000: loss = 4.188903331756592\n",
      "step = 6776200: loss = 4.489163875579834\n",
      "step = 6776400: loss = 4.926403045654297\n",
      "step = 6776600: loss = 3.4364213943481445\n",
      "step = 6776800: loss = 2.699876070022583\n",
      "step = 6777000: loss = 4.286361217498779\n",
      "step = 6777200: loss = 3.1086480617523193\n",
      "step = 6777400: loss = 5.253360271453857\n",
      "step = 6777600: loss = 4.88077449798584\n",
      "step = 6777800: loss = 3.676111936569214\n",
      "step = 6778000: loss = 4.876255035400391\n",
      "step = 6778200: loss = 4.561604022979736\n",
      "step = 6778400: loss = 3.322859525680542\n",
      "step = 6778600: loss = 5.584803104400635\n",
      "step = 6778800: loss = 3.8377363681793213\n",
      "step = 6779000: loss = 2.8197314739227295\n",
      "step = 6779200: loss = 4.812475204467773\n",
      "step = 6779400: loss = 2.6197192668914795\n",
      "step = 6779600: loss = 3.1521427631378174\n",
      "step = 6779800: loss = 4.343142032623291\n",
      "step = 6780000: loss = 3.8826611042022705\n",
      "step = 6780000: Average Return = 3.252000093460083\n",
      "step = 6780200: loss = 4.56277322769165\n",
      "step = 6780400: loss = 5.148693561553955\n",
      "step = 6780600: loss = 4.705807209014893\n",
      "step = 6780800: loss = 5.054262161254883\n",
      "step = 6781000: loss = 3.541832208633423\n",
      "step = 6781200: loss = 3.0501515865325928\n",
      "step = 6781400: loss = 4.216201305389404\n",
      "step = 6781600: loss = 4.0000505447387695\n",
      "step = 6781800: loss = 3.859729290008545\n",
      "step = 6782000: loss = 5.017688274383545\n",
      "step = 6782200: loss = 5.680851936340332\n",
      "step = 6782400: loss = 4.78537654876709\n",
      "step = 6782600: loss = 3.431852340698242\n",
      "step = 6782800: loss = 3.1444332599639893\n",
      "step = 6783000: loss = 4.552761077880859\n",
      "step = 6783200: loss = 4.6914567947387695\n",
      "step = 6783400: loss = 3.6599233150482178\n",
      "step = 6783600: loss = 3.888010263442993\n",
      "step = 6783800: loss = 5.1387739181518555\n",
      "step = 6784000: loss = 5.625119686126709\n",
      "step = 6784200: loss = 4.136965274810791\n",
      "step = 6784400: loss = 4.8203558921813965\n",
      "step = 6784600: loss = 4.304743766784668\n",
      "step = 6784800: loss = 4.335356712341309\n",
      "step = 6785000: loss = 4.649661540985107\n",
      "step = 6785000: Average Return = 3.4079999923706055\n",
      "step = 6785200: loss = 3.7308263778686523\n",
      "step = 6785400: loss = 4.542900562286377\n",
      "step = 6785600: loss = 4.376772403717041\n",
      "step = 6785800: loss = 4.217054843902588\n",
      "step = 6786000: loss = 5.12637186050415\n",
      "step = 6786200: loss = 4.918496131896973\n",
      "step = 6786400: loss = 5.706923484802246\n",
      "step = 6786600: loss = 3.76241135597229\n",
      "step = 6786800: loss = 3.1120729446411133\n",
      "step = 6787000: loss = 5.4061455726623535\n",
      "step = 6787200: loss = 5.812914848327637\n",
      "step = 6787400: loss = 3.6897597312927246\n",
      "step = 6787600: loss = 3.274275541305542\n",
      "step = 6787800: loss = 4.52610969543457\n",
      "step = 6788000: loss = 3.4882397651672363\n",
      "step = 6788200: loss = 3.6088061332702637\n",
      "step = 6788400: loss = 3.480234384536743\n",
      "step = 6788600: loss = 3.949963331222534\n",
      "step = 6788800: loss = 4.795122146606445\n",
      "step = 6789000: loss = 4.180164813995361\n",
      "step = 6789200: loss = 3.02130126953125\n",
      "step = 6789400: loss = 4.388522148132324\n",
      "step = 6789600: loss = 3.3634114265441895\n",
      "step = 6789800: loss = 4.383183479309082\n",
      "step = 6790000: loss = 5.06008243560791\n",
      "step = 6790000: Average Return = 3.934000015258789\n",
      "step = 6790200: loss = 4.998621463775635\n",
      "step = 6790400: loss = 4.9138946533203125\n",
      "step = 6790600: loss = 4.775634765625\n",
      "step = 6790800: loss = 3.495894432067871\n",
      "step = 6791000: loss = 4.170426368713379\n",
      "step = 6791200: loss = 4.153999328613281\n",
      "step = 6791400: loss = 4.453832149505615\n",
      "step = 6791600: loss = 5.882935523986816\n",
      "step = 6791800: loss = 3.9915852546691895\n",
      "step = 6792000: loss = 4.204077243804932\n",
      "step = 6792200: loss = 4.7778449058532715\n",
      "step = 6792400: loss = 4.788386344909668\n",
      "step = 6792600: loss = 3.9368247985839844\n",
      "step = 6792800: loss = 3.83270001411438\n",
      "step = 6793000: loss = 3.5605690479278564\n",
      "step = 6793200: loss = 4.004144668579102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6793400: loss = 3.3823726177215576\n",
      "step = 6793600: loss = 3.3385140895843506\n",
      "step = 6793800: loss = 4.428542613983154\n",
      "step = 6794000: loss = 4.246138095855713\n",
      "step = 6794200: loss = 4.64415168762207\n",
      "step = 6794400: loss = 3.0965864658355713\n",
      "step = 6794600: loss = 4.490485668182373\n",
      "step = 6794800: loss = 3.963782548904419\n",
      "step = 6795000: loss = 2.8476223945617676\n",
      "step = 6795000: Average Return = 3.6440000534057617\n",
      "step = 6795200: loss = 4.585873126983643\n",
      "step = 6795400: loss = 4.533801078796387\n",
      "step = 6795600: loss = 3.7386107444763184\n",
      "step = 6795800: loss = 3.237401247024536\n",
      "step = 6796000: loss = 3.5070457458496094\n",
      "step = 6796200: loss = 4.515280246734619\n",
      "step = 6796400: loss = 4.779436111450195\n",
      "step = 6796600: loss = 4.597762107849121\n",
      "step = 6796800: loss = 3.2416484355926514\n",
      "step = 6797000: loss = 3.8679583072662354\n",
      "step = 6797200: loss = 4.621822834014893\n",
      "step = 6797400: loss = 4.490955352783203\n",
      "step = 6797600: loss = 5.411374568939209\n",
      "step = 6797800: loss = 4.794223308563232\n",
      "step = 6798000: loss = 5.359393119812012\n",
      "step = 6798200: loss = 5.550002098083496\n",
      "step = 6798400: loss = 3.5619959831237793\n",
      "step = 6798600: loss = 4.153072357177734\n",
      "step = 6798800: loss = 4.831063270568848\n",
      "step = 6799000: loss = 3.1289868354797363\n",
      "step = 6799200: loss = 3.9794585704803467\n",
      "step = 6799400: loss = 6.087543964385986\n",
      "step = 6799600: loss = 4.887957572937012\n",
      "step = 6799800: loss = 4.029450416564941\n",
      "step = 6800000: loss = 3.8394625186920166\n",
      "step = 6800000: Average Return = 3.880000114440918\n",
      "step = 6800200: loss = 3.7079741954803467\n",
      "step = 6800400: loss = 5.105742931365967\n",
      "step = 6800600: loss = 5.02764892578125\n",
      "step = 6800800: loss = 4.4947052001953125\n",
      "step = 6801000: loss = 3.9903008937835693\n",
      "step = 6801200: loss = 3.800092935562134\n",
      "step = 6801400: loss = 4.34236478805542\n",
      "step = 6801600: loss = 4.5279083251953125\n",
      "step = 6801800: loss = 5.548807621002197\n",
      "step = 6802000: loss = 4.745857238769531\n",
      "step = 6802200: loss = 2.5160157680511475\n",
      "step = 6802400: loss = 5.862906455993652\n",
      "step = 6802600: loss = 4.502069473266602\n",
      "step = 6802800: loss = 4.9949951171875\n",
      "step = 6803000: loss = 5.492120742797852\n",
      "step = 6803200: loss = 4.964695930480957\n",
      "step = 6803400: loss = 3.587172269821167\n",
      "step = 6803600: loss = 3.4071044921875\n",
      "step = 6803800: loss = 4.276986122131348\n",
      "step = 6804000: loss = 2.7434425354003906\n",
      "step = 6804200: loss = 4.892404556274414\n",
      "step = 6804400: loss = 3.3059651851654053\n",
      "step = 6804600: loss = 3.4573237895965576\n",
      "step = 6804800: loss = 3.886505603790283\n",
      "step = 6805000: loss = 4.342257499694824\n",
      "step = 6805000: Average Return = 3.944000005722046\n",
      "step = 6805200: loss = 4.755661964416504\n",
      "step = 6805400: loss = 5.01231575012207\n",
      "step = 6805600: loss = 3.598092555999756\n",
      "step = 6805800: loss = 4.273241996765137\n",
      "step = 6806000: loss = 4.590364456176758\n",
      "step = 6806200: loss = 4.4604315757751465\n",
      "step = 6806400: loss = 3.492762565612793\n",
      "step = 6806600: loss = 4.458079814910889\n",
      "step = 6806800: loss = 3.27055025100708\n",
      "step = 6807000: loss = 5.636219501495361\n",
      "step = 6807200: loss = 4.147449970245361\n",
      "step = 6807400: loss = 3.7564992904663086\n",
      "step = 6807600: loss = 3.117922306060791\n",
      "step = 6807800: loss = 4.23489236831665\n",
      "step = 6808000: loss = 4.532355308532715\n",
      "step = 6808200: loss = 6.020219802856445\n",
      "step = 6808400: loss = 6.072841644287109\n",
      "step = 6808600: loss = 3.369443893432617\n",
      "step = 6808800: loss = 4.205682277679443\n",
      "step = 6809000: loss = 4.067910671234131\n",
      "step = 6809200: loss = 3.984546422958374\n",
      "step = 6809400: loss = 2.8890767097473145\n",
      "step = 6809600: loss = 3.7732837200164795\n",
      "step = 6809800: loss = 5.250885486602783\n",
      "step = 6810000: loss = 3.4981067180633545\n",
      "step = 6810000: Average Return = 3.813999891281128\n",
      "step = 6810200: loss = 4.092806339263916\n",
      "step = 6810400: loss = 4.610583305358887\n",
      "step = 6810600: loss = 3.1901543140411377\n",
      "step = 6810800: loss = 3.656741142272949\n",
      "step = 6811000: loss = 4.157561779022217\n",
      "step = 6811200: loss = 4.322569370269775\n",
      "step = 6811400: loss = 3.912775993347168\n",
      "step = 6811600: loss = 4.201996326446533\n",
      "step = 6811800: loss = 4.6307477951049805\n",
      "step = 6812000: loss = 3.8115837574005127\n",
      "step = 6812200: loss = 3.749326705932617\n",
      "step = 6812400: loss = 3.7905795574188232\n",
      "step = 6812600: loss = 3.800808906555176\n",
      "step = 6812800: loss = 3.7163619995117188\n",
      "step = 6813000: loss = 2.3553080558776855\n",
      "step = 6813200: loss = 2.947333335876465\n",
      "step = 6813400: loss = 4.270394802093506\n",
      "step = 6813600: loss = 4.285219192504883\n",
      "step = 6813800: loss = 5.354889869689941\n",
      "step = 6814000: loss = 3.8620426654815674\n",
      "step = 6814200: loss = 4.1746954917907715\n",
      "step = 6814400: loss = 4.711677551269531\n",
      "step = 6814600: loss = 4.463438987731934\n",
      "step = 6814800: loss = 5.339400291442871\n",
      "step = 6815000: loss = 4.143985271453857\n",
      "step = 6815000: Average Return = 3.375999927520752\n",
      "step = 6815200: loss = 4.4737772941589355\n",
      "step = 6815400: loss = 3.551693916320801\n",
      "step = 6815600: loss = 3.6921467781066895\n",
      "step = 6815800: loss = 5.023091793060303\n",
      "step = 6816000: loss = 3.5791008472442627\n",
      "step = 6816200: loss = 4.6255598068237305\n",
      "step = 6816400: loss = 2.9230470657348633\n",
      "step = 6816600: loss = 3.8293211460113525\n",
      "step = 6816800: loss = 4.056074142456055\n",
      "step = 6817000: loss = 2.880570650100708\n",
      "step = 6817200: loss = 6.028095722198486\n",
      "step = 6817400: loss = 4.849645137786865\n",
      "step = 6817600: loss = 3.803165912628174\n",
      "step = 6817800: loss = 3.977086067199707\n",
      "step = 6818000: loss = 4.001683235168457\n",
      "step = 6818200: loss = 4.016181945800781\n",
      "step = 6818400: loss = 3.45223069190979\n",
      "step = 6818600: loss = 3.6304681301116943\n",
      "step = 6818800: loss = 4.565878868103027\n",
      "step = 6819000: loss = 3.608264684677124\n",
      "step = 6819200: loss = 3.867896556854248\n",
      "step = 6819400: loss = 4.640190124511719\n",
      "step = 6819600: loss = 3.6604702472686768\n",
      "step = 6819800: loss = 4.496160984039307\n",
      "step = 6820000: loss = 4.495242595672607\n",
      "step = 6820000: Average Return = 3.7360000610351562\n",
      "step = 6820200: loss = 4.368890762329102\n",
      "step = 6820400: loss = 4.689515113830566\n",
      "step = 6820600: loss = 3.1186511516571045\n",
      "step = 6820800: loss = 3.6760435104370117\n",
      "step = 6821000: loss = 4.1044921875\n",
      "step = 6821200: loss = 4.038080215454102\n",
      "step = 6821400: loss = 5.249569416046143\n",
      "step = 6821600: loss = 4.32260799407959\n",
      "step = 6821800: loss = 4.37530517578125\n",
      "step = 6822000: loss = 4.660061359405518\n",
      "step = 6822200: loss = 5.44536018371582\n",
      "step = 6822400: loss = 3.429879665374756\n",
      "step = 6822600: loss = 3.6645398139953613\n",
      "step = 6822800: loss = 4.497255325317383\n",
      "step = 6823000: loss = 4.315237522125244\n",
      "step = 6823200: loss = 5.048701286315918\n",
      "step = 6823400: loss = 3.8834240436553955\n",
      "step = 6823600: loss = 6.513742446899414\n",
      "step = 6823800: loss = 3.8835344314575195\n",
      "step = 6824000: loss = 4.774087429046631\n",
      "step = 6824200: loss = 2.717808723449707\n",
      "step = 6824400: loss = 5.240004062652588\n",
      "step = 6824600: loss = 3.069336414337158\n",
      "step = 6824800: loss = 5.016382694244385\n",
      "step = 6825000: loss = 3.3889403343200684\n",
      "step = 6825000: Average Return = 3.6700000762939453\n",
      "step = 6825200: loss = 4.632852554321289\n",
      "step = 6825400: loss = 2.172807455062866\n",
      "step = 6825600: loss = 3.5251476764678955\n",
      "step = 6825800: loss = 4.2586894035339355\n",
      "step = 6826000: loss = 5.22349214553833\n",
      "step = 6826200: loss = 3.780717372894287\n",
      "step = 6826400: loss = 4.2410197257995605\n",
      "step = 6826600: loss = 4.648983955383301\n",
      "step = 6826800: loss = 3.633277177810669\n",
      "step = 6827000: loss = 3.6875550746917725\n",
      "step = 6827200: loss = 4.589776992797852\n",
      "step = 6827400: loss = 4.651174545288086\n",
      "step = 6827600: loss = 4.618048667907715\n",
      "step = 6827800: loss = 5.302880764007568\n",
      "step = 6828000: loss = 3.6464152336120605\n",
      "step = 6828200: loss = 4.7478437423706055\n",
      "step = 6828400: loss = 3.170902729034424\n",
      "step = 6828600: loss = 5.068857192993164\n",
      "step = 6828800: loss = 4.288506507873535\n",
      "step = 6829000: loss = 3.7701826095581055\n",
      "step = 6829200: loss = 3.6457536220550537\n",
      "step = 6829400: loss = 4.782087326049805\n",
      "step = 6829600: loss = 4.60984468460083\n",
      "step = 6829800: loss = 4.327476978302002\n",
      "step = 6830000: loss = 3.4157285690307617\n",
      "step = 6830000: Average Return = 3.9079999923706055\n",
      "step = 6830200: loss = 4.691544532775879\n",
      "step = 6830400: loss = 3.7633538246154785\n",
      "step = 6830600: loss = 2.9120097160339355\n",
      "step = 6830800: loss = 5.834808826446533\n",
      "step = 6831000: loss = 3.986363410949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6831200: loss = 5.000812530517578\n",
      "step = 6831400: loss = 4.844035625457764\n",
      "step = 6831600: loss = 3.9187614917755127\n",
      "step = 6831800: loss = 5.5015153884887695\n",
      "step = 6832000: loss = 4.1682610511779785\n",
      "step = 6832200: loss = 3.0862996578216553\n",
      "step = 6832400: loss = 4.355040550231934\n",
      "step = 6832600: loss = 3.697665214538574\n",
      "step = 6832800: loss = 4.287742614746094\n",
      "step = 6833000: loss = 5.041090488433838\n",
      "step = 6833200: loss = 4.151019096374512\n",
      "step = 6833400: loss = 5.2527618408203125\n",
      "step = 6833600: loss = 2.857957601547241\n",
      "step = 6833800: loss = 3.934495210647583\n",
      "step = 6834000: loss = 4.401918411254883\n",
      "step = 6834200: loss = 3.417694568634033\n",
      "step = 6834400: loss = 5.576544761657715\n",
      "step = 6834600: loss = 4.788842678070068\n",
      "step = 6834800: loss = 4.334284782409668\n",
      "step = 6835000: loss = 5.561241149902344\n",
      "step = 6835000: Average Return = 3.5460000038146973\n",
      "step = 6835200: loss = 5.034045696258545\n",
      "step = 6835400: loss = 5.705831050872803\n",
      "step = 6835600: loss = 4.246219158172607\n",
      "step = 6835800: loss = 5.252003192901611\n",
      "step = 6836000: loss = 4.570138931274414\n",
      "step = 6836200: loss = 5.264476299285889\n",
      "step = 6836400: loss = 5.241914749145508\n",
      "step = 6836600: loss = 5.136019706726074\n",
      "step = 6836800: loss = 3.6870787143707275\n",
      "step = 6837000: loss = 5.86760139465332\n",
      "step = 6837200: loss = 3.393094062805176\n",
      "step = 6837400: loss = 4.893904685974121\n",
      "step = 6837600: loss = 5.383820533752441\n",
      "step = 6837800: loss = 3.7643072605133057\n",
      "step = 6838000: loss = 3.5300822257995605\n",
      "step = 6838200: loss = 2.911165475845337\n",
      "step = 6838400: loss = 4.675209045410156\n",
      "step = 6838600: loss = 4.829687595367432\n",
      "step = 6838800: loss = 5.179646015167236\n",
      "step = 6839000: loss = 5.690727233886719\n",
      "step = 6839200: loss = 3.63451886177063\n",
      "step = 6839400: loss = 4.972385883331299\n",
      "step = 6839600: loss = 4.745537757873535\n",
      "step = 6839800: loss = 3.790419340133667\n",
      "step = 6840000: loss = 5.980727195739746\n",
      "step = 6840000: Average Return = 3.6760001182556152\n",
      "step = 6840200: loss = 2.757136106491089\n",
      "step = 6840400: loss = 2.8836584091186523\n",
      "step = 6840600: loss = 5.356802463531494\n",
      "step = 6840800: loss = 4.171526908874512\n",
      "step = 6841000: loss = 4.486434459686279\n",
      "step = 6841200: loss = 5.488101482391357\n",
      "step = 6841400: loss = 2.8683691024780273\n",
      "step = 6841600: loss = 5.064546585083008\n",
      "step = 6841800: loss = 3.700761556625366\n",
      "step = 6842000: loss = 4.3107123374938965\n",
      "step = 6842200: loss = 3.7091972827911377\n",
      "step = 6842400: loss = 4.391731262207031\n",
      "step = 6842600: loss = 5.29802131652832\n",
      "step = 6842800: loss = 4.260541915893555\n",
      "step = 6843000: loss = 3.343960762023926\n",
      "step = 6843200: loss = 5.523385047912598\n",
      "step = 6843400: loss = 4.136516094207764\n",
      "step = 6843600: loss = 3.2484700679779053\n",
      "step = 6843800: loss = 5.410843372344971\n",
      "step = 6844000: loss = 4.31044340133667\n",
      "step = 6844200: loss = 2.6847100257873535\n",
      "step = 6844400: loss = 3.0742900371551514\n",
      "step = 6844600: loss = 5.041235446929932\n",
      "step = 6844800: loss = 2.8689661026000977\n",
      "step = 6845000: loss = 3.1528918743133545\n",
      "step = 6845000: Average Return = 3.946000099182129\n",
      "step = 6845200: loss = 3.778276205062866\n",
      "step = 6845400: loss = 7.205516338348389\n",
      "step = 6845600: loss = 3.045424699783325\n",
      "step = 6845800: loss = 4.058764457702637\n",
      "step = 6846000: loss = 3.4408133029937744\n",
      "step = 6846200: loss = 4.33266544342041\n",
      "step = 6846400: loss = 3.6200201511383057\n",
      "step = 6846600: loss = 5.148262977600098\n",
      "step = 6846800: loss = 5.324532508850098\n",
      "step = 6847000: loss = 5.825118064880371\n",
      "step = 6847200: loss = 4.231794834136963\n",
      "step = 6847400: loss = 4.595544338226318\n",
      "step = 6847600: loss = 2.9612951278686523\n",
      "step = 6847800: loss = 4.12888765335083\n",
      "step = 6848000: loss = 6.012423992156982\n",
      "step = 6848200: loss = 3.794062376022339\n",
      "step = 6848400: loss = 3.6832332611083984\n",
      "step = 6848600: loss = 2.9481310844421387\n",
      "step = 6848800: loss = 4.232206344604492\n",
      "step = 6849000: loss = 4.76457405090332\n",
      "step = 6849200: loss = 4.302051544189453\n",
      "step = 6849400: loss = 5.246083736419678\n",
      "step = 6849600: loss = 4.1601104736328125\n",
      "step = 6849800: loss = 4.221145153045654\n",
      "step = 6850000: loss = 3.5116536617279053\n",
      "step = 6850000: Average Return = 3.4600000381469727\n",
      "step = 6850200: loss = 3.6179563999176025\n",
      "step = 6850400: loss = 4.262044429779053\n",
      "step = 6850600: loss = 5.441537857055664\n",
      "step = 6850800: loss = 3.582005739212036\n",
      "step = 6851000: loss = 5.103045463562012\n",
      "step = 6851200: loss = 3.949638843536377\n",
      "step = 6851400: loss = 3.8845956325531006\n",
      "step = 6851600: loss = 3.488279342651367\n",
      "step = 6851800: loss = 4.198152542114258\n",
      "step = 6852000: loss = 3.4306390285491943\n",
      "step = 6852200: loss = 4.360093116760254\n",
      "step = 6852400: loss = 5.402883052825928\n",
      "step = 6852600: loss = 5.03499698638916\n",
      "step = 6852800: loss = 4.062873840332031\n",
      "step = 6853000: loss = 4.1423020362854\n",
      "step = 6853200: loss = 5.520388603210449\n",
      "step = 6853400: loss = 5.019630432128906\n",
      "step = 6853600: loss = 4.960747241973877\n",
      "step = 6853800: loss = 4.254846096038818\n",
      "step = 6854000: loss = 3.5572729110717773\n",
      "step = 6854200: loss = 4.239086627960205\n",
      "step = 6854400: loss = 4.10620641708374\n",
      "step = 6854600: loss = 3.6065709590911865\n",
      "step = 6854800: loss = 4.595970153808594\n",
      "step = 6855000: loss = 5.81992769241333\n",
      "step = 6855000: Average Return = 3.687999963760376\n",
      "step = 6855200: loss = 4.28845739364624\n",
      "step = 6855400: loss = 4.661829471588135\n",
      "step = 6855600: loss = 3.8235363960266113\n",
      "step = 6855800: loss = 4.364319324493408\n",
      "step = 6856000: loss = 4.197230339050293\n",
      "step = 6856200: loss = 5.715685844421387\n",
      "step = 6856400: loss = 4.946386814117432\n",
      "step = 6856600: loss = 3.7676239013671875\n",
      "step = 6856800: loss = 5.035355567932129\n",
      "step = 6857000: loss = 4.3910017013549805\n",
      "step = 6857200: loss = 4.65027379989624\n",
      "step = 6857400: loss = 3.9601733684539795\n",
      "step = 6857600: loss = 4.388751029968262\n",
      "step = 6857800: loss = 5.201867580413818\n",
      "step = 6858000: loss = 3.0191776752471924\n",
      "step = 6858200: loss = 4.946910858154297\n",
      "step = 6858400: loss = 4.440074920654297\n",
      "step = 6858600: loss = 4.0555219650268555\n",
      "step = 6858800: loss = 3.453364849090576\n",
      "step = 6859000: loss = 5.77713680267334\n",
      "step = 6859200: loss = 4.0637078285217285\n",
      "step = 6859400: loss = 3.3940188884735107\n",
      "step = 6859600: loss = 3.2617290019989014\n",
      "step = 6859800: loss = 4.771500587463379\n",
      "step = 6860000: loss = 4.821003437042236\n",
      "step = 6860000: Average Return = 3.7660000324249268\n",
      "step = 6860200: loss = 4.102779865264893\n",
      "step = 6860400: loss = 5.294854640960693\n",
      "step = 6860600: loss = 3.913769245147705\n",
      "step = 6860800: loss = 5.433921813964844\n",
      "step = 6861000: loss = 4.203494548797607\n",
      "step = 6861200: loss = 3.5741114616394043\n",
      "step = 6861400: loss = 5.106439590454102\n",
      "step = 6861600: loss = 3.7066802978515625\n",
      "step = 6861800: loss = 3.2694342136383057\n",
      "step = 6862000: loss = 4.692216873168945\n",
      "step = 6862200: loss = 4.944310188293457\n",
      "step = 6862400: loss = 4.4417405128479\n",
      "step = 6862600: loss = 4.925373554229736\n",
      "step = 6862800: loss = 5.521651268005371\n",
      "step = 6863000: loss = 4.5713019371032715\n",
      "step = 6863200: loss = 3.9771440029144287\n",
      "step = 6863400: loss = 4.8235015869140625\n",
      "step = 6863600: loss = 4.356015682220459\n",
      "step = 6863800: loss = 4.809986591339111\n",
      "step = 6864000: loss = 4.301455020904541\n",
      "step = 6864200: loss = 4.511958599090576\n",
      "step = 6864400: loss = 3.8218607902526855\n",
      "step = 6864600: loss = 3.631592035293579\n",
      "step = 6864800: loss = 4.979551315307617\n",
      "step = 6865000: loss = 4.284171104431152\n",
      "step = 6865000: Average Return = 3.865999937057495\n",
      "step = 6865200: loss = 4.689378261566162\n",
      "step = 6865400: loss = 4.281314849853516\n",
      "step = 6865600: loss = 4.677462577819824\n",
      "step = 6865800: loss = 4.746764183044434\n",
      "step = 6866000: loss = 5.282723903656006\n",
      "step = 6866200: loss = 4.180384635925293\n",
      "step = 6866400: loss = 4.415984630584717\n",
      "step = 6866600: loss = 4.137668609619141\n",
      "step = 6866800: loss = 3.4303743839263916\n",
      "step = 6867000: loss = 5.476754188537598\n",
      "step = 6867200: loss = 3.9443187713623047\n",
      "step = 6867400: loss = 4.476762294769287\n",
      "step = 6867600: loss = 3.2106339931488037\n",
      "step = 6867800: loss = 4.046291351318359\n",
      "step = 6868000: loss = 2.54437255859375\n",
      "step = 6868200: loss = 4.838051795959473\n",
      "step = 6868400: loss = 3.3204619884490967\n",
      "step = 6868600: loss = 4.344091892242432\n",
      "step = 6868800: loss = 6.65704345703125\n",
      "step = 6869000: loss = 3.3923535346984863\n",
      "step = 6869200: loss = 4.899019241333008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6869400: loss = 5.268792152404785\n",
      "step = 6869600: loss = 5.368092060089111\n",
      "step = 6869800: loss = 4.389887809753418\n",
      "step = 6870000: loss = 5.395503997802734\n",
      "step = 6870000: Average Return = 3.799999952316284\n",
      "step = 6870200: loss = 3.0682497024536133\n",
      "step = 6870400: loss = 4.901968955993652\n",
      "step = 6870600: loss = 2.6546175479888916\n",
      "step = 6870800: loss = 5.569167137145996\n",
      "step = 6871000: loss = 5.294205188751221\n",
      "step = 6871200: loss = 5.186280250549316\n",
      "step = 6871400: loss = 4.216839790344238\n",
      "step = 6871600: loss = 4.4672770500183105\n",
      "step = 6871800: loss = 5.560649871826172\n",
      "step = 6872000: loss = 4.515106678009033\n",
      "step = 6872200: loss = 4.376273155212402\n",
      "step = 6872400: loss = 3.467550039291382\n",
      "step = 6872600: loss = 4.5829176902771\n",
      "step = 6872800: loss = 2.836230516433716\n",
      "step = 6873000: loss = 4.302285194396973\n",
      "step = 6873200: loss = 3.6106603145599365\n",
      "step = 6873400: loss = 4.275452613830566\n",
      "step = 6873600: loss = 4.007187843322754\n",
      "step = 6873800: loss = 3.396845817565918\n",
      "step = 6874000: loss = 3.509336471557617\n",
      "step = 6874200: loss = 4.959441661834717\n",
      "step = 6874400: loss = 3.66457200050354\n",
      "step = 6874600: loss = 5.075753211975098\n",
      "step = 6874800: loss = 5.260297775268555\n",
      "step = 6875000: loss = 4.750197410583496\n",
      "step = 6875000: Average Return = 3.805999994277954\n",
      "step = 6875200: loss = 4.243679523468018\n",
      "step = 6875400: loss = 2.941957712173462\n",
      "step = 6875600: loss = 4.334220886230469\n",
      "step = 6875800: loss = 3.4753499031066895\n",
      "step = 6876000: loss = 4.324663162231445\n",
      "step = 6876200: loss = 3.192539930343628\n",
      "step = 6876400: loss = 4.2939863204956055\n",
      "step = 6876600: loss = 4.051897048950195\n",
      "step = 6876800: loss = 4.9452104568481445\n",
      "step = 6877000: loss = 4.51212215423584\n",
      "step = 6877200: loss = 4.8958001136779785\n",
      "step = 6877400: loss = 4.6700825691223145\n",
      "step = 6877600: loss = 4.585414886474609\n",
      "step = 6877800: loss = 4.169917106628418\n",
      "step = 6878000: loss = 4.893527030944824\n",
      "step = 6878200: loss = 3.5528411865234375\n",
      "step = 6878400: loss = 2.9419970512390137\n",
      "step = 6878600: loss = 4.2287278175354\n",
      "step = 6878800: loss = 3.8404507637023926\n",
      "step = 6879000: loss = 2.6229028701782227\n",
      "step = 6879200: loss = 3.712763547897339\n",
      "step = 6879400: loss = 5.086378574371338\n",
      "step = 6879600: loss = 4.64640474319458\n",
      "step = 6879800: loss = 4.363390922546387\n",
      "step = 6880000: loss = 2.9455113410949707\n",
      "step = 6880000: Average Return = 3.8540000915527344\n",
      "step = 6880200: loss = 5.21705961227417\n",
      "step = 6880400: loss = 4.597026348114014\n",
      "step = 6880600: loss = 2.9672374725341797\n",
      "step = 6880800: loss = 4.831211090087891\n",
      "step = 6881000: loss = 6.427258014678955\n",
      "step = 6881200: loss = 3.3854787349700928\n",
      "step = 6881400: loss = 3.712592601776123\n",
      "step = 6881600: loss = 3.4280574321746826\n",
      "step = 6881800: loss = 3.29518723487854\n",
      "step = 6882000: loss = 3.671095609664917\n",
      "step = 6882200: loss = 4.517262935638428\n",
      "step = 6882400: loss = 2.4595842361450195\n",
      "step = 6882600: loss = 3.48125958442688\n",
      "step = 6882800: loss = 4.036181449890137\n",
      "step = 6883000: loss = 4.426201820373535\n",
      "step = 6883200: loss = 3.671895742416382\n",
      "step = 6883400: loss = 2.9288203716278076\n",
      "step = 6883600: loss = 4.755768775939941\n",
      "step = 6883800: loss = 2.9166057109832764\n",
      "step = 6884000: loss = 4.036867141723633\n",
      "step = 6884200: loss = 3.963881254196167\n",
      "step = 6884400: loss = 3.940098285675049\n",
      "step = 6884600: loss = 2.5733184814453125\n",
      "step = 6884800: loss = 4.1365485191345215\n",
      "step = 6885000: loss = 3.713411808013916\n",
      "step = 6885000: Average Return = 3.940000057220459\n",
      "step = 6885200: loss = 5.295018672943115\n",
      "step = 6885400: loss = 4.083821773529053\n",
      "step = 6885600: loss = 4.207208633422852\n",
      "step = 6885800: loss = 6.179506778717041\n",
      "step = 6886000: loss = 4.334469795227051\n",
      "step = 6886200: loss = 3.810732841491699\n",
      "step = 6886400: loss = 3.1329638957977295\n",
      "step = 6886600: loss = 3.7329940795898438\n",
      "step = 6886800: loss = 5.056972980499268\n",
      "step = 6887000: loss = 5.084097385406494\n",
      "step = 6887200: loss = 3.456496477127075\n",
      "step = 6887400: loss = 5.0580339431762695\n",
      "step = 6887600: loss = 5.805794715881348\n",
      "step = 6887800: loss = 4.601176738739014\n",
      "step = 6888000: loss = 3.229727029800415\n",
      "step = 6888200: loss = 4.427459239959717\n",
      "step = 6888400: loss = 4.364406108856201\n",
      "step = 6888600: loss = 4.7423810958862305\n",
      "step = 6888800: loss = 3.8231847286224365\n",
      "step = 6889000: loss = 2.4437317848205566\n",
      "step = 6889200: loss = 4.754124164581299\n",
      "step = 6889400: loss = 6.732565879821777\n",
      "step = 6889600: loss = 5.175463676452637\n",
      "step = 6889800: loss = 4.443592548370361\n",
      "step = 6890000: loss = 3.768183708190918\n",
      "step = 6890000: Average Return = 3.6440000534057617\n",
      "step = 6890200: loss = 4.305102348327637\n",
      "step = 6890400: loss = 3.7776033878326416\n",
      "step = 6890600: loss = 3.2873048782348633\n",
      "step = 6890800: loss = 3.7857627868652344\n",
      "step = 6891000: loss = 3.6512391567230225\n",
      "step = 6891200: loss = 3.6271440982818604\n",
      "step = 6891400: loss = 5.085498332977295\n",
      "step = 6891600: loss = 2.9301440715789795\n",
      "step = 6891800: loss = 3.6815929412841797\n",
      "step = 6892000: loss = 2.7210052013397217\n",
      "step = 6892200: loss = 4.734410285949707\n",
      "step = 6892400: loss = 3.166468858718872\n",
      "step = 6892600: loss = 6.088343143463135\n",
      "step = 6892800: loss = 3.595255136489868\n",
      "step = 6893000: loss = 5.30946159362793\n",
      "step = 6893200: loss = 3.562352180480957\n",
      "step = 6893400: loss = 4.707833290100098\n",
      "step = 6893600: loss = 2.846519708633423\n",
      "step = 6893800: loss = 5.843828201293945\n",
      "step = 6894000: loss = 3.9795939922332764\n",
      "step = 6894200: loss = 3.315321683883667\n",
      "step = 6894400: loss = 5.986288547515869\n",
      "step = 6894600: loss = 4.261347770690918\n",
      "step = 6894800: loss = 4.0441694259643555\n",
      "step = 6895000: loss = 4.52541446685791\n",
      "step = 6895000: Average Return = 4.065999984741211\n",
      "step = 6895200: loss = 4.7674126625061035\n",
      "step = 6895400: loss = 2.6733574867248535\n",
      "step = 6895600: loss = 5.14088249206543\n",
      "step = 6895800: loss = 4.044776439666748\n",
      "step = 6896000: loss = 5.050079345703125\n",
      "step = 6896200: loss = 3.1656336784362793\n",
      "step = 6896400: loss = 4.595895767211914\n",
      "step = 6896600: loss = 4.900007724761963\n",
      "step = 6896800: loss = 4.235921382904053\n",
      "step = 6897000: loss = 3.965640068054199\n",
      "step = 6897200: loss = 4.64875602722168\n",
      "step = 6897400: loss = 3.446608304977417\n",
      "step = 6897600: loss = 5.608981609344482\n",
      "step = 6897800: loss = 3.780803680419922\n",
      "step = 6898000: loss = 6.558350563049316\n",
      "step = 6898200: loss = 3.5286266803741455\n",
      "step = 6898400: loss = 3.3508360385894775\n",
      "step = 6898600: loss = 4.5037736892700195\n",
      "step = 6898800: loss = 3.9895646572113037\n",
      "step = 6899000: loss = 2.1450388431549072\n",
      "step = 6899200: loss = 3.945467948913574\n",
      "step = 6899400: loss = 4.241739273071289\n",
      "step = 6899600: loss = 2.512756109237671\n",
      "step = 6899800: loss = 5.299859523773193\n",
      "step = 6900000: loss = 4.3820109367370605\n",
      "step = 6900000: Average Return = 3.492000102996826\n",
      "step = 6900200: loss = 3.781935930252075\n",
      "step = 6900400: loss = 4.636139392852783\n",
      "step = 6900600: loss = 4.554146766662598\n",
      "step = 6900800: loss = 5.0076398849487305\n",
      "step = 6901000: loss = 4.8560099601745605\n",
      "step = 6901200: loss = 3.488554000854492\n",
      "step = 6901400: loss = 4.946868419647217\n",
      "step = 6901600: loss = 3.7118966579437256\n",
      "step = 6901800: loss = 4.168395519256592\n",
      "step = 6902000: loss = 3.795687675476074\n",
      "step = 6902200: loss = 3.7904393672943115\n",
      "step = 6902400: loss = 4.105759143829346\n",
      "step = 6902600: loss = 4.467741012573242\n",
      "step = 6902800: loss = 3.900283098220825\n",
      "step = 6903000: loss = 4.417900085449219\n",
      "step = 6903200: loss = 3.722182512283325\n",
      "step = 6903400: loss = 4.287605285644531\n",
      "step = 6903600: loss = 4.762231349945068\n",
      "step = 6903800: loss = 4.373490333557129\n",
      "step = 6904000: loss = 4.481837749481201\n",
      "step = 6904200: loss = 2.6814889907836914\n",
      "step = 6904400: loss = 4.423460483551025\n",
      "step = 6904600: loss = 6.132026672363281\n",
      "step = 6904800: loss = 5.345864772796631\n",
      "step = 6905000: loss = 4.201748847961426\n",
      "step = 6905000: Average Return = 3.950000047683716\n",
      "step = 6905200: loss = 5.0303544998168945\n",
      "step = 6905400: loss = 3.6326894760131836\n",
      "step = 6905600: loss = 4.628473281860352\n",
      "step = 6905800: loss = 5.11527681350708\n",
      "step = 6906000: loss = 3.684854030609131\n",
      "step = 6906200: loss = 3.8325743675231934\n",
      "step = 6906400: loss = 4.055388927459717\n",
      "step = 6906600: loss = 4.20468807220459\n",
      "step = 6906800: loss = 3.962634563446045\n",
      "step = 6907000: loss = 4.249292373657227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6907200: loss = 5.008524417877197\n",
      "step = 6907400: loss = 4.471097469329834\n",
      "step = 6907600: loss = 5.599893569946289\n",
      "step = 6907800: loss = 5.082119464874268\n",
      "step = 6908000: loss = 4.500255107879639\n",
      "step = 6908200: loss = 3.542991876602173\n",
      "step = 6908400: loss = 4.46883487701416\n",
      "step = 6908600: loss = 4.021073341369629\n",
      "step = 6908800: loss = 3.6033031940460205\n",
      "step = 6909000: loss = 3.981748580932617\n",
      "step = 6909200: loss = 2.834782600402832\n",
      "step = 6909400: loss = 4.7064690589904785\n",
      "step = 6909600: loss = 3.56791615486145\n",
      "step = 6909800: loss = 4.228169918060303\n",
      "step = 6910000: loss = 4.311771869659424\n",
      "step = 6910000: Average Return = 3.569999933242798\n",
      "step = 6910200: loss = 2.7220587730407715\n",
      "step = 6910400: loss = 4.086409091949463\n",
      "step = 6910600: loss = 4.074497699737549\n",
      "step = 6910800: loss = 4.770052433013916\n",
      "step = 6911000: loss = 4.134711742401123\n",
      "step = 6911200: loss = 4.574396133422852\n",
      "step = 6911400: loss = 4.513888835906982\n",
      "step = 6911600: loss = 3.8353826999664307\n",
      "step = 6911800: loss = 4.644445896148682\n",
      "step = 6912000: loss = 4.024848937988281\n",
      "step = 6912200: loss = 3.825366735458374\n",
      "step = 6912400: loss = 4.6221184730529785\n",
      "step = 6912600: loss = 3.8964555263519287\n",
      "step = 6912800: loss = 5.527068138122559\n",
      "step = 6913000: loss = 5.640665531158447\n",
      "step = 6913200: loss = 3.8438618183135986\n",
      "step = 6913400: loss = 4.358320236206055\n",
      "step = 6913600: loss = 3.1291890144348145\n",
      "step = 6913800: loss = 4.076610088348389\n",
      "step = 6914000: loss = 3.828713893890381\n",
      "step = 6914200: loss = 3.501420259475708\n",
      "step = 6914400: loss = 4.322789669036865\n",
      "step = 6914600: loss = 5.246755123138428\n",
      "step = 6914800: loss = 4.331645488739014\n",
      "step = 6915000: loss = 5.146599769592285\n",
      "step = 6915000: Average Return = 3.7160000801086426\n",
      "step = 6915200: loss = 5.549441814422607\n",
      "step = 6915400: loss = 4.554996013641357\n",
      "step = 6915600: loss = 5.080590724945068\n",
      "step = 6915800: loss = 3.537452220916748\n",
      "step = 6916000: loss = 5.268943786621094\n",
      "step = 6916200: loss = 3.753554105758667\n",
      "step = 6916400: loss = 3.2154994010925293\n",
      "step = 6916600: loss = 2.666501522064209\n",
      "step = 6916800: loss = 4.613533973693848\n",
      "step = 6917000: loss = 3.9816291332244873\n",
      "step = 6917200: loss = 3.3651466369628906\n",
      "step = 6917400: loss = 3.5850634574890137\n",
      "step = 6917600: loss = 4.740842342376709\n",
      "step = 6917800: loss = 4.261250972747803\n",
      "step = 6918000: loss = 4.540960788726807\n",
      "step = 6918200: loss = 4.830045223236084\n",
      "step = 6918400: loss = 4.376201152801514\n",
      "step = 6918600: loss = 2.5180182456970215\n",
      "step = 6918800: loss = 4.649206638336182\n",
      "step = 6919000: loss = 3.951270580291748\n",
      "step = 6919200: loss = 4.454335689544678\n",
      "step = 6919400: loss = 4.458989143371582\n",
      "step = 6919600: loss = 4.302762508392334\n",
      "step = 6919800: loss = 4.9199347496032715\n",
      "step = 6920000: loss = 4.3469157218933105\n",
      "step = 6920000: Average Return = 3.8980000019073486\n",
      "step = 6920200: loss = 4.702643394470215\n",
      "step = 6920400: loss = 4.174131870269775\n",
      "step = 6920600: loss = 3.9232945442199707\n",
      "step = 6920800: loss = 4.283341407775879\n",
      "step = 6921000: loss = 4.3127288818359375\n",
      "step = 6921200: loss = 4.787709712982178\n",
      "step = 6921400: loss = 3.77553391456604\n",
      "step = 6921600: loss = 3.9955716133117676\n",
      "step = 6921800: loss = 6.225885391235352\n",
      "step = 6922000: loss = 3.7168502807617188\n",
      "step = 6922200: loss = 4.707309246063232\n",
      "step = 6922400: loss = 3.5912060737609863\n",
      "step = 6922600: loss = 5.057493686676025\n",
      "step = 6922800: loss = 3.384159564971924\n",
      "step = 6923000: loss = 4.9291181564331055\n",
      "step = 6923200: loss = 5.272347450256348\n",
      "step = 6923400: loss = 4.408918380737305\n",
      "step = 6923600: loss = 4.684814453125\n",
      "step = 6923800: loss = 3.5278587341308594\n",
      "step = 6924000: loss = 5.696253299713135\n",
      "step = 6924200: loss = 4.148357391357422\n",
      "step = 6924400: loss = 4.164663791656494\n",
      "step = 6924600: loss = 3.9980340003967285\n",
      "step = 6924800: loss = 4.647151947021484\n",
      "step = 6925000: loss = 5.070676326751709\n",
      "step = 6925000: Average Return = 3.7799999713897705\n",
      "step = 6925200: loss = 4.1064348220825195\n",
      "step = 6925400: loss = 3.3258166313171387\n",
      "step = 6925600: loss = 4.208799362182617\n",
      "step = 6925800: loss = 3.7667224407196045\n",
      "step = 6926000: loss = 5.096363544464111\n",
      "step = 6926200: loss = 4.840582370758057\n",
      "step = 6926400: loss = 3.1775310039520264\n",
      "step = 6926600: loss = 4.467456817626953\n",
      "step = 6926800: loss = 3.985795736312866\n",
      "step = 6927000: loss = 5.608572959899902\n",
      "step = 6927200: loss = 4.31705904006958\n",
      "step = 6927400: loss = 4.590531349182129\n",
      "step = 6927600: loss = 4.3791117668151855\n",
      "step = 6927800: loss = 4.645993709564209\n",
      "step = 6928000: loss = 3.9712538719177246\n",
      "step = 6928200: loss = 3.2688395977020264\n",
      "step = 6928400: loss = 3.8777098655700684\n",
      "step = 6928600: loss = 4.71998405456543\n",
      "step = 6928800: loss = 3.965604066848755\n",
      "step = 6929000: loss = 4.380311489105225\n",
      "step = 6929200: loss = 3.6953933238983154\n",
      "step = 6929400: loss = 4.500572204589844\n",
      "step = 6929600: loss = 4.858714580535889\n",
      "step = 6929800: loss = 4.123644828796387\n",
      "step = 6930000: loss = 4.55428409576416\n",
      "step = 6930000: Average Return = 3.869999885559082\n",
      "step = 6930200: loss = 2.6132771968841553\n",
      "step = 6930400: loss = 4.531586170196533\n",
      "step = 6930600: loss = 4.708289623260498\n",
      "step = 6930800: loss = 4.582050800323486\n",
      "step = 6931000: loss = 4.327090740203857\n",
      "step = 6931200: loss = 5.135827541351318\n",
      "step = 6931400: loss = 3.683117628097534\n",
      "step = 6931600: loss = 3.509791135787964\n",
      "step = 6931800: loss = 4.673283576965332\n",
      "step = 6932000: loss = 3.745724678039551\n",
      "step = 6932200: loss = 3.8779335021972656\n",
      "step = 6932400: loss = 5.406821250915527\n",
      "step = 6932600: loss = 3.4193859100341797\n",
      "step = 6932800: loss = 3.7650301456451416\n",
      "step = 6933000: loss = 4.424257278442383\n",
      "step = 6933200: loss = 3.32369327545166\n",
      "step = 6933400: loss = 3.631303548812866\n",
      "step = 6933600: loss = 3.9500224590301514\n",
      "step = 6933800: loss = 4.106980323791504\n",
      "step = 6934000: loss = 3.281141996383667\n",
      "step = 6934200: loss = 5.454185485839844\n",
      "step = 6934400: loss = 3.6899044513702393\n",
      "step = 6934600: loss = 4.108452796936035\n",
      "step = 6934800: loss = 3.377425193786621\n",
      "step = 6935000: loss = 4.659101963043213\n",
      "step = 6935000: Average Return = 3.7100000381469727\n",
      "step = 6935200: loss = 4.734274864196777\n",
      "step = 6935400: loss = 2.6116411685943604\n",
      "step = 6935600: loss = 3.5487871170043945\n",
      "step = 6935800: loss = 3.9551286697387695\n",
      "step = 6936000: loss = 3.8172264099121094\n",
      "step = 6936200: loss = 3.246363401412964\n",
      "step = 6936400: loss = 3.797119617462158\n",
      "step = 6936600: loss = 3.487959861755371\n",
      "step = 6936800: loss = 3.0370874404907227\n",
      "step = 6937000: loss = 4.7435736656188965\n",
      "step = 6937200: loss = 3.991786479949951\n",
      "step = 6937400: loss = 4.014591217041016\n",
      "step = 6937600: loss = 5.356022834777832\n",
      "step = 6937800: loss = 4.737876892089844\n",
      "step = 6938000: loss = 4.542405128479004\n",
      "step = 6938200: loss = 4.301602840423584\n",
      "step = 6938400: loss = 6.891233921051025\n",
      "step = 6938600: loss = 3.7092061042785645\n",
      "step = 6938800: loss = 3.419607400894165\n",
      "step = 6939000: loss = 2.8871397972106934\n",
      "step = 6939200: loss = 5.1827592849731445\n",
      "step = 6939400: loss = 5.144001483917236\n",
      "step = 6939600: loss = 4.450437068939209\n",
      "step = 6939800: loss = 2.8133721351623535\n",
      "step = 6940000: loss = 3.4151077270507812\n",
      "step = 6940000: Average Return = 4.007999897003174\n",
      "step = 6940200: loss = 4.11453914642334\n",
      "step = 6940400: loss = 6.892431259155273\n",
      "step = 6940600: loss = 3.553642511367798\n",
      "step = 6940800: loss = 5.14372444152832\n",
      "step = 6941000: loss = 3.9522504806518555\n",
      "step = 6941200: loss = 3.8197410106658936\n",
      "step = 6941400: loss = 4.089162349700928\n",
      "step = 6941600: loss = 3.6298985481262207\n",
      "step = 6941800: loss = 4.383799076080322\n",
      "step = 6942000: loss = 4.908008098602295\n",
      "step = 6942200: loss = 3.2356250286102295\n",
      "step = 6942400: loss = 4.572945594787598\n",
      "step = 6942600: loss = 4.038570880889893\n",
      "step = 6942800: loss = 5.971103191375732\n",
      "step = 6943000: loss = 2.943525552749634\n",
      "step = 6943200: loss = 5.58342170715332\n",
      "step = 6943400: loss = 4.9698567390441895\n",
      "step = 6943600: loss = 3.5686731338500977\n",
      "step = 6943800: loss = 3.1794090270996094\n",
      "step = 6944000: loss = 4.260871410369873\n",
      "step = 6944200: loss = 4.884535789489746\n",
      "step = 6944400: loss = 4.051416397094727\n",
      "step = 6944600: loss = 4.445195198059082\n",
      "step = 6944800: loss = 3.615656614303589\n",
      "step = 6945000: loss = 4.449220657348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6945000: Average Return = 4.050000190734863\n",
      "step = 6945200: loss = 3.7329680919647217\n",
      "step = 6945400: loss = 4.426027297973633\n",
      "step = 6945600: loss = 4.130068302154541\n",
      "step = 6945800: loss = 3.28641414642334\n",
      "step = 6946000: loss = 3.9165499210357666\n",
      "step = 6946200: loss = 3.8909149169921875\n",
      "step = 6946400: loss = 5.52581787109375\n",
      "step = 6946600: loss = 4.109249114990234\n",
      "step = 6946800: loss = 3.2417094707489014\n",
      "step = 6947000: loss = 3.5770390033721924\n",
      "step = 6947200: loss = 4.141136169433594\n",
      "step = 6947400: loss = 3.0162453651428223\n",
      "step = 6947600: loss = 5.05057954788208\n",
      "step = 6947800: loss = 4.128719806671143\n",
      "step = 6948000: loss = 4.848967552185059\n",
      "step = 6948200: loss = 4.734541416168213\n",
      "step = 6948400: loss = 3.5972986221313477\n",
      "step = 6948600: loss = 3.7345149517059326\n",
      "step = 6948800: loss = 4.5240936279296875\n",
      "step = 6949000: loss = 3.596797466278076\n",
      "step = 6949200: loss = 4.244327068328857\n",
      "step = 6949400: loss = 3.1099889278411865\n",
      "step = 6949600: loss = 3.6921050548553467\n",
      "step = 6949800: loss = 3.843923568725586\n",
      "step = 6950000: loss = 3.301021099090576\n",
      "step = 6950000: Average Return = 3.7060000896453857\n",
      "step = 6950200: loss = 3.687734842300415\n",
      "step = 6950400: loss = 4.041475296020508\n",
      "step = 6950600: loss = 3.4988691806793213\n",
      "step = 6950800: loss = 3.8556041717529297\n",
      "step = 6951000: loss = 4.781683921813965\n",
      "step = 6951200: loss = 4.985792636871338\n",
      "step = 6951400: loss = 4.17165470123291\n",
      "step = 6951600: loss = 4.202924728393555\n",
      "step = 6951800: loss = 4.440432548522949\n",
      "step = 6952000: loss = 3.3057353496551514\n",
      "step = 6952200: loss = 5.74844217300415\n",
      "step = 6952400: loss = 4.679468631744385\n",
      "step = 6952600: loss = 4.381082534790039\n",
      "step = 6952800: loss = 2.796914577484131\n",
      "step = 6953000: loss = 3.8542168140411377\n",
      "step = 6953200: loss = 4.508944988250732\n",
      "step = 6953400: loss = 3.5956315994262695\n",
      "step = 6953600: loss = 4.5925421714782715\n",
      "step = 6953800: loss = 3.976200819015503\n",
      "step = 6954000: loss = 4.685420513153076\n",
      "step = 6954200: loss = 4.5431036949157715\n",
      "step = 6954400: loss = 5.409358978271484\n",
      "step = 6954600: loss = 3.4785706996917725\n",
      "step = 6954800: loss = 5.557058811187744\n",
      "step = 6955000: loss = 3.8772072792053223\n",
      "step = 6955000: Average Return = 3.7880001068115234\n",
      "step = 6955200: loss = 3.6499574184417725\n",
      "step = 6955400: loss = 5.206369400024414\n",
      "step = 6955600: loss = 4.052255630493164\n",
      "step = 6955800: loss = 4.170792102813721\n",
      "step = 6956000: loss = 3.18550968170166\n",
      "step = 6956200: loss = 4.4822540283203125\n",
      "step = 6956400: loss = 3.1417155265808105\n",
      "step = 6956600: loss = 4.486280918121338\n",
      "step = 6956800: loss = 3.3006529808044434\n",
      "step = 6957000: loss = 4.979065895080566\n",
      "step = 6957200: loss = 5.984307289123535\n",
      "step = 6957400: loss = 4.445960998535156\n",
      "step = 6957600: loss = 2.353347063064575\n",
      "step = 6957800: loss = 4.589949131011963\n",
      "step = 6958000: loss = 3.832468271255493\n",
      "step = 6958200: loss = 3.497471332550049\n",
      "step = 6958400: loss = 2.7290947437286377\n",
      "step = 6958600: loss = 5.278460502624512\n",
      "step = 6958800: loss = 4.054378986358643\n",
      "step = 6959000: loss = 4.908380508422852\n",
      "step = 6959200: loss = 6.299633502960205\n",
      "step = 6959400: loss = 4.3764495849609375\n",
      "step = 6959600: loss = 2.3678534030914307\n",
      "step = 6959800: loss = 3.6151797771453857\n",
      "step = 6960000: loss = 3.4699175357818604\n",
      "step = 6960000: Average Return = 3.753999948501587\n",
      "step = 6960200: loss = 3.9066948890686035\n",
      "step = 6960400: loss = 3.661600351333618\n",
      "step = 6960600: loss = 3.4488134384155273\n",
      "step = 6960800: loss = 4.301104545593262\n",
      "step = 6961000: loss = 4.306281089782715\n",
      "step = 6961200: loss = 3.589132308959961\n",
      "step = 6961400: loss = 4.575906276702881\n",
      "step = 6961600: loss = 4.324108123779297\n",
      "step = 6961800: loss = 3.9209446907043457\n",
      "step = 6962000: loss = 4.481164455413818\n",
      "step = 6962200: loss = 3.9199981689453125\n",
      "step = 6962400: loss = 5.2881646156311035\n",
      "step = 6962600: loss = 2.8239877223968506\n",
      "step = 6962800: loss = 3.6504085063934326\n",
      "step = 6963000: loss = 3.257874011993408\n",
      "step = 6963200: loss = 5.359242916107178\n",
      "step = 6963400: loss = 3.6510250568389893\n",
      "step = 6963600: loss = 3.5509297847747803\n",
      "step = 6963800: loss = 3.438328742980957\n",
      "step = 6964000: loss = 5.229671001434326\n",
      "step = 6964200: loss = 4.100775241851807\n",
      "step = 6964400: loss = 4.62505578994751\n",
      "step = 6964600: loss = 4.41284704208374\n",
      "step = 6964800: loss = 3.713129758834839\n",
      "step = 6965000: loss = 4.975222587585449\n",
      "step = 6965000: Average Return = 3.7219998836517334\n",
      "step = 6965200: loss = 4.212996006011963\n",
      "step = 6965400: loss = 3.402972936630249\n",
      "step = 6965600: loss = 5.272916316986084\n",
      "step = 6965800: loss = 5.325237274169922\n",
      "step = 6966000: loss = 2.570857524871826\n",
      "step = 6966200: loss = 4.167916774749756\n",
      "step = 6966400: loss = 4.401623249053955\n",
      "step = 6966600: loss = 4.25014066696167\n",
      "step = 6966800: loss = 4.989736080169678\n",
      "step = 6967000: loss = 3.863460063934326\n",
      "step = 6967200: loss = 4.1273298263549805\n",
      "step = 6967400: loss = 3.693488121032715\n",
      "step = 6967600: loss = 5.268866062164307\n",
      "step = 6967800: loss = 3.881591796875\n",
      "step = 6968000: loss = 3.006525754928589\n",
      "step = 6968200: loss = 3.9928362369537354\n",
      "step = 6968400: loss = 3.0273447036743164\n",
      "step = 6968600: loss = 3.9051597118377686\n",
      "step = 6968800: loss = 3.9675238132476807\n",
      "step = 6969000: loss = 4.329319477081299\n",
      "step = 6969200: loss = 4.519380569458008\n",
      "step = 6969400: loss = 5.803783416748047\n",
      "step = 6969600: loss = 4.9334001541137695\n",
      "step = 6969800: loss = 3.8955912590026855\n",
      "step = 6970000: loss = 2.999099016189575\n",
      "step = 6970000: Average Return = 4.211999893188477\n",
      "step = 6970200: loss = 4.049351692199707\n",
      "step = 6970400: loss = 3.739229679107666\n",
      "step = 6970600: loss = 4.247767448425293\n",
      "step = 6970800: loss = 3.229609251022339\n",
      "step = 6971000: loss = 4.414687633514404\n",
      "step = 6971200: loss = 4.240023612976074\n",
      "step = 6971400: loss = 3.9796745777130127\n",
      "step = 6971600: loss = 4.738580226898193\n",
      "step = 6971800: loss = 5.434072017669678\n",
      "step = 6972000: loss = 4.683139324188232\n",
      "step = 6972200: loss = 4.312690734863281\n",
      "step = 6972400: loss = 2.870384454727173\n",
      "step = 6972600: loss = 5.2664713859558105\n",
      "step = 6972800: loss = 3.5483434200286865\n",
      "step = 6973000: loss = 4.68154239654541\n",
      "step = 6973200: loss = 6.08445405960083\n",
      "step = 6973400: loss = 4.014119625091553\n",
      "step = 6973600: loss = 3.3028371334075928\n",
      "step = 6973800: loss = 3.8421947956085205\n",
      "step = 6974000: loss = 2.9687774181365967\n",
      "step = 6974200: loss = 4.335075378417969\n",
      "step = 6974400: loss = 3.593066692352295\n",
      "step = 6974600: loss = 5.37839412689209\n",
      "step = 6974800: loss = 4.947464942932129\n",
      "step = 6975000: loss = 3.0322914123535156\n",
      "step = 6975000: Average Return = 3.8340001106262207\n",
      "step = 6975200: loss = 5.428552150726318\n",
      "step = 6975400: loss = 6.220959186553955\n",
      "step = 6975600: loss = 3.931387186050415\n",
      "step = 6975800: loss = 5.112923622131348\n",
      "step = 6976000: loss = 5.222751140594482\n",
      "step = 6976200: loss = 3.546534538269043\n",
      "step = 6976400: loss = 3.181378126144409\n",
      "step = 6976600: loss = 3.8212482929229736\n",
      "step = 6976800: loss = 4.375540256500244\n",
      "step = 6977000: loss = 3.7035911083221436\n",
      "step = 6977200: loss = 3.970104932785034\n",
      "step = 6977400: loss = 4.198226451873779\n",
      "step = 6977600: loss = 3.6177799701690674\n",
      "step = 6977800: loss = 2.605090379714966\n",
      "step = 6978000: loss = 4.120582103729248\n",
      "step = 6978200: loss = 4.60233211517334\n",
      "step = 6978400: loss = 5.225165843963623\n",
      "step = 6978600: loss = 2.675349473953247\n",
      "step = 6978800: loss = 2.847317695617676\n",
      "step = 6979000: loss = 5.042307376861572\n",
      "step = 6979200: loss = 3.479901075363159\n",
      "step = 6979400: loss = 5.046496391296387\n",
      "step = 6979600: loss = 4.658207893371582\n",
      "step = 6979800: loss = 3.3099663257598877\n",
      "step = 6980000: loss = 3.0772526264190674\n",
      "step = 6980000: Average Return = 3.865999937057495\n",
      "step = 6980200: loss = 3.822532892227173\n",
      "step = 6980400: loss = 4.671792984008789\n",
      "step = 6980600: loss = 2.8998076915740967\n",
      "step = 6980800: loss = 5.950767517089844\n",
      "step = 6981000: loss = 3.854079246520996\n",
      "step = 6981200: loss = 3.943687677383423\n",
      "step = 6981400: loss = 3.8430395126342773\n",
      "step = 6981600: loss = 3.467529296875\n",
      "step = 6981800: loss = 3.557194232940674\n",
      "step = 6982000: loss = 3.7023963928222656\n",
      "step = 6982200: loss = 3.4125914573669434\n",
      "step = 6982400: loss = 4.596525192260742\n",
      "step = 6982600: loss = 4.208871364593506\n",
      "step = 6982800: loss = 3.6462554931640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 6983000: loss = 3.358060121536255\n",
      "step = 6983200: loss = 4.478181838989258\n",
      "step = 6983400: loss = 3.891979932785034\n",
      "step = 6983600: loss = 3.522662878036499\n",
      "step = 6983800: loss = 4.368640422821045\n",
      "step = 6984000: loss = 3.9789793491363525\n",
      "step = 6984200: loss = 4.17987585067749\n",
      "step = 6984400: loss = 5.200779914855957\n",
      "step = 6984600: loss = 5.850362300872803\n",
      "step = 6984800: loss = 4.373265266418457\n",
      "step = 6985000: loss = 3.204685688018799\n",
      "step = 6985000: Average Return = 3.7660000324249268\n",
      "step = 6985200: loss = 4.4768242835998535\n",
      "step = 6985400: loss = 4.3160834312438965\n",
      "step = 6985600: loss = 3.4197356700897217\n",
      "step = 6985800: loss = 5.102095603942871\n",
      "step = 6986000: loss = 4.196985244750977\n",
      "step = 6986200: loss = 4.288804531097412\n",
      "step = 6986400: loss = 4.795173645019531\n",
      "step = 6986600: loss = 3.5070159435272217\n",
      "step = 6986800: loss = 3.5564491748809814\n",
      "step = 6987000: loss = 4.388482570648193\n",
      "step = 6987200: loss = 3.422914981842041\n",
      "step = 6987400: loss = 4.9982709884643555\n",
      "step = 6987600: loss = 3.441477060317993\n",
      "step = 6987800: loss = 4.097851276397705\n",
      "step = 6988000: loss = 5.550876140594482\n",
      "step = 6988200: loss = 4.510583400726318\n",
      "step = 6988400: loss = 5.1668701171875\n",
      "step = 6988600: loss = 2.453125238418579\n",
      "step = 6988800: loss = 5.478559494018555\n",
      "step = 6989000: loss = 4.330883026123047\n",
      "step = 6989200: loss = 3.9281270503997803\n",
      "step = 6989400: loss = 4.308379173278809\n",
      "step = 6989600: loss = 4.087710857391357\n",
      "step = 6989800: loss = 3.536855936050415\n",
      "step = 6990000: loss = 5.668712139129639\n",
      "step = 6990000: Average Return = 3.552000045776367\n",
      "step = 6990200: loss = 4.114156246185303\n",
      "step = 6990400: loss = 5.430925369262695\n",
      "step = 6990600: loss = 5.881812572479248\n",
      "step = 6990800: loss = 3.597637176513672\n",
      "step = 6991000: loss = 2.5093071460723877\n",
      "step = 6991200: loss = 4.348457336425781\n",
      "step = 6991400: loss = 4.749729633331299\n",
      "step = 6991600: loss = 2.898205280303955\n",
      "step = 6991800: loss = 3.9278953075408936\n",
      "step = 6992000: loss = 4.234235763549805\n",
      "step = 6992200: loss = 3.7152960300445557\n",
      "step = 6992400: loss = 4.113345623016357\n",
      "step = 6992600: loss = 3.7816314697265625\n",
      "step = 6992800: loss = 2.918860673904419\n",
      "step = 6993000: loss = 4.989689826965332\n",
      "step = 6993200: loss = 3.648348808288574\n",
      "step = 6993400: loss = 4.236114978790283\n",
      "step = 6993600: loss = 4.379208087921143\n",
      "step = 6993800: loss = 4.456423282623291\n",
      "step = 6994000: loss = 4.312124729156494\n",
      "step = 6994200: loss = 4.37691068649292\n",
      "step = 6994400: loss = 3.1786820888519287\n",
      "step = 6994600: loss = 4.7060227394104\n",
      "step = 6994800: loss = 3.3122854232788086\n",
      "step = 6995000: loss = 3.9017279148101807\n",
      "step = 6995000: Average Return = 3.696000099182129\n",
      "step = 6995200: loss = 4.0256242752075195\n",
      "step = 6995400: loss = 5.213932991027832\n",
      "step = 6995600: loss = 2.9219844341278076\n",
      "step = 6995800: loss = 4.1530680656433105\n",
      "step = 6996000: loss = 3.9781248569488525\n",
      "step = 6996200: loss = 4.288829326629639\n",
      "step = 6996400: loss = 4.337979793548584\n",
      "step = 6996600: loss = 5.203494548797607\n",
      "step = 6996800: loss = 4.022826194763184\n",
      "step = 6997000: loss = 3.836038112640381\n",
      "step = 6997200: loss = 3.868666887283325\n",
      "step = 6997400: loss = 3.523372173309326\n",
      "step = 6997600: loss = 5.414135456085205\n",
      "step = 6997800: loss = 3.301642656326294\n",
      "step = 6998000: loss = 3.7605583667755127\n",
      "step = 6998200: loss = 4.097352981567383\n",
      "step = 6998400: loss = 4.952170372009277\n",
      "step = 6998600: loss = 4.234432220458984\n",
      "step = 6998800: loss = 3.4722230434417725\n",
      "step = 6999000: loss = 3.8510258197784424\n",
      "step = 6999200: loss = 5.879146575927734\n",
      "step = 6999400: loss = 4.168734550476074\n",
      "step = 6999600: loss = 4.723155975341797\n",
      "step = 6999800: loss = 5.470508098602295\n",
      "step = 7000000: loss = 3.7591552734375\n",
      "step = 7000000: Average Return = 3.996000051498413\n",
      "step = 7000200: loss = 2.8783071041107178\n",
      "step = 7000400: loss = 4.498804092407227\n",
      "step = 7000600: loss = 3.892210006713867\n",
      "step = 7000800: loss = 4.961195468902588\n",
      "step = 7001000: loss = 4.822832107543945\n",
      "step = 7001200: loss = 5.0363922119140625\n",
      "step = 7001400: loss = 4.253521919250488\n",
      "step = 7001600: loss = 4.332113742828369\n",
      "step = 7001800: loss = 2.9985995292663574\n",
      "step = 7002000: loss = 3.576411008834839\n",
      "step = 7002200: loss = 3.1797516345977783\n",
      "step = 7002400: loss = 4.085781097412109\n",
      "step = 7002600: loss = 5.281351089477539\n",
      "step = 7002800: loss = 4.821713924407959\n",
      "step = 7003000: loss = 2.6541202068328857\n",
      "step = 7003200: loss = 6.70019006729126\n",
      "step = 7003400: loss = 3.976912498474121\n",
      "step = 7003600: loss = 3.5081725120544434\n",
      "step = 7003800: loss = 4.116589546203613\n",
      "step = 7004000: loss = 3.0744378566741943\n",
      "step = 7004200: loss = 4.7415056228637695\n",
      "step = 7004400: loss = 4.1909708976745605\n",
      "step = 7004600: loss = 3.4454166889190674\n",
      "step = 7004800: loss = 3.230907917022705\n",
      "step = 7005000: loss = 5.600942134857178\n",
      "step = 7005000: Average Return = 3.746000051498413\n",
      "step = 7005200: loss = 3.4058361053466797\n",
      "step = 7005400: loss = 4.044347286224365\n",
      "step = 7005600: loss = 5.905579566955566\n",
      "step = 7005800: loss = 3.5263853073120117\n",
      "step = 7006000: loss = 4.572670936584473\n",
      "step = 7006200: loss = 3.649735689163208\n",
      "step = 7006400: loss = 4.803124904632568\n",
      "step = 7006600: loss = 5.532843589782715\n",
      "step = 7006800: loss = 3.339430093765259\n",
      "step = 7007000: loss = 3.645625591278076\n",
      "step = 7007200: loss = 3.4729325771331787\n",
      "step = 7007400: loss = 3.448539972305298\n",
      "step = 7007600: loss = 3.7912614345550537\n",
      "step = 7007800: loss = 4.4615278244018555\n",
      "step = 7008000: loss = 2.78216290473938\n",
      "step = 7008200: loss = 4.410358905792236\n",
      "step = 7008400: loss = 4.502138614654541\n",
      "step = 7008600: loss = 3.70356822013855\n",
      "step = 7008800: loss = 4.38832426071167\n",
      "step = 7009000: loss = 4.235427379608154\n",
      "step = 7009200: loss = 3.80413818359375\n",
      "step = 7009400: loss = 4.095961093902588\n",
      "step = 7009600: loss = 3.889371156692505\n",
      "step = 7009800: loss = 3.6734619140625\n",
      "step = 7010000: loss = 3.8144614696502686\n",
      "step = 7010000: Average Return = 3.7160000801086426\n",
      "step = 7010200: loss = 3.6222047805786133\n",
      "step = 7010400: loss = 3.6789019107818604\n",
      "step = 7010600: loss = 3.866109609603882\n",
      "step = 7010800: loss = 3.64514422416687\n",
      "step = 7011000: loss = 3.7405269145965576\n",
      "step = 7011200: loss = 4.6191325187683105\n",
      "step = 7011400: loss = 3.7899250984191895\n",
      "step = 7011600: loss = 4.72407341003418\n",
      "step = 7011800: loss = 2.9662327766418457\n",
      "step = 7012000: loss = 5.361138820648193\n",
      "step = 7012200: loss = 5.152082920074463\n",
      "step = 7012400: loss = 3.6256942749023438\n",
      "step = 7012600: loss = 3.288188934326172\n",
      "step = 7012800: loss = 5.034807205200195\n",
      "step = 7013000: loss = 4.791437149047852\n",
      "step = 7013200: loss = 4.536941051483154\n",
      "step = 7013400: loss = 3.2574992179870605\n",
      "step = 7013600: loss = 2.1779379844665527\n",
      "step = 7013800: loss = 5.131890773773193\n",
      "step = 7014000: loss = 3.847280263900757\n",
      "step = 7014200: loss = 6.079446792602539\n",
      "step = 7014400: loss = 4.209988117218018\n",
      "step = 7014600: loss = 4.107934951782227\n",
      "step = 7014800: loss = 4.443297863006592\n",
      "step = 7015000: loss = 5.182771682739258\n",
      "step = 7015000: Average Return = 3.549999952316284\n",
      "step = 7015200: loss = 3.8637707233428955\n",
      "step = 7015400: loss = 3.519426107406616\n",
      "step = 7015600: loss = 4.018143653869629\n",
      "step = 7015800: loss = 3.750915765762329\n",
      "step = 7016000: loss = 4.132108688354492\n",
      "step = 7016200: loss = 4.916112899780273\n",
      "step = 7016400: loss = 3.7840511798858643\n",
      "step = 7016600: loss = 2.882100820541382\n",
      "step = 7016800: loss = 2.648386240005493\n",
      "step = 7017000: loss = 3.2653613090515137\n",
      "step = 7017200: loss = 2.808628559112549\n",
      "step = 7017400: loss = 3.773501396179199\n",
      "step = 7017600: loss = 3.3074684143066406\n",
      "step = 7017800: loss = 2.3592779636383057\n",
      "step = 7018000: loss = 4.679666042327881\n",
      "step = 7018200: loss = 5.327534198760986\n",
      "step = 7018400: loss = 4.294024467468262\n",
      "step = 7018600: loss = 5.501350402832031\n",
      "step = 7018800: loss = 4.473474979400635\n",
      "step = 7019000: loss = 3.119621753692627\n",
      "step = 7019200: loss = 3.913459300994873\n",
      "step = 7019400: loss = 4.455254554748535\n",
      "step = 7019600: loss = 4.4615302085876465\n",
      "step = 7019800: loss = 5.992913722991943\n",
      "step = 7020000: loss = 3.503774881362915\n",
      "step = 7020000: Average Return = 3.8940000534057617\n",
      "step = 7020200: loss = 6.339451313018799\n",
      "step = 7020400: loss = 2.671271324157715\n",
      "step = 7020600: loss = 5.548450946807861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7020800: loss = 2.8810439109802246\n",
      "step = 7021000: loss = 4.758373737335205\n",
      "step = 7021200: loss = 3.9060182571411133\n",
      "step = 7021400: loss = 3.941267967224121\n",
      "step = 7021600: loss = 5.154537677764893\n",
      "step = 7021800: loss = 4.208099842071533\n",
      "step = 7022000: loss = 4.2696990966796875\n",
      "step = 7022200: loss = 3.883169412612915\n",
      "step = 7022400: loss = 4.565516471862793\n",
      "step = 7022600: loss = 4.207765579223633\n",
      "step = 7022800: loss = 4.514678478240967\n",
      "step = 7023000: loss = 5.34979772567749\n",
      "step = 7023200: loss = 4.035694599151611\n",
      "step = 7023400: loss = 2.909904718399048\n",
      "step = 7023600: loss = 3.7734673023223877\n",
      "step = 7023800: loss = 5.451479434967041\n",
      "step = 7024000: loss = 3.788991928100586\n",
      "step = 7024200: loss = 4.129766464233398\n",
      "step = 7024400: loss = 4.360753536224365\n",
      "step = 7024600: loss = 3.380476951599121\n",
      "step = 7024800: loss = 3.9702131748199463\n",
      "step = 7025000: loss = 3.737095832824707\n",
      "step = 7025000: Average Return = 4.216000080108643\n",
      "step = 7025200: loss = 3.6382317543029785\n",
      "step = 7025400: loss = 5.848001956939697\n",
      "step = 7025600: loss = 3.9872214794158936\n",
      "step = 7025800: loss = 3.729200601577759\n",
      "step = 7026000: loss = 3.350076198577881\n",
      "step = 7026200: loss = 4.376598358154297\n",
      "step = 7026400: loss = 2.982734203338623\n",
      "step = 7026600: loss = 5.792929172515869\n",
      "step = 7026800: loss = 3.996286630630493\n",
      "step = 7027000: loss = 3.531428098678589\n",
      "step = 7027200: loss = 4.791971206665039\n",
      "step = 7027400: loss = 4.0239081382751465\n",
      "step = 7027600: loss = 3.4483895301818848\n",
      "step = 7027800: loss = 4.039572238922119\n",
      "step = 7028000: loss = 3.340031623840332\n",
      "step = 7028200: loss = 3.3178398609161377\n",
      "step = 7028400: loss = 4.3749847412109375\n",
      "step = 7028600: loss = 3.4022765159606934\n",
      "step = 7028800: loss = 3.418081521987915\n",
      "step = 7029000: loss = 4.775043964385986\n",
      "step = 7029200: loss = 3.992689371109009\n",
      "step = 7029400: loss = 3.5988590717315674\n",
      "step = 7029600: loss = 3.3483855724334717\n",
      "step = 7029800: loss = 2.912086009979248\n",
      "step = 7030000: loss = 3.4524645805358887\n",
      "step = 7030000: Average Return = 4.1519999504089355\n",
      "step = 7030200: loss = 2.989716053009033\n",
      "step = 7030400: loss = 3.556917667388916\n",
      "step = 7030600: loss = 4.5713067054748535\n",
      "step = 7030800: loss = 4.896772384643555\n",
      "step = 7031000: loss = 4.7096476554870605\n",
      "step = 7031200: loss = 4.11013650894165\n",
      "step = 7031400: loss = 4.610466957092285\n",
      "step = 7031600: loss = 4.310950756072998\n",
      "step = 7031800: loss = 3.3647732734680176\n",
      "step = 7032000: loss = 3.908478260040283\n",
      "step = 7032200: loss = 5.294985771179199\n",
      "step = 7032400: loss = 3.91524338722229\n",
      "step = 7032600: loss = 4.20463228225708\n",
      "step = 7032800: loss = 4.874608039855957\n",
      "step = 7033000: loss = 4.908627986907959\n",
      "step = 7033200: loss = 4.823530673980713\n",
      "step = 7033400: loss = 4.118844032287598\n",
      "step = 7033600: loss = 2.8338208198547363\n",
      "step = 7033800: loss = 3.329108238220215\n",
      "step = 7034000: loss = 4.022746562957764\n",
      "step = 7034200: loss = 3.844343423843384\n",
      "step = 7034400: loss = 2.9406816959381104\n",
      "step = 7034600: loss = 4.9872236251831055\n",
      "step = 7034800: loss = 2.834576368331909\n",
      "step = 7035000: loss = 3.028764009475708\n",
      "step = 7035000: Average Return = 3.888000011444092\n",
      "step = 7035200: loss = 4.967824935913086\n",
      "step = 7035400: loss = 3.8246238231658936\n",
      "step = 7035600: loss = 3.752563953399658\n",
      "step = 7035800: loss = 4.699132442474365\n",
      "step = 7036000: loss = 3.677497148513794\n",
      "step = 7036200: loss = 3.9629344940185547\n",
      "step = 7036400: loss = 5.0711469650268555\n",
      "step = 7036600: loss = 3.5709054470062256\n",
      "step = 7036800: loss = 2.603135108947754\n",
      "step = 7037000: loss = 3.2294421195983887\n",
      "step = 7037200: loss = 3.297534704208374\n",
      "step = 7037400: loss = 2.718564748764038\n",
      "step = 7037600: loss = 4.0312981605529785\n",
      "step = 7037800: loss = 3.7598330974578857\n",
      "step = 7038000: loss = 3.91361141204834\n",
      "step = 7038200: loss = 3.438347101211548\n",
      "step = 7038400: loss = 3.488996744155884\n",
      "step = 7038600: loss = 3.6150639057159424\n",
      "step = 7038800: loss = 4.20112419128418\n",
      "step = 7039000: loss = 3.759835720062256\n",
      "step = 7039200: loss = 3.134427070617676\n",
      "step = 7039400: loss = 4.023841857910156\n",
      "step = 7039600: loss = 4.122378349304199\n",
      "step = 7039800: loss = 4.500687599182129\n",
      "step = 7040000: loss = 3.6989123821258545\n",
      "step = 7040000: Average Return = 3.509999990463257\n",
      "step = 7040200: loss = 4.666131019592285\n",
      "step = 7040400: loss = 3.928731679916382\n",
      "step = 7040600: loss = 2.288694143295288\n",
      "step = 7040800: loss = 3.733771800994873\n",
      "step = 7041000: loss = 2.9947762489318848\n",
      "step = 7041200: loss = 3.9154818058013916\n",
      "step = 7041400: loss = 4.355412483215332\n",
      "step = 7041600: loss = 4.18634557723999\n",
      "step = 7041800: loss = 3.6928083896636963\n",
      "step = 7042000: loss = 5.301287651062012\n",
      "step = 7042200: loss = 2.267505645751953\n",
      "step = 7042400: loss = 4.10034704208374\n",
      "step = 7042600: loss = 4.167807102203369\n",
      "step = 7042800: loss = 4.083591938018799\n",
      "step = 7043000: loss = 4.12155294418335\n",
      "step = 7043200: loss = 4.411205291748047\n",
      "step = 7043400: loss = 3.932673215866089\n",
      "step = 7043600: loss = 4.310836315155029\n",
      "step = 7043800: loss = 4.600963115692139\n",
      "step = 7044000: loss = 4.22389030456543\n",
      "step = 7044200: loss = 3.6995558738708496\n",
      "step = 7044400: loss = 4.001543998718262\n",
      "step = 7044600: loss = 4.985305309295654\n",
      "step = 7044800: loss = 3.443233013153076\n",
      "step = 7045000: loss = 4.264975547790527\n",
      "step = 7045000: Average Return = 3.6700000762939453\n",
      "step = 7045200: loss = 4.731273174285889\n",
      "step = 7045400: loss = 3.1951372623443604\n",
      "step = 7045600: loss = 4.9391398429870605\n",
      "step = 7045800: loss = 4.811075210571289\n",
      "step = 7046000: loss = 3.6369171142578125\n",
      "step = 7046200: loss = 3.9825942516326904\n",
      "step = 7046400: loss = 4.012391090393066\n",
      "step = 7046600: loss = 4.14035701751709\n",
      "step = 7046800: loss = 3.682438373565674\n",
      "step = 7047000: loss = 5.361018180847168\n",
      "step = 7047200: loss = 3.3161590099334717\n",
      "step = 7047400: loss = 4.0900559425354\n",
      "step = 7047600: loss = 4.434722900390625\n",
      "step = 7047800: loss = 4.279378414154053\n",
      "step = 7048000: loss = 5.0048933029174805\n",
      "step = 7048200: loss = 4.015535831451416\n",
      "step = 7048400: loss = 4.89178466796875\n",
      "step = 7048600: loss = 3.5883281230926514\n",
      "step = 7048800: loss = 4.472952842712402\n",
      "step = 7049000: loss = 4.417821884155273\n",
      "step = 7049200: loss = 4.025303363800049\n",
      "step = 7049400: loss = 2.8842217922210693\n",
      "step = 7049600: loss = 3.9159584045410156\n",
      "step = 7049800: loss = 3.6906700134277344\n",
      "step = 7050000: loss = 4.891772747039795\n",
      "step = 7050000: Average Return = 4.0879998207092285\n",
      "step = 7050200: loss = 3.8768153190612793\n",
      "step = 7050400: loss = 4.924901962280273\n",
      "step = 7050600: loss = 5.728743553161621\n",
      "step = 7050800: loss = 3.8446602821350098\n",
      "step = 7051000: loss = 3.351921319961548\n",
      "step = 7051200: loss = 3.613051652908325\n",
      "step = 7051400: loss = 4.192921161651611\n",
      "step = 7051600: loss = 3.747622013092041\n",
      "step = 7051800: loss = 2.8672924041748047\n",
      "step = 7052000: loss = 4.699007511138916\n",
      "step = 7052200: loss = 4.548168182373047\n",
      "step = 7052400: loss = 4.7290167808532715\n",
      "step = 7052600: loss = 2.8114006519317627\n",
      "step = 7052800: loss = 4.754272937774658\n",
      "step = 7053000: loss = 3.158703565597534\n",
      "step = 7053200: loss = 2.994518995285034\n",
      "step = 7053400: loss = 3.10665225982666\n",
      "step = 7053600: loss = 4.44024133682251\n",
      "step = 7053800: loss = 3.993241548538208\n",
      "step = 7054000: loss = 3.9397387504577637\n",
      "step = 7054200: loss = 5.775330543518066\n",
      "step = 7054400: loss = 4.84119987487793\n",
      "step = 7054600: loss = 4.159305095672607\n",
      "step = 7054800: loss = 4.425228595733643\n",
      "step = 7055000: loss = 4.555817604064941\n",
      "step = 7055000: Average Return = 3.565999984741211\n",
      "step = 7055200: loss = 3.9030158519744873\n",
      "step = 7055400: loss = 4.6602888107299805\n",
      "step = 7055600: loss = 3.6371841430664062\n",
      "step = 7055800: loss = 3.447291851043701\n",
      "step = 7056000: loss = 3.852832317352295\n",
      "step = 7056200: loss = 3.847756862640381\n",
      "step = 7056400: loss = 3.225637435913086\n",
      "step = 7056600: loss = 5.070580005645752\n",
      "step = 7056800: loss = 3.719574451446533\n",
      "step = 7057000: loss = 3.7014753818511963\n",
      "step = 7057200: loss = 3.0118675231933594\n",
      "step = 7057400: loss = 2.0299453735351562\n",
      "step = 7057600: loss = 4.018959999084473\n",
      "step = 7057800: loss = 3.2134580612182617\n",
      "step = 7058000: loss = 3.5972158908843994\n",
      "step = 7058200: loss = 6.800230979919434\n",
      "step = 7058400: loss = 4.272801876068115\n",
      "step = 7058600: loss = 4.174595832824707\n",
      "step = 7058800: loss = 5.497513294219971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7059000: loss = 4.215341567993164\n",
      "step = 7059200: loss = 4.8263139724731445\n",
      "step = 7059400: loss = 4.1905622482299805\n",
      "step = 7059600: loss = 4.4847941398620605\n",
      "step = 7059800: loss = 4.764014720916748\n",
      "step = 7060000: loss = 3.105435609817505\n",
      "step = 7060000: Average Return = 4.406000137329102\n",
      "step = 7060200: loss = 3.257366895675659\n",
      "step = 7060400: loss = 3.6868739128112793\n",
      "step = 7060600: loss = 4.729396343231201\n",
      "step = 7060800: loss = 4.640610218048096\n",
      "step = 7061000: loss = 3.8937437534332275\n",
      "step = 7061200: loss = 4.791386604309082\n",
      "step = 7061400: loss = 2.9002575874328613\n",
      "step = 7061600: loss = 3.758146286010742\n",
      "step = 7061800: loss = 4.792892932891846\n",
      "step = 7062000: loss = 3.901418924331665\n",
      "step = 7062200: loss = 3.4807896614074707\n",
      "step = 7062400: loss = 3.455416202545166\n",
      "step = 7062600: loss = 2.4458870887756348\n",
      "step = 7062800: loss = 4.098262310028076\n",
      "step = 7063000: loss = 5.40174674987793\n",
      "step = 7063200: loss = 2.919224262237549\n",
      "step = 7063400: loss = 4.702253341674805\n",
      "step = 7063600: loss = 5.071629047393799\n",
      "step = 7063800: loss = 4.324617385864258\n",
      "step = 7064000: loss = 4.923110485076904\n",
      "step = 7064200: loss = 3.2157580852508545\n",
      "step = 7064400: loss = 5.531937122344971\n",
      "step = 7064600: loss = 4.002777576446533\n",
      "step = 7064800: loss = 4.637993335723877\n",
      "step = 7065000: loss = 5.471335411071777\n",
      "step = 7065000: Average Return = 3.7920000553131104\n",
      "step = 7065200: loss = 3.9426863193511963\n",
      "step = 7065400: loss = 4.474257946014404\n",
      "step = 7065600: loss = 3.6035759449005127\n",
      "step = 7065800: loss = 4.820184230804443\n",
      "step = 7066000: loss = 4.488273620605469\n",
      "step = 7066200: loss = 3.226813316345215\n",
      "step = 7066400: loss = 3.0501723289489746\n",
      "step = 7066600: loss = 2.644056558609009\n",
      "step = 7066800: loss = 2.845158100128174\n",
      "step = 7067000: loss = 4.819343090057373\n",
      "step = 7067200: loss = 4.841968059539795\n",
      "step = 7067400: loss = 4.131326675415039\n",
      "step = 7067600: loss = 4.594287395477295\n",
      "step = 7067800: loss = 4.702376365661621\n",
      "step = 7068000: loss = 3.555009603500366\n",
      "step = 7068200: loss = 4.861705303192139\n",
      "step = 7068400: loss = 5.058060646057129\n",
      "step = 7068600: loss = 4.163949012756348\n",
      "step = 7068800: loss = 2.934704542160034\n",
      "step = 7069000: loss = 4.216188430786133\n",
      "step = 7069200: loss = 4.626291275024414\n",
      "step = 7069400: loss = 2.418900489807129\n",
      "step = 7069600: loss = 5.917849540710449\n",
      "step = 7069800: loss = 4.393642902374268\n",
      "step = 7070000: loss = 4.328583717346191\n",
      "step = 7070000: Average Return = 3.6740000247955322\n",
      "step = 7070200: loss = 3.1515398025512695\n",
      "step = 7070400: loss = 4.181432723999023\n",
      "step = 7070600: loss = 4.356955528259277\n",
      "step = 7070800: loss = 4.1975226402282715\n",
      "step = 7071000: loss = 4.572394847869873\n",
      "step = 7071200: loss = 4.22163724899292\n",
      "step = 7071400: loss = 5.376501560211182\n",
      "step = 7071600: loss = 5.72450065612793\n",
      "step = 7071800: loss = 4.662201881408691\n",
      "step = 7072000: loss = 3.798353910446167\n",
      "step = 7072200: loss = 4.8252763748168945\n",
      "step = 7072400: loss = 4.310789585113525\n",
      "step = 7072600: loss = 3.9578874111175537\n",
      "step = 7072800: loss = 2.6865620613098145\n",
      "step = 7073000: loss = 4.919813632965088\n",
      "step = 7073200: loss = 3.411560535430908\n",
      "step = 7073400: loss = 3.5044894218444824\n",
      "step = 7073600: loss = 4.153745174407959\n",
      "step = 7073800: loss = 3.6585562229156494\n",
      "step = 7074000: loss = 3.086101531982422\n",
      "step = 7074200: loss = 3.604916572570801\n",
      "step = 7074400: loss = 4.547506809234619\n",
      "step = 7074600: loss = 3.968740701675415\n",
      "step = 7074800: loss = 3.1802384853363037\n",
      "step = 7075000: loss = 3.801903009414673\n",
      "step = 7075000: Average Return = 3.671999931335449\n",
      "step = 7075200: loss = 5.298923015594482\n",
      "step = 7075400: loss = 6.139873027801514\n",
      "step = 7075600: loss = 3.299945592880249\n",
      "step = 7075800: loss = 4.2863078117370605\n",
      "step = 7076000: loss = 3.5257225036621094\n",
      "step = 7076200: loss = 3.6056325435638428\n",
      "step = 7076400: loss = 4.305037498474121\n",
      "step = 7076600: loss = 5.526889324188232\n",
      "step = 7076800: loss = 4.166532516479492\n",
      "step = 7077000: loss = 3.582686424255371\n",
      "step = 7077200: loss = 5.416987895965576\n",
      "step = 7077400: loss = 5.691849231719971\n",
      "step = 7077600: loss = 4.16845703125\n",
      "step = 7077800: loss = 3.336773633956909\n",
      "step = 7078000: loss = 3.951050281524658\n",
      "step = 7078200: loss = 4.72225284576416\n",
      "step = 7078400: loss = 5.271540641784668\n",
      "step = 7078600: loss = 3.1765873432159424\n",
      "step = 7078800: loss = 2.821316719055176\n",
      "step = 7079000: loss = 4.890807151794434\n",
      "step = 7079200: loss = 4.196804046630859\n",
      "step = 7079400: loss = 4.947098731994629\n",
      "step = 7079600: loss = 3.9736571311950684\n",
      "step = 7079800: loss = 5.319305896759033\n",
      "step = 7080000: loss = 6.01072359085083\n",
      "step = 7080000: Average Return = 3.7860000133514404\n",
      "step = 7080200: loss = 4.268075466156006\n",
      "step = 7080400: loss = 3.926910400390625\n",
      "step = 7080600: loss = 3.357699394226074\n",
      "step = 7080800: loss = 4.566788196563721\n",
      "step = 7081000: loss = 3.0186610221862793\n",
      "step = 7081200: loss = 3.9898223876953125\n",
      "step = 7081400: loss = 5.727038383483887\n",
      "step = 7081600: loss = 3.6536712646484375\n",
      "step = 7081800: loss = 4.133088111877441\n",
      "step = 7082000: loss = 2.477611541748047\n",
      "step = 7082200: loss = 4.6384735107421875\n",
      "step = 7082400: loss = 3.1630587577819824\n",
      "step = 7082600: loss = 5.244543552398682\n",
      "step = 7082800: loss = 4.730169773101807\n",
      "step = 7083000: loss = 3.001406192779541\n",
      "step = 7083200: loss = 3.5786876678466797\n",
      "step = 7083400: loss = 3.3105108737945557\n",
      "step = 7083600: loss = 4.4311017990112305\n",
      "step = 7083800: loss = 3.4713094234466553\n",
      "step = 7084000: loss = 5.09201717376709\n",
      "step = 7084200: loss = 3.7870779037475586\n",
      "step = 7084400: loss = 5.214013576507568\n",
      "step = 7084600: loss = 3.0670006275177\n",
      "step = 7084800: loss = 3.868549108505249\n",
      "step = 7085000: loss = 5.257734298706055\n",
      "step = 7085000: Average Return = 3.822000026702881\n",
      "step = 7085200: loss = 4.174868583679199\n",
      "step = 7085400: loss = 5.130876541137695\n",
      "step = 7085600: loss = 2.947892427444458\n",
      "step = 7085800: loss = 3.5611047744750977\n",
      "step = 7086000: loss = 5.0569000244140625\n",
      "step = 7086200: loss = 2.7740986347198486\n",
      "step = 7086400: loss = 4.806454658508301\n",
      "step = 7086600: loss = 5.001417636871338\n",
      "step = 7086800: loss = 3.9308085441589355\n",
      "step = 7087000: loss = 5.539024353027344\n",
      "step = 7087200: loss = 5.12489128112793\n",
      "step = 7087400: loss = 4.286537170410156\n",
      "step = 7087600: loss = 3.826033353805542\n",
      "step = 7087800: loss = 4.026201248168945\n",
      "step = 7088000: loss = 3.957184076309204\n",
      "step = 7088200: loss = 4.292729377746582\n",
      "step = 7088400: loss = 3.0603277683258057\n",
      "step = 7088600: loss = 3.9212443828582764\n",
      "step = 7088800: loss = 3.615213632583618\n",
      "step = 7089000: loss = 5.144595623016357\n",
      "step = 7089200: loss = 4.246755123138428\n",
      "step = 7089400: loss = 4.914512634277344\n",
      "step = 7089600: loss = 4.231832027435303\n",
      "step = 7089800: loss = 4.0481648445129395\n",
      "step = 7090000: loss = 3.029846668243408\n",
      "step = 7090000: Average Return = 3.6059999465942383\n",
      "step = 7090200: loss = 4.135039329528809\n",
      "step = 7090400: loss = 4.622203350067139\n",
      "step = 7090600: loss = 3.899876594543457\n",
      "step = 7090800: loss = 3.6101369857788086\n",
      "step = 7091000: loss = 4.0758280754089355\n",
      "step = 7091200: loss = 4.2050628662109375\n",
      "step = 7091400: loss = 3.346525192260742\n",
      "step = 7091600: loss = 4.079354763031006\n",
      "step = 7091800: loss = 3.3230416774749756\n",
      "step = 7092000: loss = 2.638190507888794\n",
      "step = 7092200: loss = 4.032723903656006\n",
      "step = 7092400: loss = 5.17691707611084\n",
      "step = 7092600: loss = 4.383014678955078\n",
      "step = 7092800: loss = 3.696643590927124\n",
      "step = 7093000: loss = 4.102432727813721\n",
      "step = 7093200: loss = 4.367283344268799\n",
      "step = 7093400: loss = 4.9740166664123535\n",
      "step = 7093600: loss = 5.027579307556152\n",
      "step = 7093800: loss = 5.003464221954346\n",
      "step = 7094000: loss = 4.041088581085205\n",
      "step = 7094200: loss = 4.242495536804199\n",
      "step = 7094400: loss = 5.474411487579346\n",
      "step = 7094600: loss = 5.113429069519043\n",
      "step = 7094800: loss = 4.386232376098633\n",
      "step = 7095000: loss = 3.606830358505249\n",
      "step = 7095000: Average Return = 4.021999835968018\n",
      "step = 7095200: loss = 4.173480033874512\n",
      "step = 7095400: loss = 3.1023855209350586\n",
      "step = 7095600: loss = 3.5925190448760986\n",
      "step = 7095800: loss = 3.721968650817871\n",
      "step = 7096000: loss = 4.237597465515137\n",
      "step = 7096200: loss = 4.1985883712768555\n",
      "step = 7096400: loss = 4.06343936920166\n",
      "step = 7096600: loss = 5.3550639152526855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7096800: loss = 4.654162406921387\n",
      "step = 7097000: loss = 5.185159683227539\n",
      "step = 7097200: loss = 5.064602851867676\n",
      "step = 7097400: loss = 2.6275668144226074\n",
      "step = 7097600: loss = 3.2948405742645264\n",
      "step = 7097800: loss = 6.215354919433594\n",
      "step = 7098000: loss = 3.3229711055755615\n",
      "step = 7098200: loss = 4.505946159362793\n",
      "step = 7098400: loss = 3.868083953857422\n",
      "step = 7098600: loss = 4.090545654296875\n",
      "step = 7098800: loss = 2.88731050491333\n",
      "step = 7099000: loss = 4.337634086608887\n",
      "step = 7099200: loss = 4.238895893096924\n",
      "step = 7099400: loss = 5.781961917877197\n",
      "step = 7099600: loss = 4.453878879547119\n",
      "step = 7099800: loss = 3.3342905044555664\n",
      "step = 7100000: loss = 4.129609107971191\n",
      "step = 7100000: Average Return = 3.4539999961853027\n",
      "step = 7100200: loss = 3.704427480697632\n",
      "step = 7100400: loss = 5.145837306976318\n",
      "step = 7100600: loss = 3.4978747367858887\n",
      "step = 7100800: loss = 4.469105243682861\n",
      "step = 7101000: loss = 4.563694477081299\n",
      "step = 7101200: loss = 4.851559162139893\n",
      "step = 7101400: loss = 4.552809238433838\n",
      "step = 7101600: loss = 2.5229923725128174\n",
      "step = 7101800: loss = 4.256422519683838\n",
      "step = 7102000: loss = 3.221393346786499\n",
      "step = 7102200: loss = 3.1806352138519287\n",
      "step = 7102400: loss = 4.525374889373779\n",
      "step = 7102600: loss = 4.2695417404174805\n",
      "step = 7102800: loss = 3.8412926197052\n",
      "step = 7103000: loss = 3.8397631645202637\n",
      "step = 7103200: loss = 4.963444232940674\n",
      "step = 7103400: loss = 4.2171854972839355\n",
      "step = 7103600: loss = 4.4433770179748535\n",
      "step = 7103800: loss = 5.447081565856934\n",
      "step = 7104000: loss = 4.623003005981445\n",
      "step = 7104200: loss = 3.9866905212402344\n",
      "step = 7104400: loss = 4.021803855895996\n",
      "step = 7104600: loss = 5.389557361602783\n",
      "step = 7104800: loss = 3.6166605949401855\n",
      "step = 7105000: loss = 4.394629001617432\n",
      "step = 7105000: Average Return = 3.558000087738037\n",
      "step = 7105200: loss = 5.002323627471924\n",
      "step = 7105400: loss = 4.8909101486206055\n",
      "step = 7105600: loss = 4.346645832061768\n",
      "step = 7105800: loss = 3.856034517288208\n",
      "step = 7106000: loss = 3.320125102996826\n",
      "step = 7106200: loss = 3.272568941116333\n",
      "step = 7106400: loss = 3.5527172088623047\n",
      "step = 7106600: loss = 4.253684043884277\n",
      "step = 7106800: loss = 4.2816362380981445\n",
      "step = 7107000: loss = 2.7163305282592773\n",
      "step = 7107200: loss = 4.554924964904785\n",
      "step = 7107400: loss = 4.231773376464844\n",
      "step = 7107600: loss = 5.353797912597656\n",
      "step = 7107800: loss = 2.848240613937378\n",
      "step = 7108000: loss = 7.0364909172058105\n",
      "step = 7108200: loss = 3.7465736865997314\n",
      "step = 7108400: loss = 3.8640220165252686\n",
      "step = 7108600: loss = 5.68510627746582\n",
      "step = 7108800: loss = 5.270508289337158\n",
      "step = 7109000: loss = 5.37255859375\n",
      "step = 7109200: loss = 5.217690944671631\n",
      "step = 7109400: loss = 3.519896984100342\n",
      "step = 7109600: loss = 4.507202625274658\n",
      "step = 7109800: loss = 5.368499755859375\n",
      "step = 7110000: loss = 4.873542785644531\n",
      "step = 7110000: Average Return = 3.6619999408721924\n",
      "step = 7110200: loss = 3.703636884689331\n",
      "step = 7110400: loss = 4.240217685699463\n",
      "step = 7110600: loss = 4.427188396453857\n",
      "step = 7110800: loss = 3.891179084777832\n",
      "step = 7111000: loss = 4.369996070861816\n",
      "step = 7111200: loss = 4.614123344421387\n",
      "step = 7111400: loss = 2.565204620361328\n",
      "step = 7111600: loss = 4.971282005310059\n",
      "step = 7111800: loss = 4.688643932342529\n",
      "step = 7112000: loss = 6.291769981384277\n",
      "step = 7112200: loss = 4.265921592712402\n",
      "step = 7112400: loss = 3.859998941421509\n",
      "step = 7112600: loss = 3.657227039337158\n",
      "step = 7112800: loss = 4.150280952453613\n",
      "step = 7113000: loss = 5.044890403747559\n",
      "step = 7113200: loss = 3.7425107955932617\n",
      "step = 7113400: loss = 3.150662660598755\n",
      "step = 7113600: loss = 3.2120585441589355\n",
      "step = 7113800: loss = 3.7302305698394775\n",
      "step = 7114000: loss = 4.408692359924316\n",
      "step = 7114200: loss = 4.964015483856201\n",
      "step = 7114400: loss = 5.207016944885254\n",
      "step = 7114600: loss = 3.753124713897705\n",
      "step = 7114800: loss = 3.8542754650115967\n",
      "step = 7115000: loss = 3.541217565536499\n",
      "step = 7115000: Average Return = 3.8540000915527344\n",
      "step = 7115200: loss = 4.669877052307129\n",
      "step = 7115400: loss = 5.591921329498291\n",
      "step = 7115600: loss = 4.018786907196045\n",
      "step = 7115800: loss = 3.3595855236053467\n",
      "step = 7116000: loss = 3.62835693359375\n",
      "step = 7116200: loss = 3.617220401763916\n",
      "step = 7116400: loss = 3.004232168197632\n",
      "step = 7116600: loss = 3.7298009395599365\n",
      "step = 7116800: loss = 3.2427890300750732\n",
      "step = 7117000: loss = 4.868895530700684\n",
      "step = 7117200: loss = 4.41554594039917\n",
      "step = 7117400: loss = 5.463497638702393\n",
      "step = 7117600: loss = 5.220269203186035\n",
      "step = 7117800: loss = 4.285496234893799\n",
      "step = 7118000: loss = 5.888304710388184\n",
      "step = 7118200: loss = 5.434586048126221\n",
      "step = 7118400: loss = 4.432490825653076\n",
      "step = 7118600: loss = 4.313925266265869\n",
      "step = 7118800: loss = 5.009896278381348\n",
      "step = 7119000: loss = 5.361793518066406\n",
      "step = 7119200: loss = 4.636996269226074\n",
      "step = 7119400: loss = 4.776182651519775\n",
      "step = 7119600: loss = 3.3758342266082764\n",
      "step = 7119800: loss = 3.9373435974121094\n",
      "step = 7120000: loss = 3.0233876705169678\n",
      "step = 7120000: Average Return = 3.7899999618530273\n",
      "step = 7120200: loss = 4.423184871673584\n",
      "step = 7120400: loss = 5.3400797843933105\n",
      "step = 7120600: loss = 5.223078727722168\n",
      "step = 7120800: loss = 3.1503050327301025\n",
      "step = 7121000: loss = 4.343391418457031\n",
      "step = 7121200: loss = 3.3169970512390137\n",
      "step = 7121400: loss = 4.466684341430664\n",
      "step = 7121600: loss = 3.9031178951263428\n",
      "step = 7121800: loss = 2.8839879035949707\n",
      "step = 7122000: loss = 5.882546424865723\n",
      "step = 7122200: loss = 5.304589748382568\n",
      "step = 7122400: loss = 3.8696835041046143\n",
      "step = 7122600: loss = 4.87346887588501\n",
      "step = 7122800: loss = 3.705385684967041\n",
      "step = 7123000: loss = 5.248990535736084\n",
      "step = 7123200: loss = 3.8662800788879395\n",
      "step = 7123400: loss = 3.9112820625305176\n",
      "step = 7123600: loss = 3.3189244270324707\n",
      "step = 7123800: loss = 4.304145812988281\n",
      "step = 7124000: loss = 3.374410390853882\n",
      "step = 7124200: loss = 3.989774703979492\n",
      "step = 7124400: loss = 4.08740758895874\n",
      "step = 7124600: loss = 3.469747304916382\n",
      "step = 7124800: loss = 5.307331562042236\n",
      "step = 7125000: loss = 4.471845626831055\n",
      "step = 7125000: Average Return = 4.021999835968018\n",
      "step = 7125200: loss = 5.5238871574401855\n",
      "step = 7125400: loss = 4.544327735900879\n",
      "step = 7125600: loss = 5.316807746887207\n",
      "step = 7125800: loss = 4.336071014404297\n",
      "step = 7126000: loss = 5.639148235321045\n",
      "step = 7126200: loss = 3.4313149452209473\n",
      "step = 7126400: loss = 3.782893180847168\n",
      "step = 7126600: loss = 4.151379108428955\n",
      "step = 7126800: loss = 3.342554807662964\n",
      "step = 7127000: loss = 3.120180606842041\n",
      "step = 7127200: loss = 4.316118240356445\n",
      "step = 7127400: loss = 5.564921855926514\n",
      "step = 7127600: loss = 4.16896915435791\n",
      "step = 7127800: loss = 3.493122100830078\n",
      "step = 7128000: loss = 5.0923075675964355\n",
      "step = 7128200: loss = 3.8267037868499756\n",
      "step = 7128400: loss = 4.147848606109619\n",
      "step = 7128600: loss = 5.644057273864746\n",
      "step = 7128800: loss = 5.319552898406982\n",
      "step = 7129000: loss = 3.782816171646118\n",
      "step = 7129200: loss = 3.841031312942505\n",
      "step = 7129400: loss = 2.931617259979248\n",
      "step = 7129600: loss = 5.2641119956970215\n",
      "step = 7129800: loss = 3.925100088119507\n",
      "step = 7130000: loss = 5.362664699554443\n",
      "step = 7130000: Average Return = 3.7339999675750732\n",
      "step = 7130200: loss = 4.7272257804870605\n",
      "step = 7130400: loss = 4.8120832443237305\n",
      "step = 7130600: loss = 3.365377426147461\n",
      "step = 7130800: loss = 4.0314106941223145\n",
      "step = 7131000: loss = 3.4482054710388184\n",
      "step = 7131200: loss = 3.785787343978882\n",
      "step = 7131400: loss = 3.726853847503662\n",
      "step = 7131600: loss = 5.805298805236816\n",
      "step = 7131800: loss = 4.674720764160156\n",
      "step = 7132000: loss = 4.627833843231201\n",
      "step = 7132200: loss = 3.5282018184661865\n",
      "step = 7132400: loss = 3.4216558933258057\n",
      "step = 7132600: loss = 2.8230419158935547\n",
      "step = 7132800: loss = 3.975800037384033\n",
      "step = 7133000: loss = 3.0585358142852783\n",
      "step = 7133200: loss = 4.596863269805908\n",
      "step = 7133400: loss = 4.2474822998046875\n",
      "step = 7133600: loss = 4.562691688537598\n",
      "step = 7133800: loss = 4.52458381652832\n",
      "step = 7134000: loss = 3.796844959259033\n",
      "step = 7134200: loss = 3.8743138313293457\n",
      "step = 7134400: loss = 3.317960023880005\n",
      "step = 7134600: loss = 2.4653613567352295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7134800: loss = 5.1733269691467285\n",
      "step = 7135000: loss = 6.398867130279541\n",
      "step = 7135000: Average Return = 4.119999885559082\n",
      "step = 7135200: loss = 3.145603656768799\n",
      "step = 7135400: loss = 4.910528659820557\n",
      "step = 7135600: loss = 3.285592794418335\n",
      "step = 7135800: loss = 3.6127166748046875\n",
      "step = 7136000: loss = 4.6729044914245605\n",
      "step = 7136200: loss = 4.298862934112549\n",
      "step = 7136400: loss = 4.9162397384643555\n",
      "step = 7136600: loss = 4.4642181396484375\n",
      "step = 7136800: loss = 3.8015129566192627\n",
      "step = 7137000: loss = 3.8984739780426025\n",
      "step = 7137200: loss = 4.907519340515137\n",
      "step = 7137400: loss = 4.269600868225098\n",
      "step = 7137600: loss = 4.173521518707275\n",
      "step = 7137800: loss = 2.4046900272369385\n",
      "step = 7138000: loss = 3.5695323944091797\n",
      "step = 7138200: loss = 3.9977564811706543\n",
      "step = 7138400: loss = 2.9086620807647705\n",
      "step = 7138600: loss = 5.090811729431152\n",
      "step = 7138800: loss = 3.6390745639801025\n",
      "step = 7139000: loss = 3.9165496826171875\n",
      "step = 7139200: loss = 4.19284725189209\n",
      "step = 7139400: loss = 3.4608912467956543\n",
      "step = 7139600: loss = 2.1608083248138428\n",
      "step = 7139800: loss = 3.65055775642395\n",
      "step = 7140000: loss = 3.3648016452789307\n",
      "step = 7140000: Average Return = 3.9260001182556152\n",
      "step = 7140200: loss = 4.567964553833008\n",
      "step = 7140400: loss = 3.8774304389953613\n",
      "step = 7140600: loss = 4.035693168640137\n",
      "step = 7140800: loss = 5.096266746520996\n",
      "step = 7141000: loss = 2.8360509872436523\n",
      "step = 7141200: loss = 3.0259945392608643\n",
      "step = 7141400: loss = 4.980545997619629\n",
      "step = 7141600: loss = 4.739721775054932\n",
      "step = 7141800: loss = 2.850579261779785\n",
      "step = 7142000: loss = 4.4596848487854\n",
      "step = 7142200: loss = 5.240233421325684\n",
      "step = 7142400: loss = 3.6776745319366455\n",
      "step = 7142600: loss = 4.763385772705078\n",
      "step = 7142800: loss = 3.838578462600708\n",
      "step = 7143000: loss = 3.7720947265625\n",
      "step = 7143200: loss = 2.8872640132904053\n",
      "step = 7143400: loss = 2.6194980144500732\n",
      "step = 7143600: loss = 4.2550048828125\n",
      "step = 7143800: loss = 5.023586750030518\n",
      "step = 7144000: loss = 4.129056453704834\n",
      "step = 7144200: loss = 4.624520301818848\n",
      "step = 7144400: loss = 3.966862916946411\n",
      "step = 7144600: loss = 3.8387017250061035\n",
      "step = 7144800: loss = 5.161893367767334\n",
      "step = 7145000: loss = 3.8149824142456055\n",
      "step = 7145000: Average Return = 4.007999897003174\n",
      "step = 7145200: loss = 5.324487686157227\n",
      "step = 7145400: loss = 3.908001661300659\n",
      "step = 7145600: loss = 4.7544097900390625\n",
      "step = 7145800: loss = 4.40790319442749\n",
      "step = 7146000: loss = 4.063525676727295\n",
      "step = 7146200: loss = 4.224636554718018\n",
      "step = 7146400: loss = 3.9027199745178223\n",
      "step = 7146600: loss = 3.729268789291382\n",
      "step = 7146800: loss = 4.259030342102051\n",
      "step = 7147000: loss = 3.849090576171875\n",
      "step = 7147200: loss = 5.11600923538208\n",
      "step = 7147400: loss = 5.208005428314209\n",
      "step = 7147600: loss = 4.27448844909668\n",
      "step = 7147800: loss = 4.240630626678467\n",
      "step = 7148000: loss = 3.361423969268799\n",
      "step = 7148200: loss = 4.596911430358887\n",
      "step = 7148400: loss = 5.634881973266602\n",
      "step = 7148600: loss = 4.530474662780762\n",
      "step = 7148800: loss = 4.5135979652404785\n",
      "step = 7149000: loss = 4.032974720001221\n",
      "step = 7149200: loss = 3.5329558849334717\n",
      "step = 7149400: loss = 3.959117889404297\n",
      "step = 7149600: loss = 3.549229621887207\n",
      "step = 7149800: loss = 4.782900810241699\n",
      "step = 7150000: loss = 4.921281814575195\n",
      "step = 7150000: Average Return = 4.093999862670898\n",
      "step = 7150200: loss = 4.314251899719238\n",
      "step = 7150400: loss = 4.160536289215088\n",
      "step = 7150600: loss = 4.640235900878906\n",
      "step = 7150800: loss = 3.553736448287964\n",
      "step = 7151000: loss = 4.850821495056152\n",
      "step = 7151200: loss = 4.239174842834473\n",
      "step = 7151400: loss = 2.7176566123962402\n",
      "step = 7151600: loss = 5.138724327087402\n",
      "step = 7151800: loss = 3.9295661449432373\n",
      "step = 7152000: loss = 4.804430961608887\n",
      "step = 7152200: loss = 5.107945919036865\n",
      "step = 7152400: loss = 5.321303367614746\n",
      "step = 7152600: loss = 3.6961770057678223\n",
      "step = 7152800: loss = 4.327714920043945\n",
      "step = 7153000: loss = 3.711601972579956\n",
      "step = 7153200: loss = 4.235271453857422\n",
      "step = 7153400: loss = 3.8767004013061523\n",
      "step = 7153600: loss = 3.7734200954437256\n",
      "step = 7153800: loss = 5.035858154296875\n",
      "step = 7154000: loss = 4.318319797515869\n",
      "step = 7154200: loss = 3.7242302894592285\n",
      "step = 7154400: loss = 4.910891532897949\n",
      "step = 7154600: loss = 4.072915077209473\n",
      "step = 7154800: loss = 4.650432109832764\n",
      "step = 7155000: loss = 5.495636940002441\n",
      "step = 7155000: Average Return = 3.549999952316284\n",
      "step = 7155200: loss = 6.280273914337158\n",
      "step = 7155400: loss = 5.059727191925049\n",
      "step = 7155600: loss = 5.330489158630371\n",
      "step = 7155800: loss = 3.3665173053741455\n",
      "step = 7156000: loss = 3.729882001876831\n",
      "step = 7156200: loss = 3.8503406047821045\n",
      "step = 7156400: loss = 4.932638645172119\n",
      "step = 7156600: loss = 4.442244052886963\n",
      "step = 7156800: loss = 5.014793395996094\n",
      "step = 7157000: loss = 4.107816219329834\n",
      "step = 7157200: loss = 3.6589531898498535\n",
      "step = 7157400: loss = 4.4295854568481445\n",
      "step = 7157600: loss = 4.992083549499512\n",
      "step = 7157800: loss = 4.271996974945068\n",
      "step = 7158000: loss = 3.1237733364105225\n",
      "step = 7158200: loss = 2.8924083709716797\n",
      "step = 7158400: loss = 3.42327618598938\n",
      "step = 7158600: loss = 4.767202854156494\n",
      "step = 7158800: loss = 5.210445880889893\n",
      "step = 7159000: loss = 4.509603023529053\n",
      "step = 7159200: loss = 3.6881513595581055\n",
      "step = 7159400: loss = 4.022312641143799\n",
      "step = 7159600: loss = 3.767369031906128\n",
      "step = 7159800: loss = 4.2171454429626465\n",
      "step = 7160000: loss = 4.772233963012695\n",
      "step = 7160000: Average Return = 3.805999994277954\n",
      "step = 7160200: loss = 3.8873391151428223\n",
      "step = 7160400: loss = 2.2410600185394287\n",
      "step = 7160600: loss = 3.1671080589294434\n",
      "step = 7160800: loss = 4.072678089141846\n",
      "step = 7161000: loss = 4.272430419921875\n",
      "step = 7161200: loss = 4.20105504989624\n",
      "step = 7161400: loss = 3.872699499130249\n",
      "step = 7161600: loss = 3.544901132583618\n",
      "step = 7161800: loss = 4.267515659332275\n",
      "step = 7162000: loss = 3.935256004333496\n",
      "step = 7162200: loss = 5.06486177444458\n",
      "step = 7162400: loss = 3.240694999694824\n",
      "step = 7162600: loss = 3.617000102996826\n",
      "step = 7162800: loss = 3.39961576461792\n",
      "step = 7163000: loss = 6.383556842803955\n",
      "step = 7163200: loss = 3.5755562782287598\n",
      "step = 7163400: loss = 3.7440080642700195\n",
      "step = 7163600: loss = 6.121360778808594\n",
      "step = 7163800: loss = 6.400411128997803\n",
      "step = 7164000: loss = 3.690549850463867\n",
      "step = 7164200: loss = 4.876794338226318\n",
      "step = 7164400: loss = 4.143299579620361\n",
      "step = 7164600: loss = 2.4931581020355225\n",
      "step = 7164800: loss = 4.38299560546875\n",
      "step = 7165000: loss = 3.3733999729156494\n",
      "step = 7165000: Average Return = 3.7660000324249268\n",
      "step = 7165200: loss = 3.1914868354797363\n",
      "step = 7165400: loss = 3.855705976486206\n",
      "step = 7165600: loss = 3.56014347076416\n",
      "step = 7165800: loss = 4.666275501251221\n",
      "step = 7166000: loss = 3.7855491638183594\n",
      "step = 7166200: loss = 3.6804583072662354\n",
      "step = 7166400: loss = 2.9163308143615723\n",
      "step = 7166600: loss = 3.668272018432617\n",
      "step = 7166800: loss = 4.806473731994629\n",
      "step = 7167000: loss = 4.72202205657959\n",
      "step = 7167200: loss = 4.25148868560791\n",
      "step = 7167400: loss = 3.5697383880615234\n",
      "step = 7167600: loss = 4.86986780166626\n",
      "step = 7167800: loss = 5.115937232971191\n",
      "step = 7168000: loss = 5.36000919342041\n",
      "step = 7168200: loss = 3.8011040687561035\n",
      "step = 7168400: loss = 4.26115608215332\n",
      "step = 7168600: loss = 3.6525890827178955\n",
      "step = 7168800: loss = 4.999324321746826\n",
      "step = 7169000: loss = 3.9690911769866943\n",
      "step = 7169200: loss = 3.661466598510742\n",
      "step = 7169400: loss = 3.891907215118408\n",
      "step = 7169600: loss = 5.175506591796875\n",
      "step = 7169800: loss = 3.9754607677459717\n",
      "step = 7170000: loss = 4.910813331604004\n",
      "step = 7170000: Average Return = 3.375999927520752\n",
      "step = 7170200: loss = 4.794772148132324\n",
      "step = 7170400: loss = 3.8213133811950684\n",
      "step = 7170600: loss = 4.081576347351074\n",
      "step = 7170800: loss = 3.3241055011749268\n",
      "step = 7171000: loss = 4.447620868682861\n",
      "step = 7171200: loss = 3.9901249408721924\n",
      "step = 7171400: loss = 4.858479022979736\n",
      "step = 7171600: loss = 4.3550028800964355\n",
      "step = 7171800: loss = 3.7831411361694336\n",
      "step = 7172000: loss = 4.575479030609131\n",
      "step = 7172200: loss = 4.271575927734375\n",
      "step = 7172400: loss = 3.862342119216919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7172600: loss = 5.090634346008301\n",
      "step = 7172800: loss = 4.168609619140625\n",
      "step = 7173000: loss = 4.70703649520874\n",
      "step = 7173200: loss = 4.755272388458252\n",
      "step = 7173400: loss = 4.168472766876221\n",
      "step = 7173600: loss = 3.881523370742798\n",
      "step = 7173800: loss = 5.719793796539307\n",
      "step = 7174000: loss = 3.9296038150787354\n",
      "step = 7174200: loss = 4.041571140289307\n",
      "step = 7174400: loss = 4.21513032913208\n",
      "step = 7174600: loss = 4.693690299987793\n",
      "step = 7174800: loss = 3.2579565048217773\n",
      "step = 7175000: loss = 3.3997740745544434\n",
      "step = 7175000: Average Return = 3.619999885559082\n",
      "step = 7175200: loss = 4.53959846496582\n",
      "step = 7175400: loss = 2.4333925247192383\n",
      "step = 7175600: loss = 4.704176425933838\n",
      "step = 7175800: loss = 3.8587987422943115\n",
      "step = 7176000: loss = 4.504924774169922\n",
      "step = 7176200: loss = 3.3480653762817383\n",
      "step = 7176400: loss = 4.020325183868408\n",
      "step = 7176600: loss = 3.743381977081299\n",
      "step = 7176800: loss = 3.0447349548339844\n",
      "step = 7177000: loss = 4.060785293579102\n",
      "step = 7177200: loss = 4.589939594268799\n",
      "step = 7177400: loss = 3.6823620796203613\n",
      "step = 7177600: loss = 3.2426462173461914\n",
      "step = 7177800: loss = 5.417513370513916\n",
      "step = 7178000: loss = 4.19044828414917\n",
      "step = 7178200: loss = 4.121379375457764\n",
      "step = 7178400: loss = 3.8561201095581055\n",
      "step = 7178600: loss = 3.409227132797241\n",
      "step = 7178800: loss = 3.49568510055542\n",
      "step = 7179000: loss = 3.7671751976013184\n",
      "step = 7179200: loss = 3.0524303913116455\n",
      "step = 7179400: loss = 3.397226572036743\n",
      "step = 7179600: loss = 1.9295774698257446\n",
      "step = 7179800: loss = 4.0173563957214355\n",
      "step = 7180000: loss = 4.467457294464111\n",
      "step = 7180000: Average Return = 3.7079999446868896\n",
      "step = 7180200: loss = 4.7970709800720215\n",
      "step = 7180400: loss = 3.8571276664733887\n",
      "step = 7180600: loss = 4.880318641662598\n",
      "step = 7180800: loss = 4.939948081970215\n",
      "step = 7181000: loss = 4.827250003814697\n",
      "step = 7181200: loss = 5.324484825134277\n",
      "step = 7181400: loss = 5.331332683563232\n",
      "step = 7181600: loss = 3.76810359954834\n",
      "step = 7181800: loss = 3.1318140029907227\n",
      "step = 7182000: loss = 3.6524009704589844\n",
      "step = 7182200: loss = 5.077545166015625\n",
      "step = 7182400: loss = 4.984742641448975\n",
      "step = 7182600: loss = 3.427549362182617\n",
      "step = 7182800: loss = 5.331143856048584\n",
      "step = 7183000: loss = 3.269320011138916\n",
      "step = 7183200: loss = 6.097529888153076\n",
      "step = 7183400: loss = 4.691071510314941\n",
      "step = 7183600: loss = 5.09014892578125\n",
      "step = 7183800: loss = 3.9308974742889404\n",
      "step = 7184000: loss = 4.846381187438965\n",
      "step = 7184200: loss = 4.414492130279541\n",
      "step = 7184400: loss = 3.1726183891296387\n",
      "step = 7184600: loss = 4.659433364868164\n",
      "step = 7184800: loss = 3.5943877696990967\n",
      "step = 7185000: loss = 3.6883609294891357\n",
      "step = 7185000: Average Return = 3.941999912261963\n",
      "step = 7185200: loss = 5.061544895172119\n",
      "step = 7185400: loss = 4.675354957580566\n",
      "step = 7185600: loss = 4.305445671081543\n",
      "step = 7185800: loss = 4.283321380615234\n",
      "step = 7186000: loss = 5.204862117767334\n",
      "step = 7186200: loss = 5.57459831237793\n",
      "step = 7186400: loss = 4.85695743560791\n",
      "step = 7186600: loss = 4.025400161743164\n",
      "step = 7186800: loss = 3.915695905685425\n",
      "step = 7187000: loss = 4.318815231323242\n",
      "step = 7187200: loss = 4.271798133850098\n",
      "step = 7187400: loss = 3.9738543033599854\n",
      "step = 7187600: loss = 4.85909366607666\n",
      "step = 7187800: loss = 3.864229679107666\n",
      "step = 7188000: loss = 4.46551513671875\n",
      "step = 7188200: loss = 4.140666484832764\n",
      "step = 7188400: loss = 5.091174125671387\n",
      "step = 7188600: loss = 5.905727863311768\n",
      "step = 7188800: loss = 4.0433878898620605\n",
      "step = 7189000: loss = 4.284433841705322\n",
      "step = 7189200: loss = 4.232039928436279\n",
      "step = 7189400: loss = 4.854359149932861\n",
      "step = 7189600: loss = 3.7908499240875244\n",
      "step = 7189800: loss = 4.4721150398254395\n",
      "step = 7190000: loss = 4.46755838394165\n",
      "step = 7190000: Average Return = 3.7899999618530273\n",
      "step = 7190200: loss = 3.8590383529663086\n",
      "step = 7190400: loss = 4.049976825714111\n",
      "step = 7190600: loss = 4.881781101226807\n",
      "step = 7190800: loss = 4.482297420501709\n",
      "step = 7191000: loss = 6.176107883453369\n",
      "step = 7191200: loss = 4.40250301361084\n",
      "step = 7191400: loss = 2.4780995845794678\n",
      "step = 7191600: loss = 4.518529415130615\n",
      "step = 7191800: loss = 3.3443052768707275\n",
      "step = 7192000: loss = 3.958634614944458\n",
      "step = 7192200: loss = 4.143824577331543\n",
      "step = 7192400: loss = 3.0846588611602783\n",
      "step = 7192600: loss = 4.166713714599609\n",
      "step = 7192800: loss = 5.656573295593262\n",
      "step = 7193000: loss = 4.058692455291748\n",
      "step = 7193200: loss = 6.047577381134033\n",
      "step = 7193400: loss = 4.546741962432861\n",
      "step = 7193600: loss = 3.658520460128784\n",
      "step = 7193800: loss = 4.123605728149414\n",
      "step = 7194000: loss = 4.969698905944824\n",
      "step = 7194200: loss = 3.878382444381714\n",
      "step = 7194400: loss = 3.5605242252349854\n",
      "step = 7194600: loss = 3.9695699214935303\n",
      "step = 7194800: loss = 4.269540786743164\n",
      "step = 7195000: loss = 3.4462883472442627\n",
      "step = 7195000: Average Return = 3.7720000743865967\n",
      "step = 7195200: loss = 5.730349063873291\n",
      "step = 7195400: loss = 4.229360103607178\n",
      "step = 7195600: loss = 4.59104585647583\n",
      "step = 7195800: loss = 2.935006856918335\n",
      "step = 7196000: loss = 3.7446398735046387\n",
      "step = 7196200: loss = 4.469310760498047\n",
      "step = 7196400: loss = 3.9366424083709717\n",
      "step = 7196600: loss = 3.678914785385132\n",
      "step = 7196800: loss = 2.531367063522339\n",
      "step = 7197000: loss = 4.867030620574951\n",
      "step = 7197200: loss = 4.464290618896484\n",
      "step = 7197400: loss = 3.337491035461426\n",
      "step = 7197600: loss = 5.473074913024902\n",
      "step = 7197800: loss = 4.227287292480469\n",
      "step = 7198000: loss = 5.295235633850098\n",
      "step = 7198200: loss = 4.992371559143066\n",
      "step = 7198400: loss = 3.194289445877075\n",
      "step = 7198600: loss = 3.661226511001587\n",
      "step = 7198800: loss = 4.074456214904785\n",
      "step = 7199000: loss = 5.899019718170166\n",
      "step = 7199200: loss = 3.913909912109375\n",
      "step = 7199400: loss = 3.661996603012085\n",
      "step = 7199600: loss = 3.982790946960449\n",
      "step = 7199800: loss = 3.269718647003174\n",
      "step = 7200000: loss = 4.005427360534668\n",
      "step = 7200000: Average Return = 3.390000104904175\n",
      "step = 7200200: loss = 4.759588241577148\n",
      "step = 7200400: loss = 5.189583778381348\n",
      "step = 7200600: loss = 3.895569324493408\n",
      "step = 7200800: loss = 4.109030246734619\n",
      "step = 7201000: loss = 3.183427572250366\n",
      "step = 7201200: loss = 6.349254131317139\n",
      "step = 7201400: loss = 5.175166130065918\n",
      "step = 7201600: loss = 2.933527708053589\n",
      "step = 7201800: loss = 2.434842586517334\n",
      "step = 7202000: loss = 4.44450569152832\n",
      "step = 7202200: loss = 4.164048194885254\n",
      "step = 7202400: loss = 3.9029014110565186\n",
      "step = 7202600: loss = 4.471587181091309\n",
      "step = 7202800: loss = 3.4761857986450195\n",
      "step = 7203000: loss = 3.8382153511047363\n",
      "step = 7203200: loss = 5.497731685638428\n",
      "step = 7203400: loss = 3.783531427383423\n",
      "step = 7203600: loss = 3.586068868637085\n",
      "step = 7203800: loss = 3.6985960006713867\n",
      "step = 7204000: loss = 3.62319016456604\n",
      "step = 7204200: loss = 4.384210109710693\n",
      "step = 7204400: loss = 4.738360404968262\n",
      "step = 7204600: loss = 4.031177043914795\n",
      "step = 7204800: loss = 5.718530654907227\n",
      "step = 7205000: loss = 4.914339065551758\n",
      "step = 7205000: Average Return = 3.921999931335449\n",
      "step = 7205200: loss = 2.910330057144165\n",
      "step = 7205400: loss = 4.464484214782715\n",
      "step = 7205600: loss = 2.8791139125823975\n",
      "step = 7205800: loss = 5.429636001586914\n",
      "step = 7206000: loss = 5.177226543426514\n",
      "step = 7206200: loss = 4.680680274963379\n",
      "step = 7206400: loss = 4.549306392669678\n",
      "step = 7206600: loss = 4.400681018829346\n",
      "step = 7206800: loss = 5.490660190582275\n",
      "step = 7207000: loss = 3.9826884269714355\n",
      "step = 7207200: loss = 4.14211368560791\n",
      "step = 7207400: loss = 6.072424411773682\n",
      "step = 7207600: loss = 2.9643185138702393\n",
      "step = 7207800: loss = 4.875683784484863\n",
      "step = 7208000: loss = 4.588136672973633\n",
      "step = 7208200: loss = 3.5218162536621094\n",
      "step = 7208400: loss = 4.337546348571777\n",
      "step = 7208600: loss = 2.878261089324951\n",
      "step = 7208800: loss = 4.602107048034668\n",
      "step = 7209000: loss = 4.188545227050781\n",
      "step = 7209200: loss = 4.563888072967529\n",
      "step = 7209400: loss = 4.5533061027526855\n",
      "step = 7209600: loss = 4.7768168449401855\n",
      "step = 7209800: loss = 5.278293132781982\n",
      "step = 7210000: loss = 3.858987331390381\n",
      "step = 7210000: Average Return = 3.99399995803833\n",
      "step = 7210200: loss = 5.108521461486816\n",
      "step = 7210400: loss = 4.246744632720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7210600: loss = 5.3684515953063965\n",
      "step = 7210800: loss = 3.9266891479492188\n",
      "step = 7211000: loss = 1.755560278892517\n",
      "step = 7211200: loss = 4.284847736358643\n",
      "step = 7211400: loss = 4.49729585647583\n",
      "step = 7211600: loss = 4.614400386810303\n",
      "step = 7211800: loss = 4.005825042724609\n",
      "step = 7212000: loss = 4.282187461853027\n",
      "step = 7212200: loss = 3.6829609870910645\n",
      "step = 7212400: loss = 4.2693915367126465\n",
      "step = 7212600: loss = 5.387907028198242\n",
      "step = 7212800: loss = 4.768205642700195\n",
      "step = 7213000: loss = 3.4505083560943604\n",
      "step = 7213200: loss = 3.7996294498443604\n",
      "step = 7213400: loss = 4.11665678024292\n",
      "step = 7213600: loss = 5.704102516174316\n",
      "step = 7213800: loss = 4.308841228485107\n",
      "step = 7214000: loss = 3.8344783782958984\n",
      "step = 7214200: loss = 4.6983819007873535\n",
      "step = 7214400: loss = 3.8310062885284424\n",
      "step = 7214600: loss = 4.60064172744751\n",
      "step = 7214800: loss = 3.1255252361297607\n",
      "step = 7215000: loss = 4.634775161743164\n",
      "step = 7215000: Average Return = 3.638000011444092\n",
      "step = 7215200: loss = 5.459653377532959\n",
      "step = 7215400: loss = 3.854403495788574\n",
      "step = 7215600: loss = 5.21936559677124\n",
      "step = 7215800: loss = 5.259617328643799\n",
      "step = 7216000: loss = 3.6455681324005127\n",
      "step = 7216200: loss = 3.8414788246154785\n",
      "step = 7216400: loss = 3.6623244285583496\n",
      "step = 7216600: loss = 4.34633207321167\n",
      "step = 7216800: loss = 4.242600440979004\n",
      "step = 7217000: loss = 3.9325695037841797\n",
      "step = 7217200: loss = 3.685401439666748\n",
      "step = 7217400: loss = 3.5301578044891357\n",
      "step = 7217600: loss = 5.877777099609375\n",
      "step = 7217800: loss = 4.444770812988281\n",
      "step = 7218000: loss = 3.620638370513916\n",
      "step = 7218200: loss = 4.004451751708984\n",
      "step = 7218400: loss = 4.217227458953857\n",
      "step = 7218600: loss = 6.715485572814941\n",
      "step = 7218800: loss = 6.2455363273620605\n",
      "step = 7219000: loss = 4.808400630950928\n",
      "step = 7219200: loss = 3.104606866836548\n",
      "step = 7219400: loss = 3.0385375022888184\n",
      "step = 7219600: loss = 5.153865814208984\n",
      "step = 7219800: loss = 4.859217643737793\n",
      "step = 7220000: loss = 3.2837932109832764\n",
      "step = 7220000: Average Return = 3.9860000610351562\n",
      "step = 7220200: loss = 6.310174465179443\n",
      "step = 7220400: loss = 5.619260311126709\n",
      "step = 7220600: loss = 2.392514228820801\n",
      "step = 7220800: loss = 3.804870843887329\n",
      "step = 7221000: loss = 3.1517369747161865\n",
      "step = 7221200: loss = 4.047435760498047\n",
      "step = 7221400: loss = 3.9175801277160645\n",
      "step = 7221600: loss = 5.260307312011719\n",
      "step = 7221800: loss = 3.1386423110961914\n",
      "step = 7222000: loss = 5.224676609039307\n",
      "step = 7222200: loss = 3.4959356784820557\n",
      "step = 7222400: loss = 2.8393619060516357\n",
      "step = 7222600: loss = 4.17749547958374\n",
      "step = 7222800: loss = 4.92403507232666\n",
      "step = 7223000: loss = 4.082379341125488\n",
      "step = 7223200: loss = 4.025811672210693\n",
      "step = 7223400: loss = 4.198915481567383\n",
      "step = 7223600: loss = 4.584805011749268\n",
      "step = 7223800: loss = 4.025599956512451\n",
      "step = 7224000: loss = 5.023679256439209\n",
      "step = 7224200: loss = 5.991122722625732\n",
      "step = 7224400: loss = 3.536403179168701\n",
      "step = 7224600: loss = 3.7284388542175293\n",
      "step = 7224800: loss = 4.061911106109619\n",
      "step = 7225000: loss = 4.253976821899414\n",
      "step = 7225000: Average Return = 3.865999937057495\n",
      "step = 7225200: loss = 3.747068405151367\n",
      "step = 7225400: loss = 5.238785743713379\n",
      "step = 7225600: loss = 4.690724849700928\n",
      "step = 7225800: loss = 3.8881852626800537\n",
      "step = 7226000: loss = 5.086050987243652\n",
      "step = 7226200: loss = 3.2825746536254883\n",
      "step = 7226400: loss = 4.227619171142578\n",
      "step = 7226600: loss = 5.087305545806885\n",
      "step = 7226800: loss = 5.1776251792907715\n",
      "step = 7227000: loss = 4.361384868621826\n",
      "step = 7227200: loss = 4.669275283813477\n",
      "step = 7227400: loss = 4.163431644439697\n",
      "step = 7227600: loss = 3.8852062225341797\n",
      "step = 7227800: loss = 5.533388614654541\n",
      "step = 7228000: loss = 5.100747585296631\n",
      "step = 7228200: loss = 4.888906955718994\n",
      "step = 7228400: loss = 3.6840572357177734\n",
      "step = 7228600: loss = 4.387296199798584\n",
      "step = 7228800: loss = 3.581188201904297\n",
      "step = 7229000: loss = 3.5698182582855225\n",
      "step = 7229200: loss = 3.6540403366088867\n",
      "step = 7229400: loss = 3.841177225112915\n",
      "step = 7229600: loss = 4.285914421081543\n",
      "step = 7229800: loss = 4.125563144683838\n",
      "step = 7230000: loss = 4.165766716003418\n",
      "step = 7230000: Average Return = 3.934000015258789\n",
      "step = 7230200: loss = 4.55443811416626\n",
      "step = 7230400: loss = 4.350155353546143\n",
      "step = 7230600: loss = 3.91337251663208\n",
      "step = 7230800: loss = 3.7767601013183594\n",
      "step = 7231000: loss = 5.428887367248535\n",
      "step = 7231200: loss = 4.354544639587402\n",
      "step = 7231400: loss = 4.955653667449951\n",
      "step = 7231600: loss = 4.213436603546143\n",
      "step = 7231800: loss = 5.357011795043945\n",
      "step = 7232000: loss = 3.7687394618988037\n",
      "step = 7232200: loss = 3.442800283432007\n",
      "step = 7232400: loss = 3.2252414226531982\n",
      "step = 7232600: loss = 4.0268754959106445\n",
      "step = 7232800: loss = 4.020622253417969\n",
      "step = 7233000: loss = 3.371856689453125\n",
      "step = 7233200: loss = 3.2470712661743164\n",
      "step = 7233400: loss = 5.464754581451416\n",
      "step = 7233600: loss = 4.97321081161499\n",
      "step = 7233800: loss = 4.688814163208008\n",
      "step = 7234000: loss = 5.09884786605835\n",
      "step = 7234200: loss = 3.4339146614074707\n",
      "step = 7234400: loss = 3.0992162227630615\n",
      "step = 7234600: loss = 3.702188014984131\n",
      "step = 7234800: loss = 4.023397445678711\n",
      "step = 7235000: loss = 4.543998718261719\n",
      "step = 7235000: Average Return = 3.9179999828338623\n",
      "step = 7235200: loss = 5.169029235839844\n",
      "step = 7235400: loss = 4.390620231628418\n",
      "step = 7235600: loss = 4.048342704772949\n",
      "step = 7235800: loss = 4.6562323570251465\n",
      "step = 7236000: loss = 3.8499364852905273\n",
      "step = 7236200: loss = 5.315043926239014\n",
      "step = 7236400: loss = 3.679130792617798\n",
      "step = 7236600: loss = 4.205192565917969\n",
      "step = 7236800: loss = 4.842785835266113\n",
      "step = 7237000: loss = 4.562012195587158\n",
      "step = 7237200: loss = 3.481091260910034\n",
      "step = 7237400: loss = 3.8023290634155273\n",
      "step = 7237600: loss = 4.067243576049805\n",
      "step = 7237800: loss = 4.126151084899902\n",
      "step = 7238000: loss = 5.8158464431762695\n",
      "step = 7238200: loss = 4.760138034820557\n",
      "step = 7238400: loss = 5.22496223449707\n",
      "step = 7238600: loss = 3.4838104248046875\n",
      "step = 7238800: loss = 4.2730607986450195\n",
      "step = 7239000: loss = 5.391002178192139\n",
      "step = 7239200: loss = 5.439572811126709\n",
      "step = 7239400: loss = 3.4785571098327637\n",
      "step = 7239600: loss = 3.065828800201416\n",
      "step = 7239800: loss = 5.0501251220703125\n",
      "step = 7240000: loss = 2.9826462268829346\n",
      "step = 7240000: Average Return = 4.113999843597412\n",
      "step = 7240200: loss = 3.19368314743042\n",
      "step = 7240400: loss = 3.8911404609680176\n",
      "step = 7240600: loss = 4.596380710601807\n",
      "step = 7240800: loss = 3.7275609970092773\n",
      "step = 7241000: loss = 3.353269577026367\n",
      "step = 7241200: loss = 4.483976364135742\n",
      "step = 7241400: loss = 4.490703582763672\n",
      "step = 7241600: loss = 4.511098384857178\n",
      "step = 7241800: loss = 3.104665517807007\n",
      "step = 7242000: loss = 4.593452453613281\n",
      "step = 7242200: loss = 3.951125144958496\n",
      "step = 7242400: loss = 4.376349925994873\n",
      "step = 7242600: loss = 3.9783775806427\n",
      "step = 7242800: loss = 4.290356159210205\n",
      "step = 7243000: loss = 4.704117298126221\n",
      "step = 7243200: loss = 3.9096903800964355\n",
      "step = 7243400: loss = 4.299276828765869\n",
      "step = 7243600: loss = 3.296359062194824\n",
      "step = 7243800: loss = 2.8875110149383545\n",
      "step = 7244000: loss = 3.9057631492614746\n",
      "step = 7244200: loss = 4.111840724945068\n",
      "step = 7244400: loss = 5.169713497161865\n",
      "step = 7244600: loss = 3.2821567058563232\n",
      "step = 7244800: loss = 3.4063851833343506\n",
      "step = 7245000: loss = 5.410315036773682\n",
      "step = 7245000: Average Return = 4.044000148773193\n",
      "step = 7245200: loss = 3.130152940750122\n",
      "step = 7245400: loss = 3.9026548862457275\n",
      "step = 7245600: loss = 4.184792518615723\n",
      "step = 7245800: loss = 4.34700345993042\n",
      "step = 7246000: loss = 4.3006181716918945\n",
      "step = 7246200: loss = 3.8381974697113037\n",
      "step = 7246400: loss = 3.957913637161255\n",
      "step = 7246600: loss = 4.888213157653809\n",
      "step = 7246800: loss = 3.5708258152008057\n",
      "step = 7247000: loss = 3.2175893783569336\n",
      "step = 7247200: loss = 3.9181950092315674\n",
      "step = 7247400: loss = 4.129085063934326\n",
      "step = 7247600: loss = 5.209371089935303\n",
      "step = 7247800: loss = 3.7007741928100586\n",
      "step = 7248000: loss = 4.425105571746826\n",
      "step = 7248200: loss = 3.916907787322998\n",
      "step = 7248400: loss = 4.070186614990234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7248600: loss = 4.049501419067383\n",
      "step = 7248800: loss = 4.020728588104248\n",
      "step = 7249000: loss = 3.0495445728302\n",
      "step = 7249200: loss = 4.155575275421143\n",
      "step = 7249400: loss = 3.619490385055542\n",
      "step = 7249600: loss = 3.1318583488464355\n",
      "step = 7249800: loss = 4.375550270080566\n",
      "step = 7250000: loss = 3.1955718994140625\n",
      "step = 7250000: Average Return = 3.9679999351501465\n",
      "step = 7250200: loss = 4.743757724761963\n",
      "step = 7250400: loss = 5.399250507354736\n",
      "step = 7250600: loss = 4.8735785484313965\n",
      "step = 7250800: loss = 5.92165994644165\n",
      "step = 7251000: loss = 2.6979594230651855\n",
      "step = 7251200: loss = 4.514686107635498\n",
      "step = 7251400: loss = 2.9655680656433105\n",
      "step = 7251600: loss = 3.2178893089294434\n",
      "step = 7251800: loss = 5.78584098815918\n",
      "step = 7252000: loss = 4.494884967803955\n",
      "step = 7252200: loss = 3.648378849029541\n",
      "step = 7252400: loss = 3.941152334213257\n",
      "step = 7252600: loss = 4.447263240814209\n",
      "step = 7252800: loss = 3.906331777572632\n",
      "step = 7253000: loss = 3.8768749237060547\n",
      "step = 7253200: loss = 4.891548156738281\n",
      "step = 7253400: loss = 4.202959060668945\n",
      "step = 7253600: loss = 5.502548694610596\n",
      "step = 7253800: loss = 4.283596038818359\n",
      "step = 7254000: loss = 5.187822341918945\n",
      "step = 7254200: loss = 3.762150287628174\n",
      "step = 7254400: loss = 6.339776992797852\n",
      "step = 7254600: loss = 4.444541931152344\n",
      "step = 7254800: loss = 4.3790178298950195\n",
      "step = 7255000: loss = 5.349973201751709\n",
      "step = 7255000: Average Return = 3.7139999866485596\n",
      "step = 7255200: loss = 4.244289398193359\n",
      "step = 7255400: loss = 3.907871723175049\n",
      "step = 7255600: loss = 3.326237678527832\n",
      "step = 7255800: loss = 4.529464244842529\n",
      "step = 7256000: loss = 5.028655052185059\n",
      "step = 7256200: loss = 4.53985071182251\n",
      "step = 7256400: loss = 5.090725421905518\n",
      "step = 7256600: loss = 2.827244758605957\n",
      "step = 7256800: loss = 5.062625408172607\n",
      "step = 7257000: loss = 4.947088718414307\n",
      "step = 7257200: loss = 3.453343391418457\n",
      "step = 7257400: loss = 4.082045078277588\n",
      "step = 7257600: loss = 4.004461288452148\n",
      "step = 7257800: loss = 3.7862603664398193\n",
      "step = 7258000: loss = 3.863947629928589\n",
      "step = 7258200: loss = 3.6700334548950195\n",
      "step = 7258400: loss = 4.157193660736084\n",
      "step = 7258600: loss = 3.789091110229492\n",
      "step = 7258800: loss = 3.6632158756256104\n",
      "step = 7259000: loss = 4.785222053527832\n",
      "step = 7259200: loss = 4.1060333251953125\n",
      "step = 7259400: loss = 4.099746227264404\n",
      "step = 7259600: loss = 4.33494234085083\n",
      "step = 7259800: loss = 3.9856255054473877\n",
      "step = 7260000: loss = 5.992926597595215\n",
      "step = 7260000: Average Return = 3.9079999923706055\n",
      "step = 7260200: loss = 3.308098077774048\n",
      "step = 7260400: loss = 3.5098185539245605\n",
      "step = 7260600: loss = 4.323326110839844\n",
      "step = 7260800: loss = 3.8801109790802\n",
      "step = 7261000: loss = 5.085792541503906\n",
      "step = 7261200: loss = 3.8383021354675293\n",
      "step = 7261400: loss = 4.274425983428955\n",
      "step = 7261600: loss = 4.674379348754883\n",
      "step = 7261800: loss = 4.347854137420654\n",
      "step = 7262000: loss = 4.561581134796143\n",
      "step = 7262200: loss = 3.64204740524292\n",
      "step = 7262400: loss = 3.670755624771118\n",
      "step = 7262600: loss = 3.5669808387756348\n",
      "step = 7262800: loss = 3.968559503555298\n",
      "step = 7263000: loss = 3.856757164001465\n",
      "step = 7263200: loss = 5.18110466003418\n",
      "step = 7263400: loss = 4.289544105529785\n",
      "step = 7263600: loss = 3.935089111328125\n",
      "step = 7263800: loss = 3.9460079669952393\n",
      "step = 7264000: loss = 3.321263313293457\n",
      "step = 7264200: loss = 4.594449520111084\n",
      "step = 7264400: loss = 5.300229549407959\n",
      "step = 7264600: loss = 4.227430820465088\n",
      "step = 7264800: loss = 4.339933395385742\n",
      "step = 7265000: loss = 4.133058547973633\n",
      "step = 7265000: Average Return = 3.8359999656677246\n",
      "step = 7265200: loss = 3.858126163482666\n",
      "step = 7265400: loss = 3.8137688636779785\n",
      "step = 7265600: loss = 3.6731460094451904\n",
      "step = 7265800: loss = 2.5715203285217285\n",
      "step = 7266000: loss = 5.091991901397705\n",
      "step = 7266200: loss = 4.862340450286865\n",
      "step = 7266400: loss = 3.475454568862915\n",
      "step = 7266600: loss = 4.212249279022217\n",
      "step = 7266800: loss = 4.370140075683594\n",
      "step = 7267000: loss = 3.543201208114624\n",
      "step = 7267200: loss = 3.32670521736145\n",
      "step = 7267400: loss = 4.0451788902282715\n",
      "step = 7267600: loss = 3.5519065856933594\n",
      "step = 7267800: loss = 4.246337890625\n",
      "step = 7268000: loss = 4.455944538116455\n",
      "step = 7268200: loss = 3.6057729721069336\n",
      "step = 7268400: loss = 5.85715389251709\n",
      "step = 7268600: loss = 4.833426475524902\n",
      "step = 7268800: loss = 2.5484471321105957\n",
      "step = 7269000: loss = 4.849554538726807\n",
      "step = 7269200: loss = 3.3450987339019775\n",
      "step = 7269400: loss = 3.3722686767578125\n",
      "step = 7269600: loss = 5.4436516761779785\n",
      "step = 7269800: loss = 3.9203667640686035\n",
      "step = 7270000: loss = 5.527496814727783\n",
      "step = 7270000: Average Return = 3.7200000286102295\n",
      "step = 7270200: loss = 3.3961479663848877\n",
      "step = 7270400: loss = 4.686351299285889\n",
      "step = 7270600: loss = 4.005671501159668\n",
      "step = 7270800: loss = 2.9779608249664307\n",
      "step = 7271000: loss = 5.52981424331665\n",
      "step = 7271200: loss = 3.8558437824249268\n",
      "step = 7271400: loss = 3.7129018306732178\n",
      "step = 7271600: loss = 4.755048751831055\n",
      "step = 7271800: loss = 4.7661871910095215\n",
      "step = 7272000: loss = 4.136119842529297\n",
      "step = 7272200: loss = 4.536977291107178\n",
      "step = 7272400: loss = 4.772352695465088\n",
      "step = 7272600: loss = 5.530402660369873\n",
      "step = 7272800: loss = 4.890927791595459\n",
      "step = 7273000: loss = 4.80765438079834\n",
      "step = 7273200: loss = 4.2799763679504395\n",
      "step = 7273400: loss = 4.533207416534424\n",
      "step = 7273600: loss = 4.651655197143555\n",
      "step = 7273800: loss = 3.372042179107666\n",
      "step = 7274000: loss = 3.7983462810516357\n",
      "step = 7274200: loss = 3.8035619258880615\n",
      "step = 7274400: loss = 4.100497722625732\n",
      "step = 7274600: loss = 4.489058971405029\n",
      "step = 7274800: loss = 4.045228958129883\n",
      "step = 7275000: loss = 3.698967695236206\n",
      "step = 7275000: Average Return = 3.4779999256134033\n",
      "step = 7275200: loss = 3.1743974685668945\n",
      "step = 7275400: loss = 6.391701698303223\n",
      "step = 7275600: loss = 4.372337341308594\n",
      "step = 7275800: loss = 4.187202453613281\n",
      "step = 7276000: loss = 5.438642501831055\n",
      "step = 7276200: loss = 4.1749491691589355\n",
      "step = 7276400: loss = 4.0706281661987305\n",
      "step = 7276600: loss = 5.202273845672607\n",
      "step = 7276800: loss = 3.4954018592834473\n",
      "step = 7277000: loss = 4.267667293548584\n",
      "step = 7277200: loss = 3.7960970401763916\n",
      "step = 7277400: loss = 4.291464805603027\n",
      "step = 7277600: loss = 3.395411968231201\n",
      "step = 7277800: loss = 4.24319314956665\n",
      "step = 7278000: loss = 5.250017166137695\n",
      "step = 7278200: loss = 3.7893590927124023\n",
      "step = 7278400: loss = 4.003325939178467\n",
      "step = 7278600: loss = 3.6225132942199707\n",
      "step = 7278800: loss = 3.55026912689209\n",
      "step = 7279000: loss = 4.921996116638184\n",
      "step = 7279200: loss = 4.096254348754883\n",
      "step = 7279400: loss = 4.864774227142334\n",
      "step = 7279600: loss = 5.43021821975708\n",
      "step = 7279800: loss = 4.173649311065674\n",
      "step = 7280000: loss = 5.293269157409668\n",
      "step = 7280000: Average Return = 3.4240000247955322\n",
      "step = 7280200: loss = 3.9560179710388184\n",
      "step = 7280400: loss = 3.850578784942627\n",
      "step = 7280600: loss = 4.2578325271606445\n",
      "step = 7280800: loss = 4.887057781219482\n",
      "step = 7281000: loss = 4.354559898376465\n",
      "step = 7281200: loss = 3.752852201461792\n",
      "step = 7281400: loss = 5.722375392913818\n",
      "step = 7281600: loss = 3.1286864280700684\n",
      "step = 7281800: loss = 6.040186405181885\n",
      "step = 7282000: loss = 4.177994251251221\n",
      "step = 7282200: loss = 4.622681140899658\n",
      "step = 7282400: loss = 4.667835712432861\n",
      "step = 7282600: loss = 2.8694310188293457\n",
      "step = 7282800: loss = 4.295889854431152\n",
      "step = 7283000: loss = 4.2052321434021\n",
      "step = 7283200: loss = 3.5413668155670166\n",
      "step = 7283400: loss = 5.075009822845459\n",
      "step = 7283600: loss = 4.247779846191406\n",
      "step = 7283800: loss = 3.5225746631622314\n",
      "step = 7284000: loss = 4.953234672546387\n",
      "step = 7284200: loss = 4.48732852935791\n",
      "step = 7284400: loss = 3.964167356491089\n",
      "step = 7284600: loss = 3.427199125289917\n",
      "step = 7284800: loss = 4.939438343048096\n",
      "step = 7285000: loss = 5.03507137298584\n",
      "step = 7285000: Average Return = 3.7100000381469727\n",
      "step = 7285200: loss = 3.3425307273864746\n",
      "step = 7285400: loss = 3.516490936279297\n",
      "step = 7285600: loss = 4.1652984619140625\n",
      "step = 7285800: loss = 4.428040027618408\n",
      "step = 7286000: loss = 4.7136664390563965\n",
      "step = 7286200: loss = 3.6010119915008545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7286400: loss = 4.557530879974365\n",
      "step = 7286600: loss = 4.994898796081543\n",
      "step = 7286800: loss = 4.043516159057617\n",
      "step = 7287000: loss = 3.4445128440856934\n",
      "step = 7287200: loss = 3.4117205142974854\n",
      "step = 7287400: loss = 4.634140968322754\n",
      "step = 7287600: loss = 3.6141197681427\n",
      "step = 7287800: loss = 4.524899005889893\n",
      "step = 7288000: loss = 3.8877716064453125\n",
      "step = 7288200: loss = 3.5896360874176025\n",
      "step = 7288400: loss = 3.631503105163574\n",
      "step = 7288600: loss = 3.7403526306152344\n",
      "step = 7288800: loss = 4.015655517578125\n",
      "step = 7289000: loss = 3.733030319213867\n",
      "step = 7289200: loss = 4.84677791595459\n",
      "step = 7289400: loss = 4.200585842132568\n",
      "step = 7289600: loss = 3.7229058742523193\n",
      "step = 7289800: loss = 4.4350128173828125\n",
      "step = 7290000: loss = 4.155377388000488\n",
      "step = 7290000: Average Return = 4.0320000648498535\n",
      "step = 7290200: loss = 4.0422797203063965\n",
      "step = 7290400: loss = 3.296034097671509\n",
      "step = 7290600: loss = 4.872280597686768\n",
      "step = 7290800: loss = 4.141878128051758\n",
      "step = 7291000: loss = 3.09542179107666\n",
      "step = 7291200: loss = 3.344097137451172\n",
      "step = 7291400: loss = 3.685915470123291\n",
      "step = 7291600: loss = 3.878446578979492\n",
      "step = 7291800: loss = 5.251474380493164\n",
      "step = 7292000: loss = 4.164163112640381\n",
      "step = 7292200: loss = 3.8776981830596924\n",
      "step = 7292400: loss = 4.345593452453613\n",
      "step = 7292600: loss = 5.434481143951416\n",
      "step = 7292800: loss = 2.211446523666382\n",
      "step = 7293000: loss = 4.726710319519043\n",
      "step = 7293200: loss = 4.572778701782227\n",
      "step = 7293400: loss = 6.668954849243164\n",
      "step = 7293600: loss = 3.6526615619659424\n",
      "step = 7293800: loss = 4.5243659019470215\n",
      "step = 7294000: loss = 4.051796913146973\n",
      "step = 7294200: loss = 5.16091775894165\n",
      "step = 7294400: loss = 3.7889513969421387\n",
      "step = 7294600: loss = 4.924241065979004\n",
      "step = 7294800: loss = 4.8553924560546875\n",
      "step = 7295000: loss = 3.3275444507598877\n",
      "step = 7295000: Average Return = 4.349999904632568\n",
      "step = 7295200: loss = 5.198032855987549\n",
      "step = 7295400: loss = 3.857306718826294\n",
      "step = 7295600: loss = 3.346783399581909\n",
      "step = 7295800: loss = 4.818724155426025\n",
      "step = 7296000: loss = 4.327391147613525\n",
      "step = 7296200: loss = 4.720320224761963\n",
      "step = 7296400: loss = 4.689115524291992\n",
      "step = 7296600: loss = 5.043287754058838\n",
      "step = 7296800: loss = 4.610897541046143\n",
      "step = 7297000: loss = 6.102824687957764\n",
      "step = 7297200: loss = 4.802677631378174\n",
      "step = 7297400: loss = 3.7541911602020264\n",
      "step = 7297600: loss = 5.646618366241455\n",
      "step = 7297800: loss = 4.71339225769043\n",
      "step = 7298000: loss = 3.6374244689941406\n",
      "step = 7298200: loss = 5.311661243438721\n",
      "step = 7298400: loss = 5.145053386688232\n",
      "step = 7298600: loss = 4.81837797164917\n",
      "step = 7298800: loss = 4.018399238586426\n",
      "step = 7299000: loss = 4.939626216888428\n",
      "step = 7299200: loss = 5.824843406677246\n",
      "step = 7299400: loss = 4.42048454284668\n",
      "step = 7299600: loss = 3.673518657684326\n",
      "step = 7299800: loss = 4.46744966506958\n",
      "step = 7300000: loss = 4.940345764160156\n",
      "step = 7300000: Average Return = 3.7360000610351562\n",
      "step = 7300200: loss = 3.3359837532043457\n",
      "step = 7300400: loss = 3.427785634994507\n",
      "step = 7300600: loss = 3.623746633529663\n",
      "step = 7300800: loss = 2.815326690673828\n",
      "step = 7301000: loss = 4.2691497802734375\n",
      "step = 7301200: loss = 4.5801520347595215\n",
      "step = 7301400: loss = 5.273923397064209\n",
      "step = 7301600: loss = 5.30168342590332\n",
      "step = 7301800: loss = 3.567267656326294\n",
      "step = 7302000: loss = 4.766471862792969\n",
      "step = 7302200: loss = 4.522332191467285\n",
      "step = 7302400: loss = 5.736683368682861\n",
      "step = 7302600: loss = 5.119876861572266\n",
      "step = 7302800: loss = 5.0096940994262695\n",
      "step = 7303000: loss = 5.537661075592041\n",
      "step = 7303200: loss = 3.5524730682373047\n",
      "step = 7303400: loss = 5.936935901641846\n",
      "step = 7303600: loss = 5.72655725479126\n",
      "step = 7303800: loss = 4.391785621643066\n",
      "step = 7304000: loss = 4.659635543823242\n",
      "step = 7304200: loss = 2.985595941543579\n",
      "step = 7304400: loss = 4.294732570648193\n",
      "step = 7304600: loss = 2.5870518684387207\n",
      "step = 7304800: loss = 5.563819408416748\n",
      "step = 7305000: loss = 3.1616320610046387\n",
      "step = 7305000: Average Return = 3.7119998931884766\n",
      "step = 7305200: loss = 3.8105404376983643\n",
      "step = 7305400: loss = 3.459278106689453\n",
      "step = 7305600: loss = 4.342486381530762\n",
      "step = 7305800: loss = 3.6976847648620605\n",
      "step = 7306000: loss = 5.211857795715332\n",
      "step = 7306200: loss = 5.015082359313965\n",
      "step = 7306400: loss = 6.616602420806885\n",
      "step = 7306600: loss = 4.211004734039307\n",
      "step = 7306800: loss = 4.034755229949951\n",
      "step = 7307000: loss = 3.3909497261047363\n",
      "step = 7307200: loss = 4.324935436248779\n",
      "step = 7307400: loss = 4.540194988250732\n",
      "step = 7307600: loss = 3.317387104034424\n",
      "step = 7307800: loss = 4.902087211608887\n",
      "step = 7308000: loss = 4.20849609375\n",
      "step = 7308200: loss = 2.8859753608703613\n",
      "step = 7308400: loss = 4.527639865875244\n",
      "step = 7308600: loss = 4.246987342834473\n",
      "step = 7308800: loss = 5.147886276245117\n",
      "step = 7309000: loss = 4.870418548583984\n",
      "step = 7309200: loss = 3.4270689487457275\n",
      "step = 7309400: loss = 4.204709529876709\n",
      "step = 7309600: loss = 4.091551303863525\n",
      "step = 7309800: loss = 4.530904293060303\n",
      "step = 7310000: loss = 4.238395690917969\n",
      "step = 7310000: Average Return = 3.8540000915527344\n",
      "step = 7310200: loss = 3.8250865936279297\n",
      "step = 7310400: loss = 4.997372150421143\n",
      "step = 7310600: loss = 3.6872615814208984\n",
      "step = 7310800: loss = 4.561923503875732\n",
      "step = 7311000: loss = 5.57185173034668\n",
      "step = 7311200: loss = 4.6775102615356445\n",
      "step = 7311400: loss = 3.5194499492645264\n",
      "step = 7311600: loss = 3.3470709323883057\n",
      "step = 7311800: loss = 4.005157470703125\n",
      "step = 7312000: loss = 5.084156513214111\n",
      "step = 7312200: loss = 4.487630367279053\n",
      "step = 7312400: loss = 5.194954872131348\n",
      "step = 7312600: loss = 4.6083903312683105\n",
      "step = 7312800: loss = 3.8781468868255615\n",
      "step = 7313000: loss = 5.215527057647705\n",
      "step = 7313200: loss = 3.784416913986206\n",
      "step = 7313400: loss = 4.341525554656982\n",
      "step = 7313600: loss = 2.9043524265289307\n",
      "step = 7313800: loss = 2.7559666633605957\n",
      "step = 7314000: loss = 4.959442615509033\n",
      "step = 7314200: loss = 5.644669532775879\n",
      "step = 7314400: loss = 3.5263960361480713\n",
      "step = 7314600: loss = 4.477821350097656\n",
      "step = 7314800: loss = 4.119426727294922\n",
      "step = 7315000: loss = 5.044131755828857\n",
      "step = 7315000: Average Return = 4.079999923706055\n",
      "step = 7315200: loss = 3.3991146087646484\n",
      "step = 7315400: loss = 4.39024019241333\n",
      "step = 7315600: loss = 3.824632406234741\n",
      "step = 7315800: loss = 4.739307403564453\n",
      "step = 7316000: loss = 3.155651092529297\n",
      "step = 7316200: loss = 3.5770103931427\n",
      "step = 7316400: loss = 4.527641296386719\n",
      "step = 7316600: loss = 2.736661911010742\n",
      "step = 7316800: loss = 3.0059454441070557\n",
      "step = 7317000: loss = 3.8681554794311523\n",
      "step = 7317200: loss = 5.441800594329834\n",
      "step = 7317400: loss = 5.747917175292969\n",
      "step = 7317600: loss = 5.529368877410889\n",
      "step = 7317800: loss = 4.146183490753174\n",
      "step = 7318000: loss = 4.036414623260498\n",
      "step = 7318200: loss = 5.227481365203857\n",
      "step = 7318400: loss = 3.922863721847534\n",
      "step = 7318600: loss = 5.2336883544921875\n",
      "step = 7318800: loss = 5.010634422302246\n",
      "step = 7319000: loss = 4.271679401397705\n",
      "step = 7319200: loss = 3.8844282627105713\n",
      "step = 7319400: loss = 3.2722549438476562\n",
      "step = 7319600: loss = 3.107483148574829\n",
      "step = 7319800: loss = 4.433675289154053\n",
      "step = 7320000: loss = 4.55729341506958\n",
      "step = 7320000: Average Return = 4.03000020980835\n",
      "step = 7320200: loss = 4.17301607131958\n",
      "step = 7320400: loss = 4.4778971672058105\n",
      "step = 7320600: loss = 3.7547521591186523\n",
      "step = 7320800: loss = 3.7205941677093506\n",
      "step = 7321000: loss = 3.4702377319335938\n",
      "step = 7321200: loss = 4.58296537399292\n",
      "step = 7321400: loss = 5.4014892578125\n",
      "step = 7321600: loss = 3.5638585090637207\n",
      "step = 7321800: loss = 5.163638591766357\n",
      "step = 7322000: loss = 3.952545404434204\n",
      "step = 7322200: loss = 4.4383769035339355\n",
      "step = 7322400: loss = 2.944066047668457\n",
      "step = 7322600: loss = 3.5461065769195557\n",
      "step = 7322800: loss = 4.546313762664795\n",
      "step = 7323000: loss = 3.4037129878997803\n",
      "step = 7323200: loss = 4.47076416015625\n",
      "step = 7323400: loss = 3.046032667160034\n",
      "step = 7323600: loss = 4.1811981201171875\n",
      "step = 7323800: loss = 5.290046691894531\n",
      "step = 7324000: loss = 3.7427003383636475\n",
      "step = 7324200: loss = 4.6219048500061035\n",
      "step = 7324400: loss = 4.7793869972229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7324600: loss = 3.0683703422546387\n",
      "step = 7324800: loss = 3.9279675483703613\n",
      "step = 7325000: loss = 3.9634742736816406\n",
      "step = 7325000: Average Return = 3.885999917984009\n",
      "step = 7325200: loss = 4.459737300872803\n",
      "step = 7325400: loss = 5.376370906829834\n",
      "step = 7325600: loss = 4.082558631896973\n",
      "step = 7325800: loss = 3.2614104747772217\n",
      "step = 7326000: loss = 2.783093214035034\n",
      "step = 7326200: loss = 5.494791030883789\n",
      "step = 7326400: loss = 3.1322057247161865\n",
      "step = 7326600: loss = 3.806847095489502\n",
      "step = 7326800: loss = 4.696680068969727\n",
      "step = 7327000: loss = 6.447519779205322\n",
      "step = 7327200: loss = 3.7235937118530273\n",
      "step = 7327400: loss = 3.3606958389282227\n",
      "step = 7327600: loss = 3.4020042419433594\n",
      "step = 7327800: loss = 5.709150314331055\n",
      "step = 7328000: loss = 4.769197940826416\n",
      "step = 7328200: loss = 4.519984722137451\n",
      "step = 7328400: loss = 3.595623016357422\n",
      "step = 7328600: loss = 3.9450371265411377\n",
      "step = 7328800: loss = 3.1433677673339844\n",
      "step = 7329000: loss = 4.89239501953125\n",
      "step = 7329200: loss = 4.029460430145264\n",
      "step = 7329400: loss = 3.1116511821746826\n",
      "step = 7329600: loss = 5.203908443450928\n",
      "step = 7329800: loss = 3.9661924839019775\n",
      "step = 7330000: loss = 3.483612060546875\n",
      "step = 7330000: Average Return = 4.1020002365112305\n",
      "step = 7330200: loss = 4.634319305419922\n",
      "step = 7330400: loss = 5.460291862487793\n",
      "step = 7330600: loss = 3.712527275085449\n",
      "step = 7330800: loss = 4.211585521697998\n",
      "step = 7331000: loss = 4.529611587524414\n",
      "step = 7331200: loss = 5.783348083496094\n",
      "step = 7331400: loss = 4.0388689041137695\n",
      "step = 7331600: loss = 3.1422135829925537\n",
      "step = 7331800: loss = 4.362804889678955\n",
      "step = 7332000: loss = 4.100494384765625\n",
      "step = 7332200: loss = 3.516225576400757\n",
      "step = 7332400: loss = 3.596853494644165\n",
      "step = 7332600: loss = 4.780778884887695\n",
      "step = 7332800: loss = 5.300835609436035\n",
      "step = 7333000: loss = 4.54936408996582\n",
      "step = 7333200: loss = 4.708780288696289\n",
      "step = 7333400: loss = 4.8879618644714355\n",
      "step = 7333600: loss = 5.332072734832764\n",
      "step = 7333800: loss = 3.3794472217559814\n",
      "step = 7334000: loss = 4.565447807312012\n",
      "step = 7334200: loss = 4.176167964935303\n",
      "step = 7334400: loss = 3.6554691791534424\n",
      "step = 7334600: loss = 4.414625644683838\n",
      "step = 7334800: loss = 5.19541597366333\n",
      "step = 7335000: loss = 3.9557995796203613\n",
      "step = 7335000: Average Return = 3.444000005722046\n",
      "step = 7335200: loss = 4.278601169586182\n",
      "step = 7335400: loss = 3.4129555225372314\n",
      "step = 7335600: loss = 3.964165449142456\n",
      "step = 7335800: loss = 3.108229875564575\n",
      "step = 7336000: loss = 3.8183910846710205\n",
      "step = 7336200: loss = 5.2101731300354\n",
      "step = 7336400: loss = 2.851386070251465\n",
      "step = 7336600: loss = 3.490483283996582\n",
      "step = 7336800: loss = 4.044994354248047\n",
      "step = 7337000: loss = 3.373725414276123\n",
      "step = 7337200: loss = 4.3712263107299805\n",
      "step = 7337400: loss = 3.573646068572998\n",
      "step = 7337600: loss = 4.652656555175781\n",
      "step = 7337800: loss = 4.1284332275390625\n",
      "step = 7338000: loss = 2.663684844970703\n",
      "step = 7338200: loss = 2.664261817932129\n",
      "step = 7338400: loss = 4.003108024597168\n",
      "step = 7338600: loss = 4.443995952606201\n",
      "step = 7338800: loss = 3.353677988052368\n",
      "step = 7339000: loss = 4.842854976654053\n",
      "step = 7339200: loss = 3.595982551574707\n",
      "step = 7339400: loss = 4.844130039215088\n",
      "step = 7339600: loss = 3.320164680480957\n",
      "step = 7339800: loss = 3.6998515129089355\n",
      "step = 7340000: loss = 3.80680775642395\n",
      "step = 7340000: Average Return = 3.809999942779541\n",
      "step = 7340200: loss = 5.134979248046875\n",
      "step = 7340400: loss = 4.31071138381958\n",
      "step = 7340600: loss = 4.453481197357178\n",
      "step = 7340800: loss = 3.3865966796875\n",
      "step = 7341000: loss = 3.8722851276397705\n",
      "step = 7341200: loss = 4.916380882263184\n",
      "step = 7341400: loss = 4.151558876037598\n",
      "step = 7341600: loss = 5.068831443786621\n",
      "step = 7341800: loss = 2.990748882293701\n",
      "step = 7342000: loss = 4.946901321411133\n",
      "step = 7342200: loss = 3.549081563949585\n",
      "step = 7342400: loss = 4.629532814025879\n",
      "step = 7342600: loss = 3.0379769802093506\n",
      "step = 7342800: loss = 3.4080445766448975\n",
      "step = 7343000: loss = 4.6196746826171875\n",
      "step = 7343200: loss = 2.8776614665985107\n",
      "step = 7343400: loss = 4.179213523864746\n",
      "step = 7343600: loss = 2.6157290935516357\n",
      "step = 7343800: loss = 3.8803069591522217\n",
      "step = 7344000: loss = 3.0661888122558594\n",
      "step = 7344200: loss = 3.7056286334991455\n",
      "step = 7344400: loss = 3.898371458053589\n",
      "step = 7344600: loss = 4.9806742668151855\n",
      "step = 7344800: loss = 4.247089862823486\n",
      "step = 7345000: loss = 3.4977009296417236\n",
      "step = 7345000: Average Return = 3.680000066757202\n",
      "step = 7345200: loss = 5.622042655944824\n",
      "step = 7345400: loss = 3.9378020763397217\n",
      "step = 7345600: loss = 5.646271228790283\n",
      "step = 7345800: loss = 4.834142684936523\n",
      "step = 7346000: loss = 4.370180606842041\n",
      "step = 7346200: loss = 4.391847610473633\n",
      "step = 7346400: loss = 4.034174919128418\n",
      "step = 7346600: loss = 5.17883825302124\n",
      "step = 7346800: loss = 3.3316781520843506\n",
      "step = 7347000: loss = 4.9244279861450195\n",
      "step = 7347200: loss = 4.849705696105957\n",
      "step = 7347400: loss = 5.086441516876221\n",
      "step = 7347600: loss = 4.7290849685668945\n",
      "step = 7347800: loss = 4.1131205558776855\n",
      "step = 7348000: loss = 2.3763880729675293\n",
      "step = 7348200: loss = 4.685442924499512\n",
      "step = 7348400: loss = 2.772244453430176\n",
      "step = 7348600: loss = 4.580551624298096\n",
      "step = 7348800: loss = 5.379133224487305\n",
      "step = 7349000: loss = 3.2031919956207275\n",
      "step = 7349200: loss = 4.765398025512695\n",
      "step = 7349400: loss = 4.454104423522949\n",
      "step = 7349600: loss = 4.666070461273193\n",
      "step = 7349800: loss = 4.979508876800537\n",
      "step = 7350000: loss = 4.036594867706299\n",
      "step = 7350000: Average Return = 4.076000213623047\n",
      "step = 7350200: loss = 3.855121374130249\n",
      "step = 7350400: loss = 4.12781286239624\n",
      "step = 7350600: loss = 5.514875411987305\n",
      "step = 7350800: loss = 3.812803030014038\n",
      "step = 7351000: loss = 3.7010600566864014\n",
      "step = 7351200: loss = 3.2006356716156006\n",
      "step = 7351400: loss = 5.532984733581543\n",
      "step = 7351600: loss = 4.9981207847595215\n",
      "step = 7351800: loss = 3.493317127227783\n",
      "step = 7352000: loss = 5.204958438873291\n",
      "step = 7352200: loss = 4.281994342803955\n",
      "step = 7352400: loss = 3.993159770965576\n",
      "step = 7352600: loss = 3.7065298557281494\n",
      "step = 7352800: loss = 4.249018669128418\n",
      "step = 7353000: loss = 4.435154438018799\n",
      "step = 7353200: loss = 4.5118584632873535\n",
      "step = 7353400: loss = 2.9157238006591797\n",
      "step = 7353600: loss = 4.219338893890381\n",
      "step = 7353800: loss = 3.907595634460449\n",
      "step = 7354000: loss = 4.384769439697266\n",
      "step = 7354200: loss = 4.209386348724365\n",
      "step = 7354400: loss = 3.419987201690674\n",
      "step = 7354600: loss = 4.673296928405762\n",
      "step = 7354800: loss = 3.8387060165405273\n",
      "step = 7355000: loss = 3.1100072860717773\n",
      "step = 7355000: Average Return = 3.8380000591278076\n",
      "step = 7355200: loss = 3.3787426948547363\n",
      "step = 7355400: loss = 4.9253363609313965\n",
      "step = 7355600: loss = 4.240179538726807\n",
      "step = 7355800: loss = 4.189719200134277\n",
      "step = 7356000: loss = 5.406656265258789\n",
      "step = 7356200: loss = 3.845611572265625\n",
      "step = 7356400: loss = 4.528196811676025\n",
      "step = 7356600: loss = 2.749222993850708\n",
      "step = 7356800: loss = 4.112046241760254\n",
      "step = 7357000: loss = 4.6941070556640625\n",
      "step = 7357200: loss = 3.6130948066711426\n",
      "step = 7357400: loss = 3.5255041122436523\n",
      "step = 7357600: loss = 3.6972591876983643\n",
      "step = 7357800: loss = 2.6352221965789795\n",
      "step = 7358000: loss = 4.6650166511535645\n",
      "step = 7358200: loss = 5.556966304779053\n",
      "step = 7358400: loss = 3.5805346965789795\n",
      "step = 7358600: loss = 4.856365203857422\n",
      "step = 7358800: loss = 4.6578369140625\n",
      "step = 7359000: loss = 3.8280415534973145\n",
      "step = 7359200: loss = 2.7893011569976807\n",
      "step = 7359400: loss = 4.965563774108887\n",
      "step = 7359600: loss = 3.056553602218628\n",
      "step = 7359800: loss = 3.1247472763061523\n",
      "step = 7360000: loss = 4.705113887786865\n",
      "step = 7360000: Average Return = 4.084000110626221\n",
      "step = 7360200: loss = 3.605088233947754\n",
      "step = 7360400: loss = 4.719154357910156\n",
      "step = 7360600: loss = 3.7955987453460693\n",
      "step = 7360800: loss = 4.254469871520996\n",
      "step = 7361000: loss = 4.046191692352295\n",
      "step = 7361200: loss = 4.496645927429199\n",
      "step = 7361400: loss = 3.533989191055298\n",
      "step = 7361600: loss = 4.029087066650391\n",
      "step = 7361800: loss = 4.8727521896362305\n",
      "step = 7362000: loss = 3.737994909286499\n",
      "step = 7362200: loss = 5.147181510925293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7362400: loss = 3.3155181407928467\n",
      "step = 7362600: loss = 2.9817497730255127\n",
      "step = 7362800: loss = 5.772285461425781\n",
      "step = 7363000: loss = 4.895447254180908\n",
      "step = 7363200: loss = 4.019271373748779\n",
      "step = 7363400: loss = 2.631826400756836\n",
      "step = 7363600: loss = 4.876781463623047\n",
      "step = 7363800: loss = 4.498683929443359\n",
      "step = 7364000: loss = 3.7540950775146484\n",
      "step = 7364200: loss = 4.9364399909973145\n",
      "step = 7364400: loss = 4.399410724639893\n",
      "step = 7364600: loss = 4.904702186584473\n",
      "step = 7364800: loss = 3.999131441116333\n",
      "step = 7365000: loss = 4.324873447418213\n",
      "step = 7365000: Average Return = 3.7100000381469727\n",
      "step = 7365200: loss = 5.747902870178223\n",
      "step = 7365400: loss = 4.720142364501953\n",
      "step = 7365600: loss = 4.007038593292236\n",
      "step = 7365800: loss = 5.240640163421631\n",
      "step = 7366000: loss = 5.520788669586182\n",
      "step = 7366200: loss = 3.8466436862945557\n",
      "step = 7366400: loss = 4.907010555267334\n",
      "step = 7366600: loss = 3.33111572265625\n",
      "step = 7366800: loss = 5.133821964263916\n",
      "step = 7367000: loss = 3.6346633434295654\n",
      "step = 7367200: loss = 4.334776878356934\n",
      "step = 7367400: loss = 3.5488505363464355\n",
      "step = 7367600: loss = 4.607402801513672\n",
      "step = 7367800: loss = 4.706544876098633\n",
      "step = 7368000: loss = 4.513057231903076\n",
      "step = 7368200: loss = 4.2151079177856445\n",
      "step = 7368400: loss = 3.205420970916748\n",
      "step = 7368600: loss = 4.545590877532959\n",
      "step = 7368800: loss = 4.777751922607422\n",
      "step = 7369000: loss = 4.2942681312561035\n",
      "step = 7369200: loss = 3.611551523208618\n",
      "step = 7369400: loss = 4.684232234954834\n",
      "step = 7369600: loss = 4.356541156768799\n",
      "step = 7369800: loss = 3.6597578525543213\n",
      "step = 7370000: loss = 4.61745548248291\n",
      "step = 7370000: Average Return = 3.640000104904175\n",
      "step = 7370200: loss = 5.109447479248047\n",
      "step = 7370400: loss = 3.9747753143310547\n",
      "step = 7370600: loss = 3.543320894241333\n",
      "step = 7370800: loss = 4.4071125984191895\n",
      "step = 7371000: loss = 3.2691469192504883\n",
      "step = 7371200: loss = 3.076155424118042\n",
      "step = 7371400: loss = 2.4743809700012207\n",
      "step = 7371600: loss = 3.948185443878174\n",
      "step = 7371800: loss = 4.167296886444092\n",
      "step = 7372000: loss = 3.9805052280426025\n",
      "step = 7372200: loss = 5.036730766296387\n",
      "step = 7372400: loss = 4.476076602935791\n",
      "step = 7372600: loss = 3.7479429244995117\n",
      "step = 7372800: loss = 3.209420919418335\n",
      "step = 7373000: loss = 3.2836952209472656\n",
      "step = 7373200: loss = 4.4045891761779785\n",
      "step = 7373400: loss = 4.60024356842041\n",
      "step = 7373600: loss = 5.002564430236816\n",
      "step = 7373800: loss = 4.659895420074463\n",
      "step = 7374000: loss = 2.592292070388794\n",
      "step = 7374200: loss = 4.604307651519775\n",
      "step = 7374400: loss = 4.200500965118408\n",
      "step = 7374600: loss = 3.3159987926483154\n",
      "step = 7374800: loss = 4.125955581665039\n",
      "step = 7375000: loss = 2.5152108669281006\n",
      "step = 7375000: Average Return = 4.00600004196167\n",
      "step = 7375200: loss = 4.205148696899414\n",
      "step = 7375400: loss = 4.361257076263428\n",
      "step = 7375600: loss = 3.5870561599731445\n",
      "step = 7375800: loss = 4.966002464294434\n",
      "step = 7376000: loss = 6.295907020568848\n",
      "step = 7376200: loss = 4.0415263175964355\n",
      "step = 7376400: loss = 5.228444576263428\n",
      "step = 7376600: loss = 3.7012908458709717\n",
      "step = 7376800: loss = 4.8644185066223145\n",
      "step = 7377000: loss = 5.401175498962402\n",
      "step = 7377200: loss = 4.192317962646484\n",
      "step = 7377400: loss = 4.229923248291016\n",
      "step = 7377600: loss = 3.267712354660034\n",
      "step = 7377800: loss = 3.0628464221954346\n",
      "step = 7378000: loss = 2.9896862506866455\n",
      "step = 7378200: loss = 4.871397495269775\n",
      "step = 7378400: loss = 4.1238226890563965\n",
      "step = 7378600: loss = 2.9433746337890625\n",
      "step = 7378800: loss = 3.9020628929138184\n",
      "step = 7379000: loss = 5.17104434967041\n",
      "step = 7379200: loss = 4.221484661102295\n",
      "step = 7379400: loss = 3.7438457012176514\n",
      "step = 7379600: loss = 4.069446086883545\n",
      "step = 7379800: loss = 4.296658992767334\n",
      "step = 7380000: loss = 4.046799659729004\n",
      "step = 7380000: Average Return = 3.8540000915527344\n",
      "step = 7380200: loss = 3.2450783252716064\n",
      "step = 7380400: loss = 4.612253189086914\n",
      "step = 7380600: loss = 3.5746943950653076\n",
      "step = 7380800: loss = 4.17467737197876\n",
      "step = 7381000: loss = 4.864845275878906\n",
      "step = 7381200: loss = 4.395966053009033\n",
      "step = 7381400: loss = 3.92331600189209\n",
      "step = 7381600: loss = 5.943821907043457\n",
      "step = 7381800: loss = 4.011338233947754\n",
      "step = 7382000: loss = 4.420665264129639\n",
      "step = 7382200: loss = 6.312805652618408\n",
      "step = 7382400: loss = 5.79212760925293\n",
      "step = 7382600: loss = 5.463465690612793\n",
      "step = 7382800: loss = 4.294152736663818\n",
      "step = 7383000: loss = 3.1589975357055664\n",
      "step = 7383200: loss = 5.104109764099121\n",
      "step = 7383400: loss = 4.429551601409912\n",
      "step = 7383600: loss = 3.935910940170288\n",
      "step = 7383800: loss = 4.73728609085083\n",
      "step = 7384000: loss = 5.544619560241699\n",
      "step = 7384200: loss = 3.262443780899048\n",
      "step = 7384400: loss = 3.9882447719573975\n",
      "step = 7384600: loss = 4.119422435760498\n",
      "step = 7384800: loss = 4.74769401550293\n",
      "step = 7385000: loss = 4.23025369644165\n",
      "step = 7385000: Average Return = 3.822000026702881\n",
      "step = 7385200: loss = 4.531482219696045\n",
      "step = 7385400: loss = 4.646731853485107\n",
      "step = 7385600: loss = 3.208543539047241\n",
      "step = 7385800: loss = 5.58674955368042\n",
      "step = 7386000: loss = 4.130110740661621\n",
      "step = 7386200: loss = 3.7919867038726807\n",
      "step = 7386400: loss = 3.932717800140381\n",
      "step = 7386600: loss = 3.9763591289520264\n",
      "step = 7386800: loss = 3.4738478660583496\n",
      "step = 7387000: loss = 2.779143810272217\n",
      "step = 7387200: loss = 4.107311725616455\n",
      "step = 7387400: loss = 4.171875953674316\n",
      "step = 7387600: loss = 5.07810640335083\n",
      "step = 7387800: loss = 5.31439208984375\n",
      "step = 7388000: loss = 4.752336502075195\n",
      "step = 7388200: loss = 4.033279895782471\n",
      "step = 7388400: loss = 3.649824380874634\n",
      "step = 7388600: loss = 3.220414876937866\n",
      "step = 7388800: loss = 3.201164484024048\n",
      "step = 7389000: loss = 4.500331401824951\n",
      "step = 7389200: loss = 4.064725399017334\n",
      "step = 7389400: loss = 4.1176300048828125\n",
      "step = 7389600: loss = 4.956658363342285\n",
      "step = 7389800: loss = 5.091270446777344\n",
      "step = 7390000: loss = 3.653404951095581\n",
      "step = 7390000: Average Return = 3.9040000438690186\n",
      "step = 7390200: loss = 3.0964555740356445\n",
      "step = 7390400: loss = 4.32798957824707\n",
      "step = 7390600: loss = 3.909367084503174\n",
      "step = 7390800: loss = 3.7321105003356934\n",
      "step = 7391000: loss = 3.3357913494110107\n",
      "step = 7391200: loss = 3.9932689666748047\n",
      "step = 7391400: loss = 4.3574419021606445\n",
      "step = 7391600: loss = 4.088450908660889\n",
      "step = 7391800: loss = 5.081118106842041\n",
      "step = 7392000: loss = 4.429139137268066\n",
      "step = 7392200: loss = 6.286750316619873\n",
      "step = 7392400: loss = 3.258765697479248\n",
      "step = 7392600: loss = 3.3630969524383545\n",
      "step = 7392800: loss = 4.574127197265625\n",
      "step = 7393000: loss = 4.91119909286499\n",
      "step = 7393200: loss = 3.5720269680023193\n",
      "step = 7393400: loss = 4.471461296081543\n",
      "step = 7393600: loss = 4.384452819824219\n",
      "step = 7393800: loss = 4.811642169952393\n",
      "step = 7394000: loss = 4.48026180267334\n",
      "step = 7394200: loss = 4.241099834442139\n",
      "step = 7394400: loss = 3.6981332302093506\n",
      "step = 7394600: loss = 3.5483288764953613\n",
      "step = 7394800: loss = 4.356270790100098\n",
      "step = 7395000: loss = 4.3823418617248535\n",
      "step = 7395000: Average Return = 3.615999937057495\n",
      "step = 7395200: loss = 4.341212749481201\n",
      "step = 7395400: loss = 3.6592326164245605\n",
      "step = 7395600: loss = 4.286407947540283\n",
      "step = 7395800: loss = 3.2664711475372314\n",
      "step = 7396000: loss = 4.688334941864014\n",
      "step = 7396200: loss = 3.743778705596924\n",
      "step = 7396400: loss = 4.847761154174805\n",
      "step = 7396600: loss = 4.674716472625732\n",
      "step = 7396800: loss = 4.357184886932373\n",
      "step = 7397000: loss = 5.05933141708374\n",
      "step = 7397200: loss = 4.678213596343994\n",
      "step = 7397400: loss = 3.765671968460083\n",
      "step = 7397600: loss = 3.682504177093506\n",
      "step = 7397800: loss = 4.1329803466796875\n",
      "step = 7398000: loss = 3.1627092361450195\n",
      "step = 7398200: loss = 4.148141384124756\n",
      "step = 7398400: loss = 3.570209264755249\n",
      "step = 7398600: loss = 4.92807149887085\n",
      "step = 7398800: loss = 4.838606834411621\n",
      "step = 7399000: loss = 2.5672662258148193\n",
      "step = 7399200: loss = 4.351733684539795\n",
      "step = 7399400: loss = 4.633430004119873\n",
      "step = 7399600: loss = 4.136232376098633\n",
      "step = 7399800: loss = 3.3922619819641113\n",
      "step = 7400000: loss = 4.138922691345215\n",
      "step = 7400000: Average Return = 3.8959999084472656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7400200: loss = 4.447229385375977\n",
      "step = 7400400: loss = 3.6961352825164795\n",
      "step = 7400600: loss = 3.371094226837158\n",
      "step = 7400800: loss = 4.60756778717041\n",
      "step = 7401000: loss = 4.3692498207092285\n",
      "step = 7401200: loss = 3.1921279430389404\n",
      "step = 7401400: loss = 3.1797027587890625\n",
      "step = 7401600: loss = 5.686399936676025\n",
      "step = 7401800: loss = 3.9304542541503906\n",
      "step = 7402000: loss = 4.227971076965332\n",
      "step = 7402200: loss = 4.150016784667969\n",
      "step = 7402400: loss = 3.482020139694214\n",
      "step = 7402600: loss = 4.271517753601074\n",
      "step = 7402800: loss = 4.278043746948242\n",
      "step = 7403000: loss = 3.3731706142425537\n",
      "step = 7403200: loss = 3.8818070888519287\n",
      "step = 7403400: loss = 4.157451152801514\n",
      "step = 7403600: loss = 5.518837928771973\n",
      "step = 7403800: loss = 5.400068759918213\n",
      "step = 7404000: loss = 4.6734232902526855\n",
      "step = 7404200: loss = 3.903672695159912\n",
      "step = 7404400: loss = 4.806600093841553\n",
      "step = 7404600: loss = 3.9069137573242188\n",
      "step = 7404800: loss = 5.111972332000732\n",
      "step = 7405000: loss = 5.605905532836914\n",
      "step = 7405000: Average Return = 3.9059998989105225\n",
      "step = 7405200: loss = 2.6491243839263916\n",
      "step = 7405400: loss = 4.028881549835205\n",
      "step = 7405600: loss = 4.1742753982543945\n",
      "step = 7405800: loss = 3.19368839263916\n",
      "step = 7406000: loss = 2.6585021018981934\n",
      "step = 7406200: loss = 3.4321484565734863\n",
      "step = 7406400: loss = 5.489020824432373\n",
      "step = 7406600: loss = 3.4131250381469727\n",
      "step = 7406800: loss = 3.582956552505493\n",
      "step = 7407000: loss = 4.375096321105957\n",
      "step = 7407200: loss = 4.336779594421387\n",
      "step = 7407400: loss = 3.0724878311157227\n",
      "step = 7407600: loss = 4.388319492340088\n",
      "step = 7407800: loss = 4.7354936599731445\n",
      "step = 7408000: loss = 4.777253150939941\n",
      "step = 7408200: loss = 4.497206211090088\n",
      "step = 7408400: loss = 4.2826032638549805\n",
      "step = 7408600: loss = 4.935929775238037\n",
      "step = 7408800: loss = 5.006512641906738\n",
      "step = 7409000: loss = 4.230164051055908\n",
      "step = 7409200: loss = 4.138124465942383\n",
      "step = 7409400: loss = 2.72221040725708\n",
      "step = 7409600: loss = 3.647331476211548\n",
      "step = 7409800: loss = 2.8964319229125977\n",
      "step = 7410000: loss = 3.276546001434326\n",
      "step = 7410000: Average Return = 3.9539999961853027\n",
      "step = 7410200: loss = 4.696500778198242\n",
      "step = 7410400: loss = 3.501509428024292\n",
      "step = 7410600: loss = 4.649035930633545\n",
      "step = 7410800: loss = 4.553311347961426\n",
      "step = 7411000: loss = 4.637353420257568\n",
      "step = 7411200: loss = 5.050683975219727\n",
      "step = 7411400: loss = 3.8934481143951416\n",
      "step = 7411600: loss = 3.5613133907318115\n",
      "step = 7411800: loss = 3.3390562534332275\n",
      "step = 7412000: loss = 4.322535991668701\n",
      "step = 7412200: loss = 3.09069561958313\n",
      "step = 7412400: loss = 3.170283794403076\n",
      "step = 7412600: loss = 4.008068561553955\n",
      "step = 7412800: loss = 4.461834907531738\n",
      "step = 7413000: loss = 2.91461181640625\n",
      "step = 7413200: loss = 3.5632498264312744\n",
      "step = 7413400: loss = 3.4873547554016113\n",
      "step = 7413600: loss = 4.363760471343994\n",
      "step = 7413800: loss = 3.474630117416382\n",
      "step = 7414000: loss = 3.8367748260498047\n",
      "step = 7414200: loss = 3.0440773963928223\n",
      "step = 7414400: loss = 3.7205700874328613\n",
      "step = 7414600: loss = 4.045801162719727\n",
      "step = 7414800: loss = 3.8266913890838623\n",
      "step = 7415000: loss = 5.349772930145264\n",
      "step = 7415000: Average Return = 4.079999923706055\n",
      "step = 7415200: loss = 4.259106159210205\n",
      "step = 7415400: loss = 5.026204586029053\n",
      "step = 7415600: loss = 4.517043113708496\n",
      "step = 7415800: loss = 4.01584005355835\n",
      "step = 7416000: loss = 3.977656841278076\n",
      "step = 7416200: loss = 5.105594158172607\n",
      "step = 7416400: loss = 5.073848247528076\n",
      "step = 7416600: loss = 3.022644519805908\n",
      "step = 7416800: loss = 4.6553754806518555\n",
      "step = 7417000: loss = 4.3417158126831055\n",
      "step = 7417200: loss = 6.297756195068359\n",
      "step = 7417400: loss = 3.5836730003356934\n",
      "step = 7417600: loss = 3.4920079708099365\n",
      "step = 7417800: loss = 4.389678478240967\n",
      "step = 7418000: loss = 4.193213939666748\n",
      "step = 7418200: loss = 4.964818477630615\n",
      "step = 7418400: loss = 2.250676155090332\n",
      "step = 7418600: loss = 4.990867614746094\n",
      "step = 7418800: loss = 4.696330547332764\n",
      "step = 7419000: loss = 5.508876800537109\n",
      "step = 7419200: loss = 5.069109916687012\n",
      "step = 7419400: loss = 4.295441627502441\n",
      "step = 7419600: loss = 3.4183666706085205\n",
      "step = 7419800: loss = 4.377933025360107\n",
      "step = 7420000: loss = 6.034245491027832\n",
      "step = 7420000: Average Return = 3.753999948501587\n",
      "step = 7420200: loss = 3.9108288288116455\n",
      "step = 7420400: loss = 5.066800594329834\n",
      "step = 7420600: loss = 4.028867244720459\n",
      "step = 7420800: loss = 4.6144232749938965\n",
      "step = 7421000: loss = 3.0580968856811523\n",
      "step = 7421200: loss = 4.402281284332275\n",
      "step = 7421400: loss = 4.957380771636963\n",
      "step = 7421600: loss = 4.348920822143555\n",
      "step = 7421800: loss = 4.327172756195068\n",
      "step = 7422000: loss = 4.8557562828063965\n",
      "step = 7422200: loss = 5.475867748260498\n",
      "step = 7422400: loss = 4.391285419464111\n",
      "step = 7422600: loss = 5.46811580657959\n",
      "step = 7422800: loss = 5.121661186218262\n",
      "step = 7423000: loss = 4.9684062004089355\n",
      "step = 7423200: loss = 3.3175694942474365\n",
      "step = 7423400: loss = 5.33449649810791\n",
      "step = 7423600: loss = 3.8896496295928955\n",
      "step = 7423800: loss = 3.88822078704834\n",
      "step = 7424000: loss = 3.550110340118408\n",
      "step = 7424200: loss = 3.7446441650390625\n",
      "step = 7424400: loss = 4.409927845001221\n",
      "step = 7424600: loss = 5.308201789855957\n",
      "step = 7424800: loss = 2.8449721336364746\n",
      "step = 7425000: loss = 3.7340383529663086\n",
      "step = 7425000: Average Return = 3.697999954223633\n",
      "step = 7425200: loss = 4.765486240386963\n",
      "step = 7425400: loss = 4.392172336578369\n",
      "step = 7425600: loss = 5.239694595336914\n",
      "step = 7425800: loss = 4.163206100463867\n",
      "step = 7426000: loss = 5.112284183502197\n",
      "step = 7426200: loss = 3.8282995223999023\n",
      "step = 7426400: loss = 3.68900465965271\n",
      "step = 7426600: loss = 3.039283275604248\n",
      "step = 7426800: loss = 3.063535451889038\n",
      "step = 7427000: loss = 3.4644627571105957\n",
      "step = 7427200: loss = 3.5157508850097656\n",
      "step = 7427400: loss = 4.928097248077393\n",
      "step = 7427600: loss = 4.671083450317383\n",
      "step = 7427800: loss = 4.6984405517578125\n",
      "step = 7428000: loss = 4.229851722717285\n",
      "step = 7428200: loss = 3.9412596225738525\n",
      "step = 7428400: loss = 3.7264418601989746\n",
      "step = 7428600: loss = 3.4570579528808594\n",
      "step = 7428800: loss = 4.8571343421936035\n",
      "step = 7429000: loss = 3.611884355545044\n",
      "step = 7429200: loss = 4.168637275695801\n",
      "step = 7429400: loss = 2.7453105449676514\n",
      "step = 7429600: loss = 3.954785108566284\n",
      "step = 7429800: loss = 4.205772876739502\n",
      "step = 7430000: loss = 5.362661838531494\n",
      "step = 7430000: Average Return = 3.9719998836517334\n",
      "step = 7430200: loss = 5.106491565704346\n",
      "step = 7430400: loss = 4.672091007232666\n",
      "step = 7430600: loss = 4.974847316741943\n",
      "step = 7430800: loss = 3.0257060527801514\n",
      "step = 7431000: loss = 3.889164686203003\n",
      "step = 7431200: loss = 4.196267127990723\n",
      "step = 7431400: loss = 5.726459980010986\n",
      "step = 7431600: loss = 3.9290971755981445\n",
      "step = 7431800: loss = 5.039880752563477\n",
      "step = 7432000: loss = 3.7814881801605225\n",
      "step = 7432200: loss = 2.4339325428009033\n",
      "step = 7432400: loss = 4.613747596740723\n",
      "step = 7432600: loss = 4.998635768890381\n",
      "step = 7432800: loss = 2.441362142562866\n",
      "step = 7433000: loss = 3.372107982635498\n",
      "step = 7433200: loss = 3.926314115524292\n",
      "step = 7433400: loss = 2.676424264907837\n",
      "step = 7433600: loss = 4.160683631896973\n",
      "step = 7433800: loss = 4.404773712158203\n",
      "step = 7434000: loss = 4.439413547515869\n",
      "step = 7434200: loss = 3.7051823139190674\n",
      "step = 7434400: loss = 4.656586170196533\n",
      "step = 7434600: loss = 3.1010844707489014\n",
      "step = 7434800: loss = 4.220715045928955\n",
      "step = 7435000: loss = 3.287278175354004\n",
      "step = 7435000: Average Return = 4.1519999504089355\n",
      "step = 7435200: loss = 5.209508419036865\n",
      "step = 7435400: loss = 4.640321731567383\n",
      "step = 7435600: loss = 3.037396192550659\n",
      "step = 7435800: loss = 4.660192966461182\n",
      "step = 7436000: loss = 5.62089729309082\n",
      "step = 7436200: loss = 3.987854480743408\n",
      "step = 7436400: loss = 5.289412021636963\n",
      "step = 7436600: loss = 3.7048606872558594\n",
      "step = 7436800: loss = 3.553823709487915\n",
      "step = 7437000: loss = 4.005890846252441\n",
      "step = 7437200: loss = 4.079496383666992\n",
      "step = 7437400: loss = 4.0535807609558105\n",
      "step = 7437600: loss = 3.3739194869995117\n",
      "step = 7437800: loss = 3.2403788566589355\n",
      "step = 7438000: loss = 3.6878721714019775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7438200: loss = 3.2994279861450195\n",
      "step = 7438400: loss = 5.303977012634277\n",
      "step = 7438600: loss = 4.472548961639404\n",
      "step = 7438800: loss = 3.968998908996582\n",
      "step = 7439000: loss = 4.309366703033447\n",
      "step = 7439200: loss = 4.4383063316345215\n",
      "step = 7439400: loss = 3.6125786304473877\n",
      "step = 7439600: loss = 4.301914215087891\n",
      "step = 7439800: loss = 4.669578552246094\n",
      "step = 7440000: loss = 5.286014080047607\n",
      "step = 7440000: Average Return = 3.869999885559082\n",
      "step = 7440200: loss = 4.066028118133545\n",
      "step = 7440400: loss = 4.0746917724609375\n",
      "step = 7440600: loss = 4.128541469573975\n",
      "step = 7440800: loss = 6.010529041290283\n",
      "step = 7441000: loss = 3.5529022216796875\n",
      "step = 7441200: loss = 3.675901412963867\n",
      "step = 7441400: loss = 4.579920291900635\n",
      "step = 7441600: loss = 3.754801034927368\n",
      "step = 7441800: loss = 5.577961444854736\n",
      "step = 7442000: loss = 2.9558794498443604\n",
      "step = 7442200: loss = 2.870192289352417\n",
      "step = 7442400: loss = 4.030210971832275\n",
      "step = 7442600: loss = 4.116692543029785\n",
      "step = 7442800: loss = 4.507813930511475\n",
      "step = 7443000: loss = 6.031335830688477\n",
      "step = 7443200: loss = 2.920801877975464\n",
      "step = 7443400: loss = 2.4863650798797607\n",
      "step = 7443600: loss = 4.663214206695557\n",
      "step = 7443800: loss = 4.87051248550415\n",
      "step = 7444000: loss = 3.9860293865203857\n",
      "step = 7444200: loss = 5.604926586151123\n",
      "step = 7444400: loss = 4.413664817810059\n",
      "step = 7444600: loss = 3.3152058124542236\n",
      "step = 7444800: loss = 4.7317023277282715\n",
      "step = 7445000: loss = 3.7438161373138428\n",
      "step = 7445000: Average Return = 3.7200000286102295\n",
      "step = 7445200: loss = 4.019669532775879\n",
      "step = 7445400: loss = 6.6026482582092285\n",
      "step = 7445600: loss = 2.6970715522766113\n",
      "step = 7445800: loss = 3.2597877979278564\n",
      "step = 7446000: loss = 3.382347583770752\n",
      "step = 7446200: loss = 3.8409392833709717\n",
      "step = 7446400: loss = 3.310687780380249\n",
      "step = 7446600: loss = 4.14109992980957\n",
      "step = 7446800: loss = 5.647851467132568\n",
      "step = 7447000: loss = 5.045937538146973\n",
      "step = 7447200: loss = 4.312324047088623\n",
      "step = 7447400: loss = 4.914301872253418\n",
      "step = 7447600: loss = 5.262967586517334\n",
      "step = 7447800: loss = 5.9611334800720215\n",
      "step = 7448000: loss = 4.382944107055664\n",
      "step = 7448200: loss = 3.28058123588562\n",
      "step = 7448400: loss = 4.912410736083984\n",
      "step = 7448600: loss = 4.661827087402344\n",
      "step = 7448800: loss = 3.4444644451141357\n",
      "step = 7449000: loss = 5.698462009429932\n",
      "step = 7449200: loss = 4.489437103271484\n",
      "step = 7449400: loss = 3.8704986572265625\n",
      "step = 7449600: loss = 4.862247943878174\n",
      "step = 7449800: loss = 2.661703109741211\n",
      "step = 7450000: loss = 3.676197052001953\n",
      "step = 7450000: Average Return = 3.3440001010894775\n",
      "step = 7450200: loss = 4.445292949676514\n",
      "step = 7450400: loss = 4.881724834442139\n",
      "step = 7450600: loss = 3.4320220947265625\n",
      "step = 7450800: loss = 3.558901071548462\n",
      "step = 7451000: loss = 4.596628189086914\n",
      "step = 7451200: loss = 4.692376136779785\n",
      "step = 7451400: loss = 5.4947662353515625\n",
      "step = 7451600: loss = 2.4152162075042725\n",
      "step = 7451800: loss = 4.58241605758667\n",
      "step = 7452000: loss = 5.415388107299805\n",
      "step = 7452200: loss = 4.584869861602783\n",
      "step = 7452400: loss = 2.9103386402130127\n",
      "step = 7452600: loss = 3.5863122940063477\n",
      "step = 7452800: loss = 4.846771240234375\n",
      "step = 7453000: loss = 3.484574794769287\n",
      "step = 7453200: loss = 2.8299529552459717\n",
      "step = 7453400: loss = 3.9151244163513184\n",
      "step = 7453600: loss = 4.087522029876709\n",
      "step = 7453800: loss = 3.7598981857299805\n",
      "step = 7454000: loss = 4.4806365966796875\n",
      "step = 7454200: loss = 3.5219945907592773\n",
      "step = 7454400: loss = 4.740133762359619\n",
      "step = 7454600: loss = 4.387096881866455\n",
      "step = 7454800: loss = 3.72601056098938\n",
      "step = 7455000: loss = 3.521610975265503\n",
      "step = 7455000: Average Return = 3.565999984741211\n",
      "step = 7455200: loss = 3.895331382751465\n",
      "step = 7455400: loss = 4.004927158355713\n",
      "step = 7455600: loss = 4.5142741203308105\n",
      "step = 7455800: loss = 4.868110656738281\n",
      "step = 7456000: loss = 4.387217998504639\n",
      "step = 7456200: loss = 3.8286397457122803\n",
      "step = 7456400: loss = 3.4654905796051025\n",
      "step = 7456600: loss = 4.311233043670654\n",
      "step = 7456800: loss = 3.234813690185547\n",
      "step = 7457000: loss = 4.052668571472168\n",
      "step = 7457200: loss = 5.476251125335693\n",
      "step = 7457400: loss = 4.100532054901123\n",
      "step = 7457600: loss = 5.812955379486084\n",
      "step = 7457800: loss = 4.676858901977539\n",
      "step = 7458000: loss = 4.216359615325928\n",
      "step = 7458200: loss = 4.060067653656006\n",
      "step = 7458400: loss = 2.8910930156707764\n",
      "step = 7458600: loss = 5.694584846496582\n",
      "step = 7458800: loss = 4.182974815368652\n",
      "step = 7459000: loss = 4.577972412109375\n",
      "step = 7459200: loss = 4.1664605140686035\n",
      "step = 7459400: loss = 4.327181816101074\n",
      "step = 7459600: loss = 3.6253859996795654\n",
      "step = 7459800: loss = 4.251909255981445\n",
      "step = 7460000: loss = 4.166014671325684\n",
      "step = 7460000: Average Return = 3.364000082015991\n",
      "step = 7460200: loss = 3.9415154457092285\n",
      "step = 7460400: loss = 3.3714003562927246\n",
      "step = 7460600: loss = 5.402490615844727\n",
      "step = 7460800: loss = 4.188392639160156\n",
      "step = 7461000: loss = 3.7196638584136963\n",
      "step = 7461200: loss = 5.348623752593994\n",
      "step = 7461400: loss = 5.038143157958984\n",
      "step = 7461600: loss = 3.7634265422821045\n",
      "step = 7461800: loss = 5.031907081604004\n",
      "step = 7462000: loss = 5.197968482971191\n",
      "step = 7462200: loss = 2.6139297485351562\n",
      "step = 7462400: loss = 5.044012546539307\n",
      "step = 7462600: loss = 4.144563674926758\n",
      "step = 7462800: loss = 3.732748031616211\n",
      "step = 7463000: loss = 4.2211222648620605\n",
      "step = 7463200: loss = 4.254587173461914\n",
      "step = 7463400: loss = 4.31685209274292\n",
      "step = 7463600: loss = 5.044159412384033\n",
      "step = 7463800: loss = 3.4780476093292236\n",
      "step = 7464000: loss = 3.4591081142425537\n",
      "step = 7464200: loss = 3.3359081745147705\n",
      "step = 7464400: loss = 5.310252666473389\n",
      "step = 7464600: loss = 3.918431282043457\n",
      "step = 7464800: loss = 3.3563878536224365\n",
      "step = 7465000: loss = 4.3062591552734375\n",
      "step = 7465000: Average Return = 3.76200008392334\n",
      "step = 7465200: loss = 3.5396523475646973\n",
      "step = 7465400: loss = 3.447547435760498\n",
      "step = 7465600: loss = 4.87675666809082\n",
      "step = 7465800: loss = 3.614158868789673\n",
      "step = 7466000: loss = 4.5174880027771\n",
      "step = 7466200: loss = 4.975090026855469\n",
      "step = 7466400: loss = 3.6982455253601074\n",
      "step = 7466600: loss = 5.454216003417969\n",
      "step = 7466800: loss = 3.4509358406066895\n",
      "step = 7467000: loss = 2.9482226371765137\n",
      "step = 7467200: loss = 5.347326755523682\n",
      "step = 7467400: loss = 3.6158525943756104\n",
      "step = 7467600: loss = 5.0237531661987305\n",
      "step = 7467800: loss = 4.367307186126709\n",
      "step = 7468000: loss = 4.4699859619140625\n",
      "step = 7468200: loss = 4.142534255981445\n",
      "step = 7468400: loss = 3.987872838973999\n",
      "step = 7468600: loss = 4.116379261016846\n",
      "step = 7468800: loss = 3.5191268920898438\n",
      "step = 7469000: loss = 3.60537052154541\n",
      "step = 7469200: loss = 4.396907329559326\n",
      "step = 7469400: loss = 3.6836318969726562\n",
      "step = 7469600: loss = 4.082157611846924\n",
      "step = 7469800: loss = 3.268311023712158\n",
      "step = 7470000: loss = 3.1112451553344727\n",
      "step = 7470000: Average Return = 3.507999897003174\n",
      "step = 7470200: loss = 3.7034852504730225\n",
      "step = 7470400: loss = 3.577997922897339\n",
      "step = 7470600: loss = 4.275240421295166\n",
      "step = 7470800: loss = 5.161257266998291\n",
      "step = 7471000: loss = 4.694536209106445\n",
      "step = 7471200: loss = 4.155361175537109\n",
      "step = 7471400: loss = 3.610255241394043\n",
      "step = 7471600: loss = 4.945901393890381\n",
      "step = 7471800: loss = 2.9453933238983154\n",
      "step = 7472000: loss = 5.324122428894043\n",
      "step = 7472200: loss = 5.419651985168457\n",
      "step = 7472400: loss = 5.145820617675781\n",
      "step = 7472600: loss = 4.540374755859375\n",
      "step = 7472800: loss = 5.287308216094971\n",
      "step = 7473000: loss = 4.755481243133545\n",
      "step = 7473200: loss = 3.261943817138672\n",
      "step = 7473400: loss = 3.6557888984680176\n",
      "step = 7473600: loss = 4.044336795806885\n",
      "step = 7473800: loss = 4.680164337158203\n",
      "step = 7474000: loss = 2.8176381587982178\n",
      "step = 7474200: loss = 4.324650764465332\n",
      "step = 7474400: loss = 3.948331117630005\n",
      "step = 7474600: loss = 4.34200382232666\n",
      "step = 7474800: loss = 3.4189910888671875\n",
      "step = 7475000: loss = 3.703665018081665\n",
      "step = 7475000: Average Return = 3.8499999046325684\n",
      "step = 7475200: loss = 5.100340843200684\n",
      "step = 7475400: loss = 3.540531873703003\n",
      "step = 7475600: loss = 4.860134601593018\n",
      "step = 7475800: loss = 4.031000137329102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7476000: loss = 3.258859872817993\n",
      "step = 7476200: loss = 3.6666386127471924\n",
      "step = 7476400: loss = 4.106882095336914\n",
      "step = 7476600: loss = 3.5139684677124023\n",
      "step = 7476800: loss = 4.498973369598389\n",
      "step = 7477000: loss = 4.763489246368408\n",
      "step = 7477200: loss = 3.6755166053771973\n",
      "step = 7477400: loss = 3.6784253120422363\n",
      "step = 7477600: loss = 3.7917888164520264\n",
      "step = 7477800: loss = 4.008041858673096\n",
      "step = 7478000: loss = 5.018956184387207\n",
      "step = 7478200: loss = 3.979076862335205\n",
      "step = 7478400: loss = 3.7940189838409424\n",
      "step = 7478600: loss = 3.5819931030273438\n",
      "step = 7478800: loss = 3.2852602005004883\n",
      "step = 7479000: loss = 3.9757039546966553\n",
      "step = 7479200: loss = 3.784860134124756\n",
      "step = 7479400: loss = 2.393092632293701\n",
      "step = 7479600: loss = 5.122678756713867\n",
      "step = 7479800: loss = 4.003669261932373\n",
      "step = 7480000: loss = 3.844606637954712\n",
      "step = 7480000: Average Return = 3.742000102996826\n",
      "step = 7480200: loss = 4.685690879821777\n",
      "step = 7480400: loss = 4.761443138122559\n",
      "step = 7480600: loss = 3.409895896911621\n",
      "step = 7480800: loss = 4.116676330566406\n",
      "step = 7481000: loss = 3.7357804775238037\n",
      "step = 7481200: loss = 4.8697190284729\n",
      "step = 7481400: loss = 4.980746269226074\n",
      "step = 7481600: loss = 4.821394920349121\n",
      "step = 7481800: loss = 4.880729675292969\n",
      "step = 7482000: loss = 3.5699212551116943\n",
      "step = 7482200: loss = 3.01865291595459\n",
      "step = 7482400: loss = 4.0430588722229\n",
      "step = 7482600: loss = 4.774966716766357\n",
      "step = 7482800: loss = 4.148231506347656\n",
      "step = 7483000: loss = 4.330873489379883\n",
      "step = 7483200: loss = 4.405701160430908\n",
      "step = 7483400: loss = 4.502583980560303\n",
      "step = 7483600: loss = 3.2398061752319336\n",
      "step = 7483800: loss = 4.728453636169434\n",
      "step = 7484000: loss = 3.9382951259613037\n",
      "step = 7484200: loss = 4.379222393035889\n",
      "step = 7484400: loss = 5.178920745849609\n",
      "step = 7484600: loss = 4.047272205352783\n",
      "step = 7484800: loss = 4.114358901977539\n",
      "step = 7485000: loss = 4.168603897094727\n",
      "step = 7485000: Average Return = 3.9040000438690186\n",
      "step = 7485200: loss = 4.43358039855957\n",
      "step = 7485400: loss = 4.3845343589782715\n",
      "step = 7485600: loss = 4.951438903808594\n",
      "step = 7485800: loss = 3.891587495803833\n",
      "step = 7486000: loss = 4.271023273468018\n",
      "step = 7486200: loss = 3.6092095375061035\n",
      "step = 7486400: loss = 3.820136547088623\n",
      "step = 7486600: loss = 3.639953851699829\n",
      "step = 7486800: loss = 4.530762672424316\n",
      "step = 7487000: loss = 5.079291343688965\n",
      "step = 7487200: loss = 3.0725131034851074\n",
      "step = 7487400: loss = 3.816166400909424\n",
      "step = 7487600: loss = 4.007387161254883\n",
      "step = 7487800: loss = 4.52542781829834\n",
      "step = 7488000: loss = 4.174858093261719\n",
      "step = 7488200: loss = 4.346155166625977\n",
      "step = 7488400: loss = 3.6826226711273193\n",
      "step = 7488600: loss = 4.897788047790527\n",
      "step = 7488800: loss = 4.398776531219482\n",
      "step = 7489000: loss = 4.092092990875244\n",
      "step = 7489200: loss = 4.642371654510498\n",
      "step = 7489400: loss = 4.755000591278076\n",
      "step = 7489600: loss = 4.550109386444092\n",
      "step = 7489800: loss = 4.990489959716797\n",
      "step = 7490000: loss = 4.175580024719238\n",
      "step = 7490000: Average Return = 3.818000078201294\n",
      "step = 7490200: loss = 4.205729961395264\n",
      "step = 7490400: loss = 4.980463027954102\n",
      "step = 7490600: loss = 4.8269500732421875\n",
      "step = 7490800: loss = 6.267181396484375\n",
      "step = 7491000: loss = 4.414219379425049\n",
      "step = 7491200: loss = 6.857208251953125\n",
      "step = 7491400: loss = 5.050184726715088\n",
      "step = 7491600: loss = 4.557777404785156\n",
      "step = 7491800: loss = 5.025637149810791\n",
      "step = 7492000: loss = 5.14173698425293\n",
      "step = 7492200: loss = 3.685138463973999\n",
      "step = 7492400: loss = 4.053318500518799\n",
      "step = 7492600: loss = 3.7570180892944336\n",
      "step = 7492800: loss = 5.010388374328613\n",
      "step = 7493000: loss = 4.1768927574157715\n",
      "step = 7493200: loss = 3.270355463027954\n",
      "step = 7493400: loss = 3.758356809616089\n",
      "step = 7493600: loss = 5.870170593261719\n",
      "step = 7493800: loss = 2.7630958557128906\n",
      "step = 7494000: loss = 3.8813934326171875\n",
      "step = 7494200: loss = 3.5339159965515137\n",
      "step = 7494400: loss = 5.100927829742432\n",
      "step = 7494600: loss = 2.0184690952301025\n",
      "step = 7494800: loss = 3.9064364433288574\n",
      "step = 7495000: loss = 3.451528310775757\n",
      "step = 7495000: Average Return = 3.8980000019073486\n",
      "step = 7495200: loss = 4.463249206542969\n",
      "step = 7495400: loss = 3.1528117656707764\n",
      "step = 7495600: loss = 3.1091690063476562\n",
      "step = 7495800: loss = 3.8567540645599365\n",
      "step = 7496000: loss = 4.887892246246338\n",
      "step = 7496200: loss = 5.742750644683838\n",
      "step = 7496400: loss = 4.407251358032227\n",
      "step = 7496600: loss = 4.173416614532471\n",
      "step = 7496800: loss = 5.381340980529785\n",
      "step = 7497000: loss = 4.9922003746032715\n",
      "step = 7497200: loss = 4.193020820617676\n",
      "step = 7497400: loss = 3.1376495361328125\n",
      "step = 7497600: loss = 3.497607946395874\n",
      "step = 7497800: loss = 3.902891159057617\n",
      "step = 7498000: loss = 3.969089984893799\n",
      "step = 7498200: loss = 4.661818027496338\n",
      "step = 7498400: loss = 4.720070838928223\n",
      "step = 7498600: loss = 4.738865852355957\n",
      "step = 7498800: loss = 3.8433477878570557\n",
      "step = 7499000: loss = 5.03999137878418\n",
      "step = 7499200: loss = 2.9948318004608154\n",
      "step = 7499400: loss = 4.968306064605713\n",
      "step = 7499600: loss = 5.965843200683594\n",
      "step = 7499800: loss = 4.138126850128174\n",
      "step = 7500000: loss = 2.225018262863159\n",
      "step = 7500000: Average Return = 3.869999885559082\n",
      "step = 7500200: loss = 4.282222747802734\n",
      "step = 7500400: loss = 4.078123092651367\n",
      "step = 7500600: loss = 4.237985134124756\n",
      "step = 7500800: loss = 3.8749325275421143\n",
      "step = 7501000: loss = 4.269773960113525\n",
      "step = 7501200: loss = 4.1565961837768555\n",
      "step = 7501400: loss = 4.542131423950195\n",
      "step = 7501600: loss = 4.415615081787109\n",
      "step = 7501800: loss = 3.718583345413208\n",
      "step = 7502000: loss = 4.759004592895508\n",
      "step = 7502200: loss = 3.6571474075317383\n",
      "step = 7502400: loss = 5.394862174987793\n",
      "step = 7502600: loss = 4.78068208694458\n",
      "step = 7502800: loss = 3.714555501937866\n",
      "step = 7503000: loss = 4.7893829345703125\n",
      "step = 7503200: loss = 5.506108283996582\n",
      "step = 7503400: loss = 4.334627151489258\n",
      "step = 7503600: loss = 5.033531188964844\n",
      "step = 7503800: loss = 4.382191181182861\n",
      "step = 7504000: loss = 3.5388002395629883\n",
      "step = 7504200: loss = 3.384593963623047\n",
      "step = 7504400: loss = 4.397792816162109\n",
      "step = 7504600: loss = 5.246287822723389\n",
      "step = 7504800: loss = 4.826935768127441\n",
      "step = 7505000: loss = 2.505847930908203\n",
      "step = 7505000: Average Return = 4.110000133514404\n",
      "step = 7505200: loss = 4.33380126953125\n",
      "step = 7505400: loss = 4.075643062591553\n",
      "step = 7505600: loss = 5.159266948699951\n",
      "step = 7505800: loss = 3.2361247539520264\n",
      "step = 7506000: loss = 3.496535062789917\n",
      "step = 7506200: loss = 3.4813461303710938\n",
      "step = 7506400: loss = 3.7795417308807373\n",
      "step = 7506600: loss = 4.089023113250732\n",
      "step = 7506800: loss = 4.137484073638916\n",
      "step = 7507000: loss = 3.17748761177063\n",
      "step = 7507200: loss = 3.2168376445770264\n",
      "step = 7507400: loss = 3.3369805812835693\n",
      "step = 7507600: loss = 4.8851823806762695\n",
      "step = 7507800: loss = 2.8368968963623047\n",
      "step = 7508000: loss = 3.1611642837524414\n",
      "step = 7508200: loss = 5.31386661529541\n",
      "step = 7508400: loss = 3.653249740600586\n",
      "step = 7508600: loss = 3.972285270690918\n",
      "step = 7508800: loss = 2.909289836883545\n",
      "step = 7509000: loss = 3.9194891452789307\n",
      "step = 7509200: loss = 4.7160725593566895\n",
      "step = 7509400: loss = 3.4760258197784424\n",
      "step = 7509600: loss = 3.8162901401519775\n",
      "step = 7509800: loss = 4.3026885986328125\n",
      "step = 7510000: loss = 4.346034526824951\n",
      "step = 7510000: Average Return = 3.4260001182556152\n",
      "step = 7510200: loss = 3.245713472366333\n",
      "step = 7510400: loss = 4.02913761138916\n",
      "step = 7510600: loss = 4.893681049346924\n",
      "step = 7510800: loss = 5.056870460510254\n",
      "step = 7511000: loss = 4.580259799957275\n",
      "step = 7511200: loss = 4.014177322387695\n",
      "step = 7511400: loss = 4.961845397949219\n",
      "step = 7511600: loss = 4.0032572746276855\n",
      "step = 7511800: loss = 5.181413650512695\n",
      "step = 7512000: loss = 4.976511001586914\n",
      "step = 7512200: loss = 3.9326388835906982\n",
      "step = 7512400: loss = 5.179943561553955\n",
      "step = 7512600: loss = 3.355353593826294\n",
      "step = 7512800: loss = 3.6767005920410156\n",
      "step = 7513000: loss = 3.707827568054199\n",
      "step = 7513200: loss = 4.556584358215332\n",
      "step = 7513400: loss = 3.469254732131958\n",
      "step = 7513600: loss = 3.7829854488372803\n",
      "step = 7513800: loss = 4.251968860626221\n",
      "step = 7514000: loss = 4.263546466827393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7514200: loss = 4.709856033325195\n",
      "step = 7514400: loss = 3.651249408721924\n",
      "step = 7514600: loss = 4.050919532775879\n",
      "step = 7514800: loss = 3.827277183532715\n",
      "step = 7515000: loss = 2.856942653656006\n",
      "step = 7515000: Average Return = 4.230000019073486\n",
      "step = 7515200: loss = 3.8804008960723877\n",
      "step = 7515400: loss = 4.544236183166504\n",
      "step = 7515600: loss = 3.7098381519317627\n",
      "step = 7515800: loss = 3.438685894012451\n",
      "step = 7516000: loss = 5.6759185791015625\n",
      "step = 7516200: loss = 6.188982963562012\n",
      "step = 7516400: loss = 4.356800556182861\n",
      "step = 7516600: loss = 4.044402599334717\n",
      "step = 7516800: loss = 4.283596515655518\n",
      "step = 7517000: loss = 3.4487133026123047\n",
      "step = 7517200: loss = 3.556175470352173\n",
      "step = 7517400: loss = 4.616389751434326\n",
      "step = 7517600: loss = 6.057120323181152\n",
      "step = 7517800: loss = 2.9336202144622803\n",
      "step = 7518000: loss = 2.9400713443756104\n",
      "step = 7518200: loss = 4.895273208618164\n",
      "step = 7518400: loss = 4.250969886779785\n",
      "step = 7518600: loss = 3.814581871032715\n",
      "step = 7518800: loss = 3.215672016143799\n",
      "step = 7519000: loss = 5.36590576171875\n",
      "step = 7519200: loss = 3.777667760848999\n",
      "step = 7519400: loss = 5.305779457092285\n",
      "step = 7519600: loss = 4.3167290687561035\n",
      "step = 7519800: loss = 6.495185375213623\n",
      "step = 7520000: loss = 3.438908576965332\n",
      "step = 7520000: Average Return = 3.9119999408721924\n",
      "step = 7520200: loss = 4.088810443878174\n",
      "step = 7520400: loss = 4.620031833648682\n",
      "step = 7520600: loss = 3.69189453125\n",
      "step = 7520800: loss = 4.2173895835876465\n",
      "step = 7521000: loss = 3.7832250595092773\n",
      "step = 7521200: loss = 4.2067131996154785\n",
      "step = 7521400: loss = 3.1418139934539795\n",
      "step = 7521600: loss = 3.983154773712158\n",
      "step = 7521800: loss = 4.249544620513916\n",
      "step = 7522000: loss = 3.9730846881866455\n",
      "step = 7522200: loss = 3.8036346435546875\n",
      "step = 7522400: loss = 4.538256645202637\n",
      "step = 7522600: loss = 3.127681255340576\n",
      "step = 7522800: loss = 6.005023002624512\n",
      "step = 7523000: loss = 3.0725364685058594\n",
      "step = 7523200: loss = 3.401489734649658\n",
      "step = 7523400: loss = 3.547806978225708\n",
      "step = 7523600: loss = 4.2709503173828125\n",
      "step = 7523800: loss = 3.724783182144165\n",
      "step = 7524000: loss = 3.5543699264526367\n",
      "step = 7524200: loss = 3.533686399459839\n",
      "step = 7524400: loss = 5.176905632019043\n",
      "step = 7524600: loss = 4.155242919921875\n",
      "step = 7524800: loss = 3.3493425846099854\n",
      "step = 7525000: loss = 4.115825653076172\n",
      "step = 7525000: Average Return = 3.874000072479248\n",
      "step = 7525200: loss = 3.8134560585021973\n",
      "step = 7525400: loss = 5.473437786102295\n",
      "step = 7525600: loss = 5.507964134216309\n",
      "step = 7525800: loss = 3.8005218505859375\n",
      "step = 7526000: loss = 5.690438747406006\n",
      "step = 7526200: loss = 4.475122451782227\n",
      "step = 7526400: loss = 2.267010450363159\n",
      "step = 7526600: loss = 4.035041332244873\n",
      "step = 7526800: loss = 3.4342362880706787\n",
      "step = 7527000: loss = 4.513521194458008\n",
      "step = 7527200: loss = 4.101597309112549\n",
      "step = 7527400: loss = 4.055682182312012\n",
      "step = 7527600: loss = 3.4658079147338867\n",
      "step = 7527800: loss = 3.6715941429138184\n",
      "step = 7528000: loss = 4.171964168548584\n",
      "step = 7528200: loss = 4.194823741912842\n",
      "step = 7528400: loss = 4.349512577056885\n",
      "step = 7528600: loss = 4.776924133300781\n",
      "step = 7528800: loss = 4.80576229095459\n",
      "step = 7529000: loss = 3.7622299194335938\n",
      "step = 7529200: loss = 3.80399227142334\n",
      "step = 7529400: loss = 3.362924814224243\n",
      "step = 7529600: loss = 4.409539222717285\n",
      "step = 7529800: loss = 4.101816177368164\n",
      "step = 7530000: loss = 3.89335560798645\n",
      "step = 7530000: Average Return = 4.050000190734863\n",
      "step = 7530200: loss = 4.151726722717285\n",
      "step = 7530400: loss = 3.930359363555908\n",
      "step = 7530600: loss = 3.495569944381714\n",
      "step = 7530800: loss = 4.9188690185546875\n",
      "step = 7531000: loss = 3.6266252994537354\n",
      "step = 7531200: loss = 3.5957205295562744\n",
      "step = 7531400: loss = 3.339982748031616\n",
      "step = 7531600: loss = 4.289822101593018\n",
      "step = 7531800: loss = 4.296417713165283\n",
      "step = 7532000: loss = 5.655541896820068\n",
      "step = 7532200: loss = 3.4833672046661377\n",
      "step = 7532400: loss = 3.0803751945495605\n",
      "step = 7532600: loss = 5.140096187591553\n",
      "step = 7532800: loss = 3.126516103744507\n",
      "step = 7533000: loss = 4.118137836456299\n",
      "step = 7533200: loss = 2.6405160427093506\n",
      "step = 7533400: loss = 4.075811862945557\n",
      "step = 7533600: loss = 4.189033508300781\n",
      "step = 7533800: loss = 4.411717414855957\n",
      "step = 7534000: loss = 4.562110900878906\n",
      "step = 7534200: loss = 4.644351959228516\n",
      "step = 7534400: loss = 5.665412425994873\n",
      "step = 7534600: loss = 4.238473415374756\n",
      "step = 7534800: loss = 4.8454437255859375\n",
      "step = 7535000: loss = 3.842839002609253\n",
      "step = 7535000: Average Return = 3.888000011444092\n",
      "step = 7535200: loss = 4.623908042907715\n",
      "step = 7535400: loss = 3.8132143020629883\n",
      "step = 7535600: loss = 4.9488019943237305\n",
      "step = 7535800: loss = 5.236269474029541\n",
      "step = 7536000: loss = 5.489491939544678\n",
      "step = 7536200: loss = 4.10105037689209\n",
      "step = 7536400: loss = 4.143341541290283\n",
      "step = 7536600: loss = 3.784937620162964\n",
      "step = 7536800: loss = 4.3664727210998535\n",
      "step = 7537000: loss = 3.8740391731262207\n",
      "step = 7537200: loss = 3.1357054710388184\n",
      "step = 7537400: loss = 4.369109630584717\n",
      "step = 7537600: loss = 3.580883741378784\n",
      "step = 7537800: loss = 5.262828350067139\n",
      "step = 7538000: loss = 4.423935890197754\n",
      "step = 7538200: loss = 3.7545957565307617\n",
      "step = 7538400: loss = 4.1615471839904785\n",
      "step = 7538600: loss = 5.298685073852539\n",
      "step = 7538800: loss = 4.682909965515137\n",
      "step = 7539000: loss = 3.2843947410583496\n",
      "step = 7539200: loss = 4.046627044677734\n",
      "step = 7539400: loss = 3.3595526218414307\n",
      "step = 7539600: loss = 4.341963291168213\n",
      "step = 7539800: loss = 4.721855163574219\n",
      "step = 7540000: loss = 3.355882406234741\n",
      "step = 7540000: Average Return = 3.7720000743865967\n",
      "step = 7540200: loss = 3.8165159225463867\n",
      "step = 7540400: loss = 4.226892948150635\n",
      "step = 7540600: loss = 4.96338415145874\n",
      "step = 7540800: loss = 4.94386625289917\n",
      "step = 7541000: loss = 3.7819042205810547\n",
      "step = 7541200: loss = 4.813596248626709\n",
      "step = 7541400: loss = 5.083802700042725\n",
      "step = 7541600: loss = 3.9579594135284424\n",
      "step = 7541800: loss = 3.741229772567749\n",
      "step = 7542000: loss = 5.102898597717285\n",
      "step = 7542200: loss = 4.091228485107422\n",
      "step = 7542400: loss = 4.038843154907227\n",
      "step = 7542600: loss = 5.542877197265625\n",
      "step = 7542800: loss = 4.248873710632324\n",
      "step = 7543000: loss = 5.735825061798096\n",
      "step = 7543200: loss = 4.34893798828125\n",
      "step = 7543400: loss = 3.721804141998291\n",
      "step = 7543600: loss = 4.386573791503906\n",
      "step = 7543800: loss = 3.9168496131896973\n",
      "step = 7544000: loss = 3.1235857009887695\n",
      "step = 7544200: loss = 3.9149837493896484\n",
      "step = 7544400: loss = 3.6722428798675537\n",
      "step = 7544600: loss = 6.165874004364014\n",
      "step = 7544800: loss = 5.51383638381958\n",
      "step = 7545000: loss = 4.585920333862305\n",
      "step = 7545000: Average Return = 4.00600004196167\n",
      "step = 7545200: loss = 4.382864475250244\n",
      "step = 7545400: loss = 4.529118061065674\n",
      "step = 7545600: loss = 5.023369312286377\n",
      "step = 7545800: loss = 2.9246327877044678\n",
      "step = 7546000: loss = 4.1872429847717285\n",
      "step = 7546200: loss = 5.037120819091797\n",
      "step = 7546400: loss = 4.552821159362793\n",
      "step = 7546600: loss = 3.706976890563965\n",
      "step = 7546800: loss = 3.6535496711730957\n",
      "step = 7547000: loss = 4.30644416809082\n",
      "step = 7547200: loss = 3.8728795051574707\n",
      "step = 7547400: loss = 3.173794984817505\n",
      "step = 7547600: loss = 4.941586017608643\n",
      "step = 7547800: loss = 3.836118698120117\n",
      "step = 7548000: loss = 4.75412654876709\n",
      "step = 7548200: loss = 3.970099449157715\n",
      "step = 7548400: loss = 4.005945682525635\n",
      "step = 7548600: loss = 5.547616004943848\n",
      "step = 7548800: loss = 3.881110191345215\n",
      "step = 7549000: loss = 3.951052665710449\n",
      "step = 7549200: loss = 3.1604692935943604\n",
      "step = 7549400: loss = 4.030951023101807\n",
      "step = 7549600: loss = 4.387129306793213\n",
      "step = 7549800: loss = 2.9719107151031494\n",
      "step = 7550000: loss = 4.863758087158203\n",
      "step = 7550000: Average Return = 4.285999774932861\n",
      "step = 7550200: loss = 3.8047289848327637\n",
      "step = 7550400: loss = 4.124448299407959\n",
      "step = 7550600: loss = 4.337921619415283\n",
      "step = 7550800: loss = 4.102930545806885\n",
      "step = 7551000: loss = 4.637715816497803\n",
      "step = 7551200: loss = 5.3928022384643555\n",
      "step = 7551400: loss = 3.685211181640625\n",
      "step = 7551600: loss = 4.580371379852295\n",
      "step = 7551800: loss = 2.7551634311676025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7552000: loss = 4.762892723083496\n",
      "step = 7552200: loss = 2.8922274112701416\n",
      "step = 7552400: loss = 4.043050289154053\n",
      "step = 7552600: loss = 3.6638689041137695\n",
      "step = 7552800: loss = 2.9714629650115967\n",
      "step = 7553000: loss = 3.5908684730529785\n",
      "step = 7553200: loss = 4.18757438659668\n",
      "step = 7553400: loss = 5.157598972320557\n",
      "step = 7553600: loss = 4.276816368103027\n",
      "step = 7553800: loss = 4.313300609588623\n",
      "step = 7554000: loss = 3.1269469261169434\n",
      "step = 7554200: loss = 3.9078636169433594\n",
      "step = 7554400: loss = 2.858846426010132\n",
      "step = 7554600: loss = 4.213656902313232\n",
      "step = 7554800: loss = 3.767186164855957\n",
      "step = 7555000: loss = 4.784791469573975\n",
      "step = 7555000: Average Return = 3.7119998931884766\n",
      "step = 7555200: loss = 4.347754001617432\n",
      "step = 7555400: loss = 3.3414599895477295\n",
      "step = 7555600: loss = 4.937972068786621\n",
      "step = 7555800: loss = 3.7630913257598877\n",
      "step = 7556000: loss = 4.508668899536133\n",
      "step = 7556200: loss = 4.303723335266113\n",
      "step = 7556400: loss = 4.342391490936279\n",
      "step = 7556600: loss = 4.961515426635742\n",
      "step = 7556800: loss = 3.6687605381011963\n",
      "step = 7557000: loss = 4.772471904754639\n",
      "step = 7557200: loss = 2.310109853744507\n",
      "step = 7557400: loss = 2.433159828186035\n",
      "step = 7557600: loss = 3.894777774810791\n",
      "step = 7557800: loss = 5.130722522735596\n",
      "step = 7558000: loss = 3.252394914627075\n",
      "step = 7558200: loss = 3.340848684310913\n",
      "step = 7558400: loss = 4.467533111572266\n",
      "step = 7558600: loss = 4.401892185211182\n",
      "step = 7558800: loss = 6.054408550262451\n",
      "step = 7559000: loss = 3.3001720905303955\n",
      "step = 7559200: loss = 5.363682746887207\n",
      "step = 7559400: loss = 4.345560073852539\n",
      "step = 7559600: loss = 4.7445807456970215\n",
      "step = 7559800: loss = 4.496792316436768\n",
      "step = 7560000: loss = 4.254648208618164\n",
      "step = 7560000: Average Return = 3.874000072479248\n",
      "step = 7560200: loss = 4.927841663360596\n",
      "step = 7560400: loss = 3.964596748352051\n",
      "step = 7560600: loss = 2.900792121887207\n",
      "step = 7560800: loss = 3.997891664505005\n",
      "step = 7561000: loss = 2.989614248275757\n",
      "step = 7561200: loss = 3.38834285736084\n",
      "step = 7561400: loss = 5.12199068069458\n",
      "step = 7561600: loss = 4.349111080169678\n",
      "step = 7561800: loss = 4.020296573638916\n",
      "step = 7562000: loss = 3.890537738800049\n",
      "step = 7562200: loss = 4.8734450340271\n",
      "step = 7562400: loss = 5.593757629394531\n",
      "step = 7562600: loss = 3.3015873432159424\n",
      "step = 7562800: loss = 4.577975749969482\n",
      "step = 7563000: loss = 4.772455215454102\n",
      "step = 7563200: loss = 3.5832290649414062\n",
      "step = 7563400: loss = 4.104259490966797\n",
      "step = 7563600: loss = 4.034685134887695\n",
      "step = 7563800: loss = 3.123852491378784\n",
      "step = 7564000: loss = 5.3758134841918945\n",
      "step = 7564200: loss = 4.112791538238525\n",
      "step = 7564400: loss = 5.184412956237793\n",
      "step = 7564600: loss = 4.2809672355651855\n",
      "step = 7564800: loss = 4.297198295593262\n",
      "step = 7565000: loss = 5.167809963226318\n",
      "step = 7565000: Average Return = 4.1579999923706055\n",
      "step = 7565200: loss = 3.94840145111084\n",
      "step = 7565400: loss = 3.3935132026672363\n",
      "step = 7565600: loss = 4.956431865692139\n",
      "step = 7565800: loss = 5.11149787902832\n",
      "step = 7566000: loss = 3.207139253616333\n",
      "step = 7566200: loss = 4.69753885269165\n",
      "step = 7566400: loss = 4.685074329376221\n",
      "step = 7566600: loss = 3.788450241088867\n",
      "step = 7566800: loss = 3.8106701374053955\n",
      "step = 7567000: loss = 3.631554365158081\n",
      "step = 7567200: loss = 4.184079647064209\n",
      "step = 7567400: loss = 3.8193418979644775\n",
      "step = 7567600: loss = 5.015483856201172\n",
      "step = 7567800: loss = 3.6176207065582275\n",
      "step = 7568000: loss = 4.605502128601074\n",
      "step = 7568200: loss = 4.4790778160095215\n",
      "step = 7568400: loss = 5.806326866149902\n",
      "step = 7568600: loss = 3.780512571334839\n",
      "step = 7568800: loss = 3.399456739425659\n",
      "step = 7569000: loss = 4.866266250610352\n",
      "step = 7569200: loss = 4.875095844268799\n",
      "step = 7569400: loss = 5.396480560302734\n",
      "step = 7569600: loss = 3.44095516204834\n",
      "step = 7569800: loss = 3.432420015335083\n",
      "step = 7570000: loss = 4.7067670822143555\n",
      "step = 7570000: Average Return = 3.7699999809265137\n",
      "step = 7570200: loss = 4.594318866729736\n",
      "step = 7570400: loss = 3.1614973545074463\n",
      "step = 7570600: loss = 3.3417391777038574\n",
      "step = 7570800: loss = 4.482762336730957\n",
      "step = 7571000: loss = 2.8886027336120605\n",
      "step = 7571200: loss = 3.5700347423553467\n",
      "step = 7571400: loss = 4.962925434112549\n",
      "step = 7571600: loss = 2.9953622817993164\n",
      "step = 7571800: loss = 4.411725997924805\n",
      "step = 7572000: loss = 4.51425838470459\n",
      "step = 7572200: loss = 5.637693881988525\n",
      "step = 7572400: loss = 5.830362319946289\n",
      "step = 7572600: loss = 6.078553199768066\n",
      "step = 7572800: loss = 4.044892311096191\n",
      "step = 7573000: loss = 3.2534563541412354\n",
      "step = 7573200: loss = 3.949976921081543\n",
      "step = 7573400: loss = 4.4771037101745605\n",
      "step = 7573600: loss = 4.066448211669922\n",
      "step = 7573800: loss = 5.1919474601745605\n",
      "step = 7574000: loss = 4.0723347663879395\n",
      "step = 7574200: loss = 4.527623653411865\n",
      "step = 7574400: loss = 4.770749092102051\n",
      "step = 7574600: loss = 3.837594509124756\n",
      "step = 7574800: loss = 4.403502941131592\n",
      "step = 7575000: loss = 3.875542163848877\n",
      "step = 7575000: Average Return = 4.438000202178955\n",
      "step = 7575200: loss = 4.17145299911499\n",
      "step = 7575400: loss = 4.083973407745361\n",
      "step = 7575600: loss = 4.385403633117676\n",
      "step = 7575800: loss = 4.47465705871582\n",
      "step = 7576000: loss = 2.0034995079040527\n",
      "step = 7576200: loss = 3.8478312492370605\n",
      "step = 7576400: loss = 3.8383641242980957\n",
      "step = 7576600: loss = 3.2527809143066406\n",
      "step = 7576800: loss = 3.200519323348999\n",
      "step = 7577000: loss = 4.635695934295654\n",
      "step = 7577200: loss = 5.375579833984375\n",
      "step = 7577400: loss = 4.707573890686035\n",
      "step = 7577600: loss = 4.177315711975098\n",
      "step = 7577800: loss = 4.124413013458252\n",
      "step = 7578000: loss = 4.450400352478027\n",
      "step = 7578200: loss = 4.569777011871338\n",
      "step = 7578400: loss = 4.333926200866699\n",
      "step = 7578600: loss = 4.350371837615967\n",
      "step = 7578800: loss = 3.823420524597168\n",
      "step = 7579000: loss = 3.02227520942688\n",
      "step = 7579200: loss = 3.2885029315948486\n",
      "step = 7579400: loss = 3.6441991329193115\n",
      "step = 7579600: loss = 4.193269729614258\n",
      "step = 7579800: loss = 4.647757053375244\n",
      "step = 7580000: loss = 3.6426305770874023\n",
      "step = 7580000: Average Return = 4.01200008392334\n",
      "step = 7580200: loss = 3.8100523948669434\n",
      "step = 7580400: loss = 3.847083330154419\n",
      "step = 7580600: loss = 3.0918493270874023\n",
      "step = 7580800: loss = 4.697286605834961\n",
      "step = 7581000: loss = 4.301130771636963\n",
      "step = 7581200: loss = 4.537304401397705\n",
      "step = 7581400: loss = 5.58965539932251\n",
      "step = 7581600: loss = 2.2226474285125732\n",
      "step = 7581800: loss = 4.397777557373047\n",
      "step = 7582000: loss = 4.1717424392700195\n",
      "step = 7582200: loss = 4.510031223297119\n",
      "step = 7582400: loss = 4.1713104248046875\n",
      "step = 7582600: loss = 3.342975378036499\n",
      "step = 7582800: loss = 3.969569683074951\n",
      "step = 7583000: loss = 2.1942245960235596\n",
      "step = 7583200: loss = 3.7411816120147705\n",
      "step = 7583400: loss = 5.372284889221191\n",
      "step = 7583600: loss = 2.663910388946533\n",
      "step = 7583800: loss = 4.335738182067871\n",
      "step = 7584000: loss = 2.588776111602783\n",
      "step = 7584200: loss = 5.027268886566162\n",
      "step = 7584400: loss = 4.713434219360352\n",
      "step = 7584600: loss = 4.7834343910217285\n",
      "step = 7584800: loss = 4.137783527374268\n",
      "step = 7585000: loss = 2.9989771842956543\n",
      "step = 7585000: Average Return = 3.7899999618530273\n",
      "step = 7585200: loss = 4.600403785705566\n",
      "step = 7585400: loss = 3.5378689765930176\n",
      "step = 7585600: loss = 3.3234095573425293\n",
      "step = 7585800: loss = 4.614890098571777\n",
      "step = 7586000: loss = 4.030041217803955\n",
      "step = 7586200: loss = 3.0269439220428467\n",
      "step = 7586400: loss = 5.282769680023193\n",
      "step = 7586600: loss = 3.9861879348754883\n",
      "step = 7586800: loss = 4.254594802856445\n",
      "step = 7587000: loss = 5.3551836013793945\n",
      "step = 7587200: loss = 3.5006484985351562\n",
      "step = 7587400: loss = 4.183858394622803\n",
      "step = 7587600: loss = 4.542466163635254\n",
      "step = 7587800: loss = 5.265664100646973\n",
      "step = 7588000: loss = 4.284872055053711\n",
      "step = 7588200: loss = 3.9366514682769775\n",
      "step = 7588400: loss = 4.516611576080322\n",
      "step = 7588600: loss = 4.1605987548828125\n",
      "step = 7588800: loss = 3.5470778942108154\n",
      "step = 7589000: loss = 3.762653350830078\n",
      "step = 7589200: loss = 2.28232479095459\n",
      "step = 7589400: loss = 5.021565914154053\n",
      "step = 7589600: loss = 4.312003135681152\n",
      "step = 7589800: loss = 3.8510613441467285\n",
      "step = 7590000: loss = 5.678293228149414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7590000: Average Return = 3.874000072479248\n",
      "step = 7590200: loss = 3.7945187091827393\n",
      "step = 7590400: loss = 6.407734394073486\n",
      "step = 7590600: loss = 4.021975040435791\n",
      "step = 7590800: loss = 4.558872222900391\n",
      "step = 7591000: loss = 3.9738881587982178\n",
      "step = 7591200: loss = 3.542562246322632\n",
      "step = 7591400: loss = 4.876863479614258\n",
      "step = 7591600: loss = 6.052774906158447\n",
      "step = 7591800: loss = 4.501041412353516\n",
      "step = 7592000: loss = 3.66994309425354\n",
      "step = 7592200: loss = 2.5966246128082275\n",
      "step = 7592400: loss = 2.8064393997192383\n",
      "step = 7592600: loss = 3.6653056144714355\n",
      "step = 7592800: loss = 3.8348939418792725\n",
      "step = 7593000: loss = 4.431382179260254\n",
      "step = 7593200: loss = 3.2999870777130127\n",
      "step = 7593400: loss = 3.9843695163726807\n",
      "step = 7593600: loss = 3.077613353729248\n",
      "step = 7593800: loss = 3.775623083114624\n",
      "step = 7594000: loss = 3.946547746658325\n",
      "step = 7594200: loss = 4.969974040985107\n",
      "step = 7594400: loss = 5.530066967010498\n",
      "step = 7594600: loss = 4.60692024230957\n",
      "step = 7594800: loss = 4.4166154861450195\n",
      "step = 7595000: loss = 4.293062210083008\n",
      "step = 7595000: Average Return = 3.7160000801086426\n",
      "step = 7595200: loss = 4.194137096405029\n",
      "step = 7595400: loss = 4.202285289764404\n",
      "step = 7595600: loss = 3.6668646335601807\n",
      "step = 7595800: loss = 5.1823835372924805\n",
      "step = 7596000: loss = 4.9036359786987305\n",
      "step = 7596200: loss = 4.878966331481934\n",
      "step = 7596400: loss = 4.1082563400268555\n",
      "step = 7596600: loss = 4.624876499176025\n",
      "step = 7596800: loss = 4.214678764343262\n",
      "step = 7597000: loss = 4.148149490356445\n",
      "step = 7597200: loss = 4.6123762130737305\n",
      "step = 7597400: loss = 4.22811222076416\n",
      "step = 7597600: loss = 2.5417277812957764\n",
      "step = 7597800: loss = 3.518078327178955\n",
      "step = 7598000: loss = 6.061892986297607\n",
      "step = 7598200: loss = 3.6522655487060547\n",
      "step = 7598400: loss = 3.522333860397339\n",
      "step = 7598600: loss = 2.273392677307129\n",
      "step = 7598800: loss = 2.6835057735443115\n",
      "step = 7599000: loss = 5.237479209899902\n",
      "step = 7599200: loss = 3.72346568107605\n",
      "step = 7599400: loss = 4.420877933502197\n",
      "step = 7599600: loss = 4.60240364074707\n",
      "step = 7599800: loss = 3.718829870223999\n",
      "step = 7600000: loss = 4.406226634979248\n",
      "step = 7600000: Average Return = 3.3919999599456787\n",
      "step = 7600200: loss = 3.49930477142334\n",
      "step = 7600400: loss = 3.0744740962982178\n",
      "step = 7600600: loss = 5.474604606628418\n",
      "step = 7600800: loss = 3.960529088973999\n",
      "step = 7601000: loss = 4.548851490020752\n",
      "step = 7601200: loss = 4.657141208648682\n",
      "step = 7601400: loss = 4.541839122772217\n",
      "step = 7601600: loss = 4.125494003295898\n",
      "step = 7601800: loss = 3.012119770050049\n",
      "step = 7602000: loss = 4.494816303253174\n",
      "step = 7602200: loss = 4.2036662101745605\n",
      "step = 7602400: loss = 5.260587215423584\n",
      "step = 7602600: loss = 4.410521030426025\n",
      "step = 7602800: loss = 2.769777536392212\n",
      "step = 7603000: loss = 3.8365447521209717\n",
      "step = 7603200: loss = 3.4631881713867188\n",
      "step = 7603400: loss = 4.598520755767822\n",
      "step = 7603600: loss = 3.0757033824920654\n",
      "step = 7603800: loss = 5.057567119598389\n",
      "step = 7604000: loss = 3.771075963973999\n",
      "step = 7604200: loss = 2.344841957092285\n",
      "step = 7604400: loss = 4.583151340484619\n",
      "step = 7604600: loss = 4.709118366241455\n",
      "step = 7604800: loss = 3.5173027515411377\n",
      "step = 7605000: loss = 4.278680801391602\n",
      "step = 7605000: Average Return = 3.9539999961853027\n",
      "step = 7605200: loss = 3.479104518890381\n",
      "step = 7605400: loss = 4.004768371582031\n",
      "step = 7605600: loss = 4.797645092010498\n",
      "step = 7605800: loss = 3.5133168697357178\n",
      "step = 7606000: loss = 2.7258052825927734\n",
      "step = 7606200: loss = 5.511275291442871\n",
      "step = 7606400: loss = 4.207962989807129\n",
      "step = 7606600: loss = 2.692337989807129\n",
      "step = 7606800: loss = 3.8714439868927\n",
      "step = 7607000: loss = 2.8990490436553955\n",
      "step = 7607200: loss = 3.5098185539245605\n",
      "step = 7607400: loss = 3.7344987392425537\n",
      "step = 7607600: loss = 3.45253849029541\n",
      "step = 7607800: loss = 5.000326633453369\n",
      "step = 7608000: loss = 3.8778915405273438\n",
      "step = 7608200: loss = 4.149988174438477\n",
      "step = 7608400: loss = 4.65223503112793\n",
      "step = 7608600: loss = 4.529097080230713\n",
      "step = 7608800: loss = 4.332671642303467\n",
      "step = 7609000: loss = 4.1606855392456055\n",
      "step = 7609200: loss = 3.8113059997558594\n",
      "step = 7609400: loss = 4.709356784820557\n",
      "step = 7609600: loss = 5.748748779296875\n",
      "step = 7609800: loss = 3.352138042449951\n",
      "step = 7610000: loss = 3.496908187866211\n",
      "step = 7610000: Average Return = 3.7820000648498535\n",
      "step = 7610200: loss = 3.241305351257324\n",
      "step = 7610400: loss = 3.957806348800659\n",
      "step = 7610600: loss = 4.602932453155518\n",
      "step = 7610800: loss = 5.364534378051758\n",
      "step = 7611000: loss = 4.496631145477295\n",
      "step = 7611200: loss = 4.85917854309082\n",
      "step = 7611400: loss = 3.9496145248413086\n",
      "step = 7611600: loss = 3.910956382751465\n",
      "step = 7611800: loss = 4.353798866271973\n",
      "step = 7612000: loss = 3.284241199493408\n",
      "step = 7612200: loss = 4.745881080627441\n",
      "step = 7612400: loss = 4.625648498535156\n",
      "step = 7612600: loss = 3.944037914276123\n",
      "step = 7612800: loss = 3.624253511428833\n",
      "step = 7613000: loss = 3.462716817855835\n",
      "step = 7613200: loss = 4.289709568023682\n",
      "step = 7613400: loss = 3.9917097091674805\n",
      "step = 7613600: loss = 4.595600605010986\n",
      "step = 7613800: loss = 5.1361894607543945\n",
      "step = 7614000: loss = 4.504678726196289\n",
      "step = 7614200: loss = 3.621445894241333\n",
      "step = 7614400: loss = 2.427530527114868\n",
      "step = 7614600: loss = 4.13838005065918\n",
      "step = 7614800: loss = 4.137706279754639\n",
      "step = 7615000: loss = 5.678779602050781\n",
      "step = 7615000: Average Return = 3.9619998931884766\n",
      "step = 7615200: loss = 3.5713560581207275\n",
      "step = 7615400: loss = 4.69236946105957\n",
      "step = 7615600: loss = 3.841245651245117\n",
      "step = 7615800: loss = 5.2035393714904785\n",
      "step = 7616000: loss = 2.9281005859375\n",
      "step = 7616200: loss = 3.6413280963897705\n",
      "step = 7616400: loss = 4.2322678565979\n",
      "step = 7616600: loss = 3.6000471115112305\n",
      "step = 7616800: loss = 5.563485145568848\n",
      "step = 7617000: loss = 5.565857887268066\n",
      "step = 7617200: loss = 5.019453525543213\n",
      "step = 7617400: loss = 3.9584617614746094\n",
      "step = 7617600: loss = 4.5438947677612305\n",
      "step = 7617800: loss = 4.29499626159668\n",
      "step = 7618000: loss = 4.674799919128418\n",
      "step = 7618200: loss = 3.9024276733398438\n",
      "step = 7618400: loss = 5.322471618652344\n",
      "step = 7618600: loss = 3.495182514190674\n",
      "step = 7618800: loss = 3.0162272453308105\n",
      "step = 7619000: loss = 3.7293713092803955\n",
      "step = 7619200: loss = 4.603316307067871\n",
      "step = 7619400: loss = 3.059643268585205\n",
      "step = 7619600: loss = 3.8708372116088867\n",
      "step = 7619800: loss = 4.88338565826416\n",
      "step = 7620000: loss = 3.6141767501831055\n",
      "step = 7620000: Average Return = 3.812000036239624\n",
      "step = 7620200: loss = 3.557445526123047\n",
      "step = 7620400: loss = 3.851105213165283\n",
      "step = 7620600: loss = 4.246523857116699\n",
      "step = 7620800: loss = 4.233950614929199\n",
      "step = 7621000: loss = 3.426246166229248\n",
      "step = 7621200: loss = 2.191673994064331\n",
      "step = 7621400: loss = 4.594852447509766\n",
      "step = 7621600: loss = 3.7289247512817383\n",
      "step = 7621800: loss = 4.035177230834961\n",
      "step = 7622000: loss = 5.085134983062744\n",
      "step = 7622200: loss = 4.920423984527588\n",
      "step = 7622400: loss = 4.611536979675293\n",
      "step = 7622600: loss = 3.6652352809906006\n",
      "step = 7622800: loss = 4.253758430480957\n",
      "step = 7623000: loss = 3.639375925064087\n",
      "step = 7623200: loss = 4.48137092590332\n",
      "step = 7623400: loss = 3.3534862995147705\n",
      "step = 7623600: loss = 4.43682861328125\n",
      "step = 7623800: loss = 4.49882173538208\n",
      "step = 7624000: loss = 3.340210437774658\n",
      "step = 7624200: loss = 4.377469539642334\n",
      "step = 7624400: loss = 3.946012496948242\n",
      "step = 7624600: loss = 3.147674083709717\n",
      "step = 7624800: loss = 2.9201245307922363\n",
      "step = 7625000: loss = 4.564208984375\n",
      "step = 7625000: Average Return = 3.828000068664551\n",
      "step = 7625200: loss = 4.582303524017334\n",
      "step = 7625400: loss = 3.747734785079956\n",
      "step = 7625600: loss = 4.396227836608887\n",
      "step = 7625800: loss = 4.361670017242432\n",
      "step = 7626000: loss = 5.230396747589111\n",
      "step = 7626200: loss = 5.753538608551025\n",
      "step = 7626400: loss = 3.8637053966522217\n",
      "step = 7626600: loss = 5.117809772491455\n",
      "step = 7626800: loss = 3.496872901916504\n",
      "step = 7627000: loss = 3.9257378578186035\n",
      "step = 7627200: loss = 2.7736434936523438\n",
      "step = 7627400: loss = 4.75029182434082\n",
      "step = 7627600: loss = 3.0615577697753906\n",
      "step = 7627800: loss = 4.745501518249512\n",
      "step = 7628000: loss = 4.18336820602417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7628200: loss = 5.295083999633789\n",
      "step = 7628400: loss = 4.460451126098633\n",
      "step = 7628600: loss = 3.227879047393799\n",
      "step = 7628800: loss = 4.281043529510498\n",
      "step = 7629000: loss = 3.81762957572937\n",
      "step = 7629200: loss = 2.9789798259735107\n",
      "step = 7629400: loss = 4.210666179656982\n",
      "step = 7629600: loss = 3.850520133972168\n",
      "step = 7629800: loss = 6.394344329833984\n",
      "step = 7630000: loss = 5.359087944030762\n",
      "step = 7630000: Average Return = 3.940000057220459\n",
      "step = 7630200: loss = 4.476598262786865\n",
      "step = 7630400: loss = 4.74662971496582\n",
      "step = 7630600: loss = 3.963512420654297\n",
      "step = 7630800: loss = 5.910053730010986\n",
      "step = 7631000: loss = 5.209101676940918\n",
      "step = 7631200: loss = 4.245002269744873\n",
      "step = 7631400: loss = 6.304507732391357\n",
      "step = 7631600: loss = 5.736163139343262\n",
      "step = 7631800: loss = 3.1298887729644775\n",
      "step = 7632000: loss = 2.682898759841919\n",
      "step = 7632200: loss = 3.725504159927368\n",
      "step = 7632400: loss = 3.7397375106811523\n",
      "step = 7632600: loss = 5.793368816375732\n",
      "step = 7632800: loss = 4.183518409729004\n",
      "step = 7633000: loss = 4.646668434143066\n",
      "step = 7633200: loss = 3.7250421047210693\n",
      "step = 7633400: loss = 4.571253776550293\n",
      "step = 7633600: loss = 5.743391036987305\n",
      "step = 7633800: loss = 4.642575263977051\n",
      "step = 7634000: loss = 3.6203713417053223\n",
      "step = 7634200: loss = 4.080193996429443\n",
      "step = 7634400: loss = 3.0817370414733887\n",
      "step = 7634600: loss = 3.823559284210205\n",
      "step = 7634800: loss = 2.6014528274536133\n",
      "step = 7635000: loss = 5.249952793121338\n",
      "step = 7635000: Average Return = 3.9800000190734863\n",
      "step = 7635200: loss = 4.245651721954346\n",
      "step = 7635400: loss = 2.5023722648620605\n",
      "step = 7635600: loss = 4.891127109527588\n",
      "step = 7635800: loss = 3.5700011253356934\n",
      "step = 7636000: loss = 3.4622294902801514\n",
      "step = 7636200: loss = 4.549056529998779\n",
      "step = 7636400: loss = 2.976306438446045\n",
      "step = 7636600: loss = 3.544753074645996\n",
      "step = 7636800: loss = 3.8324525356292725\n",
      "step = 7637000: loss = 3.5008769035339355\n",
      "step = 7637200: loss = 4.631932735443115\n",
      "step = 7637400: loss = 4.000967979431152\n",
      "step = 7637600: loss = 3.332780599594116\n",
      "step = 7637800: loss = 3.5117669105529785\n",
      "step = 7638000: loss = 6.058234691619873\n",
      "step = 7638200: loss = 2.90981388092041\n",
      "step = 7638400: loss = 6.0708441734313965\n",
      "step = 7638600: loss = 4.324477672576904\n",
      "step = 7638800: loss = 3.4827795028686523\n",
      "step = 7639000: loss = 3.2304327487945557\n",
      "step = 7639200: loss = 3.064236879348755\n",
      "step = 7639400: loss = 3.7824883460998535\n",
      "step = 7639600: loss = 4.360241413116455\n",
      "step = 7639800: loss = 4.1339006423950195\n",
      "step = 7640000: loss = 4.409322261810303\n",
      "step = 7640000: Average Return = 4.007999897003174\n",
      "step = 7640200: loss = 3.7631514072418213\n",
      "step = 7640400: loss = 4.087034702301025\n",
      "step = 7640600: loss = 4.763266563415527\n",
      "step = 7640800: loss = 4.169606685638428\n",
      "step = 7641000: loss = 3.724351167678833\n",
      "step = 7641200: loss = 5.777589797973633\n",
      "step = 7641400: loss = 4.565323829650879\n",
      "step = 7641600: loss = 4.184961795806885\n",
      "step = 7641800: loss = 3.2724249362945557\n",
      "step = 7642000: loss = 3.7592711448669434\n",
      "step = 7642200: loss = 4.396500587463379\n",
      "step = 7642400: loss = 3.0577380657196045\n",
      "step = 7642600: loss = 5.194441795349121\n",
      "step = 7642800: loss = 3.3372349739074707\n",
      "step = 7643000: loss = 4.387889385223389\n",
      "step = 7643200: loss = 3.842167377471924\n",
      "step = 7643400: loss = 5.168051242828369\n",
      "step = 7643600: loss = 4.249500751495361\n",
      "step = 7643800: loss = 3.353146553039551\n",
      "step = 7644000: loss = 4.568996906280518\n",
      "step = 7644200: loss = 4.411872386932373\n",
      "step = 7644400: loss = 4.166735649108887\n",
      "step = 7644600: loss = 3.743490219116211\n",
      "step = 7644800: loss = 4.912252902984619\n",
      "step = 7645000: loss = 4.813531875610352\n",
      "step = 7645000: Average Return = 3.7320001125335693\n",
      "step = 7645200: loss = 4.444832801818848\n",
      "step = 7645400: loss = 3.8474409580230713\n",
      "step = 7645600: loss = 5.691155910491943\n",
      "step = 7645800: loss = 3.7405619621276855\n",
      "step = 7646000: loss = 4.1679205894470215\n",
      "step = 7646200: loss = 4.262646675109863\n",
      "step = 7646400: loss = 3.620206832885742\n",
      "step = 7646600: loss = 6.106875419616699\n",
      "step = 7646800: loss = 3.5090179443359375\n",
      "step = 7647000: loss = 4.351583957672119\n",
      "step = 7647200: loss = 3.531789541244507\n",
      "step = 7647400: loss = 3.0515060424804688\n",
      "step = 7647600: loss = 3.8041024208068848\n",
      "step = 7647800: loss = 6.117746353149414\n",
      "step = 7648000: loss = 3.7062695026397705\n",
      "step = 7648200: loss = 3.935103416442871\n",
      "step = 7648400: loss = 6.309376239776611\n",
      "step = 7648600: loss = 4.167832374572754\n",
      "step = 7648800: loss = 3.7477874755859375\n",
      "step = 7649000: loss = 5.257336139678955\n",
      "step = 7649200: loss = 5.280101299285889\n",
      "step = 7649400: loss = 3.0902419090270996\n",
      "step = 7649600: loss = 4.251798629760742\n",
      "step = 7649800: loss = 5.7689409255981445\n",
      "step = 7650000: loss = 3.6535727977752686\n",
      "step = 7650000: Average Return = 3.7960000038146973\n",
      "step = 7650200: loss = 4.091926097869873\n",
      "step = 7650400: loss = 4.633828163146973\n",
      "step = 7650600: loss = 3.3170368671417236\n",
      "step = 7650800: loss = 3.7610044479370117\n",
      "step = 7651000: loss = 3.9522852897644043\n",
      "step = 7651200: loss = 3.898916482925415\n",
      "step = 7651400: loss = 3.7475883960723877\n",
      "step = 7651600: loss = 4.661080837249756\n",
      "step = 7651800: loss = 4.033422946929932\n",
      "step = 7652000: loss = 4.656656265258789\n",
      "step = 7652200: loss = 2.7498199939727783\n",
      "step = 7652400: loss = 4.881365776062012\n",
      "step = 7652600: loss = 3.729442596435547\n",
      "step = 7652800: loss = 4.654044151306152\n",
      "step = 7653000: loss = 3.742143392562866\n",
      "step = 7653200: loss = 4.212993621826172\n",
      "step = 7653400: loss = 2.625549793243408\n",
      "step = 7653600: loss = 4.382576942443848\n",
      "step = 7653800: loss = 4.657809257507324\n",
      "step = 7654000: loss = 4.30772590637207\n",
      "step = 7654200: loss = 4.4032368659973145\n",
      "step = 7654400: loss = 3.80679988861084\n",
      "step = 7654600: loss = 4.5333685874938965\n",
      "step = 7654800: loss = 5.21539306640625\n",
      "step = 7655000: loss = 3.7608449459075928\n",
      "step = 7655000: Average Return = 3.6059999465942383\n",
      "step = 7655200: loss = 5.15872859954834\n",
      "step = 7655400: loss = 4.621358394622803\n",
      "step = 7655600: loss = 3.5386712551116943\n",
      "step = 7655800: loss = 4.40147590637207\n",
      "step = 7656000: loss = 2.9573631286621094\n",
      "step = 7656200: loss = 3.7234137058258057\n",
      "step = 7656400: loss = 3.747358560562134\n",
      "step = 7656600: loss = 3.864943504333496\n",
      "step = 7656800: loss = 4.100708961486816\n",
      "step = 7657000: loss = 4.329786777496338\n",
      "step = 7657200: loss = 3.746548652648926\n",
      "step = 7657400: loss = 4.545979022979736\n",
      "step = 7657600: loss = 3.7623558044433594\n",
      "step = 7657800: loss = 3.3899357318878174\n",
      "step = 7658000: loss = 3.178826332092285\n",
      "step = 7658200: loss = 3.4986608028411865\n",
      "step = 7658400: loss = 3.2481627464294434\n",
      "step = 7658600: loss = 3.7778704166412354\n",
      "step = 7658800: loss = 3.363891839981079\n",
      "step = 7659000: loss = 3.9812474250793457\n",
      "step = 7659200: loss = 4.14162540435791\n",
      "step = 7659400: loss = 4.304621696472168\n",
      "step = 7659600: loss = 3.496952533721924\n",
      "step = 7659800: loss = 2.9308831691741943\n",
      "step = 7660000: loss = 3.63515305519104\n",
      "step = 7660000: Average Return = 3.937999963760376\n",
      "step = 7660200: loss = 4.515552520751953\n",
      "step = 7660400: loss = 3.690131425857544\n",
      "step = 7660600: loss = 4.109911918640137\n",
      "step = 7660800: loss = 3.4741427898406982\n",
      "step = 7661000: loss = 4.125264644622803\n",
      "step = 7661200: loss = 3.6857848167419434\n",
      "step = 7661400: loss = 3.595228910446167\n",
      "step = 7661600: loss = 4.154257774353027\n",
      "step = 7661800: loss = 3.962867259979248\n",
      "step = 7662000: loss = 4.349096775054932\n",
      "step = 7662200: loss = 3.6368556022644043\n",
      "step = 7662400: loss = 3.677861452102661\n",
      "step = 7662600: loss = 4.322707653045654\n",
      "step = 7662800: loss = 3.591282844543457\n",
      "step = 7663000: loss = 3.4912126064300537\n",
      "step = 7663200: loss = 3.6974706649780273\n",
      "step = 7663400: loss = 3.1911773681640625\n",
      "step = 7663600: loss = 4.61140775680542\n",
      "step = 7663800: loss = 4.592244625091553\n",
      "step = 7664000: loss = 4.248909950256348\n",
      "step = 7664200: loss = 3.14003586769104\n",
      "step = 7664400: loss = 3.088243007659912\n",
      "step = 7664600: loss = 3.5307202339172363\n",
      "step = 7664800: loss = 5.148167610168457\n",
      "step = 7665000: loss = 3.2443864345550537\n",
      "step = 7665000: Average Return = 3.5940001010894775\n",
      "step = 7665200: loss = 3.689228057861328\n",
      "step = 7665400: loss = 3.460911273956299\n",
      "step = 7665600: loss = 2.9054760932922363\n",
      "step = 7665800: loss = 2.9901061058044434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7666000: loss = 4.026736736297607\n",
      "step = 7666200: loss = 2.4610514640808105\n",
      "step = 7666400: loss = 3.391239643096924\n",
      "step = 7666600: loss = 3.3449461460113525\n",
      "step = 7666800: loss = 4.030251979827881\n",
      "step = 7667000: loss = 4.9322509765625\n",
      "step = 7667200: loss = 3.804462432861328\n",
      "step = 7667400: loss = 5.127498626708984\n",
      "step = 7667600: loss = 3.534379720687866\n",
      "step = 7667800: loss = 5.5731706619262695\n",
      "step = 7668000: loss = 3.639846086502075\n",
      "step = 7668200: loss = 4.189273834228516\n",
      "step = 7668400: loss = 3.1039276123046875\n",
      "step = 7668600: loss = 5.876523494720459\n",
      "step = 7668800: loss = 4.256253719329834\n",
      "step = 7669000: loss = 2.1683902740478516\n",
      "step = 7669200: loss = 3.24960994720459\n",
      "step = 7669400: loss = 3.5918283462524414\n",
      "step = 7669600: loss = 4.099466323852539\n",
      "step = 7669800: loss = 4.366819381713867\n",
      "step = 7670000: loss = 2.731132745742798\n",
      "step = 7670000: Average Return = 4.085999965667725\n",
      "step = 7670200: loss = 4.253963470458984\n",
      "step = 7670400: loss = 4.933370113372803\n",
      "step = 7670600: loss = 3.021768093109131\n",
      "step = 7670800: loss = 3.431652784347534\n",
      "step = 7671000: loss = 4.250389099121094\n",
      "step = 7671200: loss = 2.9338488578796387\n",
      "step = 7671400: loss = 3.6175694465637207\n",
      "step = 7671600: loss = 3.8106961250305176\n",
      "step = 7671800: loss = 4.485006332397461\n",
      "step = 7672000: loss = 3.8607594966888428\n",
      "step = 7672200: loss = 4.748882293701172\n",
      "step = 7672400: loss = 3.7519121170043945\n",
      "step = 7672600: loss = 4.335679531097412\n",
      "step = 7672800: loss = 4.3312811851501465\n",
      "step = 7673000: loss = 4.158225059509277\n",
      "step = 7673200: loss = 4.72144079208374\n",
      "step = 7673400: loss = 4.964767932891846\n",
      "step = 7673600: loss = 5.680197238922119\n",
      "step = 7673800: loss = 4.257370948791504\n",
      "step = 7674000: loss = 5.344351768493652\n",
      "step = 7674200: loss = 3.996793746948242\n",
      "step = 7674400: loss = 3.3062617778778076\n",
      "step = 7674600: loss = 3.5455076694488525\n",
      "step = 7674800: loss = 4.829057693481445\n",
      "step = 7675000: loss = 4.612903118133545\n",
      "step = 7675000: Average Return = 3.6519999504089355\n",
      "step = 7675200: loss = 3.8087685108184814\n",
      "step = 7675400: loss = 3.8269200325012207\n",
      "step = 7675600: loss = 4.708443641662598\n",
      "step = 7675800: loss = 3.817192792892456\n",
      "step = 7676000: loss = 4.313321590423584\n",
      "step = 7676200: loss = 4.223303318023682\n",
      "step = 7676400: loss = 3.826953649520874\n",
      "step = 7676600: loss = 4.528420925140381\n",
      "step = 7676800: loss = 4.306384086608887\n",
      "step = 7677000: loss = 3.142474889755249\n",
      "step = 7677200: loss = 5.137843132019043\n",
      "step = 7677400: loss = 2.658513069152832\n",
      "step = 7677600: loss = 4.707726001739502\n",
      "step = 7677800: loss = 5.41566276550293\n",
      "step = 7678000: loss = 4.725959777832031\n",
      "step = 7678200: loss = 4.168042182922363\n",
      "step = 7678400: loss = 3.985463857650757\n",
      "step = 7678600: loss = 4.023496150970459\n",
      "step = 7678800: loss = 5.020611763000488\n",
      "step = 7679000: loss = 4.066650867462158\n",
      "step = 7679200: loss = 4.60238790512085\n",
      "step = 7679400: loss = 3.660341501235962\n",
      "step = 7679600: loss = 4.836239337921143\n",
      "step = 7679800: loss = 5.1724042892456055\n",
      "step = 7680000: loss = 4.401614665985107\n",
      "step = 7680000: Average Return = 3.9839999675750732\n",
      "step = 7680200: loss = 4.569320201873779\n",
      "step = 7680400: loss = 3.302290916442871\n",
      "step = 7680600: loss = 5.303924560546875\n",
      "step = 7680800: loss = 4.279830455780029\n",
      "step = 7681000: loss = 3.930788516998291\n",
      "step = 7681200: loss = 4.505978107452393\n",
      "step = 7681400: loss = 4.241637229919434\n",
      "step = 7681600: loss = 4.398543357849121\n",
      "step = 7681800: loss = 3.4110710620880127\n",
      "step = 7682000: loss = 4.287852764129639\n",
      "step = 7682200: loss = 4.900839328765869\n",
      "step = 7682400: loss = 4.900416851043701\n",
      "step = 7682600: loss = 4.4673848152160645\n",
      "step = 7682800: loss = 4.3647141456604\n",
      "step = 7683000: loss = 3.376683473587036\n",
      "step = 7683200: loss = 4.355618953704834\n",
      "step = 7683400: loss = 4.2405009269714355\n",
      "step = 7683600: loss = 3.814549207687378\n",
      "step = 7683800: loss = 4.052331924438477\n",
      "step = 7684000: loss = 4.159178733825684\n",
      "step = 7684200: loss = 4.867974281311035\n",
      "step = 7684400: loss = 4.202814102172852\n",
      "step = 7684600: loss = 3.721344470977783\n",
      "step = 7684800: loss = 4.7866740226745605\n",
      "step = 7685000: loss = 2.9252679347991943\n",
      "step = 7685000: Average Return = 3.625999927520752\n",
      "step = 7685200: loss = 4.2364182472229\n",
      "step = 7685400: loss = 5.246652603149414\n",
      "step = 7685600: loss = 3.7388055324554443\n",
      "step = 7685800: loss = 4.271913051605225\n",
      "step = 7686000: loss = 3.888699769973755\n",
      "step = 7686200: loss = 4.765521049499512\n",
      "step = 7686400: loss = 3.5698394775390625\n",
      "step = 7686600: loss = 3.719851493835449\n",
      "step = 7686800: loss = 4.268404006958008\n",
      "step = 7687000: loss = 4.152387619018555\n",
      "step = 7687200: loss = 3.3924975395202637\n",
      "step = 7687400: loss = 3.260573625564575\n",
      "step = 7687600: loss = 4.936325550079346\n",
      "step = 7687800: loss = 3.2044923305511475\n",
      "step = 7688000: loss = 3.9310314655303955\n",
      "step = 7688200: loss = 3.115550994873047\n",
      "step = 7688400: loss = 3.7801830768585205\n",
      "step = 7688600: loss = 5.355591773986816\n",
      "step = 7688800: loss = 4.098773956298828\n",
      "step = 7689000: loss = 5.967660427093506\n",
      "step = 7689200: loss = 4.692736625671387\n",
      "step = 7689400: loss = 4.019783020019531\n",
      "step = 7689600: loss = 4.738051414489746\n",
      "step = 7689800: loss = 4.708780288696289\n",
      "step = 7690000: loss = 5.23912239074707\n",
      "step = 7690000: Average Return = 3.875999927520752\n",
      "step = 7690200: loss = 3.8328356742858887\n",
      "step = 7690400: loss = 4.557706356048584\n",
      "step = 7690600: loss = 5.866732597351074\n",
      "step = 7690800: loss = 3.0201258659362793\n",
      "step = 7691000: loss = 4.350796699523926\n",
      "step = 7691200: loss = 5.191915035247803\n",
      "step = 7691400: loss = 3.464942455291748\n",
      "step = 7691600: loss = 4.879819393157959\n",
      "step = 7691800: loss = 3.3282995223999023\n",
      "step = 7692000: loss = 3.58467960357666\n",
      "step = 7692200: loss = 4.2338409423828125\n",
      "step = 7692400: loss = 3.836665630340576\n",
      "step = 7692600: loss = 4.16580867767334\n",
      "step = 7692800: loss = 3.6425998210906982\n",
      "step = 7693000: loss = 5.774694919586182\n",
      "step = 7693200: loss = 3.3022258281707764\n",
      "step = 7693400: loss = 3.2514030933380127\n",
      "step = 7693600: loss = 4.68170690536499\n",
      "step = 7693800: loss = 4.0810227394104\n",
      "step = 7694000: loss = 3.806361675262451\n",
      "step = 7694200: loss = 3.3829619884490967\n",
      "step = 7694400: loss = 4.806157112121582\n",
      "step = 7694600: loss = 4.744152545928955\n",
      "step = 7694800: loss = 2.6853346824645996\n",
      "step = 7695000: loss = 3.5339252948760986\n",
      "step = 7695000: Average Return = 4.076000213623047\n",
      "step = 7695200: loss = 2.4853532314300537\n",
      "step = 7695400: loss = 4.874602794647217\n",
      "step = 7695600: loss = 2.8006608486175537\n",
      "step = 7695800: loss = 3.4947595596313477\n",
      "step = 7696000: loss = 3.642484664916992\n",
      "step = 7696200: loss = 4.968318462371826\n",
      "step = 7696400: loss = 3.695591449737549\n",
      "step = 7696600: loss = 3.8370463848114014\n",
      "step = 7696800: loss = 4.336169242858887\n",
      "step = 7697000: loss = 4.4850311279296875\n",
      "step = 7697200: loss = 3.5174102783203125\n",
      "step = 7697400: loss = 3.2259154319763184\n",
      "step = 7697600: loss = 3.854916572570801\n",
      "step = 7697800: loss = 5.557257652282715\n",
      "step = 7698000: loss = 3.231041193008423\n",
      "step = 7698200: loss = 3.40480637550354\n",
      "step = 7698400: loss = 3.6619677543640137\n",
      "step = 7698600: loss = 4.452667713165283\n",
      "step = 7698800: loss = 3.439443826675415\n",
      "step = 7699000: loss = 4.091975688934326\n",
      "step = 7699200: loss = 4.80951452255249\n",
      "step = 7699400: loss = 3.490318775177002\n",
      "step = 7699600: loss = 3.5766515731811523\n",
      "step = 7699800: loss = 3.2064049243927\n",
      "step = 7700000: loss = 4.364618301391602\n",
      "step = 7700000: Average Return = 3.625999927520752\n",
      "step = 7700200: loss = 2.9359636306762695\n",
      "step = 7700400: loss = 4.166460037231445\n",
      "step = 7700600: loss = 4.4549102783203125\n",
      "step = 7700800: loss = 5.209024429321289\n",
      "step = 7701000: loss = 2.726372241973877\n",
      "step = 7701200: loss = 3.778831958770752\n",
      "step = 7701400: loss = 4.248042583465576\n",
      "step = 7701600: loss = 5.063263416290283\n",
      "step = 7701800: loss = 5.267796516418457\n",
      "step = 7702000: loss = 3.723407745361328\n",
      "step = 7702200: loss = 5.376349449157715\n",
      "step = 7702400: loss = 4.74554967880249\n",
      "step = 7702600: loss = 5.578242778778076\n",
      "step = 7702800: loss = 5.482947826385498\n",
      "step = 7703000: loss = 4.063126564025879\n",
      "step = 7703200: loss = 5.539334774017334\n",
      "step = 7703400: loss = 3.5674192905426025\n",
      "step = 7703600: loss = 4.346511363983154\n",
      "step = 7703800: loss = 4.486021518707275\n",
      "step = 7704000: loss = 3.4607174396514893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7704200: loss = 4.86433219909668\n",
      "step = 7704400: loss = 3.7623119354248047\n",
      "step = 7704600: loss = 4.2638092041015625\n",
      "step = 7704800: loss = 4.573488235473633\n",
      "step = 7705000: loss = 4.033432483673096\n",
      "step = 7705000: Average Return = 4.105999946594238\n",
      "step = 7705200: loss = 5.837568283081055\n",
      "step = 7705400: loss = 5.4334845542907715\n",
      "step = 7705600: loss = 4.564702987670898\n",
      "step = 7705800: loss = 4.988409996032715\n",
      "step = 7706000: loss = 4.391640663146973\n",
      "step = 7706200: loss = 4.588255405426025\n",
      "step = 7706400: loss = 3.5630221366882324\n",
      "step = 7706600: loss = 3.7058913707733154\n",
      "step = 7706800: loss = 3.3796751499176025\n",
      "step = 7707000: loss = 4.517665386199951\n",
      "step = 7707200: loss = 4.79200553894043\n",
      "step = 7707400: loss = 3.992985963821411\n",
      "step = 7707600: loss = 3.9971957206726074\n",
      "step = 7707800: loss = 3.7745561599731445\n",
      "step = 7708000: loss = 4.049400329589844\n",
      "step = 7708200: loss = 4.720385551452637\n",
      "step = 7708400: loss = 3.8687520027160645\n",
      "step = 7708600: loss = 4.850253582000732\n",
      "step = 7708800: loss = 3.364614725112915\n",
      "step = 7709000: loss = 4.526308536529541\n",
      "step = 7709200: loss = 4.7672810554504395\n",
      "step = 7709400: loss = 3.6496598720550537\n",
      "step = 7709600: loss = 3.395772695541382\n",
      "step = 7709800: loss = 5.599311351776123\n",
      "step = 7710000: loss = 4.987198352813721\n",
      "step = 7710000: Average Return = 4.263999938964844\n",
      "step = 7710200: loss = 4.116955280303955\n",
      "step = 7710400: loss = 2.6412603855133057\n",
      "step = 7710600: loss = 4.364143371582031\n",
      "step = 7710800: loss = 3.947122812271118\n",
      "step = 7711000: loss = 4.27557373046875\n",
      "step = 7711200: loss = 3.7414019107818604\n",
      "step = 7711400: loss = 4.729610919952393\n",
      "step = 7711600: loss = 4.034043312072754\n",
      "step = 7711800: loss = 4.8538737297058105\n",
      "step = 7712000: loss = 4.271252155303955\n",
      "step = 7712200: loss = 2.9343626499176025\n",
      "step = 7712400: loss = 4.223894119262695\n",
      "step = 7712600: loss = 5.699769020080566\n",
      "step = 7712800: loss = 2.987091541290283\n",
      "step = 7713000: loss = 5.751759052276611\n",
      "step = 7713200: loss = 4.949554920196533\n",
      "step = 7713400: loss = 4.238072872161865\n",
      "step = 7713600: loss = 5.048226356506348\n",
      "step = 7713800: loss = 4.681971549987793\n",
      "step = 7714000: loss = 5.5449652671813965\n",
      "step = 7714200: loss = 3.6391308307647705\n",
      "step = 7714400: loss = 5.263044834136963\n",
      "step = 7714600: loss = 3.8759734630584717\n",
      "step = 7714800: loss = 4.426167964935303\n",
      "step = 7715000: loss = 5.755688190460205\n",
      "step = 7715000: Average Return = 3.927999973297119\n",
      "step = 7715200: loss = 4.517124652862549\n",
      "step = 7715400: loss = 5.373888969421387\n",
      "step = 7715600: loss = 3.423868417739868\n",
      "step = 7715800: loss = 4.028210163116455\n",
      "step = 7716000: loss = 5.234684467315674\n",
      "step = 7716200: loss = 4.685239791870117\n",
      "step = 7716400: loss = 4.107177257537842\n",
      "step = 7716600: loss = 4.206423282623291\n",
      "step = 7716800: loss = 5.788788795471191\n",
      "step = 7717000: loss = 4.655945777893066\n",
      "step = 7717200: loss = 4.353184223175049\n",
      "step = 7717400: loss = 5.687413215637207\n",
      "step = 7717600: loss = 3.8436574935913086\n",
      "step = 7717800: loss = 5.337294578552246\n",
      "step = 7718000: loss = 4.242284297943115\n",
      "step = 7718200: loss = 3.992252826690674\n",
      "step = 7718400: loss = 4.748424530029297\n",
      "step = 7718600: loss = 3.8832054138183594\n",
      "step = 7718800: loss = 3.479935884475708\n",
      "step = 7719000: loss = 3.699160575866699\n",
      "step = 7719200: loss = 3.7184672355651855\n",
      "step = 7719400: loss = 4.023929595947266\n",
      "step = 7719600: loss = 4.7928009033203125\n",
      "step = 7719800: loss = 3.8942790031433105\n",
      "step = 7720000: loss = 4.961724281311035\n",
      "step = 7720000: Average Return = 3.7679998874664307\n",
      "step = 7720200: loss = 3.8305227756500244\n",
      "step = 7720400: loss = 4.880703449249268\n",
      "step = 7720600: loss = 5.365435600280762\n",
      "step = 7720800: loss = 3.9016454219818115\n",
      "step = 7721000: loss = 3.0822126865386963\n",
      "step = 7721200: loss = 2.84478497505188\n",
      "step = 7721400: loss = 4.63776969909668\n",
      "step = 7721600: loss = 4.349734306335449\n",
      "step = 7721800: loss = 4.426930904388428\n",
      "step = 7722000: loss = 4.471527576446533\n",
      "step = 7722200: loss = 4.000446796417236\n",
      "step = 7722400: loss = 5.312707424163818\n",
      "step = 7722600: loss = 3.6185216903686523\n",
      "step = 7722800: loss = 4.525277137756348\n",
      "step = 7723000: loss = 5.238856792449951\n",
      "step = 7723200: loss = 4.285393238067627\n",
      "step = 7723400: loss = 4.1140546798706055\n",
      "step = 7723600: loss = 4.2608561515808105\n",
      "step = 7723800: loss = 5.0651021003723145\n",
      "step = 7724000: loss = 4.698700904846191\n",
      "step = 7724200: loss = 4.576904773712158\n",
      "step = 7724400: loss = 4.407159328460693\n",
      "step = 7724600: loss = 3.677738666534424\n",
      "step = 7724800: loss = 4.3770365715026855\n",
      "step = 7725000: loss = 5.154874801635742\n",
      "step = 7725000: Average Return = 3.0899999141693115\n",
      "step = 7725200: loss = 4.017707347869873\n",
      "step = 7725400: loss = 3.2940523624420166\n",
      "step = 7725600: loss = 4.389291763305664\n",
      "step = 7725800: loss = 4.480693340301514\n",
      "step = 7726000: loss = 3.5775654315948486\n",
      "step = 7726200: loss = 4.8399577140808105\n",
      "step = 7726400: loss = 4.602207660675049\n",
      "step = 7726600: loss = 3.714940309524536\n",
      "step = 7726800: loss = 3.781522035598755\n",
      "step = 7727000: loss = 4.552891731262207\n",
      "step = 7727200: loss = 3.184861898422241\n",
      "step = 7727400: loss = 5.611699104309082\n",
      "step = 7727600: loss = 5.370980739593506\n",
      "step = 7727800: loss = 3.456987142562866\n",
      "step = 7728000: loss = 4.4748854637146\n",
      "step = 7728200: loss = 3.8757545948028564\n",
      "step = 7728400: loss = 4.784317970275879\n",
      "step = 7728600: loss = 5.890124320983887\n",
      "step = 7728800: loss = 3.86855149269104\n",
      "step = 7729000: loss = 3.3432507514953613\n",
      "step = 7729200: loss = 5.083209037780762\n",
      "step = 7729400: loss = 3.1094322204589844\n",
      "step = 7729600: loss = 5.681595325469971\n",
      "step = 7729800: loss = 4.970878601074219\n",
      "step = 7730000: loss = 4.105539321899414\n",
      "step = 7730000: Average Return = 3.8959999084472656\n",
      "step = 7730200: loss = 4.787617206573486\n",
      "step = 7730400: loss = 4.324565887451172\n",
      "step = 7730600: loss = 3.758906841278076\n",
      "step = 7730800: loss = 3.101175546646118\n",
      "step = 7731000: loss = 4.38227653503418\n",
      "step = 7731200: loss = 3.5747838020324707\n",
      "step = 7731400: loss = 4.212123394012451\n",
      "step = 7731600: loss = 5.0548858642578125\n",
      "step = 7731800: loss = 4.9524712562561035\n",
      "step = 7732000: loss = 4.481792449951172\n",
      "step = 7732200: loss = 3.4152984619140625\n",
      "step = 7732400: loss = 3.8971245288848877\n",
      "step = 7732600: loss = 3.988245725631714\n",
      "step = 7732800: loss = 4.486440181732178\n",
      "step = 7733000: loss = 4.557344436645508\n",
      "step = 7733200: loss = 3.8251571655273438\n",
      "step = 7733400: loss = 4.959673881530762\n",
      "step = 7733600: loss = 2.2831695079803467\n",
      "step = 7733800: loss = 4.257679462432861\n",
      "step = 7734000: loss = 3.274855852127075\n",
      "step = 7734200: loss = 4.195533752441406\n",
      "step = 7734400: loss = 3.679626941680908\n",
      "step = 7734600: loss = 4.399274826049805\n",
      "step = 7734800: loss = 4.15189266204834\n",
      "step = 7735000: loss = 4.414651393890381\n",
      "step = 7735000: Average Return = 3.7360000610351562\n",
      "step = 7735200: loss = 3.5736284255981445\n",
      "step = 7735400: loss = 4.152026176452637\n",
      "step = 7735600: loss = 3.4872331619262695\n",
      "step = 7735800: loss = 4.658491134643555\n",
      "step = 7736000: loss = 5.744635581970215\n",
      "step = 7736200: loss = 3.57332706451416\n",
      "step = 7736400: loss = 3.6723287105560303\n",
      "step = 7736600: loss = 4.737087249755859\n",
      "step = 7736800: loss = 3.755190849304199\n",
      "step = 7737000: loss = 3.78629469871521\n",
      "step = 7737200: loss = 3.3745031356811523\n",
      "step = 7737400: loss = 4.785536766052246\n",
      "step = 7737600: loss = 3.3895623683929443\n",
      "step = 7737800: loss = 6.034173488616943\n",
      "step = 7738000: loss = 4.840864181518555\n",
      "step = 7738200: loss = 4.964885234832764\n",
      "step = 7738400: loss = 5.232992649078369\n",
      "step = 7738600: loss = 4.9807586669921875\n",
      "step = 7738800: loss = 3.5486583709716797\n",
      "step = 7739000: loss = 5.637964725494385\n",
      "step = 7739200: loss = 3.9718704223632812\n",
      "step = 7739400: loss = 4.001942157745361\n",
      "step = 7739600: loss = 4.719902992248535\n",
      "step = 7739800: loss = 4.034189224243164\n",
      "step = 7740000: loss = 4.140906810760498\n",
      "step = 7740000: Average Return = 3.799999952316284\n",
      "step = 7740200: loss = 3.898191452026367\n",
      "step = 7740400: loss = 4.240538120269775\n",
      "step = 7740600: loss = 1.9839881658554077\n",
      "step = 7740800: loss = 5.282279968261719\n",
      "step = 7741000: loss = 4.644250392913818\n",
      "step = 7741200: loss = 5.095292091369629\n",
      "step = 7741400: loss = 3.9599010944366455\n",
      "step = 7741600: loss = 4.777313232421875\n",
      "step = 7741800: loss = 3.093548536300659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7742000: loss = 5.39124870300293\n",
      "step = 7742200: loss = 4.160288333892822\n",
      "step = 7742400: loss = 4.501047611236572\n",
      "step = 7742600: loss = 4.833315849304199\n",
      "step = 7742800: loss = 5.518339157104492\n",
      "step = 7743000: loss = 4.146244525909424\n",
      "step = 7743200: loss = 4.030829429626465\n",
      "step = 7743400: loss = 3.9022209644317627\n",
      "step = 7743600: loss = 4.091706275939941\n",
      "step = 7743800: loss = 5.065854549407959\n",
      "step = 7744000: loss = 3.7768349647521973\n",
      "step = 7744200: loss = 4.427227020263672\n",
      "step = 7744400: loss = 3.5555174350738525\n",
      "step = 7744600: loss = 5.044317722320557\n",
      "step = 7744800: loss = 4.09165096282959\n",
      "step = 7745000: loss = 4.707871437072754\n",
      "step = 7745000: Average Return = 3.5360000133514404\n",
      "step = 7745200: loss = 3.530271530151367\n",
      "step = 7745400: loss = 4.414891242980957\n",
      "step = 7745600: loss = 4.383063316345215\n",
      "step = 7745800: loss = 4.909018516540527\n",
      "step = 7746000: loss = 3.3740081787109375\n",
      "step = 7746200: loss = 3.333163022994995\n",
      "step = 7746400: loss = 3.416609048843384\n",
      "step = 7746600: loss = 3.985501527786255\n",
      "step = 7746800: loss = 3.9853675365448\n",
      "step = 7747000: loss = 3.8097362518310547\n",
      "step = 7747200: loss = 4.040339946746826\n",
      "step = 7747400: loss = 3.783971071243286\n",
      "step = 7747600: loss = 5.695472717285156\n",
      "step = 7747800: loss = 2.9387753009796143\n",
      "step = 7748000: loss = 5.190225601196289\n",
      "step = 7748200: loss = 4.804862976074219\n",
      "step = 7748400: loss = 3.407879590988159\n",
      "step = 7748600: loss = 4.069495677947998\n",
      "step = 7748800: loss = 3.5146472454071045\n",
      "step = 7749000: loss = 3.09140944480896\n",
      "step = 7749200: loss = 6.0006256103515625\n",
      "step = 7749400: loss = 2.2786269187927246\n",
      "step = 7749600: loss = 3.803060293197632\n",
      "step = 7749800: loss = 3.4797592163085938\n",
      "step = 7750000: loss = 3.1167821884155273\n",
      "step = 7750000: Average Return = 3.9079999923706055\n",
      "step = 7750200: loss = 2.8288214206695557\n",
      "step = 7750400: loss = 4.002992630004883\n",
      "step = 7750600: loss = 5.152302265167236\n",
      "step = 7750800: loss = 5.120247840881348\n",
      "step = 7751000: loss = 3.119049549102783\n",
      "step = 7751200: loss = 4.473873615264893\n",
      "step = 7751400: loss = 3.7301440238952637\n",
      "step = 7751600: loss = 2.4615213871002197\n",
      "step = 7751800: loss = 4.636586666107178\n",
      "step = 7752000: loss = 4.768831253051758\n",
      "step = 7752200: loss = 4.433053493499756\n",
      "step = 7752400: loss = 5.510367393493652\n",
      "step = 7752600: loss = 4.413034439086914\n",
      "step = 7752800: loss = 3.285266637802124\n",
      "step = 7753000: loss = 4.792633056640625\n",
      "step = 7753200: loss = 2.8788204193115234\n",
      "step = 7753400: loss = 5.064509391784668\n",
      "step = 7753600: loss = 4.6387224197387695\n",
      "step = 7753800: loss = 4.757604122161865\n",
      "step = 7754000: loss = 4.212288856506348\n",
      "step = 7754200: loss = 4.286079406738281\n",
      "step = 7754400: loss = 4.152497291564941\n",
      "step = 7754600: loss = 3.6287078857421875\n",
      "step = 7754800: loss = 4.525895595550537\n",
      "step = 7755000: loss = 3.502934455871582\n",
      "step = 7755000: Average Return = 3.9800000190734863\n",
      "step = 7755200: loss = 5.040286540985107\n",
      "step = 7755400: loss = 4.048305034637451\n",
      "step = 7755600: loss = 3.3197898864746094\n",
      "step = 7755800: loss = 4.514132499694824\n",
      "step = 7756000: loss = 4.623451232910156\n",
      "step = 7756200: loss = 4.156851291656494\n",
      "step = 7756400: loss = 3.9097988605499268\n",
      "step = 7756600: loss = 4.142749786376953\n",
      "step = 7756800: loss = 4.1377410888671875\n",
      "step = 7757000: loss = 2.737602710723877\n",
      "step = 7757200: loss = 5.49505615234375\n",
      "step = 7757400: loss = 3.914799213409424\n",
      "step = 7757600: loss = 4.618645191192627\n",
      "step = 7757800: loss = 3.7536418437957764\n",
      "step = 7758000: loss = 3.8110313415527344\n",
      "step = 7758200: loss = 5.2303876876831055\n",
      "step = 7758400: loss = 4.602885723114014\n",
      "step = 7758600: loss = 5.623504638671875\n",
      "step = 7758800: loss = 5.043762683868408\n",
      "step = 7759000: loss = 4.574254512786865\n",
      "step = 7759200: loss = 5.1179585456848145\n",
      "step = 7759400: loss = 4.827929496765137\n",
      "step = 7759600: loss = 4.631484508514404\n",
      "step = 7759800: loss = 5.759321212768555\n",
      "step = 7760000: loss = 5.1395487785339355\n",
      "step = 7760000: Average Return = 3.944000005722046\n",
      "step = 7760200: loss = 3.703505277633667\n",
      "step = 7760400: loss = 4.207162857055664\n",
      "step = 7760600: loss = 4.3724894523620605\n",
      "step = 7760800: loss = 3.303231716156006\n",
      "step = 7761000: loss = 5.718761920928955\n",
      "step = 7761200: loss = 3.625650644302368\n",
      "step = 7761400: loss = 4.35946798324585\n",
      "step = 7761600: loss = 3.9595611095428467\n",
      "step = 7761800: loss = 3.073711395263672\n",
      "step = 7762000: loss = 4.249016284942627\n",
      "step = 7762200: loss = 4.1451873779296875\n",
      "step = 7762400: loss = 3.682239294052124\n",
      "step = 7762600: loss = 4.340148448944092\n",
      "step = 7762800: loss = 4.317663669586182\n",
      "step = 7763000: loss = 3.8824729919433594\n",
      "step = 7763200: loss = 4.446098327636719\n",
      "step = 7763400: loss = 2.1812705993652344\n",
      "step = 7763600: loss = 3.75138258934021\n",
      "step = 7763800: loss = 3.942957878112793\n",
      "step = 7764000: loss = 3.981121778488159\n",
      "step = 7764200: loss = 3.332625150680542\n",
      "step = 7764400: loss = 4.99304723739624\n",
      "step = 7764600: loss = 4.935194492340088\n",
      "step = 7764800: loss = 4.411570072174072\n",
      "step = 7765000: loss = 4.474600791931152\n",
      "step = 7765000: Average Return = 3.7260000705718994\n",
      "step = 7765200: loss = 3.8017613887786865\n",
      "step = 7765400: loss = 4.388286113739014\n",
      "step = 7765600: loss = 3.706125497817993\n",
      "step = 7765800: loss = 4.610311985015869\n",
      "step = 7766000: loss = 5.455521106719971\n",
      "step = 7766200: loss = 5.484687805175781\n",
      "step = 7766400: loss = 3.930842876434326\n",
      "step = 7766600: loss = 4.539230823516846\n",
      "step = 7766800: loss = 5.314321041107178\n",
      "step = 7767000: loss = 5.320084095001221\n",
      "step = 7767200: loss = 3.262622594833374\n",
      "step = 7767400: loss = 4.486758708953857\n",
      "step = 7767600: loss = 3.824414014816284\n",
      "step = 7767800: loss = 4.52225923538208\n",
      "step = 7768000: loss = 4.363745212554932\n",
      "step = 7768200: loss = 5.303027153015137\n",
      "step = 7768400: loss = 3.180964231491089\n",
      "step = 7768600: loss = 4.342745780944824\n",
      "step = 7768800: loss = 3.0179381370544434\n",
      "step = 7769000: loss = 4.954074382781982\n",
      "step = 7769200: loss = 4.065578460693359\n",
      "step = 7769400: loss = 5.45985746383667\n",
      "step = 7769600: loss = 3.531904935836792\n",
      "step = 7769800: loss = 3.749262809753418\n",
      "step = 7770000: loss = 3.1158740520477295\n",
      "step = 7770000: Average Return = 3.4739999771118164\n",
      "step = 7770200: loss = 3.9338128566741943\n",
      "step = 7770400: loss = 4.952670097351074\n",
      "step = 7770600: loss = 3.2562015056610107\n",
      "step = 7770800: loss = 4.976239204406738\n",
      "step = 7771000: loss = 4.951657772064209\n",
      "step = 7771200: loss = 4.75787878036499\n",
      "step = 7771400: loss = 3.8387603759765625\n",
      "step = 7771600: loss = 3.3157713413238525\n",
      "step = 7771800: loss = 3.912332057952881\n",
      "step = 7772000: loss = 4.41091775894165\n",
      "step = 7772200: loss = 5.396245002746582\n",
      "step = 7772400: loss = 5.319462776184082\n",
      "step = 7772600: loss = 3.7851359844207764\n",
      "step = 7772800: loss = 3.787692070007324\n",
      "step = 7773000: loss = 3.405832529067993\n",
      "step = 7773200: loss = 4.011630535125732\n",
      "step = 7773400: loss = 3.7318737506866455\n",
      "step = 7773600: loss = 3.083871603012085\n",
      "step = 7773800: loss = 4.286591053009033\n",
      "step = 7774000: loss = 4.894191265106201\n",
      "step = 7774200: loss = 4.921724796295166\n",
      "step = 7774400: loss = 4.165454387664795\n",
      "step = 7774600: loss = 4.177175998687744\n",
      "step = 7774800: loss = 3.6598100662231445\n",
      "step = 7775000: loss = 4.224276542663574\n",
      "step = 7775000: Average Return = 3.9760000705718994\n",
      "step = 7775200: loss = 4.4863104820251465\n",
      "step = 7775400: loss = 2.8067638874053955\n",
      "step = 7775600: loss = 4.2797675132751465\n",
      "step = 7775800: loss = 3.839318037033081\n",
      "step = 7776000: loss = 3.6383216381073\n",
      "step = 7776200: loss = 3.1163158416748047\n",
      "step = 7776400: loss = 4.666923522949219\n",
      "step = 7776600: loss = 3.86088490486145\n",
      "step = 7776800: loss = 2.9469358921051025\n",
      "step = 7777000: loss = 4.511123180389404\n",
      "step = 7777200: loss = 3.6190056800842285\n",
      "step = 7777400: loss = 6.125201225280762\n",
      "step = 7777600: loss = 3.862765312194824\n",
      "step = 7777800: loss = 3.405425548553467\n",
      "step = 7778000: loss = 1.9306129217147827\n",
      "step = 7778200: loss = 4.4308085441589355\n",
      "step = 7778400: loss = 3.275002956390381\n",
      "step = 7778600: loss = 3.816307783126831\n",
      "step = 7778800: loss = 4.936275959014893\n",
      "step = 7779000: loss = 4.573359489440918\n",
      "step = 7779200: loss = 5.07322883605957\n",
      "step = 7779400: loss = 5.4140167236328125\n",
      "step = 7779600: loss = 4.668202877044678\n",
      "step = 7779800: loss = 5.371960163116455\n",
      "step = 7780000: loss = 3.992567300796509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7780000: Average Return = 3.746000051498413\n",
      "step = 7780200: loss = 3.3990283012390137\n",
      "step = 7780400: loss = 6.434168338775635\n",
      "step = 7780600: loss = 3.9492452144622803\n",
      "step = 7780800: loss = 5.058953762054443\n",
      "step = 7781000: loss = 4.106662273406982\n",
      "step = 7781200: loss = 4.1548752784729\n",
      "step = 7781400: loss = 4.243334770202637\n",
      "step = 7781600: loss = 3.694688320159912\n",
      "step = 7781800: loss = 3.6665914058685303\n",
      "step = 7782000: loss = 3.6005308628082275\n",
      "step = 7782200: loss = 4.890676975250244\n",
      "step = 7782400: loss = 4.447490215301514\n",
      "step = 7782600: loss = 3.2153570652008057\n",
      "step = 7782800: loss = 4.858771324157715\n",
      "step = 7783000: loss = 3.2476556301116943\n",
      "step = 7783200: loss = 4.79011344909668\n",
      "step = 7783400: loss = 4.1977152824401855\n",
      "step = 7783600: loss = 3.208850383758545\n",
      "step = 7783800: loss = 2.35447096824646\n",
      "step = 7784000: loss = 4.911586284637451\n",
      "step = 7784200: loss = 3.4072649478912354\n",
      "step = 7784400: loss = 5.339735984802246\n",
      "step = 7784600: loss = 4.174934387207031\n",
      "step = 7784800: loss = 4.681885242462158\n",
      "step = 7785000: loss = 2.8730578422546387\n",
      "step = 7785000: Average Return = 3.9079999923706055\n",
      "step = 7785200: loss = 3.6433959007263184\n",
      "step = 7785400: loss = 4.167312145233154\n",
      "step = 7785600: loss = 3.440225839614868\n",
      "step = 7785800: loss = 4.269319534301758\n",
      "step = 7786000: loss = 3.359546661376953\n",
      "step = 7786200: loss = 3.0986900329589844\n",
      "step = 7786400: loss = 3.01406192779541\n",
      "step = 7786600: loss = 4.377639293670654\n",
      "step = 7786800: loss = 2.4538049697875977\n",
      "step = 7787000: loss = 4.885922908782959\n",
      "step = 7787200: loss = 3.967961549758911\n",
      "step = 7787400: loss = 4.5542073249816895\n",
      "step = 7787600: loss = 4.126705169677734\n",
      "step = 7787800: loss = 4.400582313537598\n",
      "step = 7788000: loss = 5.023263931274414\n",
      "step = 7788200: loss = 6.355291843414307\n",
      "step = 7788400: loss = 4.990443229675293\n",
      "step = 7788600: loss = 4.074701309204102\n",
      "step = 7788800: loss = 4.138857841491699\n",
      "step = 7789000: loss = 4.149109840393066\n",
      "step = 7789200: loss = 4.554082870483398\n",
      "step = 7789400: loss = 3.912539482116699\n",
      "step = 7789600: loss = 3.6814329624176025\n",
      "step = 7789800: loss = 4.414042949676514\n",
      "step = 7790000: loss = 4.710963726043701\n",
      "step = 7790000: Average Return = 4.203999996185303\n",
      "step = 7790200: loss = 5.187714576721191\n",
      "step = 7790400: loss = 3.286237955093384\n",
      "step = 7790600: loss = 3.75459885597229\n",
      "step = 7790800: loss = 4.0789289474487305\n",
      "step = 7791000: loss = 4.4686784744262695\n",
      "step = 7791200: loss = 5.413737773895264\n",
      "step = 7791400: loss = 4.170802116394043\n",
      "step = 7791600: loss = 5.062088489532471\n",
      "step = 7791800: loss = 4.138888359069824\n",
      "step = 7792000: loss = 2.8728654384613037\n",
      "step = 7792200: loss = 2.8424410820007324\n",
      "step = 7792400: loss = 5.499533176422119\n",
      "step = 7792600: loss = 4.877131462097168\n",
      "step = 7792800: loss = 3.3001279830932617\n",
      "step = 7793000: loss = 4.421762466430664\n",
      "step = 7793200: loss = 4.1655778884887695\n",
      "step = 7793400: loss = 3.9382646083831787\n",
      "step = 7793600: loss = 4.788865566253662\n",
      "step = 7793800: loss = 3.060433864593506\n",
      "step = 7794000: loss = 4.480111122131348\n",
      "step = 7794200: loss = 3.845433473587036\n",
      "step = 7794400: loss = 4.97505521774292\n",
      "step = 7794600: loss = 5.410355091094971\n",
      "step = 7794800: loss = 4.393245220184326\n",
      "step = 7795000: loss = 4.174765110015869\n",
      "step = 7795000: Average Return = 3.7279999256134033\n",
      "step = 7795200: loss = 3.7756102085113525\n",
      "step = 7795400: loss = 3.08542537689209\n",
      "step = 7795600: loss = 3.6663501262664795\n",
      "step = 7795800: loss = 5.142878532409668\n",
      "step = 7796000: loss = 4.431156635284424\n",
      "step = 7796200: loss = 3.988286018371582\n",
      "step = 7796400: loss = 3.8727476596832275\n",
      "step = 7796600: loss = 4.941002368927002\n",
      "step = 7796800: loss = 4.46057653427124\n",
      "step = 7797000: loss = 4.271670341491699\n",
      "step = 7797200: loss = 4.910945415496826\n",
      "step = 7797400: loss = 4.709433555603027\n",
      "step = 7797600: loss = 6.044495582580566\n",
      "step = 7797800: loss = 5.142421722412109\n",
      "step = 7798000: loss = 3.301677703857422\n",
      "step = 7798200: loss = 4.387006759643555\n",
      "step = 7798400: loss = 3.9411730766296387\n",
      "step = 7798600: loss = 5.085377216339111\n",
      "step = 7798800: loss = 4.245411396026611\n",
      "step = 7799000: loss = 2.917524576187134\n",
      "step = 7799200: loss = 5.360025405883789\n",
      "step = 7799400: loss = 4.260618686676025\n",
      "step = 7799600: loss = 4.75296688079834\n",
      "step = 7799800: loss = 3.195667028427124\n",
      "step = 7800000: loss = 3.2879478931427\n",
      "step = 7800000: Average Return = 4.3480000495910645\n",
      "step = 7800200: loss = 5.293342590332031\n",
      "step = 7800400: loss = 3.838099241256714\n",
      "step = 7800600: loss = 3.669607162475586\n",
      "step = 7800800: loss = 4.244219779968262\n",
      "step = 7801000: loss = 3.8002572059631348\n",
      "step = 7801200: loss = 4.78223180770874\n",
      "step = 7801400: loss = 3.1135668754577637\n",
      "step = 7801600: loss = 2.923893928527832\n",
      "step = 7801800: loss = 4.103839874267578\n",
      "step = 7802000: loss = 4.468911170959473\n",
      "step = 7802200: loss = 4.835302829742432\n",
      "step = 7802400: loss = 4.423677921295166\n",
      "step = 7802600: loss = 4.686133861541748\n",
      "step = 7802800: loss = 3.1207001209259033\n",
      "step = 7803000: loss = 4.635612964630127\n",
      "step = 7803200: loss = 4.37084436416626\n",
      "step = 7803400: loss = 3.771796464920044\n",
      "step = 7803600: loss = 3.472607135772705\n",
      "step = 7803800: loss = 4.380494117736816\n",
      "step = 7804000: loss = 3.9789345264434814\n",
      "step = 7804200: loss = 4.624268531799316\n",
      "step = 7804400: loss = 4.0896525382995605\n",
      "step = 7804600: loss = 3.8533031940460205\n",
      "step = 7804800: loss = 3.9706504344940186\n",
      "step = 7805000: loss = 3.7068755626678467\n",
      "step = 7805000: Average Return = 3.931999921798706\n",
      "step = 7805200: loss = 4.3684186935424805\n",
      "step = 7805400: loss = 5.347318172454834\n",
      "step = 7805600: loss = 3.2491416931152344\n",
      "step = 7805800: loss = 4.5329389572143555\n",
      "step = 7806000: loss = 4.015390396118164\n",
      "step = 7806200: loss = 3.597358226776123\n",
      "step = 7806400: loss = 4.5661234855651855\n",
      "step = 7806600: loss = 3.646592378616333\n",
      "step = 7806800: loss = 3.521129608154297\n",
      "step = 7807000: loss = 3.7090771198272705\n",
      "step = 7807200: loss = 3.563790798187256\n",
      "step = 7807400: loss = 4.385863780975342\n",
      "step = 7807600: loss = 5.718859672546387\n",
      "step = 7807800: loss = 4.3650665283203125\n",
      "step = 7808000: loss = 3.3411951065063477\n",
      "step = 7808200: loss = 5.336335182189941\n",
      "step = 7808400: loss = 5.075781345367432\n",
      "step = 7808600: loss = 4.5831756591796875\n",
      "step = 7808800: loss = 3.7919139862060547\n",
      "step = 7809000: loss = 4.387295246124268\n",
      "step = 7809200: loss = 4.718791484832764\n",
      "step = 7809400: loss = 2.2144088745117188\n",
      "step = 7809600: loss = 5.328855037689209\n",
      "step = 7809800: loss = 4.334473133087158\n",
      "step = 7810000: loss = 4.120980739593506\n",
      "step = 7810000: Average Return = 4.041999816894531\n",
      "step = 7810200: loss = 4.823910236358643\n",
      "step = 7810400: loss = 2.5959956645965576\n",
      "step = 7810600: loss = 3.0693817138671875\n",
      "step = 7810800: loss = 4.440422058105469\n",
      "step = 7811000: loss = 5.167842864990234\n",
      "step = 7811200: loss = 4.752130508422852\n",
      "step = 7811400: loss = 2.970355749130249\n",
      "step = 7811600: loss = 5.607369422912598\n",
      "step = 7811800: loss = 4.417339324951172\n",
      "step = 7812000: loss = 3.704008102416992\n",
      "step = 7812200: loss = 4.752963066101074\n",
      "step = 7812400: loss = 3.3741488456726074\n",
      "step = 7812600: loss = 4.515899181365967\n",
      "step = 7812800: loss = 3.912785053253174\n",
      "step = 7813000: loss = 4.022617340087891\n",
      "step = 7813200: loss = 4.8905839920043945\n",
      "step = 7813400: loss = 3.2031567096710205\n",
      "step = 7813600: loss = 5.011101722717285\n",
      "step = 7813800: loss = 4.017520904541016\n",
      "step = 7814000: loss = 4.636664390563965\n",
      "step = 7814200: loss = 4.393580436706543\n",
      "step = 7814400: loss = 4.149510383605957\n",
      "step = 7814600: loss = 4.008219242095947\n",
      "step = 7814800: loss = 4.456488132476807\n",
      "step = 7815000: loss = 3.9654879570007324\n",
      "step = 7815000: Average Return = 3.940000057220459\n",
      "step = 7815200: loss = 4.032292366027832\n",
      "step = 7815400: loss = 4.091004371643066\n",
      "step = 7815600: loss = 5.23529052734375\n",
      "step = 7815800: loss = 4.75112771987915\n",
      "step = 7816000: loss = 4.108682632446289\n",
      "step = 7816200: loss = 3.99824857711792\n",
      "step = 7816400: loss = 4.1742401123046875\n",
      "step = 7816600: loss = 4.359389305114746\n",
      "step = 7816800: loss = 2.958573579788208\n",
      "step = 7817000: loss = 4.200252056121826\n",
      "step = 7817200: loss = 3.1363940238952637\n",
      "step = 7817400: loss = 5.085140228271484\n",
      "step = 7817600: loss = 3.0402004718780518\n",
      "step = 7817800: loss = 4.119537353515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7818000: loss = 4.39620304107666\n",
      "step = 7818200: loss = 3.4154531955718994\n",
      "step = 7818400: loss = 3.060988664627075\n",
      "step = 7818600: loss = 3.738495349884033\n",
      "step = 7818800: loss = 4.448676109313965\n",
      "step = 7819000: loss = 4.7408318519592285\n",
      "step = 7819200: loss = 3.347874164581299\n",
      "step = 7819400: loss = 4.311383247375488\n",
      "step = 7819600: loss = 4.728801250457764\n",
      "step = 7819800: loss = 3.8544371128082275\n",
      "step = 7820000: loss = 3.346770763397217\n",
      "step = 7820000: Average Return = 3.759999990463257\n",
      "step = 7820200: loss = 4.340574264526367\n",
      "step = 7820400: loss = 2.8016672134399414\n",
      "step = 7820600: loss = 4.027962684631348\n",
      "step = 7820800: loss = 4.413125514984131\n",
      "step = 7821000: loss = 3.332106828689575\n",
      "step = 7821200: loss = 3.670487642288208\n",
      "step = 7821400: loss = 5.810302734375\n",
      "step = 7821600: loss = 3.1369290351867676\n",
      "step = 7821800: loss = 5.943418025970459\n",
      "step = 7822000: loss = 5.082843780517578\n",
      "step = 7822200: loss = 4.4738287925720215\n",
      "step = 7822400: loss = 5.291207313537598\n",
      "step = 7822600: loss = 3.7104077339172363\n",
      "step = 7822800: loss = 4.906564712524414\n",
      "step = 7823000: loss = 5.091582298278809\n",
      "step = 7823200: loss = 4.761690616607666\n",
      "step = 7823400: loss = 5.566868782043457\n",
      "step = 7823600: loss = 2.932584047317505\n",
      "step = 7823800: loss = 4.723389625549316\n",
      "step = 7824000: loss = 4.180107593536377\n",
      "step = 7824200: loss = 4.010169982910156\n",
      "step = 7824400: loss = 4.156761169433594\n",
      "step = 7824600: loss = 3.942988872528076\n",
      "step = 7824800: loss = 5.15054178237915\n",
      "step = 7825000: loss = 3.2809462547302246\n",
      "step = 7825000: Average Return = 3.868000030517578\n",
      "step = 7825200: loss = 4.977236270904541\n",
      "step = 7825400: loss = 3.298534393310547\n",
      "step = 7825600: loss = 3.3257031440734863\n",
      "step = 7825800: loss = 3.756850242614746\n",
      "step = 7826000: loss = 4.777929782867432\n",
      "step = 7826200: loss = 4.674455642700195\n",
      "step = 7826400: loss = 4.894093990325928\n",
      "step = 7826600: loss = 3.47763991355896\n",
      "step = 7826800: loss = 5.345276355743408\n",
      "step = 7827000: loss = 3.693800210952759\n",
      "step = 7827200: loss = 4.319904804229736\n",
      "step = 7827400: loss = 4.39188289642334\n",
      "step = 7827600: loss = 3.5645976066589355\n",
      "step = 7827800: loss = 4.961212158203125\n",
      "step = 7828000: loss = 4.134401798248291\n",
      "step = 7828200: loss = 4.033083438873291\n",
      "step = 7828400: loss = 4.108058452606201\n",
      "step = 7828600: loss = 4.639974117279053\n",
      "step = 7828800: loss = 4.472371578216553\n",
      "step = 7829000: loss = 4.523697853088379\n",
      "step = 7829200: loss = 4.637269020080566\n",
      "step = 7829400: loss = 3.7645018100738525\n",
      "step = 7829600: loss = 4.856026649475098\n",
      "step = 7829800: loss = 4.23452091217041\n",
      "step = 7830000: loss = 3.754866361618042\n",
      "step = 7830000: Average Return = 3.5959999561309814\n",
      "step = 7830200: loss = 4.1621832847595215\n",
      "step = 7830400: loss = 4.463932514190674\n",
      "step = 7830600: loss = 3.537343740463257\n",
      "step = 7830800: loss = 4.033059597015381\n",
      "step = 7831000: loss = 3.377333402633667\n",
      "step = 7831200: loss = 3.8185131549835205\n",
      "step = 7831400: loss = 3.7553887367248535\n",
      "step = 7831600: loss = 4.226202487945557\n",
      "step = 7831800: loss = 3.5826876163482666\n",
      "step = 7832000: loss = 4.280205249786377\n",
      "step = 7832200: loss = 3.932257652282715\n",
      "step = 7832400: loss = 3.2624449729919434\n",
      "step = 7832600: loss = 4.527523517608643\n",
      "step = 7832800: loss = 5.201416969299316\n",
      "step = 7833000: loss = 4.394661903381348\n",
      "step = 7833200: loss = 3.7149181365966797\n",
      "step = 7833400: loss = 3.316856861114502\n",
      "step = 7833600: loss = 3.567141056060791\n",
      "step = 7833800: loss = 6.668209075927734\n",
      "step = 7834000: loss = 5.519587516784668\n",
      "step = 7834200: loss = 2.8111705780029297\n",
      "step = 7834400: loss = 5.084371089935303\n",
      "step = 7834600: loss = 4.755825996398926\n",
      "step = 7834800: loss = 3.0223000049591064\n",
      "step = 7835000: loss = 4.076473712921143\n",
      "step = 7835000: Average Return = 3.9600000381469727\n",
      "step = 7835200: loss = 4.004633903503418\n",
      "step = 7835400: loss = 4.534033298492432\n",
      "step = 7835600: loss = 4.020521640777588\n",
      "step = 7835800: loss = 4.71616792678833\n",
      "step = 7836000: loss = 4.619405269622803\n",
      "step = 7836200: loss = 4.829250812530518\n",
      "step = 7836400: loss = 3.311264991760254\n",
      "step = 7836600: loss = 3.987591505050659\n",
      "step = 7836800: loss = 5.287214279174805\n",
      "step = 7837000: loss = 4.639423847198486\n",
      "step = 7837200: loss = 5.377627372741699\n",
      "step = 7837400: loss = 4.138881683349609\n",
      "step = 7837600: loss = 4.151853084564209\n",
      "step = 7837800: loss = 4.341503620147705\n",
      "step = 7838000: loss = 4.0825042724609375\n",
      "step = 7838200: loss = 3.8152544498443604\n",
      "step = 7838400: loss = 4.422379493713379\n",
      "step = 7838600: loss = 5.087222576141357\n",
      "step = 7838800: loss = 4.324819564819336\n",
      "step = 7839000: loss = 4.043459892272949\n",
      "step = 7839200: loss = 3.400995969772339\n",
      "step = 7839400: loss = 3.848522901535034\n",
      "step = 7839600: loss = 2.8860836029052734\n",
      "step = 7839800: loss = 4.8278656005859375\n",
      "step = 7840000: loss = 3.987823486328125\n",
      "step = 7840000: Average Return = 3.6640000343322754\n",
      "step = 7840200: loss = 3.528533935546875\n",
      "step = 7840400: loss = 5.821828365325928\n",
      "step = 7840600: loss = 4.000141620635986\n",
      "step = 7840800: loss = 3.7868223190307617\n",
      "step = 7841000: loss = 3.305535316467285\n",
      "step = 7841200: loss = 2.586110830307007\n",
      "step = 7841400: loss = 3.9155704975128174\n",
      "step = 7841600: loss = 5.830551624298096\n",
      "step = 7841800: loss = 4.2942962646484375\n",
      "step = 7842000: loss = 4.438516616821289\n",
      "step = 7842200: loss = 3.877980947494507\n",
      "step = 7842400: loss = 5.419456481933594\n",
      "step = 7842600: loss = 4.819134712219238\n",
      "step = 7842800: loss = 4.69221830368042\n",
      "step = 7843000: loss = 3.599710702896118\n",
      "step = 7843200: loss = 3.7625224590301514\n",
      "step = 7843400: loss = 5.396774768829346\n",
      "step = 7843600: loss = 5.022586822509766\n",
      "step = 7843800: loss = 4.624176979064941\n",
      "step = 7844000: loss = 3.3415441513061523\n",
      "step = 7844200: loss = 3.5749244689941406\n",
      "step = 7844400: loss = 4.9470624923706055\n",
      "step = 7844600: loss = 4.393486976623535\n",
      "step = 7844800: loss = 4.766170501708984\n",
      "step = 7845000: loss = 4.5778937339782715\n",
      "step = 7845000: Average Return = 3.5339999198913574\n",
      "step = 7845200: loss = 4.383636474609375\n",
      "step = 7845400: loss = 3.6665000915527344\n",
      "step = 7845600: loss = 3.752293586730957\n",
      "step = 7845800: loss = 3.288027286529541\n",
      "step = 7846000: loss = 4.49202823638916\n",
      "step = 7846200: loss = 4.289100170135498\n",
      "step = 7846400: loss = 4.137306213378906\n",
      "step = 7846600: loss = 4.384308815002441\n",
      "step = 7846800: loss = 2.8696115016937256\n",
      "step = 7847000: loss = 3.40329647064209\n",
      "step = 7847200: loss = 2.9739701747894287\n",
      "step = 7847400: loss = 3.2582502365112305\n",
      "step = 7847600: loss = 3.522317409515381\n",
      "step = 7847800: loss = 3.178148031234741\n",
      "step = 7848000: loss = 4.265026092529297\n",
      "step = 7848200: loss = 4.846043586730957\n",
      "step = 7848400: loss = 3.1949641704559326\n",
      "step = 7848600: loss = 5.222039699554443\n",
      "step = 7848800: loss = 4.021615028381348\n",
      "step = 7849000: loss = 3.218214988708496\n",
      "step = 7849200: loss = 4.3699235916137695\n",
      "step = 7849400: loss = 4.1176910400390625\n",
      "step = 7849600: loss = 4.435275554656982\n",
      "step = 7849800: loss = 3.6595065593719482\n",
      "step = 7850000: loss = 3.998767375946045\n",
      "step = 7850000: Average Return = 3.5999999046325684\n",
      "step = 7850200: loss = 2.866093873977661\n",
      "step = 7850400: loss = 4.02954626083374\n",
      "step = 7850600: loss = 4.821157455444336\n",
      "step = 7850800: loss = 3.27654767036438\n",
      "step = 7851000: loss = 5.376201629638672\n",
      "step = 7851200: loss = 2.6564297676086426\n",
      "step = 7851400: loss = 4.295133590698242\n",
      "step = 7851600: loss = 3.953705310821533\n",
      "step = 7851800: loss = 4.61569881439209\n",
      "step = 7852000: loss = 2.9739067554473877\n",
      "step = 7852200: loss = 3.464879274368286\n",
      "step = 7852400: loss = 2.9594151973724365\n",
      "step = 7852600: loss = 5.486364841461182\n",
      "step = 7852800: loss = 3.4902429580688477\n",
      "step = 7853000: loss = 3.5800914764404297\n",
      "step = 7853200: loss = 4.528512954711914\n",
      "step = 7853400: loss = 4.516170978546143\n",
      "step = 7853600: loss = 4.406228065490723\n",
      "step = 7853800: loss = 4.558910846710205\n",
      "step = 7854000: loss = 5.758442401885986\n",
      "step = 7854200: loss = 4.074151039123535\n",
      "step = 7854400: loss = 5.191040992736816\n",
      "step = 7854600: loss = 3.241804838180542\n",
      "step = 7854800: loss = 4.375412464141846\n",
      "step = 7855000: loss = 4.663284778594971\n",
      "step = 7855000: Average Return = 3.5999999046325684\n",
      "step = 7855200: loss = 3.6345903873443604\n",
      "step = 7855400: loss = 3.437511444091797\n",
      "step = 7855600: loss = 3.78814435005188\n",
      "step = 7855800: loss = 5.311439037322998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7856000: loss = 3.0507702827453613\n",
      "step = 7856200: loss = 5.195290565490723\n",
      "step = 7856400: loss = 3.8077638149261475\n",
      "step = 7856600: loss = 3.272764205932617\n",
      "step = 7856800: loss = 5.04073429107666\n",
      "step = 7857000: loss = 3.8355696201324463\n",
      "step = 7857200: loss = 3.4334685802459717\n",
      "step = 7857400: loss = 4.319585800170898\n",
      "step = 7857600: loss = 5.4945549964904785\n",
      "step = 7857800: loss = 4.911824703216553\n",
      "step = 7858000: loss = 4.171206474304199\n",
      "step = 7858200: loss = 3.338444232940674\n",
      "step = 7858400: loss = 4.1687517166137695\n",
      "step = 7858600: loss = 5.151797771453857\n",
      "step = 7858800: loss = 4.501104354858398\n",
      "step = 7859000: loss = 5.00487756729126\n",
      "step = 7859200: loss = 4.320496559143066\n",
      "step = 7859400: loss = 4.020900249481201\n",
      "step = 7859600: loss = 2.620046615600586\n",
      "step = 7859800: loss = 4.648845672607422\n",
      "step = 7860000: loss = 4.539305210113525\n",
      "step = 7860000: Average Return = 3.815999984741211\n",
      "step = 7860200: loss = 4.837263584136963\n",
      "step = 7860400: loss = 2.784045696258545\n",
      "step = 7860600: loss = 4.800589084625244\n",
      "step = 7860800: loss = 3.808812141418457\n",
      "step = 7861000: loss = 3.821756601333618\n",
      "step = 7861200: loss = 6.21129035949707\n",
      "step = 7861400: loss = 4.12748908996582\n",
      "step = 7861600: loss = 3.485668182373047\n",
      "step = 7861800: loss = 6.2110595703125\n",
      "step = 7862000: loss = 4.1324462890625\n",
      "step = 7862200: loss = 5.274621486663818\n",
      "step = 7862400: loss = 2.941253662109375\n",
      "step = 7862600: loss = 6.663176536560059\n",
      "step = 7862800: loss = 3.9964144229888916\n",
      "step = 7863000: loss = 3.8951945304870605\n",
      "step = 7863200: loss = 4.409417629241943\n",
      "step = 7863400: loss = 3.6926989555358887\n",
      "step = 7863600: loss = 3.0992588996887207\n",
      "step = 7863800: loss = 4.000270366668701\n",
      "step = 7864000: loss = 3.9213414192199707\n",
      "step = 7864200: loss = 4.161829471588135\n",
      "step = 7864400: loss = 3.879671573638916\n",
      "step = 7864600: loss = 4.616237163543701\n",
      "step = 7864800: loss = 5.9034929275512695\n",
      "step = 7865000: loss = 3.852268695831299\n",
      "step = 7865000: Average Return = 4.013999938964844\n",
      "step = 7865200: loss = 4.4276347160339355\n",
      "step = 7865400: loss = 5.325892925262451\n",
      "step = 7865600: loss = 3.138070583343506\n",
      "step = 7865800: loss = 3.0858240127563477\n",
      "step = 7866000: loss = 6.212077617645264\n",
      "step = 7866200: loss = 3.936413526535034\n",
      "step = 7866400: loss = 3.8244526386260986\n",
      "step = 7866600: loss = 5.050346374511719\n",
      "step = 7866800: loss = 2.3448638916015625\n",
      "step = 7867000: loss = 3.80950927734375\n",
      "step = 7867200: loss = 4.721313953399658\n",
      "step = 7867400: loss = 2.839299201965332\n",
      "step = 7867600: loss = 3.9470908641815186\n",
      "step = 7867800: loss = 4.432327747344971\n",
      "step = 7868000: loss = 4.872581958770752\n",
      "step = 7868200: loss = 3.83520245552063\n",
      "step = 7868400: loss = 4.001911640167236\n",
      "step = 7868600: loss = 3.7533538341522217\n",
      "step = 7868800: loss = 4.620612144470215\n",
      "step = 7869000: loss = 3.041792631149292\n",
      "step = 7869200: loss = 5.141589164733887\n",
      "step = 7869400: loss = 5.478521823883057\n",
      "step = 7869600: loss = 3.4653615951538086\n",
      "step = 7869800: loss = 4.66512393951416\n",
      "step = 7870000: loss = 3.6220040321350098\n",
      "step = 7870000: Average Return = 3.7179999351501465\n",
      "step = 7870200: loss = 3.7456042766571045\n",
      "step = 7870400: loss = 3.6508092880249023\n",
      "step = 7870600: loss = 3.94765043258667\n",
      "step = 7870800: loss = 4.829057216644287\n",
      "step = 7871000: loss = 3.8465394973754883\n",
      "step = 7871200: loss = 4.75407600402832\n",
      "step = 7871400: loss = 3.953871965408325\n",
      "step = 7871600: loss = 4.6937174797058105\n",
      "step = 7871800: loss = 3.85030198097229\n",
      "step = 7872000: loss = 3.464933156967163\n",
      "step = 7872200: loss = 4.820962905883789\n",
      "step = 7872400: loss = 3.343801259994507\n",
      "step = 7872600: loss = 3.611813545227051\n",
      "step = 7872800: loss = 4.580377578735352\n",
      "step = 7873000: loss = 3.9015767574310303\n",
      "step = 7873200: loss = 3.4328396320343018\n",
      "step = 7873400: loss = 5.167223930358887\n",
      "step = 7873600: loss = 4.4106059074401855\n",
      "step = 7873800: loss = 4.399568557739258\n",
      "step = 7874000: loss = 3.898902416229248\n",
      "step = 7874200: loss = 3.93962025642395\n",
      "step = 7874400: loss = 3.698176860809326\n",
      "step = 7874600: loss = 3.4860849380493164\n",
      "step = 7874800: loss = 4.165730953216553\n",
      "step = 7875000: loss = 3.3263139724731445\n",
      "step = 7875000: Average Return = 3.8380000591278076\n",
      "step = 7875200: loss = 4.7967753410339355\n",
      "step = 7875400: loss = 4.184907913208008\n",
      "step = 7875600: loss = 3.8369505405426025\n",
      "step = 7875800: loss = 3.761345148086548\n",
      "step = 7876000: loss = 4.646395206451416\n",
      "step = 7876200: loss = 4.528130531311035\n",
      "step = 7876400: loss = 4.237022399902344\n",
      "step = 7876600: loss = 3.7266643047332764\n",
      "step = 7876800: loss = 5.927707195281982\n",
      "step = 7877000: loss = 4.52112340927124\n",
      "step = 7877200: loss = 4.1194868087768555\n",
      "step = 7877400: loss = 3.508211612701416\n",
      "step = 7877600: loss = 4.5342912673950195\n",
      "step = 7877800: loss = 4.155935764312744\n",
      "step = 7878000: loss = 4.1193413734436035\n",
      "step = 7878200: loss = 5.393989562988281\n",
      "step = 7878400: loss = 4.562664031982422\n",
      "step = 7878600: loss = 4.4116411209106445\n",
      "step = 7878800: loss = 3.7314164638519287\n",
      "step = 7879000: loss = 4.0275654792785645\n",
      "step = 7879200: loss = 4.2638773918151855\n",
      "step = 7879400: loss = 3.6232354640960693\n",
      "step = 7879600: loss = 3.452704429626465\n",
      "step = 7879800: loss = 4.213163375854492\n",
      "step = 7880000: loss = 3.935181140899658\n",
      "step = 7880000: Average Return = 3.7660000324249268\n",
      "step = 7880200: loss = 4.818710803985596\n",
      "step = 7880400: loss = 3.9390602111816406\n",
      "step = 7880600: loss = 3.676823854446411\n",
      "step = 7880800: loss = 3.324286937713623\n",
      "step = 7881000: loss = 3.551978588104248\n",
      "step = 7881200: loss = 4.0913801193237305\n",
      "step = 7881400: loss = 3.5300207138061523\n",
      "step = 7881600: loss = 4.534049987792969\n",
      "step = 7881800: loss = 3.7616467475891113\n",
      "step = 7882000: loss = 3.043677806854248\n",
      "step = 7882200: loss = 3.873826265335083\n",
      "step = 7882400: loss = 3.998941659927368\n",
      "step = 7882600: loss = 4.73746919631958\n",
      "step = 7882800: loss = 5.928633689880371\n",
      "step = 7883000: loss = 5.183718204498291\n",
      "step = 7883200: loss = 3.565342903137207\n",
      "step = 7883400: loss = 4.173103332519531\n",
      "step = 7883600: loss = 3.111678123474121\n",
      "step = 7883800: loss = 4.154197692871094\n",
      "step = 7884000: loss = 2.9373598098754883\n",
      "step = 7884200: loss = 3.8676841259002686\n",
      "step = 7884400: loss = 5.668707370758057\n",
      "step = 7884600: loss = 3.345566749572754\n",
      "step = 7884800: loss = 3.689262628555298\n",
      "step = 7885000: loss = 4.645563125610352\n",
      "step = 7885000: Average Return = 4.00600004196167\n",
      "step = 7885200: loss = 5.439825534820557\n",
      "step = 7885400: loss = 4.87229585647583\n",
      "step = 7885600: loss = 4.2244672775268555\n",
      "step = 7885800: loss = 4.519233226776123\n",
      "step = 7886000: loss = 3.4114179611206055\n",
      "step = 7886200: loss = 5.114354610443115\n",
      "step = 7886400: loss = 3.598727226257324\n",
      "step = 7886600: loss = 3.954535961151123\n",
      "step = 7886800: loss = 4.256653308868408\n",
      "step = 7887000: loss = 5.341736793518066\n",
      "step = 7887200: loss = 5.132900714874268\n",
      "step = 7887400: loss = 3.560281276702881\n",
      "step = 7887600: loss = 3.9910240173339844\n",
      "step = 7887800: loss = 5.0930914878845215\n",
      "step = 7888000: loss = 4.6920881271362305\n",
      "step = 7888200: loss = 4.121680736541748\n",
      "step = 7888400: loss = 4.136623382568359\n",
      "step = 7888600: loss = 5.615431785583496\n",
      "step = 7888800: loss = 3.3596293926239014\n",
      "step = 7889000: loss = 3.4858040809631348\n",
      "step = 7889200: loss = 4.44228458404541\n",
      "step = 7889400: loss = 5.789405822753906\n",
      "step = 7889600: loss = 3.7190237045288086\n",
      "step = 7889800: loss = 3.333383083343506\n",
      "step = 7890000: loss = 4.2272186279296875\n",
      "step = 7890000: Average Return = 3.369999885559082\n",
      "step = 7890200: loss = 3.539308786392212\n",
      "step = 7890400: loss = 4.583556652069092\n",
      "step = 7890600: loss = 3.92608380317688\n",
      "step = 7890800: loss = 4.459712028503418\n",
      "step = 7891000: loss = 5.46402645111084\n",
      "step = 7891200: loss = 4.8359575271606445\n",
      "step = 7891400: loss = 4.032850742340088\n",
      "step = 7891600: loss = 2.690012216567993\n",
      "step = 7891800: loss = 5.520089149475098\n",
      "step = 7892000: loss = 4.121963977813721\n",
      "step = 7892200: loss = 3.553467273712158\n",
      "step = 7892400: loss = 3.8921220302581787\n",
      "step = 7892600: loss = 3.1906590461730957\n",
      "step = 7892800: loss = 3.7808849811553955\n",
      "step = 7893000: loss = 4.768649101257324\n",
      "step = 7893200: loss = 3.7093098163604736\n",
      "step = 7893400: loss = 4.227267742156982\n",
      "step = 7893600: loss = 2.9365077018737793\n",
      "step = 7893800: loss = 3.826594352722168\n",
      "step = 7894000: loss = 4.573172092437744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7894200: loss = 3.5694797039031982\n",
      "step = 7894400: loss = 3.115863084793091\n",
      "step = 7894600: loss = 3.4782726764678955\n",
      "step = 7894800: loss = 5.244146823883057\n",
      "step = 7895000: loss = 4.9436798095703125\n",
      "step = 7895000: Average Return = 3.6419999599456787\n",
      "step = 7895200: loss = 5.34358024597168\n",
      "step = 7895400: loss = 3.5965402126312256\n",
      "step = 7895600: loss = 4.3115739822387695\n",
      "step = 7895800: loss = 4.303255558013916\n",
      "step = 7896000: loss = 3.954177141189575\n",
      "step = 7896200: loss = 3.875783920288086\n",
      "step = 7896400: loss = 3.5520811080932617\n",
      "step = 7896600: loss = 4.762290954589844\n",
      "step = 7896800: loss = 3.626621723175049\n",
      "step = 7897000: loss = 4.706101417541504\n",
      "step = 7897200: loss = 4.725160598754883\n",
      "step = 7897400: loss = 4.764647483825684\n",
      "step = 7897600: loss = 3.2916555404663086\n",
      "step = 7897800: loss = 4.199939250946045\n",
      "step = 7898000: loss = 5.235804557800293\n",
      "step = 7898200: loss = 4.073531150817871\n",
      "step = 7898400: loss = 3.791907548904419\n",
      "step = 7898600: loss = 4.93806791305542\n",
      "step = 7898800: loss = 4.3572611808776855\n",
      "step = 7899000: loss = 3.800771713256836\n",
      "step = 7899200: loss = 2.871129035949707\n",
      "step = 7899400: loss = 3.7352077960968018\n",
      "step = 7899600: loss = 4.655637264251709\n",
      "step = 7899800: loss = 3.738814115524292\n",
      "step = 7900000: loss = 4.283718585968018\n",
      "step = 7900000: Average Return = 3.7639999389648438\n",
      "step = 7900200: loss = 3.931640863418579\n",
      "step = 7900400: loss = 3.212000846862793\n",
      "step = 7900600: loss = 5.315523624420166\n",
      "step = 7900800: loss = 4.245599269866943\n",
      "step = 7901000: loss = 5.301764965057373\n",
      "step = 7901200: loss = 4.662137508392334\n",
      "step = 7901400: loss = 3.932612895965576\n",
      "step = 7901600: loss = 4.4395599365234375\n",
      "step = 7901800: loss = 4.4267096519470215\n",
      "step = 7902000: loss = 3.9876821041107178\n",
      "step = 7902200: loss = 5.490311145782471\n",
      "step = 7902400: loss = 3.276672601699829\n",
      "step = 7902600: loss = 2.9427669048309326\n",
      "step = 7902800: loss = 4.430888652801514\n",
      "step = 7903000: loss = 5.001745700836182\n",
      "step = 7903200: loss = 3.6848270893096924\n",
      "step = 7903400: loss = 3.754356622695923\n",
      "step = 7903600: loss = 2.900477170944214\n",
      "step = 7903800: loss = 4.536347389221191\n",
      "step = 7904000: loss = 3.4367635250091553\n",
      "step = 7904200: loss = 5.0699968338012695\n",
      "step = 7904400: loss = 4.172118186950684\n",
      "step = 7904600: loss = 4.162749767303467\n",
      "step = 7904800: loss = 3.457770586013794\n",
      "step = 7905000: loss = 4.4912567138671875\n",
      "step = 7905000: Average Return = 3.6419999599456787\n",
      "step = 7905200: loss = 3.9905519485473633\n",
      "step = 7905400: loss = 4.323720455169678\n",
      "step = 7905600: loss = 3.379039764404297\n",
      "step = 7905800: loss = 4.389084815979004\n",
      "step = 7906000: loss = 3.116401195526123\n",
      "step = 7906200: loss = 5.663237571716309\n",
      "step = 7906400: loss = 3.6035685539245605\n",
      "step = 7906600: loss = 5.979963302612305\n",
      "step = 7906800: loss = 5.384958267211914\n",
      "step = 7907000: loss = 3.8976316452026367\n",
      "step = 7907200: loss = 5.336672782897949\n",
      "step = 7907400: loss = 4.536762237548828\n",
      "step = 7907600: loss = 4.606243133544922\n",
      "step = 7907800: loss = 4.293700695037842\n",
      "step = 7908000: loss = 3.4561572074890137\n",
      "step = 7908200: loss = 3.5127830505371094\n",
      "step = 7908400: loss = 4.084553241729736\n",
      "step = 7908600: loss = 4.238251209259033\n",
      "step = 7908800: loss = 3.732752799987793\n",
      "step = 7909000: loss = 3.4175875186920166\n",
      "step = 7909200: loss = 4.5717082023620605\n",
      "step = 7909400: loss = 3.854409694671631\n",
      "step = 7909600: loss = 3.839905261993408\n",
      "step = 7909800: loss = 4.7116851806640625\n",
      "step = 7910000: loss = 5.996214389801025\n",
      "step = 7910000: Average Return = 3.568000078201294\n",
      "step = 7910200: loss = 4.063926696777344\n",
      "step = 7910400: loss = 3.9809374809265137\n",
      "step = 7910600: loss = 3.926575183868408\n",
      "step = 7910800: loss = 4.284057140350342\n",
      "step = 7911000: loss = 3.7230401039123535\n",
      "step = 7911200: loss = 3.9540557861328125\n",
      "step = 7911400: loss = 4.89102840423584\n",
      "step = 7911600: loss = 3.8482439517974854\n",
      "step = 7911800: loss = 4.809903621673584\n",
      "step = 7912000: loss = 3.7351667881011963\n",
      "step = 7912200: loss = 4.027341365814209\n",
      "step = 7912400: loss = 3.3634557723999023\n",
      "step = 7912600: loss = 3.3015429973602295\n",
      "step = 7912800: loss = 3.5994768142700195\n",
      "step = 7913000: loss = 4.818399429321289\n",
      "step = 7913200: loss = 3.7694060802459717\n",
      "step = 7913400: loss = 3.968186616897583\n",
      "step = 7913600: loss = 3.706429958343506\n",
      "step = 7913800: loss = 5.3961381912231445\n",
      "step = 7914000: loss = 4.913931846618652\n",
      "step = 7914200: loss = 4.0979790687561035\n",
      "step = 7914400: loss = 3.7874999046325684\n",
      "step = 7914600: loss = 5.364107608795166\n",
      "step = 7914800: loss = 4.218316555023193\n",
      "step = 7915000: loss = 4.740176677703857\n",
      "step = 7915000: Average Return = 3.431999921798706\n",
      "step = 7915200: loss = 4.815890312194824\n",
      "step = 7915400: loss = 3.746689558029175\n",
      "step = 7915600: loss = 4.311744689941406\n",
      "step = 7915800: loss = 5.019908905029297\n",
      "step = 7916000: loss = 5.093796730041504\n",
      "step = 7916200: loss = 5.124902725219727\n",
      "step = 7916400: loss = 4.53629732131958\n",
      "step = 7916600: loss = 3.3654065132141113\n",
      "step = 7916800: loss = 5.37431526184082\n",
      "step = 7917000: loss = 5.4498138427734375\n",
      "step = 7917200: loss = 3.8257482051849365\n",
      "step = 7917400: loss = 4.278777599334717\n",
      "step = 7917600: loss = 3.1430130004882812\n",
      "step = 7917800: loss = 3.299452066421509\n",
      "step = 7918000: loss = 2.486293077468872\n",
      "step = 7918200: loss = 5.754336833953857\n",
      "step = 7918400: loss = 3.6641411781311035\n",
      "step = 7918600: loss = 5.708112716674805\n",
      "step = 7918800: loss = 5.053597450256348\n",
      "step = 7919000: loss = 3.671924591064453\n",
      "step = 7919200: loss = 4.792336463928223\n",
      "step = 7919400: loss = 4.970503807067871\n",
      "step = 7919600: loss = 3.770061492919922\n",
      "step = 7919800: loss = 3.463432550430298\n",
      "step = 7920000: loss = 3.645341634750366\n",
      "step = 7920000: Average Return = 3.6579999923706055\n",
      "step = 7920200: loss = 4.268767833709717\n",
      "step = 7920400: loss = 4.104193687438965\n",
      "step = 7920600: loss = 3.828009605407715\n",
      "step = 7920800: loss = 3.1168630123138428\n",
      "step = 7921000: loss = 3.475081205368042\n",
      "step = 7921200: loss = 3.873074769973755\n",
      "step = 7921400: loss = 5.033434867858887\n",
      "step = 7921600: loss = 4.177114009857178\n",
      "step = 7921800: loss = 5.357569694519043\n",
      "step = 7922000: loss = 4.274478912353516\n",
      "step = 7922200: loss = 2.228480577468872\n",
      "step = 7922400: loss = 3.795816659927368\n",
      "step = 7922600: loss = 5.173996448516846\n",
      "step = 7922800: loss = 3.5501153469085693\n",
      "step = 7923000: loss = 4.4491376876831055\n",
      "step = 7923200: loss = 5.186515808105469\n",
      "step = 7923400: loss = 5.379246234893799\n",
      "step = 7923600: loss = 3.7799296379089355\n",
      "step = 7923800: loss = 4.2871294021606445\n",
      "step = 7924000: loss = 3.4147865772247314\n",
      "step = 7924200: loss = 5.699102878570557\n",
      "step = 7924400: loss = 3.696232795715332\n",
      "step = 7924600: loss = 6.25340461730957\n",
      "step = 7924800: loss = 4.670966625213623\n",
      "step = 7925000: loss = 4.293772220611572\n",
      "step = 7925000: Average Return = 3.802000045776367\n",
      "step = 7925200: loss = 3.853672504425049\n",
      "step = 7925400: loss = 3.204415798187256\n",
      "step = 7925600: loss = 3.895014524459839\n",
      "step = 7925800: loss = 3.6319615840911865\n",
      "step = 7926000: loss = 4.172281265258789\n",
      "step = 7926200: loss = 4.632111072540283\n",
      "step = 7926400: loss = 4.260714054107666\n",
      "step = 7926600: loss = 4.467002868652344\n",
      "step = 7926800: loss = 3.419382333755493\n",
      "step = 7927000: loss = 4.250758171081543\n",
      "step = 7927200: loss = 5.476967811584473\n",
      "step = 7927400: loss = 3.161403179168701\n",
      "step = 7927600: loss = 3.886695384979248\n",
      "step = 7927800: loss = 3.9712517261505127\n",
      "step = 7928000: loss = 5.063394069671631\n",
      "step = 7928200: loss = 4.936777591705322\n",
      "step = 7928400: loss = 4.093752861022949\n",
      "step = 7928600: loss = 3.3268954753875732\n",
      "step = 7928800: loss = 2.937894105911255\n",
      "step = 7929000: loss = 5.8057990074157715\n",
      "step = 7929200: loss = 6.058155536651611\n",
      "step = 7929400: loss = 3.8431499004364014\n",
      "step = 7929600: loss = 4.252713680267334\n",
      "step = 7929800: loss = 4.490085124969482\n",
      "step = 7930000: loss = 4.684865474700928\n",
      "step = 7930000: Average Return = 3.9100000858306885\n",
      "step = 7930200: loss = 4.269753932952881\n",
      "step = 7930400: loss = 4.5106024742126465\n",
      "step = 7930600: loss = 3.9961493015289307\n",
      "step = 7930800: loss = 5.585138320922852\n",
      "step = 7931000: loss = 5.214365005493164\n",
      "step = 7931200: loss = 4.611739635467529\n",
      "step = 7931400: loss = 5.596821308135986\n",
      "step = 7931600: loss = 2.960545063018799\n",
      "step = 7931800: loss = 5.947057247161865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7932000: loss = 3.7594823837280273\n",
      "step = 7932200: loss = 4.232670783996582\n",
      "step = 7932400: loss = 3.740694284439087\n",
      "step = 7932600: loss = 4.464343547821045\n",
      "step = 7932800: loss = 4.208187580108643\n",
      "step = 7933000: loss = 5.013518333435059\n",
      "step = 7933200: loss = 3.903343915939331\n",
      "step = 7933400: loss = 3.351418972015381\n",
      "step = 7933600: loss = 3.723548173904419\n",
      "step = 7933800: loss = 3.2224180698394775\n",
      "step = 7934000: loss = 4.85028076171875\n",
      "step = 7934200: loss = 4.737895965576172\n",
      "step = 7934400: loss = 4.020444393157959\n",
      "step = 7934600: loss = 5.340225696563721\n",
      "step = 7934800: loss = 4.023662090301514\n",
      "step = 7935000: loss = 4.520521640777588\n",
      "step = 7935000: Average Return = 3.8459999561309814\n",
      "step = 7935200: loss = 5.796579360961914\n",
      "step = 7935400: loss = 5.728095531463623\n",
      "step = 7935600: loss = 4.619009494781494\n",
      "step = 7935800: loss = 5.143773078918457\n",
      "step = 7936000: loss = 3.0146336555480957\n",
      "step = 7936200: loss = 4.033176422119141\n",
      "step = 7936400: loss = 5.318911552429199\n",
      "step = 7936600: loss = 4.4631757736206055\n",
      "step = 7936800: loss = 4.712047576904297\n",
      "step = 7937000: loss = 4.36887264251709\n",
      "step = 7937200: loss = 3.827078104019165\n",
      "step = 7937400: loss = 4.6553144454956055\n",
      "step = 7937600: loss = 3.5434324741363525\n",
      "step = 7937800: loss = 5.121928691864014\n",
      "step = 7938000: loss = 4.257981777191162\n",
      "step = 7938200: loss = 3.847339391708374\n",
      "step = 7938400: loss = 3.6164839267730713\n",
      "step = 7938600: loss = 2.8398406505584717\n",
      "step = 7938800: loss = 4.51198673248291\n",
      "step = 7939000: loss = 3.9956881999969482\n",
      "step = 7939200: loss = 3.1435461044311523\n",
      "step = 7939400: loss = 4.347146034240723\n",
      "step = 7939600: loss = 3.699615955352783\n",
      "step = 7939800: loss = 3.761960983276367\n",
      "step = 7940000: loss = 4.453075885772705\n",
      "step = 7940000: Average Return = 3.8559999465942383\n",
      "step = 7940200: loss = 3.884765148162842\n",
      "step = 7940400: loss = 6.5674896240234375\n",
      "step = 7940600: loss = 4.651183605194092\n",
      "step = 7940800: loss = 4.580519199371338\n",
      "step = 7941000: loss = 4.755851745605469\n",
      "step = 7941200: loss = 3.7985470294952393\n",
      "step = 7941400: loss = 5.335615158081055\n",
      "step = 7941600: loss = 4.417915344238281\n",
      "step = 7941800: loss = 3.825230598449707\n",
      "step = 7942000: loss = 4.276894569396973\n",
      "step = 7942200: loss = 3.6691808700561523\n",
      "step = 7942400: loss = 3.4322450160980225\n",
      "step = 7942600: loss = 4.022103309631348\n",
      "step = 7942800: loss = 3.42478346824646\n",
      "step = 7943000: loss = 3.843618154525757\n",
      "step = 7943200: loss = 5.27326774597168\n",
      "step = 7943400: loss = 3.98750901222229\n",
      "step = 7943600: loss = 4.109177589416504\n",
      "step = 7943800: loss = 5.015933513641357\n",
      "step = 7944000: loss = 4.181315898895264\n",
      "step = 7944200: loss = 5.262335300445557\n",
      "step = 7944400: loss = 5.276860237121582\n",
      "step = 7944600: loss = 3.333467960357666\n",
      "step = 7944800: loss = 4.870133399963379\n",
      "step = 7945000: loss = 3.3603689670562744\n",
      "step = 7945000: Average Return = 3.885999917984009\n",
      "step = 7945200: loss = 3.775053024291992\n",
      "step = 7945400: loss = 3.6847686767578125\n",
      "step = 7945600: loss = 4.1736626625061035\n",
      "step = 7945800: loss = 4.500024795532227\n",
      "step = 7946000: loss = 3.61264705657959\n",
      "step = 7946200: loss = 4.562196731567383\n",
      "step = 7946400: loss = 5.160664081573486\n",
      "step = 7946600: loss = 5.170840740203857\n",
      "step = 7946800: loss = 5.418837547302246\n",
      "step = 7947000: loss = 3.484656810760498\n",
      "step = 7947200: loss = 4.500217914581299\n",
      "step = 7947400: loss = 4.728641986846924\n",
      "step = 7947600: loss = 5.391704082489014\n",
      "step = 7947800: loss = 5.206085681915283\n",
      "step = 7948000: loss = 5.499324798583984\n",
      "step = 7948200: loss = 4.24229621887207\n",
      "step = 7948400: loss = 3.5763609409332275\n",
      "step = 7948600: loss = 3.9675567150115967\n",
      "step = 7948800: loss = 2.7800517082214355\n",
      "step = 7949000: loss = 3.8833632469177246\n",
      "step = 7949200: loss = 3.5323100090026855\n",
      "step = 7949400: loss = 3.134066343307495\n",
      "step = 7949600: loss = 6.690266132354736\n",
      "step = 7949800: loss = 3.4361066818237305\n",
      "step = 7950000: loss = 4.999133586883545\n",
      "step = 7950000: Average Return = 3.8480000495910645\n",
      "step = 7950200: loss = 3.4741923809051514\n",
      "step = 7950400: loss = 5.10780143737793\n",
      "step = 7950600: loss = 4.653839588165283\n",
      "step = 7950800: loss = 5.534567832946777\n",
      "step = 7951000: loss = 2.7112839221954346\n",
      "step = 7951200: loss = 4.332988739013672\n",
      "step = 7951400: loss = 4.468418598175049\n",
      "step = 7951600: loss = 4.721813678741455\n",
      "step = 7951800: loss = 4.210389137268066\n",
      "step = 7952000: loss = 3.5806939601898193\n",
      "step = 7952200: loss = 4.550402641296387\n",
      "step = 7952400: loss = 3.7536416053771973\n",
      "step = 7952600: loss = 4.363893508911133\n",
      "step = 7952800: loss = 4.021642684936523\n",
      "step = 7953000: loss = 3.6957995891571045\n",
      "step = 7953200: loss = 4.811898231506348\n",
      "step = 7953400: loss = 3.8238537311553955\n",
      "step = 7953600: loss = 3.0028903484344482\n",
      "step = 7953800: loss = 3.5826356410980225\n",
      "step = 7954000: loss = 4.041907787322998\n",
      "step = 7954200: loss = 5.337924480438232\n",
      "step = 7954400: loss = 5.203602313995361\n",
      "step = 7954600: loss = 3.1650054454803467\n",
      "step = 7954800: loss = 4.399530410766602\n",
      "step = 7955000: loss = 6.441614151000977\n",
      "step = 7955000: Average Return = 3.9760000705718994\n",
      "step = 7955200: loss = 5.645889759063721\n",
      "step = 7955400: loss = 4.534534454345703\n",
      "step = 7955600: loss = 3.247438907623291\n",
      "step = 7955800: loss = 4.09204626083374\n",
      "step = 7956000: loss = 5.394047260284424\n",
      "step = 7956200: loss = 2.2546281814575195\n",
      "step = 7956400: loss = 5.39206600189209\n",
      "step = 7956600: loss = 5.452023983001709\n",
      "step = 7956800: loss = 5.576776027679443\n",
      "step = 7957000: loss = 3.9147887229919434\n",
      "step = 7957200: loss = 3.265573740005493\n",
      "step = 7957400: loss = 4.085209369659424\n",
      "step = 7957600: loss = 3.4441914558410645\n",
      "step = 7957800: loss = 3.411900043487549\n",
      "step = 7958000: loss = 3.0364575386047363\n",
      "step = 7958200: loss = 5.057098388671875\n",
      "step = 7958400: loss = 3.7461249828338623\n",
      "step = 7958600: loss = 3.5279083251953125\n",
      "step = 7958800: loss = 3.265852689743042\n",
      "step = 7959000: loss = 4.6855292320251465\n",
      "step = 7959200: loss = 4.808650970458984\n",
      "step = 7959400: loss = 4.122889995574951\n",
      "step = 7959600: loss = 4.270449161529541\n",
      "step = 7959800: loss = 3.545886754989624\n",
      "step = 7960000: loss = 4.5367889404296875\n",
      "step = 7960000: Average Return = 4.0279998779296875\n",
      "step = 7960200: loss = 4.2915143966674805\n",
      "step = 7960400: loss = 4.376925945281982\n",
      "step = 7960600: loss = 4.600642681121826\n",
      "step = 7960800: loss = 3.34733247756958\n",
      "step = 7961000: loss = 4.234618663787842\n",
      "step = 7961200: loss = 4.399120807647705\n",
      "step = 7961400: loss = 4.267361164093018\n",
      "step = 7961600: loss = 5.367781639099121\n",
      "step = 7961800: loss = 4.654777526855469\n",
      "step = 7962000: loss = 4.452151775360107\n",
      "step = 7962200: loss = 4.631256103515625\n",
      "step = 7962400: loss = 4.325232028961182\n",
      "step = 7962600: loss = 4.5757904052734375\n",
      "step = 7962800: loss = 4.350529670715332\n",
      "step = 7963000: loss = 4.947507858276367\n",
      "step = 7963200: loss = 2.651719331741333\n",
      "step = 7963400: loss = 3.6054916381835938\n",
      "step = 7963600: loss = 5.06851053237915\n",
      "step = 7963800: loss = 3.2133452892303467\n",
      "step = 7964000: loss = 4.437179088592529\n",
      "step = 7964200: loss = 2.9944982528686523\n",
      "step = 7964400: loss = 3.4692623615264893\n",
      "step = 7964600: loss = 3.284886598587036\n",
      "step = 7964800: loss = 6.412656784057617\n",
      "step = 7965000: loss = 3.0657505989074707\n",
      "step = 7965000: Average Return = 3.819999933242798\n",
      "step = 7965200: loss = 3.5341155529022217\n",
      "step = 7965400: loss = 2.4682083129882812\n",
      "step = 7965600: loss = 5.744721412658691\n",
      "step = 7965800: loss = 4.497561931610107\n",
      "step = 7966000: loss = 3.876124382019043\n",
      "step = 7966200: loss = 4.607744216918945\n",
      "step = 7966400: loss = 3.9823079109191895\n",
      "step = 7966600: loss = 3.5032289028167725\n",
      "step = 7966800: loss = 4.875163555145264\n",
      "step = 7967000: loss = 4.996189594268799\n",
      "step = 7967200: loss = 3.406297445297241\n",
      "step = 7967400: loss = 3.377065658569336\n",
      "step = 7967600: loss = 5.051912307739258\n",
      "step = 7967800: loss = 3.8108134269714355\n",
      "step = 7968000: loss = 5.519352436065674\n",
      "step = 7968200: loss = 4.015207290649414\n",
      "step = 7968400: loss = 4.398043632507324\n",
      "step = 7968600: loss = 5.202572345733643\n",
      "step = 7968800: loss = 4.014130592346191\n",
      "step = 7969000: loss = 5.839682579040527\n",
      "step = 7969200: loss = 4.501985549926758\n",
      "step = 7969400: loss = 4.0409417152404785\n",
      "step = 7969600: loss = 4.507116794586182\n",
      "step = 7969800: loss = 4.354055881500244\n",
      "step = 7970000: loss = 4.186575412750244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 7970000: Average Return = 3.8380000591278076\n",
      "step = 7970200: loss = 5.001007556915283\n",
      "step = 7970400: loss = 6.121294975280762\n",
      "step = 7970600: loss = 3.6191351413726807\n",
      "step = 7970800: loss = 4.383029460906982\n",
      "step = 7971000: loss = 3.482393741607666\n",
      "step = 7971200: loss = 4.231024265289307\n",
      "step = 7971400: loss = 3.943636178970337\n",
      "step = 7971600: loss = 4.219400405883789\n",
      "step = 7971800: loss = 4.597409725189209\n",
      "step = 7972000: loss = 2.9386329650878906\n",
      "step = 7972200: loss = 4.218707084655762\n",
      "step = 7972400: loss = 3.8845372200012207\n",
      "step = 7972600: loss = 3.8745524883270264\n",
      "step = 7972800: loss = 5.377804279327393\n",
      "step = 7973000: loss = 3.8383069038391113\n",
      "step = 7973200: loss = 4.608864784240723\n",
      "step = 7973400: loss = 3.8265795707702637\n",
      "step = 7973600: loss = 4.451809406280518\n",
      "step = 7973800: loss = 5.2878546714782715\n",
      "step = 7974000: loss = 4.165093421936035\n",
      "step = 7974200: loss = 3.989119529724121\n",
      "step = 7974400: loss = 4.573508262634277\n",
      "step = 7974600: loss = 4.232728004455566\n",
      "step = 7974800: loss = 5.097184181213379\n",
      "step = 7975000: loss = 5.194874286651611\n",
      "step = 7975000: Average Return = 3.885999917984009\n",
      "step = 7975200: loss = 4.047682285308838\n",
      "step = 7975400: loss = 3.8838086128234863\n",
      "step = 7975600: loss = 4.596067428588867\n",
      "step = 7975800: loss = 4.493182182312012\n",
      "step = 7976000: loss = 4.139220714569092\n",
      "step = 7976200: loss = 6.0122528076171875\n",
      "step = 7976400: loss = 5.078588962554932\n",
      "step = 7976600: loss = 5.061123847961426\n",
      "step = 7976800: loss = 3.8992724418640137\n",
      "step = 7977000: loss = 2.4332544803619385\n",
      "step = 7977200: loss = 5.465110778808594\n",
      "step = 7977400: loss = 4.506567478179932\n",
      "step = 7977600: loss = 3.746126413345337\n",
      "step = 7977800: loss = 3.836817502975464\n",
      "step = 7978000: loss = 6.443105697631836\n",
      "step = 7978200: loss = 4.187631607055664\n",
      "step = 7978400: loss = 3.1347687244415283\n",
      "step = 7978600: loss = 3.747213363647461\n",
      "step = 7978800: loss = 3.894402027130127\n",
      "step = 7979000: loss = 3.8064677715301514\n",
      "step = 7979200: loss = 3.719928741455078\n",
      "step = 7979400: loss = 4.607883930206299\n",
      "step = 7979600: loss = 3.532819986343384\n",
      "step = 7979800: loss = 3.7057342529296875\n",
      "step = 7980000: loss = 4.408984661102295\n",
      "step = 7980000: Average Return = 4.007999897003174\n",
      "step = 7980200: loss = 3.9584169387817383\n",
      "step = 7980400: loss = 4.743772983551025\n",
      "step = 7980600: loss = 4.917509078979492\n",
      "step = 7980800: loss = 4.183715343475342\n",
      "step = 7981000: loss = 2.8511223793029785\n",
      "step = 7981200: loss = 3.937089681625366\n",
      "step = 7981400: loss = 3.1708061695098877\n",
      "step = 7981600: loss = 4.472715377807617\n",
      "step = 7981800: loss = 4.392055988311768\n",
      "step = 7982000: loss = 3.2905285358428955\n",
      "step = 7982200: loss = 3.2896580696105957\n",
      "step = 7982400: loss = 4.510667324066162\n",
      "step = 7982600: loss = 3.178997755050659\n",
      "step = 7982800: loss = 3.588298797607422\n",
      "step = 7983000: loss = 3.1464247703552246\n",
      "step = 7983200: loss = 3.5578179359436035\n",
      "step = 7983400: loss = 4.754480361938477\n",
      "step = 7983600: loss = 3.6446545124053955\n",
      "step = 7983800: loss = 5.715795993804932\n",
      "step = 7984000: loss = 5.421332836151123\n",
      "step = 7984200: loss = 3.808331251144409\n",
      "step = 7984400: loss = 5.471029758453369\n",
      "step = 7984600: loss = 3.324007511138916\n",
      "step = 7984800: loss = 4.143967151641846\n",
      "step = 7985000: loss = 4.037882328033447\n",
      "step = 7985000: Average Return = 4.073999881744385\n",
      "step = 7985200: loss = 3.0663769245147705\n",
      "step = 7985400: loss = 5.505620002746582\n",
      "step = 7985600: loss = 3.2877213954925537\n",
      "step = 7985800: loss = 6.0503363609313965\n",
      "step = 7986000: loss = 3.483562469482422\n",
      "step = 7986200: loss = 4.962335586547852\n",
      "step = 7986400: loss = 3.478837251663208\n",
      "step = 7986600: loss = 5.678567886352539\n",
      "step = 7986800: loss = 2.6112401485443115\n",
      "step = 7987000: loss = 4.47755765914917\n",
      "step = 7987200: loss = 4.526927471160889\n",
      "step = 7987400: loss = 4.141992568969727\n",
      "step = 7987600: loss = 5.347383975982666\n",
      "step = 7987800: loss = 3.606581449508667\n",
      "step = 7988000: loss = 3.6661219596862793\n",
      "step = 7988200: loss = 3.96498703956604\n",
      "step = 7988400: loss = 2.980503797531128\n",
      "step = 7988600: loss = 3.5093438625335693\n",
      "step = 7988800: loss = 3.3838746547698975\n",
      "step = 7989000: loss = 3.3945446014404297\n",
      "step = 7989200: loss = 3.8551318645477295\n",
      "step = 7989400: loss = 4.073892593383789\n",
      "step = 7989600: loss = 4.832032203674316\n",
      "step = 7989800: loss = 3.4598400592803955\n",
      "step = 7990000: loss = 4.865677833557129\n",
      "step = 7990000: Average Return = 4.263999938964844\n",
      "step = 7990200: loss = 4.669255256652832\n",
      "step = 7990400: loss = 3.4125707149505615\n",
      "step = 7990600: loss = 4.062995910644531\n",
      "step = 7990800: loss = 3.6323442459106445\n",
      "step = 7991000: loss = 3.579608678817749\n",
      "step = 7991200: loss = 4.013319492340088\n",
      "step = 7991400: loss = 4.768808841705322\n",
      "step = 7991600: loss = 3.932921886444092\n",
      "step = 7991800: loss = 6.723881721496582\n",
      "step = 7992000: loss = 5.985081672668457\n",
      "step = 7992200: loss = 5.725413799285889\n",
      "step = 7992400: loss = 3.995488166809082\n",
      "step = 7992600: loss = 3.820099353790283\n",
      "step = 7992800: loss = 4.973265171051025\n",
      "step = 7993000: loss = 3.1895365715026855\n",
      "step = 7993200: loss = 3.134911060333252\n",
      "step = 7993400: loss = 3.3309950828552246\n",
      "step = 7993600: loss = 4.094958305358887\n",
      "step = 7993800: loss = 5.43355655670166\n",
      "step = 7994000: loss = 5.156947135925293\n",
      "step = 7994200: loss = 5.33001708984375\n",
      "step = 7994400: loss = 3.0664427280426025\n",
      "step = 7994600: loss = 4.124994277954102\n",
      "step = 7994800: loss = 3.181612491607666\n",
      "step = 7995000: loss = 4.213669300079346\n",
      "step = 7995000: Average Return = 3.8919999599456787\n",
      "step = 7995200: loss = 3.8162524700164795\n",
      "step = 7995400: loss = 4.049173355102539\n",
      "step = 7995600: loss = 4.818515300750732\n",
      "step = 7995800: loss = 7.083874225616455\n",
      "step = 7996000: loss = 5.139333248138428\n",
      "step = 7996200: loss = 4.900073051452637\n",
      "step = 7996400: loss = 4.683112621307373\n",
      "step = 7996600: loss = 3.8189780712127686\n",
      "step = 7996800: loss = 3.3182668685913086\n",
      "step = 7997000: loss = 4.9667158126831055\n",
      "step = 7997200: loss = 4.050660133361816\n",
      "step = 7997400: loss = 4.393094539642334\n",
      "step = 7997600: loss = 4.76312780380249\n",
      "step = 7997800: loss = 4.235955238342285\n",
      "step = 7998000: loss = 3.34299898147583\n",
      "step = 7998200: loss = 4.7913312911987305\n",
      "step = 7998400: loss = 3.2487051486968994\n",
      "step = 7998600: loss = 3.3121514320373535\n",
      "step = 7998800: loss = 3.7295892238616943\n",
      "step = 7999000: loss = 3.2861366271972656\n",
      "step = 7999200: loss = 3.0523600578308105\n",
      "step = 7999400: loss = 4.248968601226807\n",
      "step = 7999600: loss = 3.561554431915283\n",
      "step = 7999800: loss = 4.237210750579834\n",
      "step = 8000000: loss = 3.58880615234375\n",
      "step = 8000000: Average Return = 3.936000108718872\n",
      "step = 8000200: loss = 4.433135986328125\n",
      "step = 8000400: loss = 4.420266628265381\n",
      "step = 8000600: loss = 4.982460975646973\n",
      "step = 8000800: loss = 4.009561061859131\n",
      "step = 8001000: loss = 4.980671405792236\n",
      "step = 8001200: loss = 4.429545879364014\n",
      "step = 8001400: loss = 4.446611404418945\n",
      "step = 8001600: loss = 4.286440849304199\n",
      "step = 8001800: loss = 3.517428159713745\n",
      "step = 8002000: loss = 4.406756401062012\n",
      "step = 8002200: loss = 5.884944915771484\n",
      "step = 8002400: loss = 3.6533939838409424\n",
      "step = 8002600: loss = 4.684750080108643\n",
      "step = 8002800: loss = 3.8502416610717773\n",
      "step = 8003000: loss = 3.552515983581543\n",
      "step = 8003200: loss = 4.268303394317627\n",
      "step = 8003400: loss = 3.753098964691162\n",
      "step = 8003600: loss = 3.6631243228912354\n",
      "step = 8003800: loss = 5.294789791107178\n",
      "step = 8004000: loss = 5.56958532333374\n",
      "step = 8004200: loss = 3.832927703857422\n",
      "step = 8004400: loss = 4.570756435394287\n",
      "step = 8004600: loss = 4.047445774078369\n",
      "step = 8004800: loss = 3.7118759155273438\n",
      "step = 8005000: loss = 4.834969520568848\n",
      "step = 8005000: Average Return = 4.010000228881836\n",
      "step = 8005200: loss = 3.2193286418914795\n",
      "step = 8005400: loss = 2.8718132972717285\n",
      "step = 8005600: loss = 4.554232120513916\n",
      "step = 8005800: loss = 3.53570556640625\n",
      "step = 8006000: loss = 4.539646625518799\n",
      "step = 8006200: loss = 4.3048095703125\n",
      "step = 8006400: loss = 5.263421535491943\n",
      "step = 8006600: loss = 4.681274890899658\n",
      "step = 8006800: loss = 5.4567108154296875\n",
      "step = 8007000: loss = 4.349080562591553\n",
      "step = 8007200: loss = 5.079538345336914\n",
      "step = 8007400: loss = 3.953120708465576\n",
      "step = 8007600: loss = 3.9484269618988037\n",
      "step = 8007800: loss = 3.7128679752349854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 8008000: loss = 4.964694976806641\n",
      "step = 8008200: loss = 5.245525360107422\n",
      "step = 8008400: loss = 4.536887168884277\n",
      "step = 8008600: loss = 4.512728691101074\n",
      "step = 8008800: loss = 4.00562047958374\n",
      "step = 8009000: loss = 3.108874559402466\n",
      "step = 8009200: loss = 2.8710031509399414\n",
      "step = 8009400: loss = 3.569605588912964\n",
      "step = 8009600: loss = 4.823826313018799\n",
      "step = 8009800: loss = 4.476442337036133\n",
      "step = 8010000: loss = 3.702120065689087\n",
      "step = 8010000: Average Return = 3.6640000343322754\n",
      "step = 8010200: loss = 2.4966037273406982\n",
      "step = 8010400: loss = 3.8472282886505127\n",
      "step = 8010600: loss = 4.19607400894165\n",
      "step = 8010800: loss = 5.117452144622803\n",
      "step = 8011000: loss = 4.356631755828857\n",
      "step = 8011200: loss = 4.197840213775635\n",
      "step = 8011400: loss = 4.0280280113220215\n",
      "step = 8011600: loss = 4.2266316413879395\n",
      "step = 8011800: loss = 4.886875629425049\n",
      "step = 8012000: loss = 2.653332471847534\n",
      "step = 8012200: loss = 5.112224102020264\n",
      "step = 8012400: loss = 4.471078395843506\n",
      "step = 8012600: loss = 4.569701671600342\n",
      "step = 8012800: loss = 4.959293842315674\n",
      "step = 8013000: loss = 4.309556007385254\n",
      "step = 8013200: loss = 3.7765004634857178\n",
      "step = 8013400: loss = 4.763741493225098\n",
      "step = 8013600: loss = 4.006232738494873\n",
      "step = 8013800: loss = 4.216298580169678\n",
      "step = 8014000: loss = 4.555331230163574\n",
      "step = 8014200: loss = 3.8087236881256104\n",
      "step = 8014400: loss = 2.6554346084594727\n",
      "step = 8014600: loss = 3.8188581466674805\n",
      "step = 8014800: loss = 3.6132445335388184\n",
      "step = 8015000: loss = 5.467062950134277\n",
      "step = 8015000: Average Return = 4.026000022888184\n",
      "step = 8015200: loss = 3.9282970428466797\n",
      "step = 8015400: loss = 4.209931373596191\n",
      "step = 8015600: loss = 4.103246688842773\n",
      "step = 8015800: loss = 4.654387950897217\n",
      "step = 8016000: loss = 3.511871576309204\n",
      "step = 8016200: loss = 5.548497676849365\n",
      "step = 8016400: loss = 5.227380275726318\n",
      "step = 8016600: loss = 4.434205532073975\n",
      "step = 8016800: loss = 5.341540336608887\n",
      "step = 8017000: loss = 5.678015232086182\n",
      "step = 8017200: loss = 5.3038177490234375\n",
      "step = 8017400: loss = 4.91496467590332\n",
      "step = 8017600: loss = 2.874335289001465\n",
      "step = 8017800: loss = 5.683563709259033\n",
      "step = 8018000: loss = 2.2619335651397705\n",
      "step = 8018200: loss = 4.862013339996338\n",
      "step = 8018400: loss = 5.131600379943848\n",
      "step = 8018600: loss = 4.2723212242126465\n",
      "step = 8018800: loss = 3.798896312713623\n",
      "step = 8019000: loss = 4.442988395690918\n",
      "step = 8019200: loss = 4.498298168182373\n",
      "step = 8019400: loss = 4.260365962982178\n",
      "step = 8019600: loss = 4.57086181640625\n",
      "step = 8019800: loss = 4.354172229766846\n",
      "step = 8020000: loss = 3.7850468158721924\n",
      "step = 8020000: Average Return = 3.558000087738037\n",
      "step = 8020200: loss = 4.361613750457764\n",
      "step = 8020400: loss = 3.9664220809936523\n",
      "step = 8020600: loss = 4.156682014465332\n",
      "step = 8020800: loss = 4.503272533416748\n",
      "step = 8021000: loss = 5.526797771453857\n",
      "step = 8021200: loss = 3.2519707679748535\n",
      "step = 8021400: loss = 4.38675594329834\n",
      "step = 8021600: loss = 4.307036876678467\n",
      "step = 8021800: loss = 3.9996395111083984\n",
      "step = 8022000: loss = 4.607034206390381\n",
      "step = 8022200: loss = 4.880640983581543\n",
      "step = 8022400: loss = 4.576858997344971\n",
      "step = 8022600: loss = 4.089399337768555\n",
      "step = 8022800: loss = 3.668494462966919\n",
      "step = 8023000: loss = 3.4385201930999756\n",
      "step = 8023200: loss = 4.717251300811768\n",
      "step = 8023400: loss = 3.475839138031006\n",
      "step = 8023600: loss = 4.3345465660095215\n",
      "step = 8023800: loss = 4.5278472900390625\n",
      "step = 8024000: loss = 5.360109329223633\n",
      "step = 8024200: loss = 4.14567756652832\n",
      "step = 8024400: loss = 3.2383787631988525\n",
      "step = 8024600: loss = 4.6628546714782715\n",
      "step = 8024800: loss = 4.036133766174316\n",
      "step = 8025000: loss = 6.851844310760498\n",
      "step = 8025000: Average Return = 4.033999919891357\n",
      "step = 8025200: loss = 4.471606731414795\n",
      "step = 8025400: loss = 3.535740852355957\n",
      "step = 8025600: loss = 3.4325621128082275\n",
      "step = 8025800: loss = 4.890922546386719\n",
      "step = 8026000: loss = 4.359985828399658\n",
      "step = 8026200: loss = 3.9994702339172363\n",
      "step = 8026400: loss = 5.62664794921875\n",
      "step = 8026600: loss = 4.182499885559082\n",
      "step = 8026800: loss = 3.928542375564575\n",
      "step = 8027000: loss = 4.528750896453857\n",
      "step = 8027200: loss = 2.9806272983551025\n",
      "step = 8027400: loss = 3.9234073162078857\n",
      "step = 8027600: loss = 4.6414947509765625\n",
      "step = 8027800: loss = 4.216981887817383\n",
      "step = 8028000: loss = 3.20169734954834\n",
      "step = 8028200: loss = 5.570243835449219\n",
      "step = 8028400: loss = 2.8525567054748535\n",
      "step = 8028600: loss = 4.963705539703369\n",
      "step = 8028800: loss = 3.3731744289398193\n",
      "step = 8029000: loss = 4.916448593139648\n",
      "step = 8029200: loss = 4.030575275421143\n",
      "step = 8029400: loss = 4.590684413909912\n",
      "step = 8029600: loss = 3.7360076904296875\n",
      "step = 8029800: loss = 3.7056922912597656\n",
      "step = 8030000: loss = 4.707206726074219\n",
      "step = 8030000: Average Return = 3.696000099182129\n",
      "step = 8030200: loss = 3.653348684310913\n",
      "step = 8030400: loss = 4.9600725173950195\n",
      "step = 8030600: loss = 2.722736120223999\n",
      "step = 8030800: loss = 5.385021686553955\n",
      "step = 8031000: loss = 4.6007843017578125\n",
      "step = 8031200: loss = 2.0578222274780273\n",
      "step = 8031400: loss = 4.962199687957764\n",
      "step = 8031600: loss = 4.289116859436035\n",
      "step = 8031800: loss = 4.981349945068359\n",
      "step = 8032000: loss = 2.7960667610168457\n",
      "step = 8032200: loss = 2.842172861099243\n",
      "step = 8032400: loss = 4.333485126495361\n",
      "step = 8032600: loss = 4.141478061676025\n",
      "step = 8032800: loss = 4.633295059204102\n",
      "step = 8033000: loss = 5.157320976257324\n",
      "step = 8033200: loss = 5.556738376617432\n",
      "step = 8033400: loss = 4.447277545928955\n",
      "step = 8033600: loss = 2.9274656772613525\n",
      "step = 8033800: loss = 4.129551887512207\n",
      "step = 8034000: loss = 3.8127169609069824\n",
      "step = 8034200: loss = 3.4909346103668213\n",
      "step = 8034400: loss = 3.9659969806671143\n",
      "step = 8034600: loss = 4.662257671356201\n",
      "step = 8034800: loss = 4.063836097717285\n",
      "step = 8035000: loss = 6.181233882904053\n",
      "step = 8035000: Average Return = 3.7160000801086426\n",
      "step = 8035200: loss = 3.2365636825561523\n",
      "step = 8035400: loss = 5.77271842956543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:253: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if policy_state is ():  # pylint: disable=literal-comparison\n",
      "C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:315: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if not (policy_state is None or policy_state is () or policy_state is []):  # pylint: disable=literal-comparison\n",
      "C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:253: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if policy_state is ():  # pylint: disable=literal-comparison\n",
      "C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:315: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if not (policy_state is None or policy_state is () or policy_state is []):  # pylint: disable=literal-comparison\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_steps_per_iteration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     experience, unused_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m      5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain(experience)\u001b[38;5;241m.\u001b[39mloss\n",
      "Cell \u001b[1;32mIn[11], line 91\u001b[0m, in \u001b[0;36mcollect_data\u001b[1;34m(env, policy, buffer, steps)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_data\u001b[39m(env, policy, buffer, steps):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m---> 91\u001b[0m         \u001b[43mcollect_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 82\u001b[0m, in \u001b[0;36mcollect_step\u001b[1;34m(environment, policy, buffer)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_step\u001b[39m(environment, policy, buffer):\n\u001b[0;32m     81\u001b[0m     time_step \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mcurrent_time_step()\n\u001b[1;32m---> 82\u001b[0m     action_step \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     next_time_step \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(action_step)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# commenting out render will make training quicker\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# environment.render(\"human\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:324\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[0;32m    323\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[1;32m--> 324\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[0;32m    327\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\utils\\common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_tf1_allowed()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\policies\\epsilon_greedy_policy.py:116\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy._action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    114\u001b[0m seed_stream \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mSeedStream(seed\u001b[38;5;241m=\u001b[39mseed, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_greedy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m greedy_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_greedy_policy\u001b[38;5;241m.\u001b[39maction(time_step, policy_state)\n\u001b[1;32m--> 116\u001b[0m random_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_random_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m outer_shape \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mget_outer_shape(time_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_spec)\n\u001b[0;32m    119\u001b[0m rng \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m    120\u001b[0m     outer_shape, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed_stream(), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_rng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py:324\u001b[0m, in \u001b[0;36mTFPolicy.action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_automatic_state_reset:\n\u001b[0;32m    323\u001b[0m   policy_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_reset_state(time_step, policy_state)\n\u001b[1;32m--> 324\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43maction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_action\u001b[39m(action, action_spec):\n\u001b[0;32m    327\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action_spec, tensor_spec\u001b[38;5;241m.\u001b[39mBoundedTensorSpec):\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\utils\\common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_tf1_allowed()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    190\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\policies\\random_tf_policy.py:170\u001b[0m, in \u001b[0;36mRandomTFPolicy._action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    167\u001b[0m     action_ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(masked_categorical\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;241m+\u001b[39m action_spec\u001b[38;5;241m.\u001b[39mminimum,\n\u001b[0;32m    168\u001b[0m                       action_spec\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    169\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     action_ \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_spec_nest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m   policy_info \u001b[38;5;241m=\u001b[39m tensor_spec\u001b[38;5;241m.\u001b[39msample_spec_nest(\n\u001b[0;32m    174\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_spec, outer_dims\u001b[38;5;241m=\u001b[39mouter_dims)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Update policy info with chosen arm features.\u001b[39;00m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\specs\\tensor_spec.py:408\u001b[0m, in \u001b[0;36msample_spec_nest\u001b[1;34m(structure, seed, outer_dims, minimum, maximum)\u001b[0m\n\u001b[0;32m    405\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpec type not supported: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(spec))\n\u001b[1;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\specs\\tensor_spec.py:400\u001b[0m, in \u001b[0;36msample_spec_nest.<locals>.sample_fn\u001b[1;34m(spec)\u001b[0m\n\u001b[0;32m    394\u001b[0m       spec_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(minimum, spec_min)\n\u001b[0;32m    395\u001b[0m     bounded_spec \u001b[38;5;241m=\u001b[39m BoundedTensorSpec(\n\u001b[0;32m    396\u001b[0m         shape\u001b[38;5;241m=\u001b[39mbounded_spec\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m    397\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mbounded_spec\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    398\u001b[0m         minimum\u001b[38;5;241m=\u001b[39mspec_min,\n\u001b[0;32m    399\u001b[0m         maximum\u001b[38;5;241m=\u001b[39mspec_max)\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msample_bounded_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounded_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mouter_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mouter_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpec type not supported: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(spec))\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tf_agents\\specs\\tensor_spec.py:288\u001b[0m, in \u001b[0;36msample_bounded_spec\u001b[1;34m(spec, seed, outer_dims)\u001b[0m\n\u001b[0;32m    285\u001b[0m     maxval \u001b[38;5;241m=\u001b[39m maxval \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    287\u001b[0m   shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(spec\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m--> 288\u001b[0m   full_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mouter_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m   res \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m    290\u001b[0m       full_shape,\n\u001b[0;32m    291\u001b[0m       minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[0;32m    292\u001b[0m       maxval\u001b[38;5;241m=\u001b[39mmaxval,\n\u001b[0;32m    293\u001b[0m       dtype\u001b[38;5;241m=\u001b[39msampling_dtype,\n\u001b[0;32m    294\u001b[0m       seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_uint8:\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1821\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   1818\u001b[0m         axis, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1819\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39massert_has_rank(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m identity(values[\u001b[38;5;241m0\u001b[39m], name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m-> 1821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:1502\u001b[0m, in \u001b[0;36mconcat_v2\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   1501\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1502\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConcatV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   1505\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        train_checkpointer.save(train_step_counter)\n",
    "        with open(\"CheckpointsRC/returns.txt\", \"w\") as txt:\n",
    "            for item in returns:\n",
    "                txt.write(str(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eeacc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1109, 10.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, train_step_counter + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea43f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrandomWins = 0\\nagentWins = 0\\nfor i in range(100000):\\n    print(i)\\n    temp = playRandom()\\n    if temp == 1:\\n        randomWins += 1\\n    elif temp == 2:\\n        agentWins += 1\\n\\nprint(\"Random Policy Wins: \" + str(randomWins))\\nprint(\"Agent Policy Wins: \" + str(agentWins))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the agent human vs ai\n",
    "def playHuman():\n",
    "    policy = agent.policy\n",
    "    running = True\n",
    "    env = eval_env\n",
    "    state = env.reset()\n",
    "    pygame.init()\n",
    "\n",
    "    while running:\n",
    "        to_move = True\n",
    "        env.render(\"human\")\n",
    "        mouse = pygame.mouse.get_pos()\n",
    "        \n",
    "        if env._envs[0].current_player == 1:\n",
    "            if 75 + 100 > mouse[0] > 75 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][0]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(0)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "            elif 275 + 100 > mouse[0] > 275 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][1]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(4)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "            elif 475 + 100 > mouse[0] > 475 and 635 + 100 > mouse[1] > 635:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.KEYDOWN:\n",
    "                        if event.key == pygame.K_r:\n",
    "                            tile = env._envs[0].tiles[env._envs[0].player_tiles[0][2]]\n",
    "                            tile.rotate_tile(1)\n",
    "                            env.render(\"human\")\n",
    "                    if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                        to_move = False\n",
    "                        state = env.step(8)\n",
    "                        env.render(\"human\")\n",
    "                        \n",
    "        elif env._envs[0].current_player == 2:\n",
    "            time.sleep(2)\n",
    "            action_step = policy.action(state)\n",
    "            env.step(action_step.action)\n",
    "            env.render(\"human\")\n",
    "            to_move = False\n",
    "\n",
    "        elif env._envs[0].current_player == -1 or env._envs[0].game_is_over():\n",
    "            env.render(\"human\")\n",
    "            print(\"Winner: Player \" + str(eval_py_env.current_player))\n",
    "            running = False\n",
    "                    \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                \n",
    "def playRandom():\n",
    "    env = eval_env\n",
    "    random = random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),env.action_spec())\n",
    "    policy = agent.policy\n",
    "    running = True\n",
    "    state = env.reset()\n",
    "    pygame.init()\n",
    "    \n",
    "    while running:\n",
    "        to_move = True\n",
    "        \n",
    "        if env._envs[0].current_player == 1 and not env._envs[0].game_is_over():\n",
    "            action_step = random.action(state)\n",
    "            state = env.step(action_step.action)\n",
    "            to_move = False\n",
    "            \n",
    "        elif env._envs[0].current_player == 2 and not env._envs[0].game_is_over():\n",
    "            action_step = policy.action(state)\n",
    "            state = env.step(action_step.action)\n",
    "            to_move = False\n",
    "\n",
    "        elif env._envs[0].current_player == -1 or env._envs[0].game_is_over():\n",
    "            running = False\n",
    "            pygame.quit()\n",
    "            return env._envs[0].remaining_players[0]\n",
    "        \n",
    "        #uncomment to render\n",
    "        #env.render(\"human\")\n",
    "                    \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                \n",
    "    return 0\n",
    "        \n",
    "playHuman()\n",
    "\n",
    "'''\n",
    "randomWins = 0\n",
    "agentWins = 0\n",
    "for i in range(100000):\n",
    "    print(i)\n",
    "    temp = playRandom()\n",
    "    if temp == 1:\n",
    "        randomWins += 1\n",
    "    elif temp == 2:\n",
    "        agentWins += 1\n",
    "\n",
    "print(\"Random Policy Wins: \" + str(randomWins))\n",
    "print(\"Agent Policy Wins: \" + str(agentWins))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12249b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
