{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bcaa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_agents==0.15.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (0.15.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.2.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (9.3.0)\n",
      "Requirement already satisfied: gin-config>=0.4.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.23.5)\n",
      "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.23.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.3.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (2.1.0)\n",
      "Requirement already satisfied: tensorflow-probability>=0.18.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (0.19.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (3.19.6)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tf_agents==0.15.0) (1.16.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from gym<=0.23.0,>=0.17.0->tf_agents==0.15.0) (0.0.8)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.4.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (5.1.1)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-probability>=0.18.0->tf_agents==0.15.0) (0.1.8)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\horridjoe\\opencv\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.11.23)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\horridjoe\\opencv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pygame\n",
    "!pip install tf_agents==0.15.0\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4929ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# player piece colors [player1, player2]\n",
    "colors = ['#FF0000', '#0000FF']\n",
    "\n",
    "# paths for each tile 1-35\n",
    "node_combinations = [ \n",
    "    [(0,3), (1,5), (2,6), (4,7)], [(0,4), (1,5), (2,6), (3,7)], [(0,3), (1,6), (2,5), (4,7)], [(0,6), (1,5), (2,4), (3,7)],\n",
    "    [(0,1), (2,3), (4,5), (6,7)], [(0,4), (1,5), (2,3), (6,7)], [(0,6), (1,5), (2,3), (4,7)], [(0,5), (1,4), (2,7), (3,6)],\n",
    "    [(0,5), (1,4), (2,6), (3,7)], [(0,3), (1,4), (2,5), (6,7)], [(0,6), (1,4), (2,5), (3,7)], [(0,5), (1,4), (2,3), (6,7)],\n",
    "    [(0,2), (1,3), (4,6), (5,7)], [(0,2), (1,3), (4,5), (6,7)], [(0,5), (1,3), (2,7), (4,6)], [(0,6), (1,3), (2,7), (4,5)],\n",
    "    [(0,4), (1,3), (2,6), (5,7)], [(0,5), (1,3), (2,6), (4,7)], [(0,4), (1,3), (2,5), (6,7)], [(0,6), (1,3), (2,5), (4,7)],\n",
    "    [(0,5), (1,3), (2,4), (6,7)], [(0,6), (1,3), (2,4), (5,7)], [(0,3), (1,2), (4,7), (5,6)], [(0,3), (1,2), (4,6), (5,7)],\n",
    "    [(0,3), (1,2), (4,5), (6,7)], [(0,4), (1,2), (3,7), (5,6)], [(0,5), (1,2), (3,7), (4,6)], [(0,6), (1,2), (3,7), (4,5)],\n",
    "    [(0,4), (1,2), (3,6), (5,7)], [(0,5), (1,2), (3,6), (4,7)], [(0,4), (1,2), (3,5), (6,7)], [(0,6), (1,2), (3,5), (4,7)],\n",
    "    [(0,5), (1,2), (3,4), (6,7)], [(0,6), (1,2), (3,4), (5,7)], [(0,7), (1,2), (3,4), (5,6)]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7550d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tile():\n",
    "    def __init__(self, tile_num, tile_connections):\n",
    "        self.tile_num = tile_num\n",
    "        self.image = pygame.image.load(\"TsuroImages/\" + str(tile_num) + \".png\")\n",
    "        self.image = pygame.transform.scale(self.image, (100, 100))\n",
    "        self.tile_connections = tile_connections\n",
    "        self.rotation = 1\n",
    "    \n",
    "    def move(self, current_node):\n",
    "        next_node = 0\n",
    "        next_player_tile = 0\n",
    "        for connection in self.tile_connections:\n",
    "            if current_node in connection:\n",
    "                n1, n2 = connection\n",
    "                if n1 == current_node:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n2)\n",
    "                else:\n",
    "                    next_node, next_player_tile, next_x, next_y = self.new_tile_node(n1)\n",
    "                return next_node, next_player_tile, next_x, next_y\n",
    "        raise Exception(\"Issue in moving players\")\n",
    "    \n",
    "    # update number of times rotation should be applied to connections and image\n",
    "    def rotate_tile(self, rotate):\n",
    "        self.image = pygame.transform.rotate(self.image, rotate * -90)\n",
    "        self.tile_connections = [tuple((element + (2 * rotate)) % 8 for element in couple ) for couple in self.tile_connections]\n",
    "        self.rotation = 1 if (self.rotation + 1 % 4 == 0) else self.rotation + 1\n",
    "        \n",
    "    def get_rotation(self):\n",
    "        return self.rotation\n",
    "    \n",
    "    def new_tile_node(self, current_node):\n",
    "        next_node = 0\n",
    "        next_x = 0\n",
    "        next_y = 0\n",
    "        next_player_tile = 0\n",
    "        match current_node:\n",
    "            case 0:\n",
    "                next_node = 3\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case 1:\n",
    "                next_node = 6\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 2:\n",
    "                next_node = 5\n",
    "                next_player_tile = -6\n",
    "                next_y = -1\n",
    "            case 3:\n",
    "                next_node = 0\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 4:\n",
    "                next_node = 7\n",
    "                next_player_tile = 1\n",
    "                next_x = 1\n",
    "            case 5:\n",
    "                next_node = 2\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 6:\n",
    "                next_node = 1\n",
    "                next_player_tile = 6\n",
    "                next_y = 1\n",
    "            case 7:\n",
    "                next_node = 4\n",
    "                next_player_tile = -1\n",
    "                next_x = -1\n",
    "            case _:\n",
    "                raise Exception(\"Issue in tile board\")\n",
    "                \n",
    "        return next_node, next_player_tile, next_x, next_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f8391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TsuroEnv(gym.Env):\n",
    "    #\n",
    "    def __init__(self):\n",
    "        self.screen = None\n",
    "        self.current_player = 1\n",
    "        self.num_tiles = 35\n",
    "        self.tile_board_size = (6, 6)\n",
    "        self.rotation_board_size = (6, 6)\n",
    "        self.player_board_size = (36,8)\n",
    "        self.num_players = 2\n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "\n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "        \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        self.rotation_board = np.zeros(self.rotation_board_size, dtype = int)\n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "\n",
    "        self.action_space = spaces.Discrete(11)\n",
    "        self.observation_space = spaces.Box(low=-1, high=140, shape=(327,))\n",
    "        \n",
    "    def get_tile_obs(self):\n",
    "        temp = self.rotation_board.flatten()\n",
    "        temp1 = self.tile_board.flatten()\n",
    "        for i in range(35):\n",
    "            temp[i] = ((temp[i] - 1) * 35) + temp1[i]\n",
    "        return temp\n",
    "        \n",
    "    # Resets the environment to default state\n",
    "    def reset(self): \n",
    "        self.current_player = 1\n",
    "        self.rotation_board = np.zeros(self.rotation_board_size, dtype = int)\n",
    "        self.tile_board = np.zeros(self.tile_board_size, dtype = int)\n",
    "        self.player_board = np.zeros(self.player_board_size, dtype = int)\n",
    "        \n",
    "        self.tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.tiles.append(Tile(i, node_combinations[i]))\n",
    "            \n",
    "        self.remaining_tiles = []\n",
    "        for i in range(self.num_tiles):\n",
    "            self.remaining_tiles.append(i)\n",
    "        random.shuffle(self.remaining_tiles)\n",
    "            \n",
    "        self.remaining_players = []\n",
    "        for i in range(self.num_players):\n",
    "            self.remaining_players.append(i+1)\n",
    "            \n",
    "        self.player_tiles = []\n",
    "        for i in range(self.num_players):\n",
    "            player_tiles = []\n",
    "            for i in range(3):\n",
    "                player_tiles.append(self.remaining_tiles.pop())\n",
    "            self.player_tiles.append(player_tiles)\n",
    "            \n",
    "        #########################################\n",
    "        #TODO: TESTING STUFF TO BE REMOVED LATER#\n",
    "        #########################################\n",
    "        for i in range(self.num_players):\n",
    "            self.player_board[random.randint(0,5)][i+1] = i+1\n",
    "            \n",
    "        initial_obs = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "\n",
    "        return initial_obs\n",
    "    \n",
    "    # Makes a move in the game based on inputs from player or AI\n",
    "    def step(self, action = -2, move = -1):\n",
    "        if move == -1:\n",
    "            card, rotate = self.get_card(action)\n",
    "        else:\n",
    "            card = move\n",
    "            rotate = 0\n",
    "        \n",
    "        action = self.player_tiles[self.current_player - 1][card]\n",
    "        \n",
    "        if action == -1:\n",
    "            observation = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "            reward = -1\n",
    "            done = 0\n",
    "            return observation, reward, done, {}\n",
    "            \n",
    "        # Removes used tile and adds new tile from deck to hand\n",
    "        self.player_tiles[self.current_player-1].remove(action)\n",
    "        if len(self.remaining_tiles) > 0:\n",
    "            self.player_tiles[self.current_player-1].append(self.remaining_tiles.pop())\n",
    "        else:\n",
    "            self.player_tiles[self.current_player-1].append(-1)\n",
    "        \n",
    "        # Rotates tile (Only used by AI)\n",
    "        self.tiles[action].rotate_tile(rotate)\n",
    "        \n",
    "        reward = 0\n",
    "        self.place_tile(action+1)\n",
    "        self.move_players()\n",
    "        reward = self.reward_function()\n",
    "        if self.game_is_over():\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "        self.current_player = self.next_player()\n",
    "        observation = np.hstack((self.player_tiles[self.current_player - 1], self.player_board.flatten(), self.get_tile_obs()))\n",
    "        return observation, reward, done, {}\n",
    "    \n",
    "    # Decides if the game is over\n",
    "    def game_is_over(self):\n",
    "        if len(self.remaining_players) <= 1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Decides the reward (Only used for AI)\n",
    "    def reward_function(self):\n",
    "        if not self.game_is_over():\n",
    "            return 1\n",
    "        if self.game_is_over() and self.remaining_players[0] == self.current_player:\n",
    "            return 5\n",
    "        return -1\n",
    "    \n",
    "    # Places tile in self.tile_board\n",
    "    def place_tile(self, tile):\n",
    "        tile_number, node_number = np.where(self.player_board == self.current_player)\n",
    "        x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "        self.tile_board[x][y] += tile\n",
    "        self.rotation_board[x][y] += self.tiles[tile-1].get_rotation()\n",
    "    \n",
    "    # Moves player piece in self.player_board\n",
    "    def move_players(self):\n",
    "        for player in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == player)\n",
    "            x, y = TsuroEnv.euclidean_division(self, tile_number)\n",
    "            x = x[0]\n",
    "            y = y[0]\n",
    "            while self.tile_board[x][y] != 0:\n",
    "                tile = self.tiles[(self.tile_board[x][y])-1]\n",
    "                next_node, next_player_tile, next_x, next_y = tile.move(node_number)\n",
    "                self.player_board[tile_number[0]][node_number[0]] = 0\n",
    "                if ((tile_number[0] % 6 == 0) and ((tile_number[0] + next_player_tile) % 6 == 5)) or (tile_number[0] + next_player_tile < 0) or (tile_number[0] + next_player_tile > 35) or ((tile_number[0] % 6 == 5) and ((tile_number[0] + next_player_tile) % 6 == 0)):\n",
    "                    self.remaining_players.remove(player)\n",
    "                    break\n",
    "                else:\n",
    "                    self.player_board[tile_number[0] + next_player_tile][next_node] = player\n",
    "                    x += next_x\n",
    "                    y += next_y\n",
    "                    tile_number, node_number = np.where(self.player_board == player)\n",
    "    \n",
    "    # Quotient and Remainder\n",
    "    def euclidean_division(self, x, y = 6):\n",
    "        return x % y, x // y\n",
    "    \n",
    "    # Action (card, rotation) from input\n",
    "    def get_card(self, x, y = 4):\n",
    "        return  x // y, x % y\n",
    "\n",
    "    # Decide whos turn it is\n",
    "    def next_player(self):\n",
    "        if len(self.remaining_players) == 0:\n",
    "            return -1\n",
    "        if self.current_player not in self.remaining_players:\n",
    "            for player in self.remaining_players:\n",
    "                if player > self.current_player:\n",
    "                    return player\n",
    "                else:\n",
    "                    return self.remaining_players[0]\n",
    "        return self.remaining_players[(self.remaining_players.index(self.current_player) + 1) % len(self.remaining_players)]\n",
    "        \n",
    "    # Render the environment\n",
    "    def render(self, mode):\n",
    "        screen = pygame.display.set_mode((650, 750))\n",
    "        screen.fill((255, 255, 255))\n",
    "\n",
    "        # Draw the game board\n",
    "        board = pygame.image.load(\"TsuroImages/board.png\")\n",
    "        board = pygame.transform.scale(board, (600, 600))\n",
    "        screen.blit(board, (25,25))\n",
    "        \n",
    "        # Draw current players hand\n",
    "        for i in range (len(self.player_tiles[self.current_player-1])):\n",
    "            tile = self.player_tiles[self.current_player-1][i]\n",
    "            screen.blit(self.tiles[tile].image, (75 + (i * 200), 635))\n",
    "        \n",
    "        # Draw the tiles on the board\n",
    "        for x in range(self.tile_board_size[0]):\n",
    "            for y in range(self.tile_board_size[1]):\n",
    "                val = self.tile_board[x][y]\n",
    "                if val != 0:\n",
    "                    tile = self.tiles[val-1]\n",
    "                    screen.blit(tile.image, (25 + x * 100, 25 + y * 100))\n",
    "                    \n",
    "        # Draw the players' pieces on the board\n",
    "        for i in self.remaining_players:\n",
    "            tile_number, node_number = np.where(self.player_board == i)\n",
    "            y_add = 0\n",
    "            x_add = 0\n",
    "            y_mult = 0\n",
    "            x_mult = 0\n",
    "            \n",
    "            match node_number[0]:\n",
    "                case 0:\n",
    "                    y_add = 35\n",
    "                case 1:\n",
    "                    x_add = 35\n",
    "                case 2:\n",
    "                    x_add = 70\n",
    "                case 3:\n",
    "                    x_add = 100\n",
    "                    y_add = 35\n",
    "                case 4:\n",
    "                    x_add = 100\n",
    "                    y_add = 70\n",
    "                case 5:\n",
    "                    x_add = 70\n",
    "                    y_add = 100\n",
    "                case 6:\n",
    "                    x_add = 35\n",
    "                    y_add = 100\n",
    "                case 7:\n",
    "                     y_add = 70\n",
    "                case _:\n",
    "                    raise Exception(\"Issue in drawing the player board\")\n",
    "                    \n",
    "            if tile_number[0] != 0:\n",
    "                x_mult, y_mult = TsuroEnv.euclidean_division(self, tile_number[0])\n",
    "            \n",
    "            pygame.draw.circle(screen, colors[i-1], (25 + x_add + (100 * x_mult), 25 + y_add + (100 * y_mult)), 5)\n",
    "            \n",
    "        # Draw text to show who won when game is over\n",
    "        if self.game_is_over() or self.current_player == -1:\n",
    "            font = pygame.font.Font('freesansbold.ttf', 32)\n",
    "            text = font.render('Player ' + str(self.current_player) + ' wins', True, '#00FF00')\n",
    "            textRect = text.get_rect()\n",
    "            textRect.center = (650 // 2, 750 // 2)\n",
    "            screen.blit(text, textRect)\n",
    "            \n",
    "        pygame.display.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8016cc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x23d96d80e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations = 2000000\n",
    "\n",
    "initial_collect_steps = 10000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "log_interval = 200\n",
    "\n",
    "num_eval_episodes = 500\n",
    "eval_interval = 5000\n",
    "\n",
    "register(\n",
    "    id='TsuroEnv',\n",
    "    entry_point=TsuroEnv,\n",
    ")\n",
    "\n",
    "env_name = \"TsuroEnv\"\n",
    "\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "fc_layer_params = (200, 100)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(num_units, activation=tf.keras.activations.relu, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(num_actions, activation=None, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03), bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step)\n",
    "    # commenting out render will make training quicker\n",
    "    # environment.render(\"human\")\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "        \n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=\"CheckpointsRC/\",\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step_counter\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34f97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "returns = []\n",
    "\n",
    "with open(\"CheckpointsRC/returns.txt\", \"r\") as txt:\n",
    "    for line in txt:\n",
    "        returns.append(line)\n",
    "        \n",
    "for i in range(len(returns)):\n",
    "    returns[i] = returns[i].strip()\n",
    "    returns[i] = float(returns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a7d75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\\nagent.train_step_counter.assign(0)\\n\\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\\nreturns = [avg_return]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Collection for new agent\n",
    "'''\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6085e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HorridJoe\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3030200: loss = 4.865280628204346\n",
      "step = 3030400: loss = 3.527041435241699\n",
      "step = 3030600: loss = 3.622375011444092\n",
      "step = 3030800: loss = 3.211477518081665\n",
      "step = 3031000: loss = 4.8109869956970215\n",
      "step = 3031200: loss = 4.624529838562012\n",
      "step = 3031400: loss = 4.654329776763916\n",
      "step = 3031600: loss = 3.8694474697113037\n",
      "step = 3031800: loss = 3.0411627292633057\n",
      "step = 3032000: loss = 5.145514965057373\n",
      "step = 3032200: loss = 3.139361619949341\n",
      "step = 3032400: loss = 3.776602029800415\n",
      "step = 3032600: loss = 3.423295497894287\n",
      "step = 3032800: loss = 3.951993703842163\n",
      "step = 3033000: loss = 5.076113224029541\n",
      "step = 3033200: loss = 4.3107171058654785\n",
      "step = 3033400: loss = 3.7145097255706787\n",
      "step = 3033600: loss = 2.6638073921203613\n",
      "step = 3033800: loss = 3.777981758117676\n",
      "step = 3034000: loss = 5.235714435577393\n",
      "step = 3034200: loss = 4.748231887817383\n",
      "step = 3034400: loss = 3.5560457706451416\n",
      "step = 3034600: loss = 3.447207450866699\n",
      "step = 3034800: loss = 3.7002742290496826\n",
      "step = 3035000: loss = 4.072951793670654\n",
      "step = 3035000: Average Return = 3.818000078201294\n",
      "step = 3035200: loss = 4.022726535797119\n",
      "step = 3035400: loss = 3.284764289855957\n",
      "step = 3035600: loss = 3.36964750289917\n",
      "step = 3035800: loss = 3.841508626937866\n",
      "step = 3036000: loss = 4.022686004638672\n",
      "step = 3036200: loss = 4.496884822845459\n",
      "step = 3036400: loss = 3.5961925983428955\n",
      "step = 3036600: loss = 3.8766448497772217\n",
      "step = 3036800: loss = 4.710397720336914\n",
      "step = 3037000: loss = 4.5365095138549805\n",
      "step = 3037200: loss = 4.316843032836914\n",
      "step = 3037400: loss = 4.9692511558532715\n",
      "step = 3037600: loss = 3.7756292819976807\n",
      "step = 3037800: loss = 3.2935521602630615\n",
      "step = 3038000: loss = 4.77503776550293\n",
      "step = 3038200: loss = 4.098999977111816\n",
      "step = 3038400: loss = 4.445324420928955\n",
      "step = 3038600: loss = 3.9068596363067627\n",
      "step = 3038800: loss = 6.35534143447876\n",
      "step = 3039000: loss = 3.9886770248413086\n",
      "step = 3039200: loss = 4.78206729888916\n",
      "step = 3039400: loss = 4.3836517333984375\n",
      "step = 3039600: loss = 4.993866920471191\n",
      "step = 3039800: loss = 2.941107988357544\n",
      "step = 3040000: loss = 3.6958961486816406\n",
      "step = 3040000: Average Return = 4.328000068664551\n",
      "step = 3040200: loss = 2.3764026165008545\n",
      "step = 3040400: loss = 3.362039089202881\n",
      "step = 3040600: loss = 3.7430241107940674\n",
      "step = 3040800: loss = 4.80045747756958\n",
      "step = 3041000: loss = 4.133530139923096\n",
      "step = 3041200: loss = 3.7819740772247314\n",
      "step = 3041400: loss = 3.795600414276123\n",
      "step = 3041600: loss = 3.915916681289673\n",
      "step = 3041800: loss = 3.6112589836120605\n",
      "step = 3042000: loss = 4.944669246673584\n",
      "step = 3042200: loss = 4.20491361618042\n",
      "step = 3042400: loss = 5.436387062072754\n",
      "step = 3042600: loss = 4.6680755615234375\n",
      "step = 3042800: loss = 3.9363853931427\n",
      "step = 3043000: loss = 5.163896560668945\n",
      "step = 3043200: loss = 2.334665060043335\n",
      "step = 3043400: loss = 4.216428279876709\n",
      "step = 3043600: loss = 4.446414470672607\n",
      "step = 3043800: loss = 2.9923763275146484\n",
      "step = 3044000: loss = 5.667749881744385\n",
      "step = 3044200: loss = 5.177482604980469\n",
      "step = 3044400: loss = 4.4990644454956055\n",
      "step = 3044600: loss = 3.4814727306365967\n",
      "step = 3044800: loss = 4.025268077850342\n",
      "step = 3045000: loss = 3.868898630142212\n",
      "step = 3045000: Average Return = 3.2279999256134033\n",
      "step = 3045200: loss = 4.961668014526367\n",
      "step = 3045400: loss = 3.563283920288086\n",
      "step = 3045600: loss = 4.743806838989258\n",
      "step = 3045800: loss = 4.145939826965332\n",
      "step = 3046000: loss = 4.349621295928955\n",
      "step = 3046200: loss = 7.327866077423096\n",
      "step = 3046400: loss = 3.7898075580596924\n",
      "step = 3046600: loss = 5.417667865753174\n",
      "step = 3046800: loss = 3.619843006134033\n",
      "step = 3047000: loss = 3.909644365310669\n",
      "step = 3047200: loss = 3.5238702297210693\n",
      "step = 3047400: loss = 4.289183139801025\n",
      "step = 3047600: loss = 4.182185173034668\n",
      "step = 3047800: loss = 3.5497283935546875\n",
      "step = 3048000: loss = 3.848330497741699\n",
      "step = 3048200: loss = 5.255791664123535\n",
      "step = 3048400: loss = 3.5801711082458496\n",
      "step = 3048600: loss = 4.766963958740234\n",
      "step = 3048800: loss = 6.24694299697876\n",
      "step = 3049000: loss = 4.239745616912842\n",
      "step = 3049200: loss = 3.837254762649536\n",
      "step = 3049400: loss = 4.093803405761719\n",
      "step = 3049600: loss = 4.494025707244873\n",
      "step = 3049800: loss = 4.98962926864624\n",
      "step = 3050000: loss = 2.943230628967285\n",
      "step = 3050000: Average Return = 3.808000087738037\n",
      "step = 3050200: loss = 4.982424736022949\n",
      "step = 3050400: loss = 3.663675546646118\n",
      "step = 3050600: loss = 3.3118951320648193\n",
      "step = 3050800: loss = 3.505457878112793\n",
      "step = 3051000: loss = 3.485996723175049\n",
      "step = 3051200: loss = 3.0454115867614746\n",
      "step = 3051400: loss = 4.056670188903809\n",
      "step = 3051600: loss = 4.148749828338623\n",
      "step = 3051800: loss = 4.548408031463623\n",
      "step = 3052000: loss = 4.703989505767822\n",
      "step = 3052200: loss = 4.148494720458984\n",
      "step = 3052400: loss = 4.139613628387451\n",
      "step = 3052600: loss = 3.458277702331543\n",
      "step = 3052800: loss = 4.671885967254639\n",
      "step = 3053000: loss = 5.378228664398193\n",
      "step = 3053200: loss = 3.6881256103515625\n",
      "step = 3053400: loss = 3.748016834259033\n",
      "step = 3053600: loss = 4.127008438110352\n",
      "step = 3053800: loss = 5.0004191398620605\n",
      "step = 3054000: loss = 2.906644821166992\n",
      "step = 3054200: loss = 5.099989414215088\n",
      "step = 3054400: loss = 3.506483793258667\n",
      "step = 3054600: loss = 4.562552452087402\n",
      "step = 3054800: loss = 3.9357218742370605\n",
      "step = 3055000: loss = 4.588212013244629\n",
      "step = 3055000: Average Return = 3.869999885559082\n",
      "step = 3055200: loss = 5.124379634857178\n",
      "step = 3055400: loss = 5.905729293823242\n",
      "step = 3055600: loss = 4.524613380432129\n",
      "step = 3055800: loss = 3.971903085708618\n",
      "step = 3056000: loss = 5.484892845153809\n",
      "step = 3056200: loss = 3.3033668994903564\n",
      "step = 3056400: loss = 5.4430952072143555\n",
      "step = 3056600: loss = 4.118038654327393\n",
      "step = 3056800: loss = 4.896650791168213\n",
      "step = 3057000: loss = 3.3516430854797363\n",
      "step = 3057200: loss = 4.725574970245361\n",
      "step = 3057400: loss = 3.6927666664123535\n",
      "step = 3057600: loss = 4.395843029022217\n",
      "step = 3057800: loss = 3.2752363681793213\n",
      "step = 3058000: loss = 3.565218925476074\n",
      "step = 3058200: loss = 3.1799159049987793\n",
      "step = 3058400: loss = 3.7897186279296875\n",
      "step = 3058600: loss = 3.787571430206299\n",
      "step = 3058800: loss = 5.876946926116943\n",
      "step = 3059000: loss = 4.017004489898682\n",
      "step = 3059200: loss = 4.273021697998047\n",
      "step = 3059400: loss = 3.025106191635132\n",
      "step = 3059600: loss = 3.314570903778076\n",
      "step = 3059800: loss = 3.0734875202178955\n",
      "step = 3060000: loss = 4.781019687652588\n",
      "step = 3060000: Average Return = 4.004000186920166\n",
      "step = 3060200: loss = 3.856602191925049\n",
      "step = 3060400: loss = 5.067267417907715\n",
      "step = 3060600: loss = 2.926936626434326\n",
      "step = 3060800: loss = 4.184024333953857\n",
      "step = 3061000: loss = 4.263064861297607\n",
      "step = 3061200: loss = 4.712471961975098\n",
      "step = 3061400: loss = 3.1600468158721924\n",
      "step = 3061600: loss = 6.204374313354492\n",
      "step = 3061800: loss = 3.038520336151123\n",
      "step = 3062000: loss = 1.963746190071106\n",
      "step = 3062200: loss = 4.172637462615967\n",
      "step = 3062400: loss = 4.656828880310059\n",
      "step = 3062600: loss = 3.0370635986328125\n",
      "step = 3062800: loss = 3.196287155151367\n",
      "step = 3063000: loss = 4.884184837341309\n",
      "step = 3063200: loss = 4.55316686630249\n",
      "step = 3063400: loss = 3.2771241664886475\n",
      "step = 3063600: loss = 4.579850673675537\n",
      "step = 3063800: loss = 4.846907615661621\n",
      "step = 3064000: loss = 5.860856056213379\n",
      "step = 3064200: loss = 5.452613353729248\n",
      "step = 3064400: loss = 4.164982318878174\n",
      "step = 3064600: loss = 4.167843341827393\n",
      "step = 3064800: loss = 3.2249038219451904\n",
      "step = 3065000: loss = 3.3825128078460693\n",
      "step = 3065000: Average Return = 4.228000164031982\n",
      "step = 3065200: loss = 5.003121852874756\n",
      "step = 3065400: loss = 4.8643670082092285\n",
      "step = 3065600: loss = 4.033550262451172\n",
      "step = 3065800: loss = 4.014604568481445\n",
      "step = 3066000: loss = 3.9909379482269287\n",
      "step = 3066200: loss = 4.771796226501465\n",
      "step = 3066400: loss = 4.461881160736084\n",
      "step = 3066600: loss = 3.0046889781951904\n",
      "step = 3066800: loss = 4.8903489112854\n",
      "step = 3067000: loss = 5.0586838722229\n",
      "step = 3067200: loss = 4.012892246246338\n",
      "step = 3067400: loss = 5.098285675048828\n",
      "step = 3067600: loss = 4.944875240325928\n",
      "step = 3067800: loss = 3.9700214862823486\n",
      "step = 3068000: loss = 4.816608905792236\n",
      "step = 3068200: loss = 4.770422458648682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3068400: loss = 5.106968402862549\n",
      "step = 3068600: loss = 4.785187721252441\n",
      "step = 3068800: loss = 4.169136047363281\n",
      "step = 3069000: loss = 3.5141441822052\n",
      "step = 3069200: loss = 4.465590953826904\n",
      "step = 3069400: loss = 4.515452861785889\n",
      "step = 3069600: loss = 4.071613788604736\n",
      "step = 3069800: loss = 4.938687801361084\n",
      "step = 3070000: loss = 4.098786354064941\n",
      "step = 3070000: Average Return = 3.609999895095825\n",
      "step = 3070200: loss = 5.8033905029296875\n",
      "step = 3070400: loss = 4.014651298522949\n",
      "step = 3070600: loss = 3.6342649459838867\n",
      "step = 3070800: loss = 4.779423236846924\n",
      "step = 3071000: loss = 3.2650418281555176\n",
      "step = 3071200: loss = 5.0377421379089355\n",
      "step = 3071400: loss = 4.021762371063232\n",
      "step = 3071600: loss = 3.9562017917633057\n",
      "step = 3071800: loss = 3.762035846710205\n",
      "step = 3072000: loss = 3.7324774265289307\n",
      "step = 3072200: loss = 4.017127513885498\n",
      "step = 3072400: loss = 3.448364734649658\n",
      "step = 3072600: loss = 4.280355930328369\n",
      "step = 3072800: loss = 4.342242240905762\n",
      "step = 3073000: loss = 5.087114334106445\n",
      "step = 3073200: loss = 5.12839937210083\n",
      "step = 3073400: loss = 4.630489826202393\n",
      "step = 3073600: loss = 3.3175294399261475\n",
      "step = 3073800: loss = 3.6072986125946045\n",
      "step = 3074000: loss = 4.500356197357178\n",
      "step = 3074200: loss = 4.629814147949219\n",
      "step = 3074400: loss = 4.616546630859375\n",
      "step = 3074600: loss = 4.063778400421143\n",
      "step = 3074800: loss = 3.9816198348999023\n",
      "step = 3075000: loss = 3.6261274814605713\n",
      "step = 3075000: Average Return = 3.630000114440918\n",
      "step = 3075200: loss = 3.8624627590179443\n",
      "step = 3075400: loss = 4.08087682723999\n",
      "step = 3075600: loss = 4.806788444519043\n",
      "step = 3075800: loss = 5.0298027992248535\n",
      "step = 3076000: loss = 4.415191650390625\n",
      "step = 3076200: loss = 5.107069969177246\n",
      "step = 3076400: loss = 4.386370658874512\n",
      "step = 3076600: loss = 5.408775329589844\n",
      "step = 3076800: loss = 4.452360153198242\n",
      "step = 3077000: loss = 4.48490571975708\n",
      "step = 3077200: loss = 4.2566633224487305\n",
      "step = 3077400: loss = 4.4867401123046875\n",
      "step = 3077600: loss = 7.14710807800293\n",
      "step = 3077800: loss = 4.6666693687438965\n",
      "step = 3078000: loss = 4.883843421936035\n",
      "step = 3078200: loss = 4.660314083099365\n",
      "step = 3078400: loss = 5.97905158996582\n",
      "step = 3078600: loss = 5.816965103149414\n",
      "step = 3078800: loss = 2.9743852615356445\n",
      "step = 3079000: loss = 4.178567409515381\n",
      "step = 3079200: loss = 5.285961627960205\n",
      "step = 3079400: loss = 2.9940056800842285\n",
      "step = 3079600: loss = 4.81502103805542\n",
      "step = 3079800: loss = 3.572000026702881\n",
      "step = 3080000: loss = 5.932227611541748\n",
      "step = 3080000: Average Return = 3.9800000190734863\n",
      "step = 3080200: loss = 4.071186542510986\n",
      "step = 3080400: loss = 3.7947230339050293\n",
      "step = 3080600: loss = 3.898961305618286\n",
      "step = 3080800: loss = 4.7569780349731445\n",
      "step = 3081000: loss = 4.614785194396973\n",
      "step = 3081200: loss = 4.810902118682861\n",
      "step = 3081400: loss = 4.5290632247924805\n",
      "step = 3081600: loss = 4.423330783843994\n",
      "step = 3081800: loss = 4.8515305519104\n",
      "step = 3082000: loss = 4.278767108917236\n",
      "step = 3082200: loss = 3.7638440132141113\n",
      "step = 3082400: loss = 5.603691101074219\n",
      "step = 3082600: loss = 3.8661603927612305\n",
      "step = 3082800: loss = 4.575450420379639\n",
      "step = 3083000: loss = 4.350188732147217\n",
      "step = 3083200: loss = 4.466916561126709\n",
      "step = 3083400: loss = 3.260526418685913\n",
      "step = 3083600: loss = 4.492607116699219\n",
      "step = 3083800: loss = 3.871241569519043\n",
      "step = 3084000: loss = 5.121909141540527\n",
      "step = 3084200: loss = 3.659397840499878\n",
      "step = 3084400: loss = 4.707862854003906\n",
      "step = 3084600: loss = 4.096071243286133\n",
      "step = 3084800: loss = 3.171337127685547\n",
      "step = 3085000: loss = 4.765482425689697\n",
      "step = 3085000: Average Return = 3.7760000228881836\n",
      "step = 3085200: loss = 5.851663112640381\n",
      "step = 3085400: loss = 4.978248596191406\n",
      "step = 3085600: loss = 3.1210217475891113\n",
      "step = 3085800: loss = 5.180397987365723\n",
      "step = 3086000: loss = 3.7091686725616455\n",
      "step = 3086200: loss = 4.239372253417969\n",
      "step = 3086400: loss = 2.800657033920288\n",
      "step = 3086600: loss = 5.491137981414795\n",
      "step = 3086800: loss = 3.111356735229492\n",
      "step = 3087000: loss = 3.69876766204834\n",
      "step = 3087200: loss = 6.51157808303833\n",
      "step = 3087400: loss = 4.516024589538574\n",
      "step = 3087600: loss = 3.858804225921631\n",
      "step = 3087800: loss = 3.818824291229248\n",
      "step = 3088000: loss = 4.4948601722717285\n",
      "step = 3088200: loss = 4.021199703216553\n",
      "step = 3088400: loss = 3.3222131729125977\n",
      "step = 3088600: loss = 3.64182710647583\n",
      "step = 3088800: loss = 4.398515701293945\n",
      "step = 3089000: loss = 5.125370979309082\n",
      "step = 3089200: loss = 3.2356317043304443\n",
      "step = 3089400: loss = 4.896349906921387\n",
      "step = 3089600: loss = 4.055505752563477\n",
      "step = 3089800: loss = 4.902259826660156\n",
      "step = 3090000: loss = 3.6182217597961426\n",
      "step = 3090000: Average Return = 3.740000009536743\n",
      "step = 3090200: loss = 5.936447620391846\n",
      "step = 3090400: loss = 4.490594387054443\n",
      "step = 3090600: loss = 5.220612525939941\n",
      "step = 3090800: loss = 4.013611316680908\n",
      "step = 3091000: loss = 1.45171320438385\n",
      "step = 3091200: loss = 3.7647464275360107\n",
      "step = 3091400: loss = 4.471484184265137\n",
      "step = 3091600: loss = 3.7835965156555176\n",
      "step = 3091800: loss = 4.3067474365234375\n",
      "step = 3092000: loss = 4.58198356628418\n",
      "step = 3092200: loss = 4.622768878936768\n",
      "step = 3092400: loss = 5.074044227600098\n",
      "step = 3092600: loss = 3.3870787620544434\n",
      "step = 3092800: loss = 4.567684173583984\n",
      "step = 3093000: loss = 4.625144004821777\n",
      "step = 3093200: loss = 5.241656303405762\n",
      "step = 3093400: loss = 3.715289831161499\n",
      "step = 3093600: loss = 3.238640785217285\n",
      "step = 3093800: loss = 5.452015399932861\n",
      "step = 3094000: loss = 6.013261795043945\n",
      "step = 3094200: loss = 4.527799606323242\n",
      "step = 3094400: loss = 3.4447174072265625\n",
      "step = 3094600: loss = 5.349762439727783\n",
      "step = 3094800: loss = 5.775400638580322\n",
      "step = 3095000: loss = 4.537276268005371\n",
      "step = 3095000: Average Return = 4.176000118255615\n",
      "step = 3095200: loss = 3.7371439933776855\n",
      "step = 3095400: loss = 6.080651760101318\n",
      "step = 3095600: loss = 3.135424852371216\n",
      "step = 3095800: loss = 5.306693077087402\n",
      "step = 3096000: loss = 3.750739574432373\n",
      "step = 3096200: loss = 3.7911760807037354\n",
      "step = 3096400: loss = 4.5644755363464355\n",
      "step = 3096600: loss = 3.9241943359375\n",
      "step = 3096800: loss = 4.546893119812012\n",
      "step = 3097000: loss = 2.847721815109253\n",
      "step = 3097200: loss = 4.071723461151123\n",
      "step = 3097400: loss = 5.45078706741333\n",
      "step = 3097600: loss = 4.486050128936768\n",
      "step = 3097800: loss = 3.3945939540863037\n",
      "step = 3098000: loss = 3.1213362216949463\n",
      "step = 3098200: loss = 4.32463264465332\n",
      "step = 3098400: loss = 4.9539031982421875\n",
      "step = 3098600: loss = 3.8511149883270264\n",
      "step = 3098800: loss = 2.8421647548675537\n",
      "step = 3099000: loss = 4.6275482177734375\n",
      "step = 3099200: loss = 3.7087364196777344\n",
      "step = 3099400: loss = 3.690800189971924\n",
      "step = 3099600: loss = 4.59682035446167\n",
      "step = 3099800: loss = 5.576370716094971\n",
      "step = 3100000: loss = 3.46488356590271\n",
      "step = 3100000: Average Return = 4.177999973297119\n",
      "step = 3100200: loss = 3.61555814743042\n",
      "step = 3100400: loss = 4.4294114112854\n",
      "step = 3100600: loss = 4.275472164154053\n",
      "step = 3100800: loss = 4.377291202545166\n",
      "step = 3101000: loss = 4.636536121368408\n",
      "step = 3101200: loss = 3.786885976791382\n",
      "step = 3101400: loss = 4.700343132019043\n",
      "step = 3101600: loss = 4.954001426696777\n",
      "step = 3101800: loss = 3.6651265621185303\n",
      "step = 3102000: loss = 3.836965560913086\n",
      "step = 3102200: loss = 4.774565696716309\n",
      "step = 3102400: loss = 3.7110557556152344\n",
      "step = 3102600: loss = 4.122636318206787\n",
      "step = 3102800: loss = 3.735360860824585\n",
      "step = 3103000: loss = 4.312969207763672\n",
      "step = 3103200: loss = 5.153032302856445\n",
      "step = 3103400: loss = 4.902110576629639\n",
      "step = 3103600: loss = 3.7395718097686768\n",
      "step = 3103800: loss = 4.94854736328125\n",
      "step = 3104000: loss = 3.2310328483581543\n",
      "step = 3104200: loss = 4.795405864715576\n",
      "step = 3104400: loss = 4.731086730957031\n",
      "step = 3104600: loss = 2.7823245525360107\n",
      "step = 3104800: loss = 5.601308345794678\n",
      "step = 3105000: loss = 4.340064525604248\n",
      "step = 3105000: Average Return = 4.085999965667725\n",
      "step = 3105200: loss = 4.60339879989624\n",
      "step = 3105400: loss = 4.974430561065674\n",
      "step = 3105600: loss = 5.344229698181152\n",
      "step = 3105800: loss = 4.481565952301025\n",
      "step = 3106000: loss = 3.8449175357818604\n",
      "step = 3106200: loss = 4.193390846252441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3106400: loss = 4.460485935211182\n",
      "step = 3106600: loss = 4.508816242218018\n",
      "step = 3106800: loss = 4.70696496963501\n",
      "step = 3107000: loss = 4.530359268188477\n",
      "step = 3107200: loss = 3.4745359420776367\n",
      "step = 3107400: loss = 4.219790458679199\n",
      "step = 3107600: loss = 3.635805606842041\n",
      "step = 3107800: loss = 4.70621395111084\n",
      "step = 3108000: loss = 4.761289119720459\n",
      "step = 3108200: loss = 3.722634792327881\n",
      "step = 3108400: loss = 4.4541215896606445\n",
      "step = 3108600: loss = 4.7540154457092285\n",
      "step = 3108800: loss = 4.6353960037231445\n",
      "step = 3109000: loss = 4.157247066497803\n",
      "step = 3109200: loss = 4.781141757965088\n",
      "step = 3109400: loss = 3.6642539501190186\n",
      "step = 3109600: loss = 2.4099326133728027\n",
      "step = 3109800: loss = 3.73052978515625\n",
      "step = 3110000: loss = 3.653463363647461\n",
      "step = 3110000: Average Return = 3.4619998931884766\n",
      "step = 3110200: loss = 4.916297435760498\n",
      "step = 3110400: loss = 4.492186546325684\n",
      "step = 3110600: loss = 3.3280627727508545\n",
      "step = 3110800: loss = 4.355429649353027\n",
      "step = 3111000: loss = 4.450211524963379\n",
      "step = 3111200: loss = 4.090962886810303\n",
      "step = 3111400: loss = 4.599534034729004\n",
      "step = 3111600: loss = 5.162758827209473\n",
      "step = 3111800: loss = 3.0545763969421387\n",
      "step = 3112000: loss = 4.99420166015625\n",
      "step = 3112200: loss = 4.3948822021484375\n",
      "step = 3112400: loss = 2.739274501800537\n",
      "step = 3112600: loss = 4.841466426849365\n",
      "step = 3112800: loss = 5.628498554229736\n",
      "step = 3113000: loss = 3.0992937088012695\n",
      "step = 3113200: loss = 3.4846413135528564\n",
      "step = 3113400: loss = 3.6034631729125977\n",
      "step = 3113600: loss = 4.3976521492004395\n",
      "step = 3113800: loss = 4.051599502563477\n",
      "step = 3114000: loss = 4.64262056350708\n",
      "step = 3114200: loss = 3.7670037746429443\n",
      "step = 3114400: loss = 3.7747414112091064\n",
      "step = 3114600: loss = 3.572784900665283\n",
      "step = 3114800: loss = 4.269442081451416\n",
      "step = 3115000: loss = 4.749076843261719\n",
      "step = 3115000: Average Return = 4.052000045776367\n",
      "step = 3115200: loss = 4.416309833526611\n",
      "step = 3115400: loss = 3.9133105278015137\n",
      "step = 3115600: loss = 3.7862768173217773\n",
      "step = 3115800: loss = 4.581361293792725\n",
      "step = 3116000: loss = 4.7937140464782715\n",
      "step = 3116200: loss = 4.5426177978515625\n",
      "step = 3116400: loss = 3.5659947395324707\n",
      "step = 3116600: loss = 3.8456408977508545\n",
      "step = 3116800: loss = 3.5507750511169434\n",
      "step = 3117000: loss = 4.862829685211182\n",
      "step = 3117200: loss = 4.887623310089111\n",
      "step = 3117400: loss = 5.085628032684326\n",
      "step = 3117600: loss = 4.024195671081543\n",
      "step = 3117800: loss = 5.053350925445557\n",
      "step = 3118000: loss = 3.395901918411255\n",
      "step = 3118200: loss = 5.020346164703369\n",
      "step = 3118400: loss = 4.663339614868164\n",
      "step = 3118600: loss = 3.169548273086548\n",
      "step = 3118800: loss = 4.605418682098389\n",
      "step = 3119000: loss = 5.107717514038086\n",
      "step = 3119200: loss = 3.5492336750030518\n",
      "step = 3119400: loss = 3.9659698009490967\n",
      "step = 3119600: loss = 3.677964448928833\n",
      "step = 3119800: loss = 3.4375786781311035\n",
      "step = 3120000: loss = 3.506458044052124\n",
      "step = 3120000: Average Return = 3.563999891281128\n",
      "step = 3120200: loss = 4.981170654296875\n",
      "step = 3120400: loss = 5.240901470184326\n",
      "step = 3120600: loss = 3.655176877975464\n",
      "step = 3120800: loss = 3.5100343227386475\n",
      "step = 3121000: loss = 3.3008508682250977\n",
      "step = 3121200: loss = 3.0945851802825928\n",
      "step = 3121400: loss = 3.885063886642456\n",
      "step = 3121600: loss = 2.3985812664031982\n",
      "step = 3121800: loss = 4.2525787353515625\n",
      "step = 3122000: loss = 3.2949774265289307\n",
      "step = 3122200: loss = 4.441207408905029\n",
      "step = 3122400: loss = 4.465182781219482\n",
      "step = 3122600: loss = 5.0078301429748535\n",
      "step = 3122800: loss = 3.5588676929473877\n",
      "step = 3123000: loss = 4.1568803787231445\n",
      "step = 3123200: loss = 4.120522499084473\n",
      "step = 3123400: loss = 5.2835693359375\n",
      "step = 3123600: loss = 5.384539604187012\n",
      "step = 3123800: loss = 4.33582878112793\n",
      "step = 3124000: loss = 3.575385570526123\n",
      "step = 3124200: loss = 5.260788440704346\n",
      "step = 3124400: loss = 4.0122575759887695\n",
      "step = 3124600: loss = 4.392323970794678\n",
      "step = 3124800: loss = 5.059701919555664\n",
      "step = 3125000: loss = 5.170844554901123\n",
      "step = 3125000: Average Return = 3.684000015258789\n",
      "step = 3125200: loss = 3.3693253993988037\n",
      "step = 3125400: loss = 4.411741733551025\n",
      "step = 3125600: loss = 3.2282304763793945\n",
      "step = 3125800: loss = 6.12258768081665\n",
      "step = 3126000: loss = 3.96114182472229\n",
      "step = 3126200: loss = 4.698909282684326\n",
      "step = 3126400: loss = 3.329542398452759\n",
      "step = 3126600: loss = 3.730921506881714\n",
      "step = 3126800: loss = 4.824202537536621\n",
      "step = 3127000: loss = 4.026800632476807\n",
      "step = 3127200: loss = 3.9126176834106445\n",
      "step = 3127400: loss = 4.420670032501221\n",
      "step = 3127600: loss = 5.612174034118652\n",
      "step = 3127800: loss = 4.549631118774414\n",
      "step = 3128000: loss = 3.8391618728637695\n",
      "step = 3128200: loss = 2.8187978267669678\n",
      "step = 3128400: loss = 3.853087902069092\n",
      "step = 3128600: loss = 3.455456256866455\n",
      "step = 3128800: loss = 6.15679931640625\n",
      "step = 3129000: loss = 3.9751789569854736\n",
      "step = 3129200: loss = 3.7769415378570557\n",
      "step = 3129400: loss = 6.4726243019104\n",
      "step = 3129600: loss = 5.7907562255859375\n",
      "step = 3129800: loss = 4.753742694854736\n",
      "step = 3130000: loss = 3.759777069091797\n",
      "step = 3130000: Average Return = 3.690000057220459\n",
      "step = 3130200: loss = 4.153310298919678\n",
      "step = 3130400: loss = 4.895866394042969\n",
      "step = 3130600: loss = 5.385112762451172\n",
      "step = 3130800: loss = 3.5827724933624268\n",
      "step = 3131000: loss = 4.992390155792236\n",
      "step = 3131200: loss = 4.437400817871094\n",
      "step = 3131400: loss = 3.4968669414520264\n",
      "step = 3131600: loss = 3.709401845932007\n",
      "step = 3131800: loss = 3.57867431640625\n",
      "step = 3132000: loss = 4.221820831298828\n",
      "step = 3132200: loss = 4.5406622886657715\n",
      "step = 3132400: loss = 3.6309947967529297\n",
      "step = 3132600: loss = 3.898578405380249\n",
      "step = 3132800: loss = 4.034134387969971\n",
      "step = 3133000: loss = 4.899083614349365\n",
      "step = 3133200: loss = 3.196798324584961\n",
      "step = 3133400: loss = 4.261579990386963\n",
      "step = 3133600: loss = 4.322937488555908\n",
      "step = 3133800: loss = 3.870931386947632\n",
      "step = 3134000: loss = 2.86257266998291\n",
      "step = 3134200: loss = 6.655572414398193\n",
      "step = 3134400: loss = 4.930804252624512\n",
      "step = 3134600: loss = 5.34787654876709\n",
      "step = 3134800: loss = 4.770259380340576\n",
      "step = 3135000: loss = 4.49592924118042\n",
      "step = 3135000: Average Return = 4.322000026702881\n",
      "step = 3135200: loss = 4.1914896965026855\n",
      "step = 3135400: loss = 5.296792030334473\n",
      "step = 3135600: loss = 2.823854923248291\n",
      "step = 3135800: loss = 4.515253067016602\n",
      "step = 3136000: loss = 3.7315709590911865\n",
      "step = 3136200: loss = 5.336368083953857\n",
      "step = 3136400: loss = 2.686138153076172\n",
      "step = 3136600: loss = 3.5291240215301514\n",
      "step = 3136800: loss = 4.814738750457764\n",
      "step = 3137000: loss = 3.1305267810821533\n",
      "step = 3137200: loss = 4.295427322387695\n",
      "step = 3137400: loss = 4.921014308929443\n",
      "step = 3137600: loss = 4.049559116363525\n",
      "step = 3137800: loss = 4.893353462219238\n",
      "step = 3138000: loss = 3.761735200881958\n",
      "step = 3138200: loss = 4.735585689544678\n",
      "step = 3138400: loss = 5.030805587768555\n",
      "step = 3138600: loss = 3.9035897254943848\n",
      "step = 3138800: loss = 5.001833915710449\n",
      "step = 3139000: loss = 4.857428073883057\n",
      "step = 3139200: loss = 3.187830686569214\n",
      "step = 3139400: loss = 3.4931371212005615\n",
      "step = 3139600: loss = 4.590686798095703\n",
      "step = 3139800: loss = 3.7897167205810547\n",
      "step = 3140000: loss = 3.7441985607147217\n",
      "step = 3140000: Average Return = 3.6459999084472656\n",
      "step = 3140200: loss = 3.466918468475342\n",
      "step = 3140400: loss = 3.3118433952331543\n",
      "step = 3140600: loss = 3.2402992248535156\n",
      "step = 3140800: loss = 5.010030746459961\n",
      "step = 3141000: loss = 4.408989429473877\n",
      "step = 3141200: loss = 3.758673667907715\n",
      "step = 3141400: loss = 3.9800713062286377\n",
      "step = 3141600: loss = 3.7421321868896484\n",
      "step = 3141800: loss = 4.128662109375\n",
      "step = 3142000: loss = 3.2529451847076416\n",
      "step = 3142200: loss = 3.8984532356262207\n",
      "step = 3142400: loss = 3.3078300952911377\n",
      "step = 3142600: loss = 3.665212392807007\n",
      "step = 3142800: loss = 3.734574556350708\n",
      "step = 3143000: loss = 3.904719829559326\n",
      "step = 3143200: loss = 4.140098571777344\n",
      "step = 3143400: loss = 3.6093950271606445\n",
      "step = 3143600: loss = 4.948164939880371\n",
      "step = 3143800: loss = 3.319399833679199\n",
      "step = 3144000: loss = 3.755343437194824\n",
      "step = 3144200: loss = 2.9201552867889404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3144400: loss = 3.3327271938323975\n",
      "step = 3144600: loss = 3.690694808959961\n",
      "step = 3144800: loss = 5.464752197265625\n",
      "step = 3145000: loss = 5.609562873840332\n",
      "step = 3145000: Average Return = 3.8580000400543213\n",
      "step = 3145200: loss = 4.920924663543701\n",
      "step = 3145400: loss = 3.615734100341797\n",
      "step = 3145600: loss = 3.5358386039733887\n",
      "step = 3145800: loss = 4.510359287261963\n",
      "step = 3146000: loss = 4.389695644378662\n",
      "step = 3146200: loss = 4.018623352050781\n",
      "step = 3146400: loss = 4.293368339538574\n",
      "step = 3146600: loss = 3.6175503730773926\n",
      "step = 3146800: loss = 4.003779888153076\n",
      "step = 3147000: loss = 4.672178745269775\n",
      "step = 3147200: loss = 4.414737224578857\n",
      "step = 3147400: loss = 3.6832470893859863\n",
      "step = 3147600: loss = 4.4980974197387695\n",
      "step = 3147800: loss = 3.341495990753174\n",
      "step = 3148000: loss = 4.290015697479248\n",
      "step = 3148200: loss = 3.9724180698394775\n",
      "step = 3148400: loss = 4.241634845733643\n",
      "step = 3148600: loss = 4.065810680389404\n",
      "step = 3148800: loss = 3.9714877605438232\n",
      "step = 3149000: loss = 3.645947217941284\n",
      "step = 3149200: loss = 3.8939292430877686\n",
      "step = 3149400: loss = 4.323016166687012\n",
      "step = 3149600: loss = 3.0695183277130127\n",
      "step = 3149800: loss = 2.9884884357452393\n",
      "step = 3150000: loss = 4.354034900665283\n",
      "step = 3150000: Average Return = 3.874000072479248\n",
      "step = 3150200: loss = 4.694206237792969\n",
      "step = 3150400: loss = 2.956514835357666\n",
      "step = 3150600: loss = 3.992002487182617\n",
      "step = 3150800: loss = 4.144232749938965\n",
      "step = 3151000: loss = 4.367587089538574\n",
      "step = 3151200: loss = 5.441499710083008\n",
      "step = 3151400: loss = 5.581943988800049\n",
      "step = 3151600: loss = 4.496094703674316\n",
      "step = 3151800: loss = 3.4099154472351074\n",
      "step = 3152000: loss = 4.362546443939209\n",
      "step = 3152200: loss = 4.1510138511657715\n",
      "step = 3152400: loss = 4.184322834014893\n",
      "step = 3152600: loss = 4.3931474685668945\n",
      "step = 3152800: loss = 4.427914142608643\n",
      "step = 3153000: loss = 4.697939395904541\n",
      "step = 3153200: loss = 4.290328502655029\n",
      "step = 3153400: loss = 4.732084274291992\n",
      "step = 3153600: loss = 4.512353897094727\n",
      "step = 3153800: loss = 4.896012306213379\n",
      "step = 3154000: loss = 5.474600791931152\n",
      "step = 3154200: loss = 4.124477863311768\n",
      "step = 3154400: loss = 6.068408012390137\n",
      "step = 3154600: loss = 2.9683380126953125\n",
      "step = 3154800: loss = 3.3989174365997314\n",
      "step = 3155000: loss = 3.9595179557800293\n",
      "step = 3155000: Average Return = 3.690000057220459\n",
      "step = 3155200: loss = 3.2354989051818848\n",
      "step = 3155400: loss = 3.847801923751831\n",
      "step = 3155600: loss = 4.558050155639648\n",
      "step = 3155800: loss = 4.158905982971191\n",
      "step = 3156000: loss = 5.450900554656982\n",
      "step = 3156200: loss = 3.5102832317352295\n",
      "step = 3156400: loss = 4.633302688598633\n",
      "step = 3156600: loss = 3.6071014404296875\n",
      "step = 3156800: loss = 3.8821165561676025\n",
      "step = 3157000: loss = 3.943241596221924\n",
      "step = 3157200: loss = 4.909959316253662\n",
      "step = 3157400: loss = 3.0040833950042725\n",
      "step = 3157600: loss = 3.7585673332214355\n",
      "step = 3157800: loss = 6.546926021575928\n",
      "step = 3158000: loss = 4.8916335105896\n",
      "step = 3158200: loss = 4.853094577789307\n",
      "step = 3158400: loss = 6.317771911621094\n",
      "step = 3158600: loss = 4.5331878662109375\n",
      "step = 3158800: loss = 4.64708137512207\n",
      "step = 3159000: loss = 2.1434662342071533\n",
      "step = 3159200: loss = 4.50627326965332\n",
      "step = 3159400: loss = 4.69705867767334\n",
      "step = 3159600: loss = 3.113969564437866\n",
      "step = 3159800: loss = 5.226647853851318\n",
      "step = 3160000: loss = 3.4163427352905273\n",
      "step = 3160000: Average Return = 3.7239999771118164\n",
      "step = 3160200: loss = 4.517690181732178\n",
      "step = 3160400: loss = 3.601719856262207\n",
      "step = 3160600: loss = 4.223213195800781\n",
      "step = 3160800: loss = 4.68885612487793\n",
      "step = 3161000: loss = 4.086772441864014\n",
      "step = 3161200: loss = 4.239160537719727\n",
      "step = 3161400: loss = 3.753521680831909\n",
      "step = 3161600: loss = 5.98612117767334\n",
      "step = 3161800: loss = 4.066653728485107\n",
      "step = 3162000: loss = 6.121589660644531\n",
      "step = 3162200: loss = 5.337270259857178\n",
      "step = 3162400: loss = 4.041802406311035\n",
      "step = 3162600: loss = 4.933755874633789\n",
      "step = 3162800: loss = 5.137460708618164\n",
      "step = 3163000: loss = 5.620359897613525\n",
      "step = 3163200: loss = 4.264797687530518\n",
      "step = 3163400: loss = 3.637338876724243\n",
      "step = 3163600: loss = 3.804553985595703\n",
      "step = 3163800: loss = 4.591570854187012\n",
      "step = 3164000: loss = 5.000734329223633\n",
      "step = 3164200: loss = 5.692899227142334\n",
      "step = 3164400: loss = 4.528347015380859\n",
      "step = 3164600: loss = 5.1297526359558105\n",
      "step = 3164800: loss = 4.271329402923584\n",
      "step = 3165000: loss = 4.728536128997803\n",
      "step = 3165000: Average Return = 3.555999994277954\n",
      "step = 3165200: loss = 2.444058895111084\n",
      "step = 3165400: loss = 3.116743803024292\n",
      "step = 3165600: loss = 3.3074235916137695\n",
      "step = 3165800: loss = 2.7026684284210205\n",
      "step = 3166000: loss = 4.392993450164795\n",
      "step = 3166200: loss = 3.04555344581604\n",
      "step = 3166400: loss = 3.271620512008667\n",
      "step = 3166600: loss = 3.239398717880249\n",
      "step = 3166800: loss = 2.901794672012329\n",
      "step = 3167000: loss = 4.454281330108643\n",
      "step = 3167200: loss = 3.314114809036255\n",
      "step = 3167400: loss = 4.53526496887207\n",
      "step = 3167600: loss = 5.188151836395264\n",
      "step = 3167800: loss = 3.9034416675567627\n",
      "step = 3168000: loss = 4.01655387878418\n",
      "step = 3168200: loss = 6.04707145690918\n",
      "step = 3168400: loss = 3.19661545753479\n",
      "step = 3168600: loss = 4.890801429748535\n",
      "step = 3168800: loss = 3.5384981632232666\n",
      "step = 3169000: loss = 3.305316925048828\n",
      "step = 3169200: loss = 5.283920764923096\n",
      "step = 3169400: loss = 4.13188362121582\n",
      "step = 3169600: loss = 4.353561878204346\n",
      "step = 3169800: loss = 3.6278069019317627\n",
      "step = 3170000: loss = 3.734416961669922\n",
      "step = 3170000: Average Return = 3.490000009536743\n",
      "step = 3170200: loss = 4.758984088897705\n",
      "step = 3170400: loss = 3.3038711547851562\n",
      "step = 3170600: loss = 4.671374797821045\n",
      "step = 3170800: loss = 4.464273452758789\n",
      "step = 3171000: loss = 4.434274196624756\n",
      "step = 3171200: loss = 2.8655214309692383\n",
      "step = 3171400: loss = 3.819166898727417\n",
      "step = 3171600: loss = 5.24129056930542\n",
      "step = 3171800: loss = 3.5133883953094482\n",
      "step = 3172000: loss = 4.297411918640137\n",
      "step = 3172200: loss = 4.548638343811035\n",
      "step = 3172400: loss = 4.865453720092773\n",
      "step = 3172600: loss = 4.18272590637207\n",
      "step = 3172800: loss = 2.5876641273498535\n",
      "step = 3173000: loss = 4.170098304748535\n",
      "step = 3173200: loss = 5.402156829833984\n",
      "step = 3173400: loss = 4.730695724487305\n",
      "step = 3173600: loss = 6.238929271697998\n",
      "step = 3173800: loss = 4.286267280578613\n",
      "step = 3174000: loss = 4.371211528778076\n",
      "step = 3174200: loss = 5.771004676818848\n",
      "step = 3174400: loss = 4.502708911895752\n",
      "step = 3174600: loss = 4.334563732147217\n",
      "step = 3174800: loss = 4.287362098693848\n",
      "step = 3175000: loss = 4.56141996383667\n",
      "step = 3175000: Average Return = 3.7100000381469727\n",
      "step = 3175200: loss = 4.82341194152832\n",
      "step = 3175400: loss = 4.492711067199707\n",
      "step = 3175600: loss = 3.8724753856658936\n",
      "step = 3175800: loss = 3.720674991607666\n",
      "step = 3176000: loss = 5.134809494018555\n",
      "step = 3176200: loss = 4.479120254516602\n",
      "step = 3176400: loss = 4.463258743286133\n",
      "step = 3176600: loss = 3.5515007972717285\n",
      "step = 3176800: loss = 5.348156452178955\n",
      "step = 3177000: loss = 5.175687313079834\n",
      "step = 3177200: loss = 3.677405834197998\n",
      "step = 3177400: loss = 3.0836410522460938\n",
      "step = 3177600: loss = 3.289841651916504\n",
      "step = 3177800: loss = 4.127134799957275\n",
      "step = 3178000: loss = 5.086639404296875\n",
      "step = 3178200: loss = 4.1920905113220215\n",
      "step = 3178400: loss = 5.818591594696045\n",
      "step = 3178600: loss = 3.92399525642395\n",
      "step = 3178800: loss = 4.563673496246338\n",
      "step = 3179000: loss = 4.658599376678467\n",
      "step = 3179200: loss = 4.745590686798096\n",
      "step = 3179400: loss = 4.36083984375\n",
      "step = 3179600: loss = 4.663437366485596\n",
      "step = 3179800: loss = 4.203887939453125\n",
      "step = 3180000: loss = 5.314526081085205\n",
      "step = 3180000: Average Return = 3.7219998836517334\n",
      "step = 3180200: loss = 2.521775484085083\n",
      "step = 3180400: loss = 3.305983304977417\n",
      "step = 3180600: loss = 3.7341392040252686\n",
      "step = 3180800: loss = 5.330848217010498\n",
      "step = 3181000: loss = 3.2281668186187744\n",
      "step = 3181200: loss = 3.764256238937378\n",
      "step = 3181400: loss = 4.735105991363525\n",
      "step = 3181600: loss = 5.133931636810303\n",
      "step = 3181800: loss = 3.787407159805298\n",
      "step = 3182000: loss = 4.5229949951171875\n",
      "step = 3182200: loss = 4.36326789855957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3182400: loss = 3.624516725540161\n",
      "step = 3182600: loss = 4.579156398773193\n",
      "step = 3182800: loss = 5.30664587020874\n",
      "step = 3183000: loss = 3.8795886039733887\n",
      "step = 3183200: loss = 4.055287837982178\n",
      "step = 3183400: loss = 4.927237510681152\n",
      "step = 3183600: loss = 4.141476631164551\n",
      "step = 3183800: loss = 3.739914655685425\n",
      "step = 3184000: loss = 4.531365871429443\n",
      "step = 3184200: loss = 4.082435131072998\n",
      "step = 3184400: loss = 3.8504905700683594\n",
      "step = 3184600: loss = 3.4059159755706787\n",
      "step = 3184800: loss = 2.9728541374206543\n",
      "step = 3185000: loss = 3.6138830184936523\n",
      "step = 3185000: Average Return = 3.9579999446868896\n",
      "step = 3185200: loss = 3.034712076187134\n",
      "step = 3185400: loss = 4.0148749351501465\n",
      "step = 3185600: loss = 4.0374627113342285\n",
      "step = 3185800: loss = 4.77305269241333\n",
      "step = 3186000: loss = 2.62392520904541\n",
      "step = 3186200: loss = 5.248274326324463\n",
      "step = 3186400: loss = 4.631844520568848\n",
      "step = 3186600: loss = 3.951167583465576\n",
      "step = 3186800: loss = 3.8158586025238037\n",
      "step = 3187000: loss = 3.615976572036743\n",
      "step = 3187200: loss = 4.237460136413574\n",
      "step = 3187400: loss = 3.7874526977539062\n",
      "step = 3187600: loss = 4.773183345794678\n",
      "step = 3187800: loss = 3.3428988456726074\n",
      "step = 3188000: loss = 4.730584621429443\n",
      "step = 3188200: loss = 5.262889385223389\n",
      "step = 3188400: loss = 3.5613090991973877\n",
      "step = 3188600: loss = 4.685162544250488\n",
      "step = 3188800: loss = 5.572041988372803\n",
      "step = 3189000: loss = 3.5556652545928955\n",
      "step = 3189200: loss = 5.324589252471924\n",
      "step = 3189400: loss = 5.801388263702393\n",
      "step = 3189600: loss = 4.595819473266602\n",
      "step = 3189800: loss = 2.720659017562866\n",
      "step = 3190000: loss = 4.421355724334717\n",
      "step = 3190000: Average Return = 3.9660000801086426\n",
      "step = 3190200: loss = 3.5241894721984863\n",
      "step = 3190400: loss = 4.878420352935791\n",
      "step = 3190600: loss = 5.221924304962158\n",
      "step = 3190800: loss = 3.98553204536438\n",
      "step = 3191000: loss = 5.577538967132568\n",
      "step = 3191200: loss = 3.1654772758483887\n",
      "step = 3191400: loss = 4.296334266662598\n",
      "step = 3191600: loss = 3.7474753856658936\n",
      "step = 3191800: loss = 3.480602264404297\n",
      "step = 3192000: loss = 4.268339157104492\n",
      "step = 3192200: loss = 4.450744152069092\n",
      "step = 3192400: loss = 3.276174306869507\n",
      "step = 3192600: loss = 4.41517972946167\n",
      "step = 3192800: loss = 4.666344165802002\n",
      "step = 3193000: loss = 3.059863805770874\n",
      "step = 3193200: loss = 4.54459285736084\n",
      "step = 3193400: loss = 3.156813383102417\n",
      "step = 3193600: loss = 4.429632663726807\n",
      "step = 3193800: loss = 3.075819730758667\n",
      "step = 3194000: loss = 5.787214279174805\n",
      "step = 3194200: loss = 5.321805477142334\n",
      "step = 3194400: loss = 4.355481147766113\n",
      "step = 3194600: loss = 3.1042120456695557\n",
      "step = 3194800: loss = 4.758874893188477\n",
      "step = 3195000: loss = 2.756171703338623\n",
      "step = 3195000: Average Return = 4.432000160217285\n",
      "step = 3195200: loss = 4.264036178588867\n",
      "step = 3195400: loss = 4.299045085906982\n",
      "step = 3195600: loss = 5.838968276977539\n",
      "step = 3195800: loss = 3.7728655338287354\n",
      "step = 3196000: loss = 5.045085906982422\n",
      "step = 3196200: loss = 4.17948579788208\n",
      "step = 3196400: loss = 3.695096015930176\n",
      "step = 3196600: loss = 4.288256645202637\n",
      "step = 3196800: loss = 4.281902313232422\n",
      "step = 3197000: loss = 4.451804161071777\n",
      "step = 3197200: loss = 5.387380123138428\n",
      "step = 3197400: loss = 4.63594913482666\n",
      "step = 3197600: loss = 4.007723808288574\n",
      "step = 3197800: loss = 3.8988823890686035\n",
      "step = 3198000: loss = 4.476096153259277\n",
      "step = 3198200: loss = 5.317594528198242\n",
      "step = 3198400: loss = 4.246203422546387\n",
      "step = 3198600: loss = 5.097209930419922\n",
      "step = 3198800: loss = 3.797922372817993\n",
      "step = 3199000: loss = 3.7783305644989014\n",
      "step = 3199200: loss = 4.003686904907227\n",
      "step = 3199400: loss = 2.760474681854248\n",
      "step = 3199600: loss = 3.7520878314971924\n",
      "step = 3199800: loss = 3.5709874629974365\n",
      "step = 3200000: loss = 3.5596609115600586\n",
      "step = 3200000: Average Return = 3.8519999980926514\n",
      "step = 3200200: loss = 4.857542991638184\n",
      "step = 3200400: loss = 3.6727678775787354\n",
      "step = 3200600: loss = 3.0156359672546387\n",
      "step = 3200800: loss = 4.1496992111206055\n",
      "step = 3201000: loss = 5.031961441040039\n",
      "step = 3201200: loss = 3.267920970916748\n",
      "step = 3201400: loss = 5.472704887390137\n",
      "step = 3201600: loss = 3.80850887298584\n",
      "step = 3201800: loss = 5.324585914611816\n",
      "step = 3202000: loss = 3.186720609664917\n",
      "step = 3202200: loss = 3.621570110321045\n",
      "step = 3202400: loss = 4.188604831695557\n",
      "step = 3202600: loss = 3.812748908996582\n",
      "step = 3202800: loss = 4.94107723236084\n",
      "step = 3203000: loss = 2.7191710472106934\n",
      "step = 3203200: loss = 4.408593654632568\n",
      "step = 3203400: loss = 4.1386003494262695\n",
      "step = 3203600: loss = 4.262670993804932\n",
      "step = 3203800: loss = 3.4965672492980957\n",
      "step = 3204000: loss = 3.78576922416687\n",
      "step = 3204200: loss = 4.238624095916748\n",
      "step = 3204400: loss = 3.9905498027801514\n",
      "step = 3204600: loss = 5.27256441116333\n",
      "step = 3204800: loss = 5.677526950836182\n",
      "step = 3205000: loss = 4.837849140167236\n",
      "step = 3205000: Average Return = 3.7739999294281006\n",
      "step = 3205200: loss = 3.908984899520874\n",
      "step = 3205400: loss = 4.424900531768799\n",
      "step = 3205600: loss = 4.328949451446533\n",
      "step = 3205800: loss = 4.405486583709717\n",
      "step = 3206000: loss = 4.980772018432617\n",
      "step = 3206200: loss = 3.6090850830078125\n",
      "step = 3206400: loss = 4.418710231781006\n",
      "step = 3206600: loss = 4.692207336425781\n",
      "step = 3206800: loss = 3.629124164581299\n",
      "step = 3207000: loss = 3.9694650173187256\n",
      "step = 3207200: loss = 3.4724037647247314\n",
      "step = 3207400: loss = 3.7299907207489014\n",
      "step = 3207600: loss = 4.448307514190674\n",
      "step = 3207800: loss = 4.061427593231201\n",
      "step = 3208000: loss = 3.824615001678467\n",
      "step = 3208200: loss = 3.664879083633423\n",
      "step = 3208400: loss = 3.8688952922821045\n",
      "step = 3208600: loss = 4.472643852233887\n",
      "step = 3208800: loss = 4.603564739227295\n",
      "step = 3209000: loss = 3.9627158641815186\n",
      "step = 3209200: loss = 4.198695182800293\n",
      "step = 3209400: loss = 4.007774353027344\n",
      "step = 3209600: loss = 4.2137556076049805\n",
      "step = 3209800: loss = 3.97395396232605\n",
      "step = 3210000: loss = 3.735858917236328\n",
      "step = 3210000: Average Return = 3.7860000133514404\n",
      "step = 3210200: loss = 4.197251319885254\n",
      "step = 3210400: loss = 3.0875940322875977\n",
      "step = 3210600: loss = 3.5278844833374023\n",
      "step = 3210800: loss = 4.485584735870361\n",
      "step = 3211000: loss = 4.484628200531006\n",
      "step = 3211200: loss = 3.689730167388916\n",
      "step = 3211400: loss = 4.13767671585083\n",
      "step = 3211600: loss = 4.668728351593018\n",
      "step = 3211800: loss = 4.7642011642456055\n",
      "step = 3212000: loss = 3.988523483276367\n",
      "step = 3212200: loss = 4.295736312866211\n",
      "step = 3212400: loss = 4.535146713256836\n",
      "step = 3212600: loss = 3.382962465286255\n",
      "step = 3212800: loss = 3.2800707817077637\n",
      "step = 3213000: loss = 4.99790096282959\n",
      "step = 3213200: loss = 4.451456069946289\n",
      "step = 3213400: loss = 4.586330890655518\n",
      "step = 3213600: loss = 4.6159868240356445\n",
      "step = 3213800: loss = 4.098461151123047\n",
      "step = 3214000: loss = 4.539813041687012\n",
      "step = 3214200: loss = 4.112093448638916\n",
      "step = 3214400: loss = 3.7646353244781494\n",
      "step = 3214600: loss = 3.172576904296875\n",
      "step = 3214800: loss = 4.316251754760742\n",
      "step = 3215000: loss = 4.30726432800293\n",
      "step = 3215000: Average Return = 4.076000213623047\n",
      "step = 3215200: loss = 3.093831777572632\n",
      "step = 3215400: loss = 3.214634418487549\n",
      "step = 3215600: loss = 4.122518062591553\n",
      "step = 3215800: loss = 4.261962890625\n",
      "step = 3216000: loss = 3.957195281982422\n",
      "step = 3216200: loss = 4.060693264007568\n",
      "step = 3216400: loss = 3.925928831100464\n",
      "step = 3216600: loss = 4.585603713989258\n",
      "step = 3216800: loss = 3.960564374923706\n",
      "step = 3217000: loss = 3.961792469024658\n",
      "step = 3217200: loss = 4.305917263031006\n",
      "step = 3217400: loss = 4.659646034240723\n",
      "step = 3217600: loss = 4.015213489532471\n",
      "step = 3217800: loss = 4.0115766525268555\n",
      "step = 3218000: loss = 3.860785961151123\n",
      "step = 3218200: loss = 4.355647563934326\n",
      "step = 3218400: loss = 3.341789484024048\n",
      "step = 3218600: loss = 3.249730110168457\n",
      "step = 3218800: loss = 5.667023181915283\n",
      "step = 3219000: loss = 5.593092918395996\n",
      "step = 3219200: loss = 4.416184425354004\n",
      "step = 3219400: loss = 3.861884593963623\n",
      "step = 3219600: loss = 3.7778899669647217\n",
      "step = 3219800: loss = 3.937624931335449\n",
      "step = 3220000: loss = 4.430568218231201\n",
      "step = 3220000: Average Return = 3.7279999256134033\n",
      "step = 3220200: loss = 4.697277069091797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3220400: loss = 4.119321823120117\n",
      "step = 3220600: loss = 3.566439390182495\n",
      "step = 3220800: loss = 4.307011127471924\n",
      "step = 3221000: loss = 3.639051675796509\n",
      "step = 3221200: loss = 4.12774133682251\n",
      "step = 3221400: loss = 4.102877616882324\n",
      "step = 3221600: loss = 3.7798397541046143\n",
      "step = 3221800: loss = 4.777569770812988\n",
      "step = 3222000: loss = 4.156084060668945\n",
      "step = 3222200: loss = 3.077054023742676\n",
      "step = 3222400: loss = 3.257650136947632\n",
      "step = 3222600: loss = 3.6573562622070312\n",
      "step = 3222800: loss = 3.6346843242645264\n",
      "step = 3223000: loss = 4.552779197692871\n",
      "step = 3223200: loss = 3.3061704635620117\n",
      "step = 3223400: loss = 4.589827537536621\n",
      "step = 3223600: loss = 4.877677917480469\n",
      "step = 3223800: loss = 4.764951229095459\n",
      "step = 3224000: loss = 5.167687892913818\n",
      "step = 3224200: loss = 4.020567417144775\n",
      "step = 3224400: loss = 4.110820293426514\n",
      "step = 3224600: loss = 5.02837610244751\n",
      "step = 3224800: loss = 5.183709621429443\n",
      "step = 3225000: loss = 3.202130079269409\n",
      "step = 3225000: Average Return = 3.563999891281128\n",
      "step = 3225200: loss = 5.1278791427612305\n",
      "step = 3225400: loss = 4.6977152824401855\n",
      "step = 3225600: loss = 4.0811381340026855\n",
      "step = 3225800: loss = 4.399782657623291\n",
      "step = 3226000: loss = 5.018885135650635\n",
      "step = 3226200: loss = 3.4007580280303955\n",
      "step = 3226400: loss = 3.678569793701172\n",
      "step = 3226600: loss = 3.748711347579956\n",
      "step = 3226800: loss = 3.1968510150909424\n",
      "step = 3227000: loss = 1.9225716590881348\n",
      "step = 3227200: loss = 3.586036205291748\n",
      "step = 3227400: loss = 3.5822813510894775\n",
      "step = 3227600: loss = 3.37880539894104\n",
      "step = 3227800: loss = 4.215832233428955\n",
      "step = 3228000: loss = 3.9921789169311523\n",
      "step = 3228200: loss = 3.672839879989624\n",
      "step = 3228400: loss = 3.757329225540161\n",
      "step = 3228600: loss = 4.429365634918213\n",
      "step = 3228800: loss = 4.873377799987793\n",
      "step = 3229000: loss = 4.13975715637207\n",
      "step = 3229200: loss = 5.079868793487549\n",
      "step = 3229400: loss = 4.750739097595215\n",
      "step = 3229600: loss = 4.6113057136535645\n",
      "step = 3229800: loss = 4.011988639831543\n",
      "step = 3230000: loss = 5.118617057800293\n",
      "step = 3230000: Average Return = 3.7760000228881836\n",
      "step = 3230200: loss = 4.249940872192383\n",
      "step = 3230400: loss = 4.767571449279785\n",
      "step = 3230600: loss = 3.9737095832824707\n",
      "step = 3230800: loss = 4.422171115875244\n",
      "step = 3231000: loss = 2.553069591522217\n",
      "step = 3231200: loss = 3.8344829082489014\n",
      "step = 3231400: loss = 4.816495418548584\n",
      "step = 3231600: loss = 4.031562328338623\n",
      "step = 3231800: loss = 4.393666744232178\n",
      "step = 3232000: loss = 3.1336166858673096\n",
      "step = 3232200: loss = 3.3587024211883545\n",
      "step = 3232400: loss = 2.5617830753326416\n",
      "step = 3232600: loss = 3.1204400062561035\n",
      "step = 3232800: loss = 3.4819958209991455\n",
      "step = 3233000: loss = 4.877408504486084\n",
      "step = 3233200: loss = 3.99234676361084\n",
      "step = 3233400: loss = 3.6626524925231934\n",
      "step = 3233600: loss = 6.289958953857422\n",
      "step = 3233800: loss = 3.0366601943969727\n",
      "step = 3234000: loss = 3.8294789791107178\n",
      "step = 3234200: loss = 5.035968780517578\n",
      "step = 3234400: loss = 3.706256866455078\n",
      "step = 3234600: loss = 4.131999492645264\n",
      "step = 3234800: loss = 4.399807929992676\n",
      "step = 3235000: loss = 4.165256977081299\n",
      "step = 3235000: Average Return = 3.568000078201294\n",
      "step = 3235200: loss = 3.002209424972534\n",
      "step = 3235400: loss = 3.215348958969116\n",
      "step = 3235600: loss = 3.1739418506622314\n",
      "step = 3235800: loss = 4.638802528381348\n",
      "step = 3236000: loss = 4.184442043304443\n",
      "step = 3236200: loss = 2.8476150035858154\n",
      "step = 3236400: loss = 4.896186351776123\n",
      "step = 3236600: loss = 4.357623100280762\n",
      "step = 3236800: loss = 2.5284337997436523\n",
      "step = 3237000: loss = 4.371842384338379\n",
      "step = 3237200: loss = 1.9756499528884888\n",
      "step = 3237400: loss = 3.6623852252960205\n",
      "step = 3237600: loss = 4.699717044830322\n",
      "step = 3237800: loss = 3.1910643577575684\n",
      "step = 3238000: loss = 3.3028087615966797\n",
      "step = 3238200: loss = 3.447065591812134\n",
      "step = 3238400: loss = 3.785801887512207\n",
      "step = 3238600: loss = 3.9504733085632324\n",
      "step = 3238800: loss = 4.231853008270264\n",
      "step = 3239000: loss = 3.1267943382263184\n",
      "step = 3239200: loss = 3.506429433822632\n",
      "step = 3239400: loss = 4.781633377075195\n",
      "step = 3239600: loss = 4.848381042480469\n",
      "step = 3239800: loss = 3.2365715503692627\n",
      "step = 3240000: loss = 5.439540386199951\n",
      "step = 3240000: Average Return = 3.8440001010894775\n",
      "step = 3240200: loss = 4.200554847717285\n",
      "step = 3240400: loss = 4.666462421417236\n",
      "step = 3240600: loss = 3.0442802906036377\n",
      "step = 3240800: loss = 2.871953010559082\n",
      "step = 3241000: loss = 4.007646083831787\n",
      "step = 3241200: loss = 3.482792377471924\n",
      "step = 3241400: loss = 3.5108656883239746\n",
      "step = 3241600: loss = 4.724778175354004\n",
      "step = 3241800: loss = 2.9683659076690674\n",
      "step = 3242000: loss = 4.177572250366211\n",
      "step = 3242200: loss = 4.327325344085693\n",
      "step = 3242400: loss = 4.32501220703125\n",
      "step = 3242600: loss = 4.832168102264404\n",
      "step = 3242800: loss = 4.844101905822754\n",
      "step = 3243000: loss = 3.274447441101074\n",
      "step = 3243200: loss = 4.6906256675720215\n",
      "step = 3243400: loss = 4.184998989105225\n",
      "step = 3243600: loss = 3.8025805950164795\n",
      "step = 3243800: loss = 3.951793670654297\n",
      "step = 3244000: loss = 4.123022556304932\n",
      "step = 3244200: loss = 4.882450580596924\n",
      "step = 3244400: loss = 5.622755527496338\n",
      "step = 3244600: loss = 4.706570148468018\n",
      "step = 3244800: loss = 3.872958183288574\n",
      "step = 3245000: loss = 4.678134441375732\n",
      "step = 3245000: Average Return = 3.559999942779541\n",
      "step = 3245200: loss = 4.504586696624756\n",
      "step = 3245400: loss = 3.9018847942352295\n",
      "step = 3245600: loss = 4.704891204833984\n",
      "step = 3245800: loss = 3.3784468173980713\n",
      "step = 3246000: loss = 5.005789279937744\n",
      "step = 3246200: loss = 3.662264347076416\n",
      "step = 3246400: loss = 4.387735366821289\n",
      "step = 3246600: loss = 4.266273021697998\n",
      "step = 3246800: loss = 3.956908702850342\n",
      "step = 3247000: loss = 4.339940547943115\n",
      "step = 3247200: loss = 3.912832021713257\n",
      "step = 3247400: loss = 5.030418395996094\n",
      "step = 3247600: loss = 5.2988176345825195\n",
      "step = 3247800: loss = 4.093199253082275\n",
      "step = 3248000: loss = 3.3165347576141357\n",
      "step = 3248200: loss = 4.890528202056885\n",
      "step = 3248400: loss = 3.090256929397583\n",
      "step = 3248600: loss = 4.232920169830322\n",
      "step = 3248800: loss = 4.093080043792725\n",
      "step = 3249000: loss = 5.527436256408691\n",
      "step = 3249200: loss = 4.0080060958862305\n",
      "step = 3249400: loss = 5.111459255218506\n",
      "step = 3249600: loss = 4.162234306335449\n",
      "step = 3249800: loss = 3.8017373085021973\n",
      "step = 3250000: loss = 5.551697731018066\n",
      "step = 3250000: Average Return = 3.49399995803833\n",
      "step = 3250200: loss = 5.0743231773376465\n",
      "step = 3250400: loss = 4.129557132720947\n",
      "step = 3250600: loss = 4.351706027984619\n",
      "step = 3250800: loss = 4.101597309112549\n",
      "step = 3251000: loss = 4.068733215332031\n",
      "step = 3251200: loss = 5.254924774169922\n",
      "step = 3251400: loss = 5.4439215660095215\n",
      "step = 3251600: loss = 4.2971625328063965\n",
      "step = 3251800: loss = 3.785914897918701\n",
      "step = 3252000: loss = 4.180489540100098\n",
      "step = 3252200: loss = 4.81898307800293\n",
      "step = 3252400: loss = 4.7342023849487305\n",
      "step = 3252600: loss = 3.3036632537841797\n",
      "step = 3252800: loss = 5.2405805587768555\n",
      "step = 3253000: loss = 3.8541669845581055\n",
      "step = 3253200: loss = 5.031970500946045\n",
      "step = 3253400: loss = 4.518427848815918\n",
      "step = 3253600: loss = 4.3041205406188965\n",
      "step = 3253800: loss = 4.495940685272217\n",
      "step = 3254000: loss = 2.924403667449951\n",
      "step = 3254200: loss = 4.2525763511657715\n",
      "step = 3254400: loss = 5.546080112457275\n",
      "step = 3254600: loss = 3.84942626953125\n",
      "step = 3254800: loss = 3.5164527893066406\n",
      "step = 3255000: loss = 4.64719295501709\n",
      "step = 3255000: Average Return = 3.757999897003174\n",
      "step = 3255200: loss = 4.6889543533325195\n",
      "step = 3255400: loss = 5.799061298370361\n",
      "step = 3255600: loss = 3.3603100776672363\n",
      "step = 3255800: loss = 4.127581596374512\n",
      "step = 3256000: loss = 4.483261585235596\n",
      "step = 3256200: loss = 4.033468723297119\n",
      "step = 3256400: loss = 3.735225200653076\n",
      "step = 3256600: loss = 3.932586908340454\n",
      "step = 3256800: loss = 4.755589485168457\n",
      "step = 3257000: loss = 3.1964898109436035\n",
      "step = 3257200: loss = 3.104407548904419\n",
      "step = 3257400: loss = 4.1753950119018555\n",
      "step = 3257600: loss = 4.039143085479736\n",
      "step = 3257800: loss = 3.447408437728882\n",
      "step = 3258000: loss = 4.632261753082275\n",
      "step = 3258200: loss = 6.551075458526611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3258400: loss = 3.8114144802093506\n",
      "step = 3258600: loss = 3.5004169940948486\n",
      "step = 3258800: loss = 5.571527004241943\n",
      "step = 3259000: loss = 4.226607799530029\n",
      "step = 3259200: loss = 3.8537700176239014\n",
      "step = 3259400: loss = 4.907050132751465\n",
      "step = 3259600: loss = 4.592785358428955\n",
      "step = 3259800: loss = 5.777918338775635\n",
      "step = 3260000: loss = 4.373765468597412\n",
      "step = 3260000: Average Return = 4.046000003814697\n",
      "step = 3260200: loss = 3.9317760467529297\n",
      "step = 3260400: loss = 5.434306621551514\n",
      "step = 3260600: loss = 4.5607075691223145\n",
      "step = 3260800: loss = 3.2129831314086914\n",
      "step = 3261000: loss = 4.629717826843262\n",
      "step = 3261200: loss = 4.147575855255127\n",
      "step = 3261400: loss = 3.961986541748047\n",
      "step = 3261600: loss = 3.4922659397125244\n",
      "step = 3261800: loss = 4.708557605743408\n",
      "step = 3262000: loss = 3.9743614196777344\n",
      "step = 3262200: loss = 2.4949822425842285\n",
      "step = 3262400: loss = 5.207374095916748\n",
      "step = 3262600: loss = 3.9526591300964355\n",
      "step = 3262800: loss = 4.149446964263916\n",
      "step = 3263000: loss = 4.48659086227417\n",
      "step = 3263200: loss = 4.129746437072754\n",
      "step = 3263400: loss = 4.256913185119629\n",
      "step = 3263600: loss = 4.01512336730957\n",
      "step = 3263800: loss = 4.7385077476501465\n",
      "step = 3264000: loss = 3.9258716106414795\n",
      "step = 3264200: loss = 4.158450126647949\n",
      "step = 3264400: loss = 5.555910587310791\n",
      "step = 3264600: loss = 4.509453773498535\n",
      "step = 3264800: loss = 4.004976272583008\n",
      "step = 3265000: loss = 2.0358214378356934\n",
      "step = 3265000: Average Return = 3.869999885559082\n",
      "step = 3265200: loss = 4.402071475982666\n",
      "step = 3265400: loss = 3.6269164085388184\n",
      "step = 3265600: loss = 4.85871696472168\n",
      "step = 3265800: loss = 3.5040104389190674\n",
      "step = 3266000: loss = 4.720854759216309\n",
      "step = 3266200: loss = 4.354682922363281\n",
      "step = 3266400: loss = 5.053964614868164\n",
      "step = 3266600: loss = 3.7671868801116943\n",
      "step = 3266800: loss = 5.421561241149902\n",
      "step = 3267000: loss = 2.9899861812591553\n",
      "step = 3267200: loss = 4.277420520782471\n",
      "step = 3267400: loss = 4.8151445388793945\n",
      "step = 3267600: loss = 4.234951496124268\n",
      "step = 3267800: loss = 4.6408915519714355\n",
      "step = 3268000: loss = 3.6607251167297363\n",
      "step = 3268200: loss = 4.12232780456543\n",
      "step = 3268400: loss = 2.7263729572296143\n",
      "step = 3268600: loss = 4.535714626312256\n",
      "step = 3268800: loss = 3.194031000137329\n",
      "step = 3269000: loss = 3.4770703315734863\n",
      "step = 3269200: loss = 4.1204376220703125\n",
      "step = 3269400: loss = 4.936881065368652\n",
      "step = 3269600: loss = 4.516077041625977\n",
      "step = 3269800: loss = 5.058334827423096\n",
      "step = 3270000: loss = 4.5093770027160645\n",
      "step = 3270000: Average Return = 3.4700000286102295\n",
      "step = 3270200: loss = 4.524938106536865\n",
      "step = 3270400: loss = 4.795094013214111\n",
      "step = 3270600: loss = 3.790278434753418\n",
      "step = 3270800: loss = 3.116177797317505\n",
      "step = 3271000: loss = 4.330440521240234\n",
      "step = 3271200: loss = 4.588794708251953\n",
      "step = 3271400: loss = 3.477750778198242\n",
      "step = 3271600: loss = 3.9231951236724854\n",
      "step = 3271800: loss = 5.049782752990723\n",
      "step = 3272000: loss = 4.621954441070557\n",
      "step = 3272200: loss = 4.6302080154418945\n",
      "step = 3272400: loss = 3.377991199493408\n",
      "step = 3272600: loss = 4.421619892120361\n",
      "step = 3272800: loss = 5.071470737457275\n",
      "step = 3273000: loss = 3.3432564735412598\n",
      "step = 3273200: loss = 4.628760814666748\n",
      "step = 3273400: loss = 3.585092782974243\n",
      "step = 3273600: loss = 4.353221893310547\n",
      "step = 3273800: loss = 4.182997226715088\n",
      "step = 3274000: loss = 5.313178062438965\n",
      "step = 3274200: loss = 4.53260612487793\n",
      "step = 3274400: loss = 3.0335168838500977\n",
      "step = 3274600: loss = 4.219757080078125\n",
      "step = 3274800: loss = 3.956923723220825\n",
      "step = 3275000: loss = 3.7677786350250244\n",
      "step = 3275000: Average Return = 4.265999794006348\n",
      "step = 3275200: loss = 4.61354923248291\n",
      "step = 3275400: loss = 4.234377861022949\n",
      "step = 3275600: loss = 6.230954647064209\n",
      "step = 3275800: loss = 3.5114622116088867\n",
      "step = 3276000: loss = 4.488102436065674\n",
      "step = 3276200: loss = 4.4327311515808105\n",
      "step = 3276400: loss = 4.006095886230469\n",
      "step = 3276600: loss = 4.7980055809021\n",
      "step = 3276800: loss = 3.9610040187835693\n",
      "step = 3277000: loss = 4.970646858215332\n",
      "step = 3277200: loss = 4.950408935546875\n",
      "step = 3277400: loss = 4.078136920928955\n",
      "step = 3277600: loss = 3.403609037399292\n",
      "step = 3277800: loss = 3.2029452323913574\n",
      "step = 3278000: loss = 3.99436092376709\n",
      "step = 3278200: loss = 4.067836284637451\n",
      "step = 3278400: loss = 3.8793270587921143\n",
      "step = 3278600: loss = 4.910975933074951\n",
      "step = 3278800: loss = 3.9336259365081787\n",
      "step = 3279000: loss = 3.5923144817352295\n",
      "step = 3279200: loss = 4.258095741271973\n",
      "step = 3279400: loss = 5.0166497230529785\n",
      "step = 3279600: loss = 4.167519569396973\n",
      "step = 3279800: loss = 4.297244071960449\n",
      "step = 3280000: loss = 3.801257848739624\n",
      "step = 3280000: Average Return = 4.090000152587891\n",
      "step = 3280200: loss = 5.368628978729248\n",
      "step = 3280400: loss = 4.253181457519531\n",
      "step = 3280600: loss = 5.063322067260742\n",
      "step = 3280800: loss = 3.792684555053711\n",
      "step = 3281000: loss = 4.267086982727051\n",
      "step = 3281200: loss = 2.8653934001922607\n",
      "step = 3281400: loss = 3.8646621704101562\n",
      "step = 3281600: loss = 4.151062488555908\n",
      "step = 3281800: loss = 4.252862453460693\n",
      "step = 3282000: loss = 5.634936332702637\n",
      "step = 3282200: loss = 4.056002140045166\n",
      "step = 3282400: loss = 3.7467265129089355\n",
      "step = 3282600: loss = 4.562629222869873\n",
      "step = 3282800: loss = 4.989997386932373\n",
      "step = 3283000: loss = 4.5710883140563965\n",
      "step = 3283200: loss = 5.39431619644165\n",
      "step = 3283400: loss = 3.987391233444214\n",
      "step = 3283600: loss = 3.423774003982544\n",
      "step = 3283800: loss = 4.92194938659668\n",
      "step = 3284000: loss = 2.7886950969696045\n",
      "step = 3284200: loss = 4.439876556396484\n",
      "step = 3284400: loss = 2.8976006507873535\n",
      "step = 3284600: loss = 4.17927885055542\n",
      "step = 3284800: loss = 5.71157693862915\n",
      "step = 3285000: loss = 5.202085971832275\n",
      "step = 3285000: Average Return = 3.9200000762939453\n",
      "step = 3285200: loss = 4.267431735992432\n",
      "step = 3285400: loss = 3.775803327560425\n",
      "step = 3285600: loss = 3.6219606399536133\n",
      "step = 3285800: loss = 4.564470291137695\n",
      "step = 3286000: loss = 3.240337371826172\n",
      "step = 3286200: loss = 3.0266716480255127\n",
      "step = 3286400: loss = 4.947325706481934\n",
      "step = 3286600: loss = 4.959465980529785\n",
      "step = 3286800: loss = 4.297821044921875\n",
      "step = 3287000: loss = 3.788774251937866\n",
      "step = 3287200: loss = 4.082075595855713\n",
      "step = 3287400: loss = 4.7263007164001465\n",
      "step = 3287600: loss = 4.453339099884033\n",
      "step = 3287800: loss = 4.781441688537598\n",
      "step = 3288000: loss = 3.9403209686279297\n",
      "step = 3288200: loss = 3.560497999191284\n",
      "step = 3288400: loss = 3.7299838066101074\n",
      "step = 3288600: loss = 3.2438390254974365\n",
      "step = 3288800: loss = 2.339360475540161\n",
      "step = 3289000: loss = 4.547909259796143\n",
      "step = 3289200: loss = 4.081777572631836\n",
      "step = 3289400: loss = 4.967756271362305\n",
      "step = 3289600: loss = 4.511971473693848\n",
      "step = 3289800: loss = 4.047547817230225\n",
      "step = 3290000: loss = 4.667968273162842\n",
      "step = 3290000: Average Return = 3.6059999465942383\n",
      "step = 3290200: loss = 4.2374043464660645\n",
      "step = 3290400: loss = 4.253549575805664\n",
      "step = 3290600: loss = 5.470595359802246\n",
      "step = 3290800: loss = 6.004109859466553\n",
      "step = 3291000: loss = 3.900665283203125\n",
      "step = 3291200: loss = 3.276231288909912\n",
      "step = 3291400: loss = 4.451398849487305\n",
      "step = 3291600: loss = 4.826442718505859\n",
      "step = 3291800: loss = 2.909125804901123\n",
      "step = 3292000: loss = 4.547793388366699\n",
      "step = 3292200: loss = 5.250237941741943\n",
      "step = 3292400: loss = 3.886348247528076\n",
      "step = 3292600: loss = 5.112545013427734\n",
      "step = 3292800: loss = 4.159018516540527\n",
      "step = 3293000: loss = 5.5273003578186035\n",
      "step = 3293200: loss = 5.11985969543457\n",
      "step = 3293400: loss = 3.846479654312134\n",
      "step = 3293600: loss = 2.817885160446167\n",
      "step = 3293800: loss = 3.6269795894622803\n",
      "step = 3294000: loss = 2.8778412342071533\n",
      "step = 3294200: loss = 4.296867370605469\n",
      "step = 3294400: loss = 3.3256542682647705\n",
      "step = 3294600: loss = 3.86576247215271\n",
      "step = 3294800: loss = 4.4022698402404785\n",
      "step = 3295000: loss = 4.617250442504883\n",
      "step = 3295000: Average Return = 4.105999946594238\n",
      "step = 3295200: loss = 4.13601541519165\n",
      "step = 3295400: loss = 4.71370792388916\n",
      "step = 3295600: loss = 5.565696716308594\n",
      "step = 3295800: loss = 3.9268064498901367\n",
      "step = 3296000: loss = 3.2405951023101807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3296200: loss = 3.4161767959594727\n",
      "step = 3296400: loss = 4.1354289054870605\n",
      "step = 3296600: loss = 3.2641055583953857\n",
      "step = 3296800: loss = 5.237972259521484\n",
      "step = 3297000: loss = 3.702474355697632\n",
      "step = 3297200: loss = 3.5846078395843506\n",
      "step = 3297400: loss = 5.496458530426025\n",
      "step = 3297600: loss = 4.176735877990723\n",
      "step = 3297800: loss = 5.138157844543457\n",
      "step = 3298000: loss = 3.778801202774048\n",
      "step = 3298200: loss = 5.134481906890869\n",
      "step = 3298400: loss = 4.033802032470703\n",
      "step = 3298600: loss = 5.105099678039551\n",
      "step = 3298800: loss = 4.445900917053223\n",
      "step = 3299000: loss = 5.2069220542907715\n",
      "step = 3299200: loss = 4.754316329956055\n",
      "step = 3299400: loss = 3.1389312744140625\n",
      "step = 3299600: loss = 4.781106472015381\n",
      "step = 3299800: loss = 6.076810836791992\n",
      "step = 3300000: loss = 4.520484924316406\n",
      "step = 3300000: Average Return = 3.5940001010894775\n",
      "step = 3300200: loss = 3.543628454208374\n",
      "step = 3300400: loss = 5.8953680992126465\n",
      "step = 3300600: loss = 3.808661937713623\n",
      "step = 3300800: loss = 4.698352336883545\n",
      "step = 3301000: loss = 3.8043315410614014\n",
      "step = 3301200: loss = 4.310916900634766\n",
      "step = 3301400: loss = 6.004303932189941\n",
      "step = 3301600: loss = 3.9034721851348877\n",
      "step = 3301800: loss = 2.7343688011169434\n",
      "step = 3302000: loss = 4.727275371551514\n",
      "step = 3302200: loss = 4.523369789123535\n",
      "step = 3302400: loss = 3.368396520614624\n",
      "step = 3302600: loss = 3.2507596015930176\n",
      "step = 3302800: loss = 4.27852725982666\n",
      "step = 3303000: loss = 4.238845348358154\n",
      "step = 3303200: loss = 4.241420269012451\n",
      "step = 3303400: loss = 3.4110898971557617\n",
      "step = 3303600: loss = 3.481440544128418\n",
      "step = 3303800: loss = 3.448580265045166\n",
      "step = 3304000: loss = 5.084288120269775\n",
      "step = 3304200: loss = 3.1007614135742188\n",
      "step = 3304400: loss = 5.221115589141846\n",
      "step = 3304600: loss = 4.193883419036865\n",
      "step = 3304800: loss = 4.086353778839111\n",
      "step = 3305000: loss = 4.437153339385986\n",
      "step = 3305000: Average Return = 3.818000078201294\n",
      "step = 3305200: loss = 3.622152328491211\n",
      "step = 3305400: loss = 5.342397212982178\n",
      "step = 3305600: loss = 3.8626039028167725\n",
      "step = 3305800: loss = 4.901636123657227\n",
      "step = 3306000: loss = 4.4582695960998535\n",
      "step = 3306200: loss = 3.989753246307373\n",
      "step = 3306400: loss = 3.946666717529297\n",
      "step = 3306600: loss = 2.991870880126953\n",
      "step = 3306800: loss = 5.344827175140381\n",
      "step = 3307000: loss = 5.244546890258789\n",
      "step = 3307200: loss = 3.4033169746398926\n",
      "step = 3307400: loss = 2.600498914718628\n",
      "step = 3307600: loss = 4.952966690063477\n",
      "step = 3307800: loss = 4.801889419555664\n",
      "step = 3308000: loss = 4.027904033660889\n",
      "step = 3308200: loss = 3.7847368717193604\n",
      "step = 3308400: loss = 3.264810800552368\n",
      "step = 3308600: loss = 5.049281597137451\n",
      "step = 3308800: loss = 3.269547700881958\n",
      "step = 3309000: loss = 3.044398546218872\n",
      "step = 3309200: loss = 3.8207430839538574\n",
      "step = 3309400: loss = 4.386085510253906\n",
      "step = 3309600: loss = 5.748166561126709\n",
      "step = 3309800: loss = 4.430030822753906\n",
      "step = 3310000: loss = 5.255009174346924\n",
      "step = 3310000: Average Return = 3.7899999618530273\n",
      "step = 3310200: loss = 3.6950976848602295\n",
      "step = 3310400: loss = 4.456960201263428\n",
      "step = 3310600: loss = 4.708268165588379\n",
      "step = 3310800: loss = 4.072364807128906\n",
      "step = 3311000: loss = 4.481013298034668\n",
      "step = 3311200: loss = 4.181744575500488\n",
      "step = 3311400: loss = 3.4534387588500977\n",
      "step = 3311600: loss = 4.221168518066406\n",
      "step = 3311800: loss = 3.703481435775757\n",
      "step = 3312000: loss = 3.5071966648101807\n",
      "step = 3312200: loss = 4.537265300750732\n",
      "step = 3312400: loss = 5.171514987945557\n",
      "step = 3312600: loss = 4.4195356369018555\n",
      "step = 3312800: loss = 3.0396759510040283\n",
      "step = 3313000: loss = 3.6855297088623047\n",
      "step = 3313200: loss = 4.923012733459473\n",
      "step = 3313400: loss = 4.486669063568115\n",
      "step = 3313600: loss = 4.293264865875244\n",
      "step = 3313800: loss = 4.414616584777832\n",
      "step = 3314000: loss = 4.224564075469971\n",
      "step = 3314200: loss = 4.081053733825684\n",
      "step = 3314400: loss = 5.086225986480713\n",
      "step = 3314600: loss = 3.708407163619995\n",
      "step = 3314800: loss = 4.4204840660095215\n",
      "step = 3315000: loss = 4.425144672393799\n",
      "step = 3315000: Average Return = 3.7100000381469727\n",
      "step = 3315200: loss = 3.9376041889190674\n",
      "step = 3315400: loss = 4.259997367858887\n",
      "step = 3315600: loss = 3.0684707164764404\n",
      "step = 3315800: loss = 4.460305213928223\n",
      "step = 3316000: loss = 3.636190176010132\n",
      "step = 3316200: loss = 3.8787121772766113\n",
      "step = 3316400: loss = 5.068624973297119\n",
      "step = 3316600: loss = 5.518385887145996\n",
      "step = 3316800: loss = 4.843807697296143\n",
      "step = 3317000: loss = 4.517579078674316\n",
      "step = 3317200: loss = 4.423206806182861\n",
      "step = 3317400: loss = 3.757441282272339\n",
      "step = 3317600: loss = 4.871844291687012\n",
      "step = 3317800: loss = 5.127788543701172\n",
      "step = 3318000: loss = 3.0628585815429688\n",
      "step = 3318200: loss = 3.7391247749328613\n",
      "step = 3318400: loss = 3.205188512802124\n",
      "step = 3318600: loss = 4.012175559997559\n",
      "step = 3318800: loss = 3.9424078464508057\n",
      "step = 3319000: loss = 4.4878082275390625\n",
      "step = 3319200: loss = 4.093516826629639\n",
      "step = 3319400: loss = 4.201822280883789\n",
      "step = 3319600: loss = 4.882989883422852\n",
      "step = 3319800: loss = 3.17868709564209\n",
      "step = 3320000: loss = 4.654831886291504\n",
      "step = 3320000: Average Return = 3.809999942779541\n",
      "step = 3320200: loss = 4.093976974487305\n",
      "step = 3320400: loss = 3.79560923576355\n",
      "step = 3320600: loss = 4.390936851501465\n",
      "step = 3320800: loss = 4.207539081573486\n",
      "step = 3321000: loss = 3.594027042388916\n",
      "step = 3321200: loss = 4.456387996673584\n",
      "step = 3321400: loss = 3.3697497844696045\n",
      "step = 3321600: loss = 4.0246663093566895\n",
      "step = 3321800: loss = 5.108229160308838\n",
      "step = 3322000: loss = 4.392664909362793\n",
      "step = 3322200: loss = 4.787637710571289\n",
      "step = 3322400: loss = 5.139697551727295\n",
      "step = 3322600: loss = 3.7740042209625244\n",
      "step = 3322800: loss = 4.131750583648682\n",
      "step = 3323000: loss = 3.983706474304199\n",
      "step = 3323200: loss = 3.3143346309661865\n",
      "step = 3323400: loss = 5.309729099273682\n",
      "step = 3323600: loss = 3.719280958175659\n",
      "step = 3323800: loss = 4.264113903045654\n",
      "step = 3324000: loss = 4.322236061096191\n",
      "step = 3324200: loss = 3.6360528469085693\n",
      "step = 3324400: loss = 3.912217378616333\n",
      "step = 3324600: loss = 4.953261852264404\n",
      "step = 3324800: loss = 4.37278938293457\n",
      "step = 3325000: loss = 4.22810697555542\n",
      "step = 3325000: Average Return = 3.7160000801086426\n",
      "step = 3325200: loss = 5.090670585632324\n",
      "step = 3325400: loss = 3.685953378677368\n",
      "step = 3325600: loss = 3.205010414123535\n",
      "step = 3325800: loss = 4.122677326202393\n",
      "step = 3326000: loss = 5.1643805503845215\n",
      "step = 3326200: loss = 4.180729866027832\n",
      "step = 3326400: loss = 4.837215900421143\n",
      "step = 3326600: loss = 4.362177848815918\n",
      "step = 3326800: loss = 5.852942943572998\n",
      "step = 3327000: loss = 4.759753704071045\n",
      "step = 3327200: loss = 5.054960250854492\n",
      "step = 3327400: loss = 5.487645149230957\n",
      "step = 3327600: loss = 4.448405742645264\n",
      "step = 3327800: loss = 4.039805889129639\n",
      "step = 3328000: loss = 4.578831672668457\n",
      "step = 3328200: loss = 5.061532020568848\n",
      "step = 3328400: loss = 4.941557884216309\n",
      "step = 3328600: loss = 4.1905059814453125\n",
      "step = 3328800: loss = 4.09574031829834\n",
      "step = 3329000: loss = 5.383183002471924\n",
      "step = 3329200: loss = 3.4025914669036865\n",
      "step = 3329400: loss = 3.3847193717956543\n",
      "step = 3329600: loss = 3.953807830810547\n",
      "step = 3329800: loss = 5.141261100769043\n",
      "step = 3330000: loss = 3.909249782562256\n",
      "step = 3330000: Average Return = 4.052000045776367\n",
      "step = 3330200: loss = 3.3542633056640625\n",
      "step = 3330400: loss = 5.951542854309082\n",
      "step = 3330600: loss = 3.6994824409484863\n",
      "step = 3330800: loss = 4.716217994689941\n",
      "step = 3331000: loss = 3.9914653301239014\n",
      "step = 3331200: loss = 5.645806789398193\n",
      "step = 3331400: loss = 3.9569151401519775\n",
      "step = 3331600: loss = 5.006566524505615\n",
      "step = 3331800: loss = 5.490942478179932\n",
      "step = 3332000: loss = 3.772688627243042\n",
      "step = 3332200: loss = 3.92657470703125\n",
      "step = 3332400: loss = 4.423995494842529\n",
      "step = 3332600: loss = 4.683578968048096\n",
      "step = 3332800: loss = 3.7835052013397217\n",
      "step = 3333000: loss = 3.298733711242676\n",
      "step = 3333200: loss = 3.3612637519836426\n",
      "step = 3333400: loss = 4.478686809539795\n",
      "step = 3333600: loss = 5.335352897644043\n",
      "step = 3333800: loss = 3.965662717819214\n",
      "step = 3334000: loss = 2.5953874588012695\n",
      "step = 3334200: loss = 5.048698425292969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3334400: loss = 4.039012908935547\n",
      "step = 3334600: loss = 3.148651361465454\n",
      "step = 3334800: loss = 2.8847033977508545\n",
      "step = 3335000: loss = 3.3169591426849365\n",
      "step = 3335000: Average Return = 3.936000108718872\n",
      "step = 3335200: loss = 4.544409275054932\n",
      "step = 3335400: loss = 4.691583633422852\n",
      "step = 3335600: loss = 3.01916241645813\n",
      "step = 3335800: loss = 3.5528600215911865\n",
      "step = 3336000: loss = 4.822474479675293\n",
      "step = 3336200: loss = 2.276752233505249\n",
      "step = 3336400: loss = 5.612685203552246\n",
      "step = 3336600: loss = 3.021550416946411\n",
      "step = 3336800: loss = 4.830492973327637\n",
      "step = 3337000: loss = 4.109214782714844\n",
      "step = 3337200: loss = 5.128211498260498\n",
      "step = 3337400: loss = 4.47838020324707\n",
      "step = 3337600: loss = 4.573136329650879\n",
      "step = 3337800: loss = 4.928555011749268\n",
      "step = 3338000: loss = 4.173975944519043\n",
      "step = 3338200: loss = 4.466869354248047\n",
      "step = 3338400: loss = 3.9045300483703613\n",
      "step = 3338600: loss = 4.8433074951171875\n",
      "step = 3338800: loss = 3.2582859992980957\n",
      "step = 3339000: loss = 5.160702228546143\n",
      "step = 3339200: loss = 3.5083959102630615\n",
      "step = 3339400: loss = 4.377254962921143\n",
      "step = 3339600: loss = 4.735918998718262\n",
      "step = 3339800: loss = 3.0773797035217285\n",
      "step = 3340000: loss = 4.865042686462402\n",
      "step = 3340000: Average Return = 3.75600004196167\n",
      "step = 3340200: loss = 5.232481479644775\n",
      "step = 3340400: loss = 3.282593250274658\n",
      "step = 3340600: loss = 4.726360321044922\n",
      "step = 3340800: loss = 4.810786247253418\n",
      "step = 3341000: loss = 3.4557244777679443\n",
      "step = 3341200: loss = 4.4714179039001465\n",
      "step = 3341400: loss = 4.503804683685303\n",
      "step = 3341600: loss = 5.31929874420166\n",
      "step = 3341800: loss = 5.242274761199951\n",
      "step = 3342000: loss = 3.991551160812378\n",
      "step = 3342200: loss = 3.6773433685302734\n",
      "step = 3342400: loss = 4.332849502563477\n",
      "step = 3342600: loss = 4.623627185821533\n",
      "step = 3342800: loss = 4.847354412078857\n",
      "step = 3343000: loss = 3.701580762863159\n",
      "step = 3343200: loss = 4.6138434410095215\n",
      "step = 3343400: loss = 4.772364616394043\n",
      "step = 3343600: loss = 4.284106254577637\n",
      "step = 3343800: loss = 5.373878002166748\n",
      "step = 3344000: loss = 4.4592485427856445\n",
      "step = 3344200: loss = 3.02079701423645\n",
      "step = 3344400: loss = 5.606813907623291\n",
      "step = 3344600: loss = 4.342865467071533\n",
      "step = 3344800: loss = 3.6849968433380127\n",
      "step = 3345000: loss = 3.697036027908325\n",
      "step = 3345000: Average Return = 4.165999889373779\n",
      "step = 3345200: loss = 4.426259517669678\n",
      "step = 3345400: loss = 3.5821168422698975\n",
      "step = 3345600: loss = 5.509865760803223\n",
      "step = 3345800: loss = 4.686675548553467\n",
      "step = 3346000: loss = 3.6796696186065674\n",
      "step = 3346200: loss = 4.499915599822998\n",
      "step = 3346400: loss = 5.74149751663208\n",
      "step = 3346600: loss = 3.827120542526245\n",
      "step = 3346800: loss = 5.858508586883545\n",
      "step = 3347000: loss = 4.562517166137695\n",
      "step = 3347200: loss = 4.172214984893799\n",
      "step = 3347400: loss = 3.478076696395874\n",
      "step = 3347600: loss = 3.823615550994873\n",
      "step = 3347800: loss = 5.240898132324219\n",
      "step = 3348000: loss = 4.8971405029296875\n",
      "step = 3348200: loss = 4.351949214935303\n",
      "step = 3348400: loss = 3.987506628036499\n",
      "step = 3348600: loss = 3.9749889373779297\n",
      "step = 3348800: loss = 4.828421115875244\n",
      "step = 3349000: loss = 2.2558281421661377\n",
      "step = 3349200: loss = 4.235388278961182\n",
      "step = 3349400: loss = 4.475825786590576\n",
      "step = 3349600: loss = 4.126096725463867\n",
      "step = 3349800: loss = 4.106114864349365\n",
      "step = 3350000: loss = 3.493403434753418\n",
      "step = 3350000: Average Return = 3.7060000896453857\n",
      "step = 3350200: loss = 4.431469917297363\n",
      "step = 3350400: loss = 5.046133518218994\n",
      "step = 3350600: loss = 3.2197227478027344\n",
      "step = 3350800: loss = 4.875555992126465\n",
      "step = 3351000: loss = 4.102555751800537\n",
      "step = 3351200: loss = 4.109209060668945\n",
      "step = 3351400: loss = 3.051525115966797\n",
      "step = 3351600: loss = 3.309213399887085\n",
      "step = 3351800: loss = 4.574641704559326\n",
      "step = 3352000: loss = 3.875392436981201\n",
      "step = 3352200: loss = 4.620584487915039\n",
      "step = 3352400: loss = 5.089341163635254\n",
      "step = 3352600: loss = 4.804681777954102\n",
      "step = 3352800: loss = 5.143637657165527\n",
      "step = 3353000: loss = 3.6143925189971924\n",
      "step = 3353200: loss = 4.147799491882324\n",
      "step = 3353400: loss = 4.484799861907959\n",
      "step = 3353600: loss = 4.324053764343262\n",
      "step = 3353800: loss = 5.079415798187256\n",
      "step = 3354000: loss = 4.374205112457275\n",
      "step = 3354200: loss = 6.840240478515625\n",
      "step = 3354400: loss = 3.7416253089904785\n",
      "step = 3354600: loss = 5.454614162445068\n",
      "step = 3354800: loss = 3.8301994800567627\n",
      "step = 3355000: loss = 4.9173479080200195\n",
      "step = 3355000: Average Return = 4.144000053405762\n",
      "step = 3355200: loss = 2.8516132831573486\n",
      "step = 3355400: loss = 5.330578327178955\n",
      "step = 3355600: loss = 4.813539981842041\n",
      "step = 3355800: loss = 3.2220723628997803\n",
      "step = 3356000: loss = 3.5637786388397217\n",
      "step = 3356200: loss = 3.4028520584106445\n",
      "step = 3356400: loss = 4.256687164306641\n",
      "step = 3356600: loss = 3.645282506942749\n",
      "step = 3356800: loss = 4.089743614196777\n",
      "step = 3357000: loss = 4.41200065612793\n",
      "step = 3357200: loss = 5.523055076599121\n",
      "step = 3357400: loss = 3.89892315864563\n",
      "step = 3357600: loss = 4.057051181793213\n",
      "step = 3357800: loss = 4.130129814147949\n",
      "step = 3358000: loss = 4.747161865234375\n",
      "step = 3358200: loss = 4.268698692321777\n",
      "step = 3358400: loss = 3.7003166675567627\n",
      "step = 3358600: loss = 3.174654722213745\n",
      "step = 3358800: loss = 3.775937795639038\n",
      "step = 3359000: loss = 5.061594486236572\n",
      "step = 3359200: loss = 4.825681686401367\n",
      "step = 3359400: loss = 6.452062606811523\n",
      "step = 3359600: loss = 4.026983737945557\n",
      "step = 3359800: loss = 3.883770704269409\n",
      "step = 3360000: loss = 4.54828405380249\n",
      "step = 3360000: Average Return = 4.070000171661377\n",
      "step = 3360200: loss = 4.5555853843688965\n",
      "step = 3360400: loss = 2.716935634613037\n",
      "step = 3360600: loss = 3.844010591506958\n",
      "step = 3360800: loss = 5.227227687835693\n",
      "step = 3361000: loss = 4.164088249206543\n",
      "step = 3361200: loss = 4.359463214874268\n",
      "step = 3361400: loss = 4.859591007232666\n",
      "step = 3361600: loss = 4.771462917327881\n",
      "step = 3361800: loss = 4.064882278442383\n",
      "step = 3362000: loss = 4.399489402770996\n",
      "step = 3362200: loss = 3.621410369873047\n",
      "step = 3362400: loss = 4.563391208648682\n",
      "step = 3362600: loss = 4.724562644958496\n",
      "step = 3362800: loss = 3.8950371742248535\n",
      "step = 3363000: loss = 3.547088146209717\n",
      "step = 3363200: loss = 3.7507498264312744\n",
      "step = 3363400: loss = 2.6156814098358154\n",
      "step = 3363600: loss = 4.629229545593262\n",
      "step = 3363800: loss = 4.7127814292907715\n",
      "step = 3364000: loss = 3.0237040519714355\n",
      "step = 3364200: loss = 4.17480993270874\n",
      "step = 3364400: loss = 5.370865345001221\n",
      "step = 3364600: loss = 3.1375091075897217\n",
      "step = 3364800: loss = 4.803300857543945\n",
      "step = 3365000: loss = 4.325817584991455\n",
      "step = 3365000: Average Return = 3.5820000171661377\n",
      "step = 3365200: loss = 3.802765130996704\n",
      "step = 3365400: loss = 3.761647939682007\n",
      "step = 3365600: loss = 3.8284835815429688\n",
      "step = 3365800: loss = 4.168423175811768\n",
      "step = 3366000: loss = 2.4090962409973145\n",
      "step = 3366200: loss = 5.107442378997803\n",
      "step = 3366400: loss = 3.6830108165740967\n",
      "step = 3366600: loss = 4.447054386138916\n",
      "step = 3366800: loss = 4.185459613800049\n",
      "step = 3367000: loss = 4.912452697753906\n",
      "step = 3367200: loss = 3.6937735080718994\n",
      "step = 3367400: loss = 3.4273626804351807\n",
      "step = 3367600: loss = 5.097649574279785\n",
      "step = 3367800: loss = 4.385472297668457\n",
      "step = 3368000: loss = 3.1207115650177\n",
      "step = 3368200: loss = 3.258127450942993\n",
      "step = 3368400: loss = 3.6294806003570557\n",
      "step = 3368600: loss = 3.9890544414520264\n",
      "step = 3368800: loss = 3.994671583175659\n",
      "step = 3369000: loss = 4.841610431671143\n",
      "step = 3369200: loss = 4.551039695739746\n",
      "step = 3369400: loss = 4.593446731567383\n",
      "step = 3369600: loss = 3.747631072998047\n",
      "step = 3369800: loss = 4.6675333976745605\n",
      "step = 3370000: loss = 3.9939022064208984\n",
      "step = 3370000: Average Return = 3.819999933242798\n",
      "step = 3370200: loss = 4.977595806121826\n",
      "step = 3370400: loss = 4.209636211395264\n",
      "step = 3370600: loss = 3.845248222351074\n",
      "step = 3370800: loss = 4.505909442901611\n",
      "step = 3371000: loss = 5.359340667724609\n",
      "step = 3371200: loss = 4.588685989379883\n",
      "step = 3371400: loss = 4.070862770080566\n",
      "step = 3371600: loss = 3.8814876079559326\n",
      "step = 3371800: loss = 3.103827953338623\n",
      "step = 3372000: loss = 4.605260372161865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3372200: loss = 4.444877624511719\n",
      "step = 3372400: loss = 4.427132606506348\n",
      "step = 3372600: loss = 4.939191818237305\n",
      "step = 3372800: loss = 4.198785781860352\n",
      "step = 3373000: loss = 4.791499614715576\n",
      "step = 3373200: loss = 4.487236022949219\n",
      "step = 3373400: loss = 3.736835479736328\n",
      "step = 3373600: loss = 4.601565837860107\n",
      "step = 3373800: loss = 4.030467510223389\n",
      "step = 3374000: loss = 5.221182823181152\n",
      "step = 3374200: loss = 4.704526424407959\n",
      "step = 3374400: loss = 3.3319406509399414\n",
      "step = 3374600: loss = 4.541637897491455\n",
      "step = 3374800: loss = 6.111550807952881\n",
      "step = 3375000: loss = 4.256559371948242\n",
      "step = 3375000: Average Return = 3.7200000286102295\n",
      "step = 3375200: loss = 3.213594913482666\n",
      "step = 3375400: loss = 3.0774285793304443\n",
      "step = 3375600: loss = 3.7910897731781006\n",
      "step = 3375800: loss = 4.024170398712158\n",
      "step = 3376000: loss = 4.071382999420166\n",
      "step = 3376200: loss = 5.388128757476807\n",
      "step = 3376400: loss = 4.197622776031494\n",
      "step = 3376600: loss = 5.866578102111816\n",
      "step = 3376800: loss = 4.660208702087402\n",
      "step = 3377000: loss = 4.444549083709717\n",
      "step = 3377200: loss = 4.35370397567749\n",
      "step = 3377400: loss = 3.2089779376983643\n",
      "step = 3377600: loss = 5.0650954246521\n",
      "step = 3377800: loss = 4.011466979980469\n",
      "step = 3378000: loss = 4.228395462036133\n",
      "step = 3378200: loss = 5.918396949768066\n",
      "step = 3378400: loss = 4.580347061157227\n",
      "step = 3378600: loss = 4.454228401184082\n",
      "step = 3378800: loss = 4.933321475982666\n",
      "step = 3379000: loss = 5.884315013885498\n",
      "step = 3379200: loss = 4.6307268142700195\n",
      "step = 3379400: loss = 5.398278713226318\n",
      "step = 3379600: loss = 5.427055358886719\n",
      "step = 3379800: loss = 3.3477373123168945\n",
      "step = 3380000: loss = 5.685832500457764\n",
      "step = 3380000: Average Return = 3.7880001068115234\n",
      "step = 3380200: loss = 3.6743011474609375\n",
      "step = 3380400: loss = 3.4754533767700195\n",
      "step = 3380600: loss = 3.40850830078125\n",
      "step = 3380800: loss = 3.956746816635132\n",
      "step = 3381000: loss = 3.4353463649749756\n",
      "step = 3381200: loss = 3.4282145500183105\n",
      "step = 3381400: loss = 4.530871868133545\n",
      "step = 3381600: loss = 4.075096130371094\n",
      "step = 3381800: loss = 5.853004455566406\n",
      "step = 3382000: loss = 2.704669713973999\n",
      "step = 3382200: loss = 6.054294109344482\n",
      "step = 3382400: loss = 4.270488739013672\n",
      "step = 3382600: loss = 5.502074241638184\n",
      "step = 3382800: loss = 5.702421188354492\n",
      "step = 3383000: loss = 4.140665054321289\n",
      "step = 3383200: loss = 4.33115291595459\n",
      "step = 3383400: loss = 4.245614528656006\n",
      "step = 3383600: loss = 5.398535251617432\n",
      "step = 3383800: loss = 3.1079962253570557\n",
      "step = 3384000: loss = 3.798607110977173\n",
      "step = 3384200: loss = 5.11307430267334\n",
      "step = 3384400: loss = 3.0066821575164795\n",
      "step = 3384600: loss = 5.940764427185059\n",
      "step = 3384800: loss = 4.013540744781494\n",
      "step = 3385000: loss = 4.174953460693359\n",
      "step = 3385000: Average Return = 3.5260000228881836\n",
      "step = 3385200: loss = 4.328218460083008\n",
      "step = 3385400: loss = 2.553994655609131\n",
      "step = 3385600: loss = 4.536603927612305\n",
      "step = 3385800: loss = 4.181186199188232\n",
      "step = 3386000: loss = 4.004544258117676\n",
      "step = 3386200: loss = 3.379230260848999\n",
      "step = 3386400: loss = 3.988529682159424\n",
      "step = 3386600: loss = 5.163424968719482\n",
      "step = 3386800: loss = 5.021174907684326\n",
      "step = 3387000: loss = 3.800661563873291\n",
      "step = 3387200: loss = 3.336467742919922\n",
      "step = 3387400: loss = 3.123422861099243\n",
      "step = 3387600: loss = 3.119952440261841\n",
      "step = 3387800: loss = 3.7470786571502686\n",
      "step = 3388000: loss = 4.422739028930664\n",
      "step = 3388200: loss = 4.537189483642578\n",
      "step = 3388400: loss = 3.2916553020477295\n",
      "step = 3388600: loss = 4.758655071258545\n",
      "step = 3388800: loss = 5.661158084869385\n",
      "step = 3389000: loss = 4.126378059387207\n",
      "step = 3389200: loss = 3.859257936477661\n",
      "step = 3389400: loss = 4.353143215179443\n",
      "step = 3389600: loss = 6.234960079193115\n",
      "step = 3389800: loss = 4.850758075714111\n",
      "step = 3390000: loss = 3.758974552154541\n",
      "step = 3390000: Average Return = 3.7880001068115234\n",
      "step = 3390200: loss = 3.823000431060791\n",
      "step = 3390400: loss = 2.94003963470459\n",
      "step = 3390600: loss = 4.463169574737549\n",
      "step = 3390800: loss = 4.869831085205078\n",
      "step = 3391000: loss = 4.25004243850708\n",
      "step = 3391200: loss = 4.824234485626221\n",
      "step = 3391400: loss = 3.9598336219787598\n",
      "step = 3391600: loss = 3.269846200942993\n",
      "step = 3391800: loss = 4.127836227416992\n",
      "step = 3392000: loss = 5.584167957305908\n",
      "step = 3392200: loss = 2.3981449604034424\n",
      "step = 3392400: loss = 3.732800245285034\n",
      "step = 3392600: loss = 4.711981296539307\n",
      "step = 3392800: loss = 3.233452081680298\n",
      "step = 3393000: loss = 3.3247854709625244\n",
      "step = 3393200: loss = 3.7765090465545654\n",
      "step = 3393400: loss = 4.349093914031982\n",
      "step = 3393600: loss = 3.8462166786193848\n",
      "step = 3393800: loss = 4.304934978485107\n",
      "step = 3394000: loss = 5.196298599243164\n",
      "step = 3394200: loss = 4.115811824798584\n",
      "step = 3394400: loss = 3.9012060165405273\n",
      "step = 3394600: loss = 3.2172558307647705\n",
      "step = 3394800: loss = 3.660254955291748\n",
      "step = 3395000: loss = 4.5612359046936035\n",
      "step = 3395000: Average Return = 3.8980000019073486\n",
      "step = 3395200: loss = 4.093359470367432\n",
      "step = 3395400: loss = 3.711832284927368\n",
      "step = 3395600: loss = 3.9297115802764893\n",
      "step = 3395800: loss = 4.5597357749938965\n",
      "step = 3396000: loss = 3.8302059173583984\n",
      "step = 3396200: loss = 4.525078773498535\n",
      "step = 3396400: loss = 3.167886257171631\n",
      "step = 3396600: loss = 5.768938064575195\n",
      "step = 3396800: loss = 4.1359405517578125\n",
      "step = 3397000: loss = 3.5295464992523193\n",
      "step = 3397200: loss = 5.295630931854248\n",
      "step = 3397400: loss = 4.3170905113220215\n",
      "step = 3397600: loss = 3.77044677734375\n",
      "step = 3397800: loss = 3.6303257942199707\n",
      "step = 3398000: loss = 5.215320110321045\n",
      "step = 3398200: loss = 6.04569673538208\n",
      "step = 3398400: loss = 4.540832042694092\n",
      "step = 3398600: loss = 3.9149622917175293\n",
      "step = 3398800: loss = 3.6868467330932617\n",
      "step = 3399000: loss = 3.5963830947875977\n",
      "step = 3399200: loss = 4.018993377685547\n",
      "step = 3399400: loss = 3.2948927879333496\n",
      "step = 3399600: loss = 3.5749611854553223\n",
      "step = 3399800: loss = 4.353971004486084\n",
      "step = 3400000: loss = 4.787055969238281\n",
      "step = 3400000: Average Return = 3.890000104904175\n",
      "step = 3400200: loss = 5.346402645111084\n",
      "step = 3400400: loss = 3.8312652111053467\n",
      "step = 3400600: loss = 4.7256646156311035\n",
      "step = 3400800: loss = 2.8825602531433105\n",
      "step = 3401000: loss = 4.382019996643066\n",
      "step = 3401200: loss = 4.007220268249512\n",
      "step = 3401400: loss = 4.294454097747803\n",
      "step = 3401600: loss = 5.116736888885498\n",
      "step = 3401800: loss = 3.4527294635772705\n",
      "step = 3402000: loss = 5.555887222290039\n",
      "step = 3402200: loss = 3.744468927383423\n",
      "step = 3402400: loss = 5.228262901306152\n",
      "step = 3402600: loss = 6.298348903656006\n",
      "step = 3402800: loss = 2.8894262313842773\n",
      "step = 3403000: loss = 4.1863555908203125\n",
      "step = 3403200: loss = 4.134137153625488\n",
      "step = 3403400: loss = 4.3965067863464355\n",
      "step = 3403600: loss = 4.372541904449463\n",
      "step = 3403800: loss = 5.3454999923706055\n",
      "step = 3404000: loss = 5.259530067443848\n",
      "step = 3404200: loss = 4.452512264251709\n",
      "step = 3404400: loss = 5.290050029754639\n",
      "step = 3404600: loss = 2.952279806137085\n",
      "step = 3404800: loss = 3.8963406085968018\n",
      "step = 3405000: loss = 4.820656776428223\n",
      "step = 3405000: Average Return = 3.8959999084472656\n",
      "step = 3405200: loss = 5.219069480895996\n",
      "step = 3405400: loss = 5.305787086486816\n",
      "step = 3405600: loss = 4.091255187988281\n",
      "step = 3405800: loss = 3.506457805633545\n",
      "step = 3406000: loss = 4.241225719451904\n",
      "step = 3406200: loss = 4.327023506164551\n",
      "step = 3406400: loss = 3.297041654586792\n",
      "step = 3406600: loss = 3.8041951656341553\n",
      "step = 3406800: loss = 2.766873836517334\n",
      "step = 3407000: loss = 4.09976053237915\n",
      "step = 3407200: loss = 4.426266193389893\n",
      "step = 3407400: loss = 4.815415859222412\n",
      "step = 3407600: loss = 3.701414108276367\n",
      "step = 3407800: loss = 4.997706413269043\n",
      "step = 3408000: loss = 3.8727848529815674\n",
      "step = 3408200: loss = 4.1951904296875\n",
      "step = 3408400: loss = 3.557990550994873\n",
      "step = 3408600: loss = 4.82777214050293\n",
      "step = 3408800: loss = 3.5930657386779785\n",
      "step = 3409000: loss = 4.092209815979004\n",
      "step = 3409200: loss = 4.288337230682373\n",
      "step = 3409400: loss = 4.590151786804199\n",
      "step = 3409600: loss = 5.243319034576416\n",
      "step = 3409800: loss = 4.2056074142456055\n",
      "step = 3410000: loss = 4.403140068054199\n",
      "step = 3410000: Average Return = 3.9860000610351562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3410200: loss = 3.0457849502563477\n",
      "step = 3410400: loss = 3.22575044631958\n",
      "step = 3410600: loss = 4.869792461395264\n",
      "step = 3410800: loss = 3.9483892917633057\n",
      "step = 3411000: loss = 3.236063241958618\n",
      "step = 3411200: loss = 5.887115478515625\n",
      "step = 3411400: loss = 4.652999401092529\n",
      "step = 3411600: loss = 6.241578102111816\n",
      "step = 3411800: loss = 4.343166351318359\n",
      "step = 3412000: loss = 3.084183692932129\n",
      "step = 3412200: loss = 3.974999189376831\n",
      "step = 3412400: loss = 2.9470231533050537\n",
      "step = 3412600: loss = 4.636295795440674\n",
      "step = 3412800: loss = 3.6318888664245605\n",
      "step = 3413000: loss = 3.6772234439849854\n",
      "step = 3413200: loss = 5.234076023101807\n",
      "step = 3413400: loss = 6.38594388961792\n",
      "step = 3413600: loss = 3.428611993789673\n",
      "step = 3413800: loss = 4.361933708190918\n",
      "step = 3414000: loss = 5.664433479309082\n",
      "step = 3414200: loss = 4.082565784454346\n",
      "step = 3414400: loss = 3.6819310188293457\n",
      "step = 3414600: loss = 3.714989423751831\n",
      "step = 3414800: loss = 4.889102458953857\n",
      "step = 3415000: loss = 5.521446228027344\n",
      "step = 3415000: Average Return = 3.6480000019073486\n",
      "step = 3415200: loss = 3.676039934158325\n",
      "step = 3415400: loss = 5.242336273193359\n",
      "step = 3415600: loss = 4.285629749298096\n",
      "step = 3415800: loss = 4.041168212890625\n",
      "step = 3416000: loss = 3.956373453140259\n",
      "step = 3416200: loss = 2.5794153213500977\n",
      "step = 3416400: loss = 5.023496627807617\n",
      "step = 3416600: loss = 4.686357021331787\n",
      "step = 3416800: loss = 2.9982495307922363\n",
      "step = 3417000: loss = 4.728567123413086\n",
      "step = 3417200: loss = 4.238348484039307\n",
      "step = 3417400: loss = 3.786226987838745\n",
      "step = 3417600: loss = 2.755066394805908\n",
      "step = 3417800: loss = 5.389346599578857\n",
      "step = 3418000: loss = 4.060153484344482\n",
      "step = 3418200: loss = 4.322074890136719\n",
      "step = 3418400: loss = 3.810845136642456\n",
      "step = 3418600: loss = 3.7305266857147217\n",
      "step = 3418800: loss = 5.983972072601318\n",
      "step = 3419000: loss = 3.795175313949585\n",
      "step = 3419200: loss = 4.192808151245117\n",
      "step = 3419400: loss = 4.4727935791015625\n",
      "step = 3419600: loss = 3.9149913787841797\n",
      "step = 3419800: loss = 3.799591541290283\n",
      "step = 3420000: loss = 3.48445463180542\n",
      "step = 3420000: Average Return = 3.628000020980835\n",
      "step = 3420200: loss = 5.02870512008667\n",
      "step = 3420400: loss = 3.6399214267730713\n",
      "step = 3420600: loss = 4.823823928833008\n",
      "step = 3420800: loss = 5.768217086791992\n",
      "step = 3421000: loss = 4.2332024574279785\n",
      "step = 3421200: loss = 4.7996826171875\n",
      "step = 3421400: loss = 4.999367713928223\n",
      "step = 3421600: loss = 3.887673854827881\n",
      "step = 3421800: loss = 3.702538251876831\n",
      "step = 3422000: loss = 6.135659217834473\n",
      "step = 3422200: loss = 4.729881763458252\n",
      "step = 3422400: loss = 3.423738956451416\n",
      "step = 3422600: loss = 4.721521854400635\n",
      "step = 3422800: loss = 4.042122840881348\n",
      "step = 3423000: loss = 4.090898513793945\n",
      "step = 3423200: loss = 2.617452383041382\n",
      "step = 3423400: loss = 4.2034783363342285\n",
      "step = 3423600: loss = 4.249471664428711\n",
      "step = 3423800: loss = 3.8020546436309814\n",
      "step = 3424000: loss = 2.7868199348449707\n",
      "step = 3424200: loss = 4.127500057220459\n",
      "step = 3424400: loss = 4.717045307159424\n",
      "step = 3424600: loss = 3.4402384757995605\n",
      "step = 3424800: loss = 4.366304397583008\n",
      "step = 3425000: loss = 3.616990089416504\n",
      "step = 3425000: Average Return = 3.700000047683716\n",
      "step = 3425200: loss = 3.5066094398498535\n",
      "step = 3425400: loss = 3.1866273880004883\n",
      "step = 3425600: loss = 3.6459832191467285\n",
      "step = 3425800: loss = 4.714532375335693\n",
      "step = 3426000: loss = 4.070723056793213\n",
      "step = 3426200: loss = 4.301897048950195\n",
      "step = 3426400: loss = 3.8277735710144043\n",
      "step = 3426600: loss = 3.783389091491699\n",
      "step = 3426800: loss = 5.614340782165527\n",
      "step = 3427000: loss = 3.799147367477417\n",
      "step = 3427200: loss = 3.7223925590515137\n",
      "step = 3427400: loss = 4.412958145141602\n",
      "step = 3427600: loss = 4.921970844268799\n",
      "step = 3427800: loss = 4.631117343902588\n",
      "step = 3428000: loss = 3.9463603496551514\n",
      "step = 3428200: loss = 4.498400688171387\n",
      "step = 3428400: loss = 3.0868306159973145\n",
      "step = 3428600: loss = 4.805631160736084\n",
      "step = 3428800: loss = 3.7950387001037598\n",
      "step = 3429000: loss = 3.605659008026123\n",
      "step = 3429200: loss = 3.218205451965332\n",
      "step = 3429400: loss = 5.918955326080322\n",
      "step = 3429600: loss = 3.1090734004974365\n",
      "step = 3429800: loss = 4.289830684661865\n",
      "step = 3430000: loss = 4.105140686035156\n",
      "step = 3430000: Average Return = 3.874000072479248\n",
      "step = 3430200: loss = 4.231417655944824\n",
      "step = 3430400: loss = 3.7468461990356445\n",
      "step = 3430600: loss = 4.451361179351807\n",
      "step = 3430800: loss = 4.122007369995117\n",
      "step = 3431000: loss = 3.9493284225463867\n",
      "step = 3431200: loss = 4.02681827545166\n",
      "step = 3431400: loss = 4.165130615234375\n",
      "step = 3431600: loss = 3.0851244926452637\n",
      "step = 3431800: loss = 4.647945880889893\n",
      "step = 3432000: loss = 5.716380596160889\n",
      "step = 3432200: loss = 3.9181621074676514\n",
      "step = 3432400: loss = 3.0524675846099854\n",
      "step = 3432600: loss = 3.667383909225464\n",
      "step = 3432800: loss = 3.909857749938965\n",
      "step = 3433000: loss = 4.315690040588379\n",
      "step = 3433200: loss = 3.746103525161743\n",
      "step = 3433400: loss = 3.2831215858459473\n",
      "step = 3433600: loss = 4.5859904289245605\n",
      "step = 3433800: loss = 3.8966851234436035\n",
      "step = 3434000: loss = 3.183640718460083\n",
      "step = 3434200: loss = 3.483262300491333\n",
      "step = 3434400: loss = 4.482913017272949\n",
      "step = 3434600: loss = 4.740437984466553\n",
      "step = 3434800: loss = 3.8490960597991943\n",
      "step = 3435000: loss = 4.3653564453125\n",
      "step = 3435000: Average Return = 3.927999973297119\n",
      "step = 3435200: loss = 4.4640631675720215\n",
      "step = 3435400: loss = 2.834484100341797\n",
      "step = 3435600: loss = 4.1403422355651855\n",
      "step = 3435800: loss = 4.280028820037842\n",
      "step = 3436000: loss = 3.8342888355255127\n",
      "step = 3436200: loss = 5.84455680847168\n",
      "step = 3436400: loss = 3.453261613845825\n",
      "step = 3436600: loss = 4.15604829788208\n",
      "step = 3436800: loss = 3.8447651863098145\n",
      "step = 3437000: loss = 3.9587292671203613\n",
      "step = 3437200: loss = 4.730280876159668\n",
      "step = 3437400: loss = 5.857339382171631\n",
      "step = 3437600: loss = 5.16223669052124\n",
      "step = 3437800: loss = 5.142045974731445\n",
      "step = 3438000: loss = 3.9163951873779297\n",
      "step = 3438200: loss = 4.1460065841674805\n",
      "step = 3438400: loss = 6.455623626708984\n",
      "step = 3438600: loss = 3.5482678413391113\n",
      "step = 3438800: loss = 5.212684154510498\n",
      "step = 3439000: loss = 3.9407365322113037\n",
      "step = 3439200: loss = 3.536146402359009\n",
      "step = 3439400: loss = 4.755181312561035\n",
      "step = 3439600: loss = 4.240350723266602\n",
      "step = 3439800: loss = 3.7749316692352295\n",
      "step = 3440000: loss = 3.7778375148773193\n",
      "step = 3440000: Average Return = 3.7699999809265137\n",
      "step = 3440200: loss = 4.259189128875732\n",
      "step = 3440400: loss = 4.314815521240234\n",
      "step = 3440600: loss = 3.490000009536743\n",
      "step = 3440800: loss = 4.360476493835449\n",
      "step = 3441000: loss = 3.210707426071167\n",
      "step = 3441200: loss = 3.066972494125366\n",
      "step = 3441400: loss = 4.074402809143066\n",
      "step = 3441600: loss = 4.47749662399292\n",
      "step = 3441800: loss = 3.38525128364563\n",
      "step = 3442000: loss = 4.486500263214111\n",
      "step = 3442200: loss = 5.095735549926758\n",
      "step = 3442400: loss = 4.350142955780029\n",
      "step = 3442600: loss = 4.430339336395264\n",
      "step = 3442800: loss = 5.191267490386963\n",
      "step = 3443000: loss = 4.513628959655762\n",
      "step = 3443200: loss = 3.5050783157348633\n",
      "step = 3443400: loss = 2.7812936305999756\n",
      "step = 3443600: loss = 3.956944465637207\n",
      "step = 3443800: loss = 4.4792375564575195\n",
      "step = 3444000: loss = 4.612045764923096\n",
      "step = 3444200: loss = 5.056460380554199\n",
      "step = 3444400: loss = 4.2735114097595215\n",
      "step = 3444600: loss = 4.659574031829834\n",
      "step = 3444800: loss = 4.827786445617676\n",
      "step = 3445000: loss = 4.298219680786133\n",
      "step = 3445000: Average Return = 3.694000005722046\n",
      "step = 3445200: loss = 4.523256301879883\n",
      "step = 3445400: loss = 4.248417377471924\n",
      "step = 3445600: loss = 2.4917755126953125\n",
      "step = 3445800: loss = 4.46455717086792\n",
      "step = 3446000: loss = 3.4948580265045166\n",
      "step = 3446200: loss = 4.947678565979004\n",
      "step = 3446400: loss = 5.403131008148193\n",
      "step = 3446600: loss = 4.535329341888428\n",
      "step = 3446800: loss = 4.07559871673584\n",
      "step = 3447000: loss = 3.6887712478637695\n",
      "step = 3447200: loss = 4.670100212097168\n",
      "step = 3447400: loss = 5.731192588806152\n",
      "step = 3447600: loss = 5.851662635803223\n",
      "step = 3447800: loss = 3.1112735271453857\n",
      "step = 3448000: loss = 3.05621337890625\n",
      "step = 3448200: loss = 4.518305778503418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3448400: loss = 6.034388065338135\n",
      "step = 3448600: loss = 3.748967170715332\n",
      "step = 3448800: loss = 2.5089821815490723\n",
      "step = 3449000: loss = 3.131290912628174\n",
      "step = 3449200: loss = 2.8859617710113525\n",
      "step = 3449400: loss = 4.253506183624268\n",
      "step = 3449600: loss = 4.335007667541504\n",
      "step = 3449800: loss = 4.196760654449463\n",
      "step = 3450000: loss = 3.596400260925293\n",
      "step = 3450000: Average Return = 3.7799999713897705\n",
      "step = 3450200: loss = 3.901709794998169\n",
      "step = 3450400: loss = 4.1845550537109375\n",
      "step = 3450600: loss = 3.743504047393799\n",
      "step = 3450800: loss = 3.984032392501831\n",
      "step = 3451000: loss = 5.7957000732421875\n",
      "step = 3451200: loss = 3.4233803749084473\n",
      "step = 3451400: loss = 4.884047508239746\n",
      "step = 3451600: loss = 3.788177013397217\n",
      "step = 3451800: loss = 5.063995361328125\n",
      "step = 3452000: loss = 3.302323579788208\n",
      "step = 3452200: loss = 4.888375282287598\n",
      "step = 3452400: loss = 4.732109546661377\n",
      "step = 3452600: loss = 4.639756202697754\n",
      "step = 3452800: loss = 2.481943368911743\n",
      "step = 3453000: loss = 3.6137285232543945\n",
      "step = 3453200: loss = 3.9826159477233887\n",
      "step = 3453400: loss = 4.426880359649658\n",
      "step = 3453600: loss = 2.6765661239624023\n",
      "step = 3453800: loss = 4.468472003936768\n",
      "step = 3454000: loss = 2.953193187713623\n",
      "step = 3454200: loss = 3.7719836235046387\n",
      "step = 3454400: loss = 3.8545892238616943\n",
      "step = 3454600: loss = 3.719816207885742\n",
      "step = 3454800: loss = 4.4197540283203125\n",
      "step = 3455000: loss = 4.271075248718262\n",
      "step = 3455000: Average Return = 3.6459999084472656\n",
      "step = 3455200: loss = 3.956618070602417\n",
      "step = 3455400: loss = 4.200516700744629\n",
      "step = 3455600: loss = 5.034928798675537\n",
      "step = 3455800: loss = 4.408332824707031\n",
      "step = 3456000: loss = 4.637253761291504\n",
      "step = 3456200: loss = 4.370115756988525\n",
      "step = 3456400: loss = 5.076871871948242\n",
      "step = 3456600: loss = 4.4137678146362305\n",
      "step = 3456800: loss = 2.434682846069336\n",
      "step = 3457000: loss = 5.29986572265625\n",
      "step = 3457200: loss = 5.01483154296875\n",
      "step = 3457400: loss = 3.435267925262451\n",
      "step = 3457600: loss = 3.8594424724578857\n",
      "step = 3457800: loss = 4.776340007781982\n",
      "step = 3458000: loss = 4.987910270690918\n",
      "step = 3458200: loss = 4.981196880340576\n",
      "step = 3458400: loss = 3.815054178237915\n",
      "step = 3458600: loss = 3.553285837173462\n",
      "step = 3458800: loss = 4.688896179199219\n",
      "step = 3459000: loss = 4.56016206741333\n",
      "step = 3459200: loss = 4.493361473083496\n",
      "step = 3459400: loss = 4.236176013946533\n",
      "step = 3459600: loss = 4.112606525421143\n",
      "step = 3459800: loss = 3.468738317489624\n",
      "step = 3460000: loss = 3.583021402359009\n",
      "step = 3460000: Average Return = 3.3540000915527344\n",
      "step = 3460200: loss = 4.661166667938232\n",
      "step = 3460400: loss = 2.3385021686553955\n",
      "step = 3460600: loss = 5.521190643310547\n",
      "step = 3460800: loss = 4.599084377288818\n",
      "step = 3461000: loss = 4.385278224945068\n",
      "step = 3461200: loss = 3.6987998485565186\n",
      "step = 3461400: loss = 5.60400915145874\n",
      "step = 3461600: loss = 4.170550346374512\n",
      "step = 3461800: loss = 4.1610002517700195\n",
      "step = 3462000: loss = 3.1195411682128906\n",
      "step = 3462200: loss = 5.253117084503174\n",
      "step = 3462400: loss = 2.7108588218688965\n",
      "step = 3462600: loss = 3.75072979927063\n",
      "step = 3462800: loss = 5.150124549865723\n",
      "step = 3463000: loss = 4.634955883026123\n",
      "step = 3463200: loss = 3.6921496391296387\n",
      "step = 3463400: loss = 3.243943452835083\n",
      "step = 3463600: loss = 4.792301654815674\n",
      "step = 3463800: loss = 2.3968868255615234\n",
      "step = 3464000: loss = 4.208772659301758\n",
      "step = 3464200: loss = 4.716968059539795\n",
      "step = 3464400: loss = 3.7553060054779053\n",
      "step = 3464600: loss = 4.019633769989014\n",
      "step = 3464800: loss = 4.065624237060547\n",
      "step = 3465000: loss = 4.105835914611816\n",
      "step = 3465000: Average Return = 3.696000099182129\n",
      "step = 3465200: loss = 3.8446853160858154\n",
      "step = 3465400: loss = 2.499337673187256\n",
      "step = 3465600: loss = 4.5154242515563965\n",
      "step = 3465800: loss = 4.2574262619018555\n",
      "step = 3466000: loss = 4.537550449371338\n",
      "step = 3466200: loss = 3.137579917907715\n",
      "step = 3466400: loss = 4.797019958496094\n",
      "step = 3466600: loss = 3.6682043075561523\n",
      "step = 3466800: loss = 4.832056045532227\n",
      "step = 3467000: loss = 4.129431247711182\n",
      "step = 3467200: loss = 4.133554458618164\n",
      "step = 3467400: loss = 5.449504375457764\n",
      "step = 3467600: loss = 5.297789096832275\n",
      "step = 3467800: loss = 3.6716790199279785\n",
      "step = 3468000: loss = 4.4208984375\n",
      "step = 3468200: loss = 3.683842182159424\n",
      "step = 3468400: loss = 3.57421612739563\n",
      "step = 3468600: loss = 4.707199573516846\n",
      "step = 3468800: loss = 2.42744779586792\n",
      "step = 3469000: loss = 5.022278308868408\n",
      "step = 3469200: loss = 2.668208599090576\n",
      "step = 3469400: loss = 3.6321771144866943\n",
      "step = 3469600: loss = 4.115900993347168\n",
      "step = 3469800: loss = 4.948493003845215\n",
      "step = 3470000: loss = 4.296164512634277\n",
      "step = 3470000: Average Return = 3.805999994277954\n",
      "step = 3470200: loss = 2.723079204559326\n",
      "step = 3470400: loss = 3.504408597946167\n",
      "step = 3470600: loss = 4.859357833862305\n",
      "step = 3470800: loss = 4.364682197570801\n",
      "step = 3471000: loss = 3.978410243988037\n",
      "step = 3471200: loss = 3.465564489364624\n",
      "step = 3471400: loss = 3.5354199409484863\n",
      "step = 3471600: loss = 5.49253511428833\n",
      "step = 3471800: loss = 3.4206318855285645\n",
      "step = 3472000: loss = 3.842122793197632\n",
      "step = 3472200: loss = 4.823552131652832\n",
      "step = 3472400: loss = 4.5885162353515625\n",
      "step = 3472600: loss = 4.113436698913574\n",
      "step = 3472800: loss = 4.545565605163574\n",
      "step = 3473000: loss = 4.99904727935791\n",
      "step = 3473200: loss = 4.216007709503174\n",
      "step = 3473400: loss = 4.78958797454834\n",
      "step = 3473600: loss = 5.818729877471924\n",
      "step = 3473800: loss = 4.229460716247559\n",
      "step = 3474000: loss = 4.96773099899292\n",
      "step = 3474200: loss = 4.399415493011475\n",
      "step = 3474400: loss = 3.5904998779296875\n",
      "step = 3474600: loss = 4.329023361206055\n",
      "step = 3474800: loss = 5.296171188354492\n",
      "step = 3475000: loss = 3.532825231552124\n",
      "step = 3475000: Average Return = 3.9519999027252197\n",
      "step = 3475200: loss = 3.560671091079712\n",
      "step = 3475400: loss = 4.502946853637695\n",
      "step = 3475600: loss = 4.532797336578369\n",
      "step = 3475800: loss = 3.1016862392425537\n",
      "step = 3476000: loss = 4.471887588500977\n",
      "step = 3476200: loss = 4.000784873962402\n",
      "step = 3476400: loss = 5.088738441467285\n",
      "step = 3476600: loss = 4.024834632873535\n",
      "step = 3476800: loss = 2.828763961791992\n",
      "step = 3477000: loss = 3.4740867614746094\n",
      "step = 3477200: loss = 3.747249126434326\n",
      "step = 3477400: loss = 4.311453342437744\n",
      "step = 3477600: loss = 4.4348673820495605\n",
      "step = 3477800: loss = 4.675929069519043\n",
      "step = 3478000: loss = 3.0561020374298096\n",
      "step = 3478200: loss = 4.448060989379883\n",
      "step = 3478400: loss = 4.630227088928223\n",
      "step = 3478600: loss = 4.192481994628906\n",
      "step = 3478800: loss = 4.608983516693115\n",
      "step = 3479000: loss = 5.822103977203369\n",
      "step = 3479200: loss = 4.684466361999512\n",
      "step = 3479400: loss = 2.1427969932556152\n",
      "step = 3479600: loss = 5.385128021240234\n",
      "step = 3479800: loss = 4.548379421234131\n",
      "step = 3480000: loss = 4.6892547607421875\n",
      "step = 3480000: Average Return = 3.6540000438690186\n",
      "step = 3480200: loss = 5.212640285491943\n",
      "step = 3480400: loss = 3.604262590408325\n",
      "step = 3480600: loss = 3.628572940826416\n",
      "step = 3480800: loss = 3.417264223098755\n",
      "step = 3481000: loss = 3.409817695617676\n",
      "step = 3481200: loss = 4.073709011077881\n",
      "step = 3481400: loss = 4.652108192443848\n",
      "step = 3481600: loss = 3.9309403896331787\n",
      "step = 3481800: loss = 3.83739972114563\n",
      "step = 3482000: loss = 4.075924396514893\n",
      "step = 3482200: loss = 3.9680349826812744\n",
      "step = 3482400: loss = 4.0937418937683105\n",
      "step = 3482600: loss = 4.487908363342285\n",
      "step = 3482800: loss = 4.067861080169678\n",
      "step = 3483000: loss = 4.2688517570495605\n",
      "step = 3483200: loss = 3.330585241317749\n",
      "step = 3483400: loss = 4.873349666595459\n",
      "step = 3483600: loss = 5.098206043243408\n",
      "step = 3483800: loss = 5.148726463317871\n",
      "step = 3484000: loss = 3.9601757526397705\n",
      "step = 3484200: loss = 3.7789201736450195\n",
      "step = 3484400: loss = 4.1939520835876465\n",
      "step = 3484600: loss = 3.7765517234802246\n",
      "step = 3484800: loss = 4.242522239685059\n",
      "step = 3485000: loss = 4.717840194702148\n",
      "step = 3485000: Average Return = 3.9100000858306885\n",
      "step = 3485200: loss = 5.339390754699707\n",
      "step = 3485400: loss = 4.753172874450684\n",
      "step = 3485600: loss = 4.309523105621338\n",
      "step = 3485800: loss = 4.855246067047119\n",
      "step = 3486000: loss = 4.258117198944092\n",
      "step = 3486200: loss = 4.096889972686768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3486400: loss = 4.152717590332031\n",
      "step = 3486600: loss = 5.043384075164795\n",
      "step = 3486800: loss = 5.160388469696045\n",
      "step = 3487000: loss = 3.871119499206543\n",
      "step = 3487200: loss = 3.85823655128479\n",
      "step = 3487400: loss = 5.016244411468506\n",
      "step = 3487600: loss = 4.477435111999512\n",
      "step = 3487800: loss = 3.924755811691284\n",
      "step = 3488000: loss = 6.1641526222229\n",
      "step = 3488200: loss = 2.996798038482666\n",
      "step = 3488400: loss = 3.368112802505493\n",
      "step = 3488600: loss = 5.155877590179443\n",
      "step = 3488800: loss = 4.466992378234863\n",
      "step = 3489000: loss = 2.192793369293213\n",
      "step = 3489200: loss = 3.280139923095703\n",
      "step = 3489400: loss = 4.709794998168945\n",
      "step = 3489600: loss = 3.0457470417022705\n",
      "step = 3489800: loss = 3.930878162384033\n",
      "step = 3490000: loss = 4.288547992706299\n",
      "step = 3490000: Average Return = 3.049999952316284\n",
      "step = 3490200: loss = 4.75518798828125\n",
      "step = 3490400: loss = 3.21559739112854\n",
      "step = 3490600: loss = 4.064176559448242\n",
      "step = 3490800: loss = 5.122468948364258\n",
      "step = 3491000: loss = 4.323927402496338\n",
      "step = 3491200: loss = 3.449432373046875\n",
      "step = 3491400: loss = 4.197139739990234\n",
      "step = 3491600: loss = 4.662295818328857\n",
      "step = 3491800: loss = 4.695512771606445\n",
      "step = 3492000: loss = 3.995783567428589\n",
      "step = 3492200: loss = 3.19631028175354\n",
      "step = 3492400: loss = 3.2717814445495605\n",
      "step = 3492600: loss = 4.636112213134766\n",
      "step = 3492800: loss = 3.5930933952331543\n",
      "step = 3493000: loss = 3.5842649936676025\n",
      "step = 3493200: loss = 3.3747496604919434\n",
      "step = 3493400: loss = 4.266836643218994\n",
      "step = 3493600: loss = 4.85675048828125\n",
      "step = 3493800: loss = 4.024021625518799\n",
      "step = 3494000: loss = 5.113101482391357\n",
      "step = 3494200: loss = 2.9464919567108154\n",
      "step = 3494400: loss = 3.207326889038086\n",
      "step = 3494600: loss = 5.84727144241333\n",
      "step = 3494800: loss = 4.717613697052002\n",
      "step = 3495000: loss = 2.9356744289398193\n",
      "step = 3495000: Average Return = 3.8380000591278076\n",
      "step = 3495200: loss = 3.1893386840820312\n",
      "step = 3495400: loss = 3.428398370742798\n",
      "step = 3495600: loss = 4.017237663269043\n",
      "step = 3495800: loss = 3.453824996948242\n",
      "step = 3496000: loss = 4.4027557373046875\n",
      "step = 3496200: loss = 4.893500328063965\n",
      "step = 3496400: loss = 4.819878578186035\n",
      "step = 3496600: loss = 4.085480690002441\n",
      "step = 3496800: loss = 3.9220378398895264\n",
      "step = 3497000: loss = 4.579893589019775\n",
      "step = 3497200: loss = 4.383504390716553\n",
      "step = 3497400: loss = 4.89581298828125\n",
      "step = 3497600: loss = 4.359942436218262\n",
      "step = 3497800: loss = 3.897690534591675\n",
      "step = 3498000: loss = 4.022912502288818\n",
      "step = 3498200: loss = 4.275914192199707\n",
      "step = 3498400: loss = 5.016392707824707\n",
      "step = 3498600: loss = 4.225318908691406\n",
      "step = 3498800: loss = 3.222766637802124\n",
      "step = 3499000: loss = 3.9404256343841553\n",
      "step = 3499200: loss = 3.7176995277404785\n",
      "step = 3499400: loss = 4.631239414215088\n",
      "step = 3499600: loss = 3.656940221786499\n",
      "step = 3499800: loss = 2.7963757514953613\n",
      "step = 3500000: loss = 4.185426235198975\n",
      "step = 3500000: Average Return = 3.611999988555908\n",
      "step = 3500200: loss = 2.605410099029541\n",
      "step = 3500400: loss = 4.3915581703186035\n",
      "step = 3500600: loss = 5.392834186553955\n",
      "step = 3500800: loss = 4.601231575012207\n",
      "step = 3501000: loss = 4.407320499420166\n",
      "step = 3501200: loss = 4.331348419189453\n",
      "step = 3501400: loss = 4.682528018951416\n",
      "step = 3501600: loss = 4.95679235458374\n",
      "step = 3501800: loss = 3.7384896278381348\n",
      "step = 3502000: loss = 2.882061719894409\n",
      "step = 3502200: loss = 3.9777276515960693\n",
      "step = 3502400: loss = 3.810333490371704\n",
      "step = 3502600: loss = 4.349573135375977\n",
      "step = 3502800: loss = 3.101925849914551\n",
      "step = 3503000: loss = 2.0605602264404297\n",
      "step = 3503200: loss = 4.923941135406494\n",
      "step = 3503400: loss = 5.127819061279297\n",
      "step = 3503600: loss = 4.187033176422119\n",
      "step = 3503800: loss = 5.427801609039307\n",
      "step = 3504000: loss = 5.179206848144531\n",
      "step = 3504200: loss = 3.475581645965576\n",
      "step = 3504400: loss = 5.139033794403076\n",
      "step = 3504600: loss = 4.358859539031982\n",
      "step = 3504800: loss = 3.2093398571014404\n",
      "step = 3505000: loss = 3.7491848468780518\n",
      "step = 3505000: Average Return = 3.997999906539917\n",
      "step = 3505200: loss = 5.5874505043029785\n",
      "step = 3505400: loss = 4.609528064727783\n",
      "step = 3505600: loss = 5.418049335479736\n",
      "step = 3505800: loss = 2.481968402862549\n",
      "step = 3506000: loss = 3.3674263954162598\n",
      "step = 3506200: loss = 4.062179088592529\n",
      "step = 3506400: loss = 4.6737213134765625\n",
      "step = 3506600: loss = 4.603413105010986\n",
      "step = 3506800: loss = 4.304437160491943\n",
      "step = 3507000: loss = 3.759291410446167\n",
      "step = 3507200: loss = 4.158144474029541\n",
      "step = 3507400: loss = 3.5645434856414795\n",
      "step = 3507600: loss = 4.743354797363281\n",
      "step = 3507800: loss = 3.9785614013671875\n",
      "step = 3508000: loss = 3.045642137527466\n",
      "step = 3508200: loss = 4.510909557342529\n",
      "step = 3508400: loss = 3.9290318489074707\n",
      "step = 3508600: loss = 3.9045066833496094\n",
      "step = 3508800: loss = 3.538496732711792\n",
      "step = 3509000: loss = 3.2942521572113037\n",
      "step = 3509200: loss = 3.923823118209839\n",
      "step = 3509400: loss = 3.4878175258636475\n",
      "step = 3509600: loss = 6.284136772155762\n",
      "step = 3509800: loss = 4.40626335144043\n",
      "step = 3510000: loss = 4.742412090301514\n",
      "step = 3510000: Average Return = 4.067999839782715\n",
      "step = 3510200: loss = 4.372261047363281\n",
      "step = 3510400: loss = 3.590877056121826\n",
      "step = 3510600: loss = 4.1271891593933105\n",
      "step = 3510800: loss = 5.767419815063477\n",
      "step = 3511000: loss = 5.225119590759277\n",
      "step = 3511200: loss = 3.7609312534332275\n",
      "step = 3511400: loss = 6.855727195739746\n",
      "step = 3511600: loss = 4.488525867462158\n",
      "step = 3511800: loss = 4.285336971282959\n",
      "step = 3512000: loss = 5.555389404296875\n",
      "step = 3512200: loss = 4.28147029876709\n",
      "step = 3512400: loss = 4.675291538238525\n",
      "step = 3512600: loss = 4.41352653503418\n",
      "step = 3512800: loss = 3.7593514919281006\n",
      "step = 3513000: loss = 3.152369976043701\n",
      "step = 3513200: loss = 3.258697748184204\n",
      "step = 3513400: loss = 3.1154801845550537\n",
      "step = 3513600: loss = 4.525137424468994\n",
      "step = 3513800: loss = 4.668396949768066\n",
      "step = 3514000: loss = 4.79203987121582\n",
      "step = 3514200: loss = 4.531872272491455\n",
      "step = 3514400: loss = 5.798634052276611\n",
      "step = 3514600: loss = 4.544960975646973\n",
      "step = 3514800: loss = 2.736060619354248\n",
      "step = 3515000: loss = 3.614652633666992\n",
      "step = 3515000: Average Return = 4.026000022888184\n",
      "step = 3515200: loss = 3.802844762802124\n",
      "step = 3515400: loss = 4.362250804901123\n",
      "step = 3515600: loss = 4.882732391357422\n",
      "step = 3515800: loss = 4.38344669342041\n",
      "step = 3516000: loss = 3.5680816173553467\n",
      "step = 3516200: loss = 3.606282949447632\n",
      "step = 3516400: loss = 3.6242244243621826\n",
      "step = 3516600: loss = 3.011183500289917\n",
      "step = 3516800: loss = 5.168971061706543\n",
      "step = 3517000: loss = 3.969391107559204\n",
      "step = 3517200: loss = 3.707094669342041\n",
      "step = 3517400: loss = 3.386307954788208\n",
      "step = 3517600: loss = 3.048755168914795\n",
      "step = 3517800: loss = 3.554482936859131\n",
      "step = 3518000: loss = 3.0350074768066406\n",
      "step = 3518200: loss = 4.783866882324219\n",
      "step = 3518400: loss = 4.771537780761719\n",
      "step = 3518600: loss = 5.022512912750244\n",
      "step = 3518800: loss = 3.3032803535461426\n",
      "step = 3519000: loss = 3.99910306930542\n",
      "step = 3519200: loss = 4.556336402893066\n",
      "step = 3519400: loss = 5.599096775054932\n",
      "step = 3519600: loss = 4.684774875640869\n",
      "step = 3519800: loss = 4.032346248626709\n",
      "step = 3520000: loss = 3.7375712394714355\n",
      "step = 3520000: Average Return = 3.052000045776367\n",
      "step = 3520200: loss = 3.565333843231201\n",
      "step = 3520400: loss = 4.360813140869141\n",
      "step = 3520600: loss = 4.246304988861084\n",
      "step = 3520800: loss = 3.5603904724121094\n",
      "step = 3521000: loss = 4.170209884643555\n",
      "step = 3521200: loss = 5.202849388122559\n",
      "step = 3521400: loss = 4.148115158081055\n",
      "step = 3521600: loss = 5.435394287109375\n",
      "step = 3521800: loss = 3.7218315601348877\n",
      "step = 3522000: loss = 4.360189914703369\n",
      "step = 3522200: loss = 3.4384799003601074\n",
      "step = 3522400: loss = 4.015285968780518\n",
      "step = 3522600: loss = 3.897648572921753\n",
      "step = 3522800: loss = 3.8051135540008545\n",
      "step = 3523000: loss = 4.974142551422119\n",
      "step = 3523200: loss = 3.6569488048553467\n",
      "step = 3523400: loss = 4.058830738067627\n",
      "step = 3523600: loss = 3.9823946952819824\n",
      "step = 3523800: loss = 4.377950668334961\n",
      "step = 3524000: loss = 5.159535884857178\n",
      "step = 3524200: loss = 4.714692115783691\n",
      "step = 3524400: loss = 3.7594897747039795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3524600: loss = 3.6206233501434326\n",
      "step = 3524800: loss = 4.225402355194092\n",
      "step = 3525000: loss = 4.147737503051758\n",
      "step = 3525000: Average Return = 3.5160000324249268\n",
      "step = 3525200: loss = 3.4849133491516113\n",
      "step = 3525400: loss = 3.0050923824310303\n",
      "step = 3525600: loss = 3.044450044631958\n",
      "step = 3525800: loss = 3.600217580795288\n",
      "step = 3526000: loss = 4.029704570770264\n",
      "step = 3526200: loss = 5.199921607971191\n",
      "step = 3526400: loss = 4.873286247253418\n",
      "step = 3526600: loss = 5.156864643096924\n",
      "step = 3526800: loss = 3.689258337020874\n",
      "step = 3527000: loss = 3.6230740547180176\n",
      "step = 3527200: loss = 3.9785430431365967\n",
      "step = 3527400: loss = 3.749329090118408\n",
      "step = 3527600: loss = 5.12803840637207\n",
      "step = 3527800: loss = 4.451034069061279\n",
      "step = 3528000: loss = 3.3747270107269287\n",
      "step = 3528200: loss = 4.776839256286621\n",
      "step = 3528400: loss = 5.056949138641357\n",
      "step = 3528600: loss = 4.893529415130615\n",
      "step = 3528800: loss = 2.9635674953460693\n",
      "step = 3529000: loss = 3.7706243991851807\n",
      "step = 3529200: loss = 3.9078288078308105\n",
      "step = 3529400: loss = 4.364306926727295\n",
      "step = 3529600: loss = 3.010931968688965\n",
      "step = 3529800: loss = 4.080468654632568\n",
      "step = 3530000: loss = 3.221620798110962\n",
      "step = 3530000: Average Return = 4.038000106811523\n",
      "step = 3530200: loss = 3.7124342918395996\n",
      "step = 3530400: loss = 3.312478542327881\n",
      "step = 3530600: loss = 4.861252307891846\n",
      "step = 3530800: loss = 4.1626057624816895\n",
      "step = 3531000: loss = 3.4490532875061035\n",
      "step = 3531200: loss = 2.9315028190612793\n",
      "step = 3531400: loss = 4.6590399742126465\n",
      "step = 3531600: loss = 4.545395374298096\n",
      "step = 3531800: loss = 3.8761181831359863\n",
      "step = 3532000: loss = 5.231371879577637\n",
      "step = 3532200: loss = 4.380622863769531\n",
      "step = 3532400: loss = 3.5480222702026367\n",
      "step = 3532600: loss = 3.3812756538391113\n",
      "step = 3532800: loss = 4.403669834136963\n",
      "step = 3533000: loss = 5.506875038146973\n",
      "step = 3533200: loss = 5.2057929039001465\n",
      "step = 3533400: loss = 2.804056406021118\n",
      "step = 3533600: loss = 3.651827096939087\n",
      "step = 3533800: loss = 5.879152774810791\n",
      "step = 3534000: loss = 3.764525890350342\n",
      "step = 3534200: loss = 4.799123287200928\n",
      "step = 3534400: loss = 4.164717197418213\n",
      "step = 3534600: loss = 3.7247793674468994\n",
      "step = 3534800: loss = 3.432392120361328\n",
      "step = 3535000: loss = 4.802427768707275\n",
      "step = 3535000: Average Return = 3.861999988555908\n",
      "step = 3535200: loss = 4.375429153442383\n",
      "step = 3535400: loss = 4.858859539031982\n",
      "step = 3535600: loss = 3.4903881549835205\n",
      "step = 3535800: loss = 4.234488487243652\n",
      "step = 3536000: loss = 4.304361343383789\n",
      "step = 3536200: loss = 4.520816326141357\n",
      "step = 3536400: loss = 4.3849639892578125\n",
      "step = 3536600: loss = 4.438693523406982\n",
      "step = 3536800: loss = 2.0278408527374268\n",
      "step = 3537000: loss = 4.021417617797852\n",
      "step = 3537200: loss = 5.043876647949219\n",
      "step = 3537400: loss = 4.756871223449707\n",
      "step = 3537600: loss = 3.781827688217163\n",
      "step = 3537800: loss = 4.790390491485596\n",
      "step = 3538000: loss = 3.3098978996276855\n",
      "step = 3538200: loss = 4.015041351318359\n",
      "step = 3538400: loss = 3.327159881591797\n",
      "step = 3538600: loss = 3.962144613265991\n",
      "step = 3538800: loss = 3.5642592906951904\n",
      "step = 3539000: loss = 6.310060977935791\n",
      "step = 3539200: loss = 3.8396055698394775\n",
      "step = 3539400: loss = 4.481822490692139\n",
      "step = 3539600: loss = 3.48728084564209\n",
      "step = 3539800: loss = 5.259143352508545\n",
      "step = 3540000: loss = 3.9377195835113525\n",
      "step = 3540000: Average Return = 3.9619998931884766\n",
      "step = 3540200: loss = 4.0629096031188965\n",
      "step = 3540400: loss = 4.453617572784424\n",
      "step = 3540600: loss = 4.1703948974609375\n",
      "step = 3540800: loss = 4.339592933654785\n",
      "step = 3541000: loss = 3.776638984680176\n",
      "step = 3541200: loss = 4.782018184661865\n",
      "step = 3541400: loss = 5.038813591003418\n",
      "step = 3541600: loss = 4.621407508850098\n",
      "step = 3541800: loss = 5.034295082092285\n",
      "step = 3542000: loss = 4.805026054382324\n",
      "step = 3542200: loss = 4.311315059661865\n",
      "step = 3542400: loss = 3.666750431060791\n",
      "step = 3542600: loss = 3.7610385417938232\n",
      "step = 3542800: loss = 6.137214660644531\n",
      "step = 3543000: loss = 4.305790424346924\n",
      "step = 3543200: loss = 2.5783534049987793\n",
      "step = 3543400: loss = 3.639493942260742\n",
      "step = 3543600: loss = 4.619775772094727\n",
      "step = 3543800: loss = 4.000628471374512\n",
      "step = 3544000: loss = 5.713171482086182\n",
      "step = 3544200: loss = 3.123619318008423\n",
      "step = 3544400: loss = 3.80039119720459\n",
      "step = 3544600: loss = 5.135971546173096\n",
      "step = 3544800: loss = 4.112132549285889\n",
      "step = 3545000: loss = 4.42128849029541\n",
      "step = 3545000: Average Return = 3.9000000953674316\n",
      "step = 3545200: loss = 2.614978551864624\n",
      "step = 3545400: loss = 4.105728626251221\n",
      "step = 3545600: loss = 4.184858798980713\n",
      "step = 3545800: loss = 3.5369207859039307\n",
      "step = 3546000: loss = 4.107819080352783\n",
      "step = 3546200: loss = 3.768477201461792\n",
      "step = 3546400: loss = 3.8628056049346924\n",
      "step = 3546600: loss = 3.4845685958862305\n",
      "step = 3546800: loss = 3.538954496383667\n",
      "step = 3547000: loss = 4.1679277420043945\n",
      "step = 3547200: loss = 4.381083965301514\n",
      "step = 3547400: loss = 4.114294052124023\n",
      "step = 3547600: loss = 3.5304510593414307\n",
      "step = 3547800: loss = 4.17968225479126\n",
      "step = 3548000: loss = 6.135705471038818\n",
      "step = 3548200: loss = 5.104567050933838\n",
      "step = 3548400: loss = 2.9532573223114014\n",
      "step = 3548600: loss = 4.479605674743652\n",
      "step = 3548800: loss = 4.1209516525268555\n",
      "step = 3549000: loss = 4.374022960662842\n",
      "step = 3549200: loss = 4.030638217926025\n",
      "step = 3549400: loss = 5.275882244110107\n",
      "step = 3549600: loss = 3.9748144149780273\n",
      "step = 3549800: loss = 4.8781352043151855\n",
      "step = 3550000: loss = 3.0714824199676514\n",
      "step = 3550000: Average Return = 3.7780001163482666\n",
      "step = 3550200: loss = 3.6237056255340576\n",
      "step = 3550400: loss = 3.782198429107666\n",
      "step = 3550600: loss = 3.652859926223755\n",
      "step = 3550800: loss = 4.131649017333984\n",
      "step = 3551000: loss = 4.594788551330566\n",
      "step = 3551200: loss = 4.497386455535889\n",
      "step = 3551400: loss = 3.048656463623047\n",
      "step = 3551600: loss = 4.885581970214844\n",
      "step = 3551800: loss = 3.3875224590301514\n",
      "step = 3552000: loss = 4.42226505279541\n",
      "step = 3552200: loss = 3.7135119438171387\n",
      "step = 3552400: loss = 4.263031005859375\n",
      "step = 3552600: loss = 3.94515061378479\n",
      "step = 3552800: loss = 3.85440731048584\n",
      "step = 3553000: loss = 2.8966097831726074\n",
      "step = 3553200: loss = 3.9382717609405518\n",
      "step = 3553400: loss = 5.416342735290527\n",
      "step = 3553600: loss = 3.1059539318084717\n",
      "step = 3553800: loss = 5.386006832122803\n",
      "step = 3554000: loss = 3.8835344314575195\n",
      "step = 3554200: loss = 5.585491180419922\n",
      "step = 3554400: loss = 5.180808067321777\n",
      "step = 3554600: loss = 4.075540542602539\n",
      "step = 3554800: loss = 3.2585504055023193\n",
      "step = 3555000: loss = 3.578321695327759\n",
      "step = 3555000: Average Return = 4.02400016784668\n",
      "step = 3555200: loss = 4.501852512359619\n",
      "step = 3555400: loss = 4.996257305145264\n",
      "step = 3555600: loss = 4.434261322021484\n",
      "step = 3555800: loss = 4.0493903160095215\n",
      "step = 3556000: loss = 4.745643615722656\n",
      "step = 3556200: loss = 3.928434371948242\n",
      "step = 3556400: loss = 5.169562816619873\n",
      "step = 3556600: loss = 3.7734081745147705\n",
      "step = 3556800: loss = 2.7892186641693115\n",
      "step = 3557000: loss = 3.146345853805542\n",
      "step = 3557200: loss = 2.2068591117858887\n",
      "step = 3557400: loss = 4.822486400604248\n",
      "step = 3557600: loss = 3.7882628440856934\n",
      "step = 3557800: loss = 4.138465881347656\n",
      "step = 3558000: loss = 3.474024534225464\n",
      "step = 3558200: loss = 4.49966287612915\n",
      "step = 3558400: loss = 3.327204704284668\n",
      "step = 3558600: loss = 5.56126594543457\n",
      "step = 3558800: loss = 5.150063514709473\n",
      "step = 3559000: loss = 4.2717976570129395\n",
      "step = 3559200: loss = 4.183319568634033\n",
      "step = 3559400: loss = 3.506899833679199\n",
      "step = 3559600: loss = 3.5614264011383057\n",
      "step = 3559800: loss = 3.465954542160034\n",
      "step = 3560000: loss = 5.354068279266357\n",
      "step = 3560000: Average Return = 3.5959999561309814\n",
      "step = 3560200: loss = 4.448164463043213\n",
      "step = 3560400: loss = 3.534111261367798\n",
      "step = 3560600: loss = 3.708630084991455\n",
      "step = 3560800: loss = 4.652570724487305\n",
      "step = 3561000: loss = 3.203843832015991\n",
      "step = 3561200: loss = 4.289038181304932\n",
      "step = 3561400: loss = 2.6123480796813965\n",
      "step = 3561600: loss = 4.966142654418945\n",
      "step = 3561800: loss = 5.193024635314941\n",
      "step = 3562000: loss = 4.26221227645874\n",
      "step = 3562200: loss = 4.852911472320557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3562400: loss = 3.7198197841644287\n",
      "step = 3562600: loss = 4.380255222320557\n",
      "step = 3562800: loss = 5.120758056640625\n",
      "step = 3563000: loss = 3.2013635635375977\n",
      "step = 3563200: loss = 3.9816088676452637\n",
      "step = 3563400: loss = 4.285969257354736\n",
      "step = 3563600: loss = 4.767332553863525\n",
      "step = 3563800: loss = 3.3422956466674805\n",
      "step = 3564000: loss = 4.263434886932373\n",
      "step = 3564200: loss = 4.398563861846924\n",
      "step = 3564400: loss = 3.2719805240631104\n",
      "step = 3564600: loss = 3.2238965034484863\n",
      "step = 3564800: loss = 5.237135887145996\n",
      "step = 3565000: loss = 3.5746989250183105\n",
      "step = 3565000: Average Return = 3.8919999599456787\n",
      "step = 3565200: loss = 3.2969090938568115\n",
      "step = 3565400: loss = 4.2929768562316895\n",
      "step = 3565600: loss = 4.813328742980957\n",
      "step = 3565800: loss = 2.6398415565490723\n",
      "step = 3566000: loss = 3.5437839031219482\n",
      "step = 3566200: loss = 3.409255266189575\n",
      "step = 3566400: loss = 4.5654296875\n",
      "step = 3566600: loss = 3.7760064601898193\n",
      "step = 3566800: loss = 4.43722677230835\n",
      "step = 3567000: loss = 4.204930305480957\n",
      "step = 3567200: loss = 3.857058048248291\n",
      "step = 3567400: loss = 4.5505547523498535\n",
      "step = 3567600: loss = 4.655757904052734\n",
      "step = 3567800: loss = 4.9142608642578125\n",
      "step = 3568000: loss = 5.965753555297852\n",
      "step = 3568200: loss = 3.969409704208374\n",
      "step = 3568400: loss = 3.7134194374084473\n",
      "step = 3568600: loss = 3.132505416870117\n",
      "step = 3568800: loss = 3.6252291202545166\n",
      "step = 3569000: loss = 4.526771068572998\n",
      "step = 3569200: loss = 5.576777935028076\n",
      "step = 3569400: loss = 4.542867183685303\n",
      "step = 3569600: loss = 5.7294921875\n",
      "step = 3569800: loss = 5.248178482055664\n",
      "step = 3570000: loss = 4.573361873626709\n",
      "step = 3570000: Average Return = 3.8420000076293945\n",
      "step = 3570200: loss = 5.08594274520874\n",
      "step = 3570400: loss = 5.136091232299805\n",
      "step = 3570600: loss = 3.2161309719085693\n",
      "step = 3570800: loss = 4.967338562011719\n",
      "step = 3571000: loss = 3.954497814178467\n",
      "step = 3571200: loss = 5.724120140075684\n",
      "step = 3571400: loss = 3.7342817783355713\n",
      "step = 3571600: loss = 4.143185615539551\n",
      "step = 3571800: loss = 4.621071815490723\n",
      "step = 3572000: loss = 3.2986035346984863\n",
      "step = 3572200: loss = 4.199110507965088\n",
      "step = 3572400: loss = 4.904611587524414\n",
      "step = 3572600: loss = 4.557887554168701\n",
      "step = 3572800: loss = 3.99432635307312\n",
      "step = 3573000: loss = 4.3921122550964355\n",
      "step = 3573200: loss = 5.027219772338867\n",
      "step = 3573400: loss = 4.324818134307861\n",
      "step = 3573600: loss = 4.887842655181885\n",
      "step = 3573800: loss = 3.1629905700683594\n",
      "step = 3574000: loss = 3.3616485595703125\n",
      "step = 3574200: loss = 3.046252965927124\n",
      "step = 3574400: loss = 5.86185884475708\n",
      "step = 3574600: loss = 5.33668327331543\n",
      "step = 3574800: loss = 5.0784101486206055\n",
      "step = 3575000: loss = 4.1520609855651855\n",
      "step = 3575000: Average Return = 3.996000051498413\n",
      "step = 3575200: loss = 3.46340274810791\n",
      "step = 3575400: loss = 3.3827266693115234\n",
      "step = 3575600: loss = 4.339114665985107\n",
      "step = 3575800: loss = 3.677210569381714\n",
      "step = 3576000: loss = 3.319251537322998\n",
      "step = 3576200: loss = 3.21657657623291\n",
      "step = 3576400: loss = 4.495936870574951\n",
      "step = 3576600: loss = 2.7536520957946777\n",
      "step = 3576800: loss = 3.5691518783569336\n",
      "step = 3577000: loss = 4.634624481201172\n",
      "step = 3577200: loss = 5.727579593658447\n",
      "step = 3577400: loss = 3.5593395233154297\n",
      "step = 3577600: loss = 5.780923843383789\n",
      "step = 3577800: loss = 4.679981708526611\n",
      "step = 3578000: loss = 4.525996685028076\n",
      "step = 3578200: loss = 4.052308559417725\n",
      "step = 3578400: loss = 3.479083776473999\n",
      "step = 3578600: loss = 4.236848831176758\n",
      "step = 3578800: loss = 6.908441066741943\n",
      "step = 3579000: loss = 5.736008167266846\n",
      "step = 3579200: loss = 4.058419704437256\n",
      "step = 3579400: loss = 5.0901570320129395\n",
      "step = 3579600: loss = 3.500596046447754\n",
      "step = 3579800: loss = 4.028524875640869\n",
      "step = 3580000: loss = 3.303907632827759\n",
      "step = 3580000: Average Return = 3.635999917984009\n",
      "step = 3580200: loss = 5.737519264221191\n",
      "step = 3580400: loss = 2.9337899684906006\n",
      "step = 3580600: loss = 4.772905349731445\n",
      "step = 3580800: loss = 3.6533496379852295\n",
      "step = 3581000: loss = 3.609395980834961\n",
      "step = 3581200: loss = 4.233133316040039\n",
      "step = 3581400: loss = 4.261157035827637\n",
      "step = 3581600: loss = 4.577093601226807\n",
      "step = 3581800: loss = 3.713921308517456\n",
      "step = 3582000: loss = 4.248528957366943\n",
      "step = 3582200: loss = 3.179743528366089\n",
      "step = 3582400: loss = 4.674287796020508\n",
      "step = 3582600: loss = 5.378902435302734\n",
      "step = 3582800: loss = 5.034829616546631\n",
      "step = 3583000: loss = 4.023529529571533\n",
      "step = 3583200: loss = 5.203193187713623\n",
      "step = 3583400: loss = 5.27781867980957\n",
      "step = 3583600: loss = 4.43812894821167\n",
      "step = 3583800: loss = 4.095459938049316\n",
      "step = 3584000: loss = 3.9798943996429443\n",
      "step = 3584200: loss = 5.154141902923584\n",
      "step = 3584400: loss = 5.021944522857666\n",
      "step = 3584600: loss = 3.618302822113037\n",
      "step = 3584800: loss = 4.666868686676025\n",
      "step = 3585000: loss = 5.155635833740234\n",
      "step = 3585000: Average Return = 3.7899999618530273\n",
      "step = 3585200: loss = 5.385711669921875\n",
      "step = 3585400: loss = 4.373489856719971\n",
      "step = 3585600: loss = 5.344205856323242\n",
      "step = 3585800: loss = 4.999870300292969\n",
      "step = 3586000: loss = 4.324526309967041\n",
      "step = 3586200: loss = 3.4677770137786865\n",
      "step = 3586400: loss = 4.7440972328186035\n",
      "step = 3586600: loss = 3.3063883781433105\n",
      "step = 3586800: loss = 4.251338481903076\n",
      "step = 3587000: loss = 3.341005563735962\n",
      "step = 3587200: loss = 3.5563573837280273\n",
      "step = 3587400: loss = 4.509810447692871\n",
      "step = 3587600: loss = 3.2882790565490723\n",
      "step = 3587800: loss = 4.4282097816467285\n",
      "step = 3588000: loss = 4.051705360412598\n",
      "step = 3588200: loss = 5.245119094848633\n",
      "step = 3588400: loss = 3.6245341300964355\n",
      "step = 3588600: loss = 3.373894691467285\n",
      "step = 3588800: loss = 5.079549312591553\n",
      "step = 3589000: loss = 4.4984822273254395\n",
      "step = 3589200: loss = 3.290558338165283\n",
      "step = 3589400: loss = 3.741392135620117\n",
      "step = 3589600: loss = 3.2821567058563232\n",
      "step = 3589800: loss = 4.800971508026123\n",
      "step = 3590000: loss = 5.955010414123535\n",
      "step = 3590000: Average Return = 3.4600000381469727\n",
      "step = 3590200: loss = 3.1868176460266113\n",
      "step = 3590400: loss = 3.6791343688964844\n",
      "step = 3590600: loss = 3.83921217918396\n",
      "step = 3590800: loss = 5.335090160369873\n",
      "step = 3591000: loss = 5.140995979309082\n",
      "step = 3591200: loss = 2.9494576454162598\n",
      "step = 3591400: loss = 2.4338269233703613\n",
      "step = 3591600: loss = 5.006601333618164\n",
      "step = 3591800: loss = 4.4812211990356445\n",
      "step = 3592000: loss = 3.625053644180298\n",
      "step = 3592200: loss = 3.4624650478363037\n",
      "step = 3592400: loss = 4.287914752960205\n",
      "step = 3592600: loss = 4.689243793487549\n",
      "step = 3592800: loss = 5.072268009185791\n",
      "step = 3593000: loss = 4.352683067321777\n",
      "step = 3593200: loss = 4.556303024291992\n",
      "step = 3593400: loss = 5.546938419342041\n",
      "step = 3593600: loss = 4.779421329498291\n",
      "step = 3593800: loss = 4.396944046020508\n",
      "step = 3594000: loss = 5.082545757293701\n",
      "step = 3594200: loss = 2.8865792751312256\n",
      "step = 3594400: loss = 5.467648983001709\n",
      "step = 3594600: loss = 4.474806308746338\n",
      "step = 3594800: loss = 3.95153546333313\n",
      "step = 3595000: loss = 4.328794002532959\n",
      "step = 3595000: Average Return = 3.5360000133514404\n",
      "step = 3595200: loss = 5.153714656829834\n",
      "step = 3595400: loss = 3.4297001361846924\n",
      "step = 3595600: loss = 3.549916982650757\n",
      "step = 3595800: loss = 2.9808428287506104\n",
      "step = 3596000: loss = 3.1118810176849365\n",
      "step = 3596200: loss = 4.576138496398926\n",
      "step = 3596400: loss = 5.913547992706299\n",
      "step = 3596600: loss = 3.806588649749756\n",
      "step = 3596800: loss = 4.425947189331055\n",
      "step = 3597000: loss = 3.780142068862915\n",
      "step = 3597200: loss = 4.522678852081299\n",
      "step = 3597400: loss = 2.3407084941864014\n",
      "step = 3597600: loss = 4.810708045959473\n",
      "step = 3597800: loss = 3.3111534118652344\n",
      "step = 3598000: loss = 3.3261899948120117\n",
      "step = 3598200: loss = 4.666833877563477\n",
      "step = 3598400: loss = 5.362977027893066\n",
      "step = 3598600: loss = 5.40979528427124\n",
      "step = 3598800: loss = 5.883748531341553\n",
      "step = 3599000: loss = 4.073155403137207\n",
      "step = 3599200: loss = 2.842965602874756\n",
      "step = 3599400: loss = 5.307977199554443\n",
      "step = 3599600: loss = 3.283430814743042\n",
      "step = 3599800: loss = 4.02207088470459\n",
      "step = 3600000: loss = 4.260867118835449\n",
      "step = 3600000: Average Return = 3.5280001163482666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3600200: loss = 3.3114373683929443\n",
      "step = 3600400: loss = 2.9411261081695557\n",
      "step = 3600600: loss = 2.7481017112731934\n",
      "step = 3600800: loss = 4.241908073425293\n",
      "step = 3601000: loss = 4.677365779876709\n",
      "step = 3601200: loss = 5.043030738830566\n",
      "step = 3601400: loss = 4.952089309692383\n",
      "step = 3601600: loss = 4.724730491638184\n",
      "step = 3601800: loss = 3.331010103225708\n",
      "step = 3602000: loss = 4.7759623527526855\n",
      "step = 3602200: loss = 3.8213539123535156\n",
      "step = 3602400: loss = 5.167862892150879\n",
      "step = 3602600: loss = 3.9912731647491455\n",
      "step = 3602800: loss = 5.661218643188477\n",
      "step = 3603000: loss = 3.8193798065185547\n",
      "step = 3603200: loss = 3.7949278354644775\n",
      "step = 3603400: loss = 4.246772766113281\n",
      "step = 3603600: loss = 3.9654622077941895\n",
      "step = 3603800: loss = 5.163138389587402\n",
      "step = 3604000: loss = 6.211339473724365\n",
      "step = 3604200: loss = 3.2251527309417725\n",
      "step = 3604400: loss = 5.740987300872803\n",
      "step = 3604600: loss = 2.6969761848449707\n",
      "step = 3604800: loss = 3.0148308277130127\n",
      "step = 3605000: loss = 4.9350504875183105\n",
      "step = 3605000: Average Return = 3.614000082015991\n",
      "step = 3605200: loss = 4.351341247558594\n",
      "step = 3605400: loss = 2.485457181930542\n",
      "step = 3605600: loss = 5.896604061126709\n",
      "step = 3605800: loss = 4.325362682342529\n",
      "step = 3606000: loss = 4.443463325500488\n",
      "step = 3606200: loss = 4.659617900848389\n",
      "step = 3606400: loss = 4.05258321762085\n",
      "step = 3606600: loss = 3.2220754623413086\n",
      "step = 3606800: loss = 4.097928524017334\n",
      "step = 3607000: loss = 4.706130504608154\n",
      "step = 3607200: loss = 4.024643421173096\n",
      "step = 3607400: loss = 5.18690824508667\n",
      "step = 3607600: loss = 5.021544456481934\n",
      "step = 3607800: loss = 5.099764347076416\n",
      "step = 3608000: loss = 3.7563936710357666\n",
      "step = 3608200: loss = 3.0956010818481445\n",
      "step = 3608400: loss = 4.6179351806640625\n",
      "step = 3608600: loss = 4.692458629608154\n",
      "step = 3608800: loss = 4.207537651062012\n",
      "step = 3609000: loss = 5.587035179138184\n",
      "step = 3609200: loss = 3.601071357727051\n",
      "step = 3609400: loss = 4.973779678344727\n",
      "step = 3609600: loss = 4.870521545410156\n",
      "step = 3609800: loss = 4.170083522796631\n",
      "step = 3610000: loss = 6.339453220367432\n",
      "step = 3610000: Average Return = 3.6419999599456787\n",
      "step = 3610200: loss = 4.317065715789795\n",
      "step = 3610400: loss = 3.947221040725708\n",
      "step = 3610600: loss = 5.791110515594482\n",
      "step = 3610800: loss = 4.8105082511901855\n",
      "step = 3611000: loss = 4.112813472747803\n",
      "step = 3611200: loss = 3.750988006591797\n",
      "step = 3611400: loss = 3.067054271697998\n",
      "step = 3611600: loss = 5.10758638381958\n",
      "step = 3611800: loss = 4.935630798339844\n",
      "step = 3612000: loss = 5.212779998779297\n",
      "step = 3612200: loss = 4.095927715301514\n",
      "step = 3612400: loss = 4.332134246826172\n",
      "step = 3612600: loss = 4.0764970779418945\n",
      "step = 3612800: loss = 3.982670307159424\n",
      "step = 3613000: loss = 4.525121212005615\n",
      "step = 3613200: loss = 3.928960084915161\n",
      "step = 3613400: loss = 3.573575973510742\n",
      "step = 3613600: loss = 4.112763404846191\n",
      "step = 3613800: loss = 3.7819225788116455\n",
      "step = 3614000: loss = 4.811746120452881\n",
      "step = 3614200: loss = 4.627652645111084\n",
      "step = 3614400: loss = 5.430269718170166\n",
      "step = 3614600: loss = 5.308228015899658\n",
      "step = 3614800: loss = 5.304493427276611\n",
      "step = 3615000: loss = 4.171509265899658\n",
      "step = 3615000: Average Return = 3.76200008392334\n",
      "step = 3615200: loss = 3.9587862491607666\n",
      "step = 3615400: loss = 2.627669095993042\n",
      "step = 3615600: loss = 5.262211799621582\n",
      "step = 3615800: loss = 4.114585876464844\n",
      "step = 3616000: loss = 3.0009989738464355\n",
      "step = 3616200: loss = 3.781327962875366\n",
      "step = 3616400: loss = 4.696805953979492\n",
      "step = 3616600: loss = 2.284383535385132\n",
      "step = 3616800: loss = 3.7837016582489014\n",
      "step = 3617000: loss = 5.7906718254089355\n",
      "step = 3617200: loss = 3.3486320972442627\n",
      "step = 3617400: loss = 4.501456260681152\n",
      "step = 3617600: loss = 3.7188498973846436\n",
      "step = 3617800: loss = 3.844503402709961\n",
      "step = 3618000: loss = 6.171226501464844\n",
      "step = 3618200: loss = 5.172369480133057\n",
      "step = 3618400: loss = 3.953754186630249\n",
      "step = 3618600: loss = 2.270418405532837\n",
      "step = 3618800: loss = 4.016096115112305\n",
      "step = 3619000: loss = 3.4878790378570557\n",
      "step = 3619200: loss = 4.884360313415527\n",
      "step = 3619400: loss = 3.663086414337158\n",
      "step = 3619600: loss = 3.2638139724731445\n",
      "step = 3619800: loss = 5.019599437713623\n",
      "step = 3620000: loss = 3.1392006874084473\n",
      "step = 3620000: Average Return = 3.609999895095825\n",
      "step = 3620200: loss = 4.148922443389893\n",
      "step = 3620400: loss = 3.7039473056793213\n",
      "step = 3620600: loss = 4.806906700134277\n",
      "step = 3620800: loss = 5.670878887176514\n",
      "step = 3621000: loss = 5.095482349395752\n",
      "step = 3621200: loss = 4.479732036590576\n",
      "step = 3621400: loss = 3.6344332695007324\n",
      "step = 3621600: loss = 3.745687246322632\n",
      "step = 3621800: loss = 4.349061965942383\n",
      "step = 3622000: loss = 4.86576509475708\n",
      "step = 3622200: loss = 3.487706422805786\n",
      "step = 3622400: loss = 4.202990531921387\n",
      "step = 3622600: loss = 3.8182787895202637\n",
      "step = 3622800: loss = 3.451638698577881\n",
      "step = 3623000: loss = 4.646995544433594\n",
      "step = 3623200: loss = 3.0886852741241455\n",
      "step = 3623400: loss = 4.852143287658691\n",
      "step = 3623600: loss = 2.9118807315826416\n",
      "step = 3623800: loss = 4.435043811798096\n",
      "step = 3624000: loss = 4.311541557312012\n",
      "step = 3624200: loss = 3.61030912399292\n",
      "step = 3624400: loss = 2.5982463359832764\n",
      "step = 3624600: loss = 6.077759742736816\n",
      "step = 3624800: loss = 4.833926200866699\n",
      "step = 3625000: loss = 3.655947208404541\n",
      "step = 3625000: Average Return = 3.497999906539917\n",
      "step = 3625200: loss = 3.2501916885375977\n",
      "step = 3625400: loss = 4.940739154815674\n",
      "step = 3625600: loss = 4.882127285003662\n",
      "step = 3625800: loss = 3.636223554611206\n",
      "step = 3626000: loss = 4.730547904968262\n",
      "step = 3626200: loss = 4.315911293029785\n",
      "step = 3626400: loss = 4.871849060058594\n",
      "step = 3626600: loss = 4.956628799438477\n",
      "step = 3626800: loss = 4.391386032104492\n",
      "step = 3627000: loss = 4.038577079772949\n",
      "step = 3627200: loss = 5.036920547485352\n",
      "step = 3627400: loss = 4.744019031524658\n",
      "step = 3627600: loss = 5.024087429046631\n",
      "step = 3627800: loss = 2.693119525909424\n",
      "step = 3628000: loss = 3.609697103500366\n",
      "step = 3628200: loss = 2.9563324451446533\n",
      "step = 3628400: loss = 3.8967247009277344\n",
      "step = 3628600: loss = 3.478557586669922\n",
      "step = 3628800: loss = 3.505324363708496\n",
      "step = 3629000: loss = 3.640422821044922\n",
      "step = 3629200: loss = 4.093462944030762\n",
      "step = 3629400: loss = 4.939169406890869\n",
      "step = 3629600: loss = 4.650737762451172\n",
      "step = 3629800: loss = 2.244230270385742\n",
      "step = 3630000: loss = 4.7870283126831055\n",
      "step = 3630000: Average Return = 3.7320001125335693\n",
      "step = 3630200: loss = 4.518813610076904\n",
      "step = 3630400: loss = 4.867913722991943\n",
      "step = 3630600: loss = 4.118181228637695\n",
      "step = 3630800: loss = 3.2711448669433594\n",
      "step = 3631000: loss = 4.441177845001221\n",
      "step = 3631200: loss = 4.214017868041992\n",
      "step = 3631400: loss = 4.779017925262451\n",
      "step = 3631600: loss = 4.585185527801514\n",
      "step = 3631800: loss = 5.786496639251709\n",
      "step = 3632000: loss = 3.42720890045166\n",
      "step = 3632200: loss = 4.78727912902832\n",
      "step = 3632400: loss = 3.1733250617980957\n",
      "step = 3632600: loss = 3.9313292503356934\n",
      "step = 3632800: loss = 3.5579779148101807\n",
      "step = 3633000: loss = 4.697053909301758\n",
      "step = 3633200: loss = 3.5156877040863037\n",
      "step = 3633400: loss = 3.692727565765381\n",
      "step = 3633600: loss = 5.359313011169434\n",
      "step = 3633800: loss = 5.712599277496338\n",
      "step = 3634000: loss = 3.6079046726226807\n",
      "step = 3634200: loss = 4.671400547027588\n",
      "step = 3634400: loss = 5.0925750732421875\n",
      "step = 3634600: loss = 4.279174327850342\n",
      "step = 3634800: loss = 4.473222732543945\n",
      "step = 3635000: loss = 4.991353511810303\n",
      "step = 3635000: Average Return = 3.990000009536743\n",
      "step = 3635200: loss = 2.8411505222320557\n",
      "step = 3635400: loss = 4.647668361663818\n",
      "step = 3635600: loss = 3.792645215988159\n",
      "step = 3635800: loss = 4.215202331542969\n",
      "step = 3636000: loss = 5.168736457824707\n",
      "step = 3636200: loss = 5.838583946228027\n",
      "step = 3636400: loss = 4.135610103607178\n",
      "step = 3636600: loss = 4.1625800132751465\n",
      "step = 3636800: loss = 5.128303050994873\n",
      "step = 3637000: loss = 4.686056613922119\n",
      "step = 3637200: loss = 2.917145252227783\n",
      "step = 3637400: loss = 3.8347649574279785\n",
      "step = 3637600: loss = 4.32951545715332\n",
      "step = 3637800: loss = 2.7439043521881104\n",
      "step = 3638000: loss = 4.579089164733887\n",
      "step = 3638200: loss = 3.325901508331299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3638400: loss = 4.681605815887451\n",
      "step = 3638600: loss = 3.6441471576690674\n",
      "step = 3638800: loss = 3.2345495223999023\n",
      "step = 3639000: loss = 3.360117197036743\n",
      "step = 3639200: loss = 5.216530799865723\n",
      "step = 3639400: loss = 4.316096782684326\n",
      "step = 3639600: loss = 3.98207688331604\n",
      "step = 3639800: loss = 3.4860637187957764\n",
      "step = 3640000: loss = 3.9410948753356934\n",
      "step = 3640000: Average Return = 3.7200000286102295\n",
      "step = 3640200: loss = 5.553517818450928\n",
      "step = 3640400: loss = 5.325861930847168\n",
      "step = 3640600: loss = 2.103933572769165\n",
      "step = 3640800: loss = 4.291720390319824\n",
      "step = 3641000: loss = 5.308399677276611\n",
      "step = 3641200: loss = 4.174703121185303\n",
      "step = 3641400: loss = 3.8858630657196045\n",
      "step = 3641600: loss = 3.804525136947632\n",
      "step = 3641800: loss = 4.833484172821045\n",
      "step = 3642000: loss = 2.055170774459839\n",
      "step = 3642200: loss = 5.076020240783691\n",
      "step = 3642400: loss = 4.937548637390137\n",
      "step = 3642600: loss = 4.783154487609863\n",
      "step = 3642800: loss = 5.0853729248046875\n",
      "step = 3643000: loss = 5.293701648712158\n",
      "step = 3643200: loss = 4.349889278411865\n",
      "step = 3643400: loss = 3.675506114959717\n",
      "step = 3643600: loss = 3.983044385910034\n",
      "step = 3643800: loss = 4.381405830383301\n",
      "step = 3644000: loss = 3.945903778076172\n",
      "step = 3644200: loss = 4.573559761047363\n",
      "step = 3644400: loss = 3.0315499305725098\n",
      "step = 3644600: loss = 3.980783224105835\n",
      "step = 3644800: loss = 4.031011581420898\n",
      "step = 3645000: loss = 3.888988971710205\n",
      "step = 3645000: Average Return = 4.03000020980835\n",
      "step = 3645200: loss = 5.07990837097168\n",
      "step = 3645400: loss = 4.254835605621338\n",
      "step = 3645600: loss = 3.0710058212280273\n",
      "step = 3645800: loss = 4.641697406768799\n",
      "step = 3646000: loss = 4.329682350158691\n",
      "step = 3646200: loss = 3.6739070415496826\n",
      "step = 3646400: loss = 4.6012701988220215\n",
      "step = 3646600: loss = 4.322080612182617\n",
      "step = 3646800: loss = 4.162387371063232\n",
      "step = 3647000: loss = 5.520314693450928\n",
      "step = 3647200: loss = 5.234362602233887\n",
      "step = 3647400: loss = 5.061864852905273\n",
      "step = 3647600: loss = 6.225605487823486\n",
      "step = 3647800: loss = 2.518990993499756\n",
      "step = 3648000: loss = 4.865406513214111\n",
      "step = 3648200: loss = 4.208706855773926\n",
      "step = 3648400: loss = 3.67937970161438\n",
      "step = 3648600: loss = 4.081082820892334\n",
      "step = 3648800: loss = 5.692720413208008\n",
      "step = 3649000: loss = 4.829779148101807\n",
      "step = 3649200: loss = 4.350708961486816\n",
      "step = 3649400: loss = 3.27694034576416\n",
      "step = 3649600: loss = 3.9392471313476562\n",
      "step = 3649800: loss = 4.0392866134643555\n",
      "step = 3650000: loss = 3.597238063812256\n",
      "step = 3650000: Average Return = 3.885999917984009\n",
      "step = 3650200: loss = 4.312067031860352\n",
      "step = 3650400: loss = 6.255401611328125\n",
      "step = 3650600: loss = 3.5137524604797363\n",
      "step = 3650800: loss = 4.2354021072387695\n",
      "step = 3651000: loss = 3.288052558898926\n",
      "step = 3651200: loss = 3.1258671283721924\n",
      "step = 3651400: loss = 4.069581031799316\n",
      "step = 3651600: loss = 4.165307998657227\n",
      "step = 3651800: loss = 4.672000885009766\n",
      "step = 3652000: loss = 3.4585604667663574\n",
      "step = 3652200: loss = 4.4110212326049805\n",
      "step = 3652400: loss = 4.067607879638672\n",
      "step = 3652600: loss = 4.0668230056762695\n",
      "step = 3652800: loss = 3.577376127243042\n",
      "step = 3653000: loss = 5.201979637145996\n",
      "step = 3653200: loss = 4.090733051300049\n",
      "step = 3653400: loss = 3.935244083404541\n",
      "step = 3653600: loss = 4.242819309234619\n",
      "step = 3653800: loss = 4.945252418518066\n",
      "step = 3654000: loss = 4.331596374511719\n",
      "step = 3654200: loss = 3.288161277770996\n",
      "step = 3654400: loss = 3.3398704528808594\n",
      "step = 3654600: loss = 3.8043572902679443\n",
      "step = 3654800: loss = 4.672258377075195\n",
      "step = 3655000: loss = 2.923231363296509\n",
      "step = 3655000: Average Return = 3.76200008392334\n",
      "step = 3655200: loss = 4.159900188446045\n",
      "step = 3655400: loss = 2.802155017852783\n",
      "step = 3655600: loss = 5.455924034118652\n",
      "step = 3655800: loss = 2.6871113777160645\n",
      "step = 3656000: loss = 3.454576253890991\n",
      "step = 3656200: loss = 4.853198051452637\n",
      "step = 3656400: loss = 3.7919180393218994\n",
      "step = 3656600: loss = 4.457934379577637\n",
      "step = 3656800: loss = 4.7646284103393555\n",
      "step = 3657000: loss = 4.3845319747924805\n",
      "step = 3657200: loss = 3.2276275157928467\n",
      "step = 3657400: loss = 4.342461585998535\n",
      "step = 3657600: loss = 3.330000877380371\n",
      "step = 3657800: loss = 5.146263122558594\n",
      "step = 3658000: loss = 3.3276619911193848\n",
      "step = 3658200: loss = 5.646981239318848\n",
      "step = 3658400: loss = 3.741422653198242\n",
      "step = 3658600: loss = 4.359389781951904\n",
      "step = 3658800: loss = 4.324594497680664\n",
      "step = 3659000: loss = 2.8139255046844482\n",
      "step = 3659200: loss = 4.043764591217041\n",
      "step = 3659400: loss = 3.905876874923706\n",
      "step = 3659600: loss = 3.4652721881866455\n",
      "step = 3659800: loss = 4.355203151702881\n",
      "step = 3660000: loss = 4.655160903930664\n",
      "step = 3660000: Average Return = 3.680000066757202\n",
      "step = 3660200: loss = 4.847728729248047\n",
      "step = 3660400: loss = 3.4377665519714355\n",
      "step = 3660600: loss = 4.21797513961792\n",
      "step = 3660800: loss = 4.857812404632568\n",
      "step = 3661000: loss = 4.653196811676025\n",
      "step = 3661200: loss = 4.894526481628418\n",
      "step = 3661400: loss = 4.334352970123291\n",
      "step = 3661600: loss = 4.171416282653809\n",
      "step = 3661800: loss = 5.017350196838379\n",
      "step = 3662000: loss = 3.686366081237793\n",
      "step = 3662200: loss = 4.675548076629639\n",
      "step = 3662400: loss = 4.46083402633667\n",
      "step = 3662600: loss = 3.635714292526245\n",
      "step = 3662800: loss = 4.540473937988281\n",
      "step = 3663000: loss = 5.615474700927734\n",
      "step = 3663200: loss = 4.905141353607178\n",
      "step = 3663400: loss = 3.487579345703125\n",
      "step = 3663600: loss = 3.8528006076812744\n",
      "step = 3663800: loss = 4.028768062591553\n",
      "step = 3664000: loss = 4.308099746704102\n",
      "step = 3664200: loss = 4.256443500518799\n",
      "step = 3664400: loss = 2.998751401901245\n",
      "step = 3664600: loss = 4.0458598136901855\n",
      "step = 3664800: loss = 4.856739521026611\n",
      "step = 3665000: loss = 4.032015800476074\n",
      "step = 3665000: Average Return = 3.888000011444092\n",
      "step = 3665200: loss = 2.9524364471435547\n",
      "step = 3665400: loss = 3.1349937915802\n",
      "step = 3665600: loss = 3.90429949760437\n",
      "step = 3665800: loss = 4.96681547164917\n",
      "step = 3666000: loss = 4.3684258460998535\n",
      "step = 3666200: loss = 5.001823425292969\n",
      "step = 3666400: loss = 3.573409080505371\n",
      "step = 3666600: loss = 4.473130226135254\n",
      "step = 3666800: loss = 5.114635944366455\n",
      "step = 3667000: loss = 3.653658390045166\n",
      "step = 3667200: loss = 5.430963516235352\n",
      "step = 3667400: loss = 4.328178405761719\n",
      "step = 3667600: loss = 5.029853820800781\n",
      "step = 3667800: loss = 3.905339241027832\n",
      "step = 3668000: loss = 4.905821800231934\n",
      "step = 3668200: loss = 5.367042541503906\n",
      "step = 3668400: loss = 3.183528423309326\n",
      "step = 3668600: loss = 3.8530352115631104\n",
      "step = 3668800: loss = 3.837581157684326\n",
      "step = 3669000: loss = 4.13662052154541\n",
      "step = 3669200: loss = 3.395156145095825\n",
      "step = 3669400: loss = 4.576411247253418\n",
      "step = 3669600: loss = 2.911893129348755\n",
      "step = 3669800: loss = 4.8645477294921875\n",
      "step = 3670000: loss = 3.581404447555542\n",
      "step = 3670000: Average Return = 4.314000129699707\n",
      "step = 3670200: loss = 4.828949928283691\n",
      "step = 3670400: loss = 5.858748435974121\n",
      "step = 3670600: loss = 4.6100287437438965\n",
      "step = 3670800: loss = 3.9553301334381104\n",
      "step = 3671000: loss = 2.966697931289673\n",
      "step = 3671200: loss = 5.353270053863525\n",
      "step = 3671400: loss = 3.395181179046631\n",
      "step = 3671600: loss = 4.3997650146484375\n",
      "step = 3671800: loss = 3.7837886810302734\n",
      "step = 3672000: loss = 4.144402503967285\n",
      "step = 3672200: loss = 5.4842095375061035\n",
      "step = 3672400: loss = 5.685959815979004\n",
      "step = 3672600: loss = 3.0446085929870605\n",
      "step = 3672800: loss = 4.473447799682617\n",
      "step = 3673000: loss = 4.705456733703613\n",
      "step = 3673200: loss = 4.686980724334717\n",
      "step = 3673400: loss = 4.2338056564331055\n",
      "step = 3673600: loss = 4.447145462036133\n",
      "step = 3673800: loss = 3.0971145629882812\n",
      "step = 3674000: loss = 3.6412432193756104\n",
      "step = 3674200: loss = 5.090054988861084\n",
      "step = 3674400: loss = 5.800936222076416\n",
      "step = 3674600: loss = 3.807377576828003\n",
      "step = 3674800: loss = 4.147904396057129\n",
      "step = 3675000: loss = 5.885572910308838\n",
      "step = 3675000: Average Return = 3.869999885559082\n",
      "step = 3675200: loss = 4.157365322113037\n",
      "step = 3675400: loss = 3.6143550872802734\n",
      "step = 3675600: loss = 4.321025848388672\n",
      "step = 3675800: loss = 5.7841033935546875\n",
      "step = 3676000: loss = 3.8451268672943115\n",
      "step = 3676200: loss = 4.557724475860596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3676400: loss = 3.5689449310302734\n",
      "step = 3676600: loss = 5.426740646362305\n",
      "step = 3676800: loss = 3.6486828327178955\n",
      "step = 3677000: loss = 3.8333616256713867\n",
      "step = 3677200: loss = 4.507110118865967\n",
      "step = 3677400: loss = 5.059630393981934\n",
      "step = 3677600: loss = 4.100925922393799\n",
      "step = 3677800: loss = 3.7227723598480225\n",
      "step = 3678000: loss = 3.2138969898223877\n",
      "step = 3678200: loss = 4.923384666442871\n",
      "step = 3678400: loss = 2.5943384170532227\n",
      "step = 3678600: loss = 2.429582118988037\n",
      "step = 3678800: loss = 5.687103748321533\n",
      "step = 3679000: loss = 2.4602715969085693\n",
      "step = 3679200: loss = 5.070460319519043\n",
      "step = 3679400: loss = 3.626354217529297\n",
      "step = 3679600: loss = 5.878307819366455\n",
      "step = 3679800: loss = 3.5296809673309326\n",
      "step = 3680000: loss = 3.937509775161743\n",
      "step = 3680000: Average Return = 3.7119998931884766\n",
      "step = 3680200: loss = 4.78031587600708\n",
      "step = 3680400: loss = 5.442485332489014\n",
      "step = 3680600: loss = 5.313732147216797\n",
      "step = 3680800: loss = 3.2413415908813477\n",
      "step = 3681000: loss = 2.951340913772583\n",
      "step = 3681200: loss = 3.9117729663848877\n",
      "step = 3681400: loss = 2.669752836227417\n",
      "step = 3681600: loss = 3.1345837116241455\n",
      "step = 3681800: loss = 4.060473442077637\n",
      "step = 3682000: loss = 3.3029744625091553\n",
      "step = 3682200: loss = 4.06801176071167\n",
      "step = 3682400: loss = 4.919200897216797\n",
      "step = 3682600: loss = 2.065380573272705\n",
      "step = 3682800: loss = 3.6418728828430176\n",
      "step = 3683000: loss = 5.082242965698242\n",
      "step = 3683200: loss = 4.564334392547607\n",
      "step = 3683400: loss = 4.067251682281494\n",
      "step = 3683600: loss = 3.8199172019958496\n",
      "step = 3683800: loss = 4.369939804077148\n",
      "step = 3684000: loss = 4.61497163772583\n",
      "step = 3684200: loss = 2.268167734146118\n",
      "step = 3684400: loss = 5.931918144226074\n",
      "step = 3684600: loss = 3.8486602306365967\n",
      "step = 3684800: loss = 3.215942859649658\n",
      "step = 3685000: loss = 3.886279821395874\n",
      "step = 3685000: Average Return = 3.555999994277954\n",
      "step = 3685200: loss = 4.348143100738525\n",
      "step = 3685400: loss = 4.965732097625732\n",
      "step = 3685600: loss = 2.316861152648926\n",
      "step = 3685800: loss = 4.879178047180176\n",
      "step = 3686000: loss = 4.172142028808594\n",
      "step = 3686200: loss = 4.041194438934326\n",
      "step = 3686400: loss = 3.1173107624053955\n",
      "step = 3686600: loss = 3.0126984119415283\n",
      "step = 3686800: loss = 5.278171062469482\n",
      "step = 3687000: loss = 3.8714916706085205\n",
      "step = 3687200: loss = 4.762932777404785\n",
      "step = 3687400: loss = 4.335633277893066\n",
      "step = 3687600: loss = 4.654174327850342\n",
      "step = 3687800: loss = 3.941113233566284\n",
      "step = 3688000: loss = 5.681906700134277\n",
      "step = 3688200: loss = 4.553901195526123\n",
      "step = 3688400: loss = 4.5036091804504395\n",
      "step = 3688600: loss = 2.717294216156006\n",
      "step = 3688800: loss = 3.212797164916992\n",
      "step = 3689000: loss = 4.050409317016602\n",
      "step = 3689200: loss = 4.883548736572266\n",
      "step = 3689400: loss = 5.177198886871338\n",
      "step = 3689600: loss = 3.6757283210754395\n",
      "step = 3689800: loss = 4.251794338226318\n",
      "step = 3690000: loss = 4.4898681640625\n",
      "step = 3690000: Average Return = 3.5820000171661377\n",
      "step = 3690200: loss = 3.260779857635498\n",
      "step = 3690400: loss = 5.273782730102539\n",
      "step = 3690600: loss = 4.689940929412842\n",
      "step = 3690800: loss = 3.46614670753479\n",
      "step = 3691000: loss = 4.823478698730469\n",
      "step = 3691200: loss = 4.149672985076904\n",
      "step = 3691400: loss = 3.8854780197143555\n",
      "step = 3691600: loss = 3.986406087875366\n",
      "step = 3691800: loss = 4.235647678375244\n",
      "step = 3692000: loss = 5.111756324768066\n",
      "step = 3692200: loss = 6.2725067138671875\n",
      "step = 3692400: loss = 4.52211856842041\n",
      "step = 3692600: loss = 4.262514591217041\n",
      "step = 3692800: loss = 3.4803225994110107\n",
      "step = 3693000: loss = 4.096097469329834\n",
      "step = 3693200: loss = 4.137978553771973\n",
      "step = 3693400: loss = 4.255318641662598\n",
      "step = 3693600: loss = 5.0661444664001465\n",
      "step = 3693800: loss = 4.605534553527832\n",
      "step = 3694000: loss = 4.008682727813721\n",
      "step = 3694200: loss = 5.60755729675293\n",
      "step = 3694400: loss = 3.0005743503570557\n",
      "step = 3694600: loss = 5.406497001647949\n",
      "step = 3694800: loss = 5.662867069244385\n",
      "step = 3695000: loss = 3.2794342041015625\n",
      "step = 3695000: Average Return = 3.7660000324249268\n",
      "step = 3695200: loss = 5.311942100524902\n",
      "step = 3695400: loss = 3.766568183898926\n",
      "step = 3695600: loss = 4.215410232543945\n",
      "step = 3695800: loss = 3.656087636947632\n",
      "step = 3696000: loss = 4.025094032287598\n",
      "step = 3696200: loss = 2.6812987327575684\n",
      "step = 3696400: loss = 3.0129594802856445\n",
      "step = 3696600: loss = 4.572023391723633\n",
      "step = 3696800: loss = 4.355361461639404\n",
      "step = 3697000: loss = 5.048022747039795\n",
      "step = 3697200: loss = 3.496887683868408\n",
      "step = 3697400: loss = 5.9136857986450195\n",
      "step = 3697600: loss = 3.1950302124023438\n",
      "step = 3697800: loss = 4.236738681793213\n",
      "step = 3698000: loss = 4.362353324890137\n",
      "step = 3698200: loss = 4.385377407073975\n",
      "step = 3698400: loss = 3.7075793743133545\n",
      "step = 3698600: loss = 5.283052921295166\n",
      "step = 3698800: loss = 4.724669933319092\n",
      "step = 3699000: loss = 5.282277584075928\n",
      "step = 3699200: loss = 3.518564462661743\n",
      "step = 3699400: loss = 3.3324742317199707\n",
      "step = 3699600: loss = 4.374924659729004\n",
      "step = 3699800: loss = 5.980413913726807\n",
      "step = 3700000: loss = 4.278870582580566\n",
      "step = 3700000: Average Return = 3.619999885559082\n",
      "step = 3700200: loss = 5.039379596710205\n",
      "step = 3700400: loss = 3.9801061153411865\n",
      "step = 3700600: loss = 2.825958728790283\n",
      "step = 3700800: loss = 5.425996780395508\n",
      "step = 3701000: loss = 4.357751369476318\n",
      "step = 3701200: loss = 3.691925525665283\n",
      "step = 3701400: loss = 4.73050594329834\n",
      "step = 3701600: loss = 5.542273044586182\n",
      "step = 3701800: loss = 4.610395431518555\n",
      "step = 3702000: loss = 4.29818868637085\n",
      "step = 3702200: loss = 2.928864002227783\n",
      "step = 3702400: loss = 6.025232315063477\n",
      "step = 3702600: loss = 4.4865264892578125\n",
      "step = 3702800: loss = 4.466472148895264\n",
      "step = 3703000: loss = 4.794941425323486\n",
      "step = 3703200: loss = 4.20290994644165\n",
      "step = 3703400: loss = 4.705349445343018\n",
      "step = 3703600: loss = 3.17375111579895\n",
      "step = 3703800: loss = 3.972442150115967\n",
      "step = 3704000: loss = 4.900944232940674\n",
      "step = 3704200: loss = 5.112082004547119\n",
      "step = 3704400: loss = 4.941296577453613\n",
      "step = 3704600: loss = 2.8150575160980225\n",
      "step = 3704800: loss = 2.4354329109191895\n",
      "step = 3705000: loss = 4.681613445281982\n",
      "step = 3705000: Average Return = 3.747999906539917\n",
      "step = 3705200: loss = 3.9741673469543457\n",
      "step = 3705400: loss = 4.893496036529541\n",
      "step = 3705600: loss = 4.444812774658203\n",
      "step = 3705800: loss = 2.91451096534729\n",
      "step = 3706000: loss = 4.044803142547607\n",
      "step = 3706200: loss = 3.94679856300354\n",
      "step = 3706400: loss = 4.6998515129089355\n",
      "step = 3706600: loss = 3.365490674972534\n",
      "step = 3706800: loss = 4.332055568695068\n",
      "step = 3707000: loss = 4.386688709259033\n",
      "step = 3707200: loss = 5.688250541687012\n",
      "step = 3707400: loss = 4.364022254943848\n",
      "step = 3707600: loss = 4.0732879638671875\n",
      "step = 3707800: loss = 3.8286821842193604\n",
      "step = 3708000: loss = 4.007261753082275\n",
      "step = 3708200: loss = 3.5783684253692627\n",
      "step = 3708400: loss = 5.073378562927246\n",
      "step = 3708600: loss = 4.528724670410156\n",
      "step = 3708800: loss = 3.5270602703094482\n",
      "step = 3709000: loss = 5.684384822845459\n",
      "step = 3709200: loss = 3.375869035720825\n",
      "step = 3709400: loss = 4.424370288848877\n",
      "step = 3709600: loss = 3.4012291431427\n",
      "step = 3709800: loss = 3.603394031524658\n",
      "step = 3710000: loss = 4.332571983337402\n",
      "step = 3710000: Average Return = 3.99399995803833\n",
      "step = 3710200: loss = 3.714355945587158\n",
      "step = 3710400: loss = 3.75895619392395\n",
      "step = 3710600: loss = 4.719607353210449\n",
      "step = 3710800: loss = 3.991755962371826\n",
      "step = 3711000: loss = 4.8880228996276855\n",
      "step = 3711200: loss = 4.207144737243652\n",
      "step = 3711400: loss = 3.485654592514038\n",
      "step = 3711600: loss = 4.152366638183594\n",
      "step = 3711800: loss = 3.4462883472442627\n",
      "step = 3712000: loss = 3.568349599838257\n",
      "step = 3712200: loss = 4.1643900871276855\n",
      "step = 3712400: loss = 4.9381513595581055\n",
      "step = 3712600: loss = 4.236223220825195\n",
      "step = 3712800: loss = 3.9172821044921875\n",
      "step = 3713000: loss = 4.439929008483887\n",
      "step = 3713200: loss = 3.5471653938293457\n",
      "step = 3713400: loss = 4.583451271057129\n",
      "step = 3713600: loss = 3.324392080307007\n",
      "step = 3713800: loss = 4.159487724304199\n",
      "step = 3714000: loss = 4.344022750854492\n",
      "step = 3714200: loss = 4.722161769866943\n",
      "step = 3714400: loss = 3.809352397918701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3714600: loss = 5.2693939208984375\n",
      "step = 3714800: loss = 4.102055549621582\n",
      "step = 3715000: loss = 4.936323642730713\n",
      "step = 3715000: Average Return = 4.084000110626221\n",
      "step = 3715200: loss = 4.705024242401123\n",
      "step = 3715400: loss = 2.4726314544677734\n",
      "step = 3715600: loss = 3.8382201194763184\n",
      "step = 3715800: loss = 3.3269307613372803\n",
      "step = 3716000: loss = 3.7898576259613037\n",
      "step = 3716200: loss = 4.795657634735107\n",
      "step = 3716400: loss = 3.6404643058776855\n",
      "step = 3716600: loss = 4.013186454772949\n",
      "step = 3716800: loss = 5.032010555267334\n",
      "step = 3717000: loss = 4.618906497955322\n",
      "step = 3717200: loss = 5.264962673187256\n",
      "step = 3717400: loss = 6.716338157653809\n",
      "step = 3717600: loss = 4.783749103546143\n",
      "step = 3717800: loss = 4.518209934234619\n",
      "step = 3718000: loss = 5.692015647888184\n",
      "step = 3718200: loss = 4.407270908355713\n",
      "step = 3718400: loss = 4.667444229125977\n",
      "step = 3718600: loss = 4.55531120300293\n",
      "step = 3718800: loss = 3.559021472930908\n",
      "step = 3719000: loss = 4.945703506469727\n",
      "step = 3719200: loss = 5.122470378875732\n",
      "step = 3719400: loss = 5.21333646774292\n",
      "step = 3719600: loss = 3.532043695449829\n",
      "step = 3719800: loss = 3.7186973094940186\n",
      "step = 3720000: loss = 3.7988359928131104\n",
      "step = 3720000: Average Return = 4.099999904632568\n",
      "step = 3720200: loss = 5.070041179656982\n",
      "step = 3720400: loss = 4.773533344268799\n",
      "step = 3720600: loss = 5.1819748878479\n",
      "step = 3720800: loss = 4.3080620765686035\n",
      "step = 3721000: loss = 4.05124568939209\n",
      "step = 3721200: loss = 4.427558898925781\n",
      "step = 3721400: loss = 3.5124218463897705\n",
      "step = 3721600: loss = 4.636435508728027\n",
      "step = 3721800: loss = 5.299262523651123\n",
      "step = 3722000: loss = 3.628467321395874\n",
      "step = 3722200: loss = 5.384298801422119\n",
      "step = 3722400: loss = 5.59669303894043\n",
      "step = 3722600: loss = 4.300745010375977\n",
      "step = 3722800: loss = 5.960267066955566\n",
      "step = 3723000: loss = 3.3760156631469727\n",
      "step = 3723200: loss = 3.4234652519226074\n",
      "step = 3723400: loss = 5.599361896514893\n",
      "step = 3723600: loss = 4.072825908660889\n",
      "step = 3723800: loss = 4.594366073608398\n",
      "step = 3724000: loss = 3.3193883895874023\n",
      "step = 3724200: loss = 5.1580729484558105\n",
      "step = 3724400: loss = 2.8009729385375977\n",
      "step = 3724600: loss = 3.401313304901123\n",
      "step = 3724800: loss = 3.5153982639312744\n",
      "step = 3725000: loss = 2.895829439163208\n",
      "step = 3725000: Average Return = 3.7279999256134033\n",
      "step = 3725200: loss = 4.342837333679199\n",
      "step = 3725400: loss = 3.199204921722412\n",
      "step = 3725600: loss = 4.1742753982543945\n",
      "step = 3725800: loss = 3.879523277282715\n",
      "step = 3726000: loss = 4.201996803283691\n",
      "step = 3726200: loss = 5.20894718170166\n",
      "step = 3726400: loss = 4.424430847167969\n",
      "step = 3726600: loss = 4.789735794067383\n",
      "step = 3726800: loss = 3.313371181488037\n",
      "step = 3727000: loss = 3.7728359699249268\n",
      "step = 3727200: loss = 3.333822011947632\n",
      "step = 3727400: loss = 4.827845573425293\n",
      "step = 3727600: loss = 4.779195308685303\n",
      "step = 3727800: loss = 3.2137815952301025\n",
      "step = 3728000: loss = 4.229506015777588\n",
      "step = 3728200: loss = 5.1816816329956055\n",
      "step = 3728400: loss = 3.68022084236145\n",
      "step = 3728600: loss = 4.955370903015137\n",
      "step = 3728800: loss = 4.201356887817383\n",
      "step = 3729000: loss = 4.5571465492248535\n",
      "step = 3729200: loss = 2.8883142471313477\n",
      "step = 3729400: loss = 3.9221370220184326\n",
      "step = 3729600: loss = 5.8938493728637695\n",
      "step = 3729800: loss = 5.872522830963135\n",
      "step = 3730000: loss = 3.891343355178833\n",
      "step = 3730000: Average Return = 3.8239998817443848\n",
      "step = 3730200: loss = 4.3830366134643555\n",
      "step = 3730400: loss = 4.279210090637207\n",
      "step = 3730600: loss = 3.411872386932373\n",
      "step = 3730800: loss = 3.157254457473755\n",
      "step = 3731000: loss = 4.491693496704102\n",
      "step = 3731200: loss = 3.2200353145599365\n",
      "step = 3731400: loss = 4.460674285888672\n",
      "step = 3731600: loss = 4.899543285369873\n",
      "step = 3731800: loss = 3.4255876541137695\n",
      "step = 3732000: loss = 5.199053764343262\n",
      "step = 3732200: loss = 6.822421550750732\n",
      "step = 3732400: loss = 4.375421047210693\n",
      "step = 3732600: loss = 3.6008496284484863\n",
      "step = 3732800: loss = 4.1466965675354\n",
      "step = 3733000: loss = 4.275702953338623\n",
      "step = 3733200: loss = 4.697811126708984\n",
      "step = 3733400: loss = 4.070407390594482\n",
      "step = 3733600: loss = 4.6911821365356445\n",
      "step = 3733800: loss = 5.782729148864746\n",
      "step = 3734000: loss = 4.053918361663818\n",
      "step = 3734200: loss = 4.924073219299316\n",
      "step = 3734400: loss = 4.360424518585205\n",
      "step = 3734600: loss = 4.179316997528076\n",
      "step = 3734800: loss = 5.325202465057373\n",
      "step = 3735000: loss = 5.012424468994141\n",
      "step = 3735000: Average Return = 3.696000099182129\n",
      "step = 3735200: loss = 3.6224684715270996\n",
      "step = 3735400: loss = 3.858405113220215\n",
      "step = 3735600: loss = 4.062239170074463\n",
      "step = 3735800: loss = 3.8540263175964355\n",
      "step = 3736000: loss = 4.531612873077393\n",
      "step = 3736200: loss = 5.4412641525268555\n",
      "step = 3736400: loss = 4.179168224334717\n",
      "step = 3736600: loss = 3.136307954788208\n",
      "step = 3736800: loss = 6.106133937835693\n",
      "step = 3737000: loss = 3.8537957668304443\n",
      "step = 3737200: loss = 3.5177907943725586\n",
      "step = 3737400: loss = 5.185931205749512\n",
      "step = 3737600: loss = 4.561755180358887\n",
      "step = 3737800: loss = 4.463298320770264\n",
      "step = 3738000: loss = 4.7781548500061035\n",
      "step = 3738200: loss = 2.7478997707366943\n",
      "step = 3738400: loss = 3.5942177772521973\n",
      "step = 3738600: loss = 3.766845226287842\n",
      "step = 3738800: loss = 4.872427463531494\n",
      "step = 3739000: loss = 4.640999794006348\n",
      "step = 3739200: loss = 4.00474739074707\n",
      "step = 3739400: loss = 3.64452862739563\n",
      "step = 3739600: loss = 4.69514274597168\n",
      "step = 3739800: loss = 4.946486473083496\n",
      "step = 3740000: loss = 4.456521987915039\n",
      "step = 3740000: Average Return = 4.015999794006348\n",
      "step = 3740200: loss = 2.907473087310791\n",
      "step = 3740400: loss = 4.1577348709106445\n",
      "step = 3740600: loss = 4.58565616607666\n",
      "step = 3740800: loss = 4.683481216430664\n",
      "step = 3741000: loss = 4.112762451171875\n",
      "step = 3741200: loss = 5.363128662109375\n",
      "step = 3741400: loss = 4.4927215576171875\n",
      "step = 3741600: loss = 4.145509719848633\n",
      "step = 3741800: loss = 5.759833335876465\n",
      "step = 3742000: loss = 4.289345741271973\n",
      "step = 3742200: loss = 4.78253698348999\n",
      "step = 3742400: loss = 4.652290344238281\n",
      "step = 3742600: loss = 3.4580352306365967\n",
      "step = 3742800: loss = 5.470526218414307\n",
      "step = 3743000: loss = 4.134181976318359\n",
      "step = 3743200: loss = 3.040902614593506\n",
      "step = 3743400: loss = 4.998561382293701\n",
      "step = 3743600: loss = 3.542759895324707\n",
      "step = 3743800: loss = 3.515582799911499\n",
      "step = 3744000: loss = 3.8524487018585205\n",
      "step = 3744200: loss = 4.26809549331665\n",
      "step = 3744400: loss = 5.3563995361328125\n",
      "step = 3744600: loss = 5.989971160888672\n",
      "step = 3744800: loss = 5.573206424713135\n",
      "step = 3745000: loss = 3.029278516769409\n",
      "step = 3745000: Average Return = 3.890000104904175\n",
      "step = 3745200: loss = 2.8634064197540283\n",
      "step = 3745400: loss = 3.569409132003784\n",
      "step = 3745600: loss = 2.8436872959136963\n",
      "step = 3745800: loss = 4.498973369598389\n",
      "step = 3746000: loss = 4.234114646911621\n",
      "step = 3746200: loss = 4.505360126495361\n",
      "step = 3746400: loss = 3.094585418701172\n",
      "step = 3746600: loss = 5.893805980682373\n",
      "step = 3746800: loss = 3.609776496887207\n",
      "step = 3747000: loss = 3.376957893371582\n",
      "step = 3747200: loss = 3.24399471282959\n",
      "step = 3747400: loss = 4.743576526641846\n",
      "step = 3747600: loss = 4.625743389129639\n",
      "step = 3747800: loss = 5.728088855743408\n",
      "step = 3748000: loss = 4.980523109436035\n",
      "step = 3748200: loss = 3.7155075073242188\n",
      "step = 3748400: loss = 4.132010459899902\n",
      "step = 3748600: loss = 6.087745189666748\n",
      "step = 3748800: loss = 2.8241164684295654\n",
      "step = 3749000: loss = 3.850811004638672\n",
      "step = 3749200: loss = 3.5185375213623047\n",
      "step = 3749400: loss = 5.401571750640869\n",
      "step = 3749600: loss = 4.026934623718262\n",
      "step = 3749800: loss = 3.966681480407715\n",
      "step = 3750000: loss = 3.7087807655334473\n",
      "step = 3750000: Average Return = 3.7920000553131104\n",
      "step = 3750200: loss = 3.461653470993042\n",
      "step = 3750400: loss = 6.0239996910095215\n",
      "step = 3750600: loss = 4.632393836975098\n",
      "step = 3750800: loss = 3.096134662628174\n",
      "step = 3751000: loss = 3.811815023422241\n",
      "step = 3751200: loss = 3.0768115520477295\n",
      "step = 3751400: loss = 4.251988887786865\n",
      "step = 3751600: loss = 4.001987457275391\n",
      "step = 3751800: loss = 2.8618223667144775\n",
      "step = 3752000: loss = 3.7954225540161133\n",
      "step = 3752200: loss = 3.387140989303589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3752400: loss = 3.9522173404693604\n",
      "step = 3752600: loss = 3.7876477241516113\n",
      "step = 3752800: loss = 3.9206056594848633\n",
      "step = 3753000: loss = 3.2965283393859863\n",
      "step = 3753200: loss = 3.4742350578308105\n",
      "step = 3753400: loss = 6.880862712860107\n",
      "step = 3753600: loss = 4.76318359375\n",
      "step = 3753800: loss = 3.8288662433624268\n",
      "step = 3754000: loss = 3.310765027999878\n",
      "step = 3754200: loss = 3.786057949066162\n",
      "step = 3754400: loss = 4.37117338180542\n",
      "step = 3754600: loss = 4.186617374420166\n",
      "step = 3754800: loss = 4.567382335662842\n",
      "step = 3755000: loss = 5.439533710479736\n",
      "step = 3755000: Average Return = 4.01800012588501\n",
      "step = 3755200: loss = 4.449375629425049\n",
      "step = 3755400: loss = 4.157309532165527\n",
      "step = 3755600: loss = 4.217251300811768\n",
      "step = 3755800: loss = 2.4720544815063477\n",
      "step = 3756000: loss = 4.470427513122559\n",
      "step = 3756200: loss = 4.500328063964844\n",
      "step = 3756400: loss = 3.857419729232788\n",
      "step = 3756600: loss = 4.019069671630859\n",
      "step = 3756800: loss = 3.629615306854248\n",
      "step = 3757000: loss = 4.376224040985107\n",
      "step = 3757200: loss = 4.21419620513916\n",
      "step = 3757400: loss = 5.640749454498291\n",
      "step = 3757600: loss = 2.483090400695801\n",
      "step = 3757800: loss = 3.46972393989563\n",
      "step = 3758000: loss = 4.276704788208008\n",
      "step = 3758200: loss = 4.164813995361328\n",
      "step = 3758400: loss = 3.7622880935668945\n",
      "step = 3758600: loss = 4.539565086364746\n",
      "step = 3758800: loss = 4.434479713439941\n",
      "step = 3759000: loss = 3.2491729259490967\n",
      "step = 3759200: loss = 3.281179666519165\n",
      "step = 3759400: loss = 4.953240871429443\n",
      "step = 3759600: loss = 3.9837396144866943\n",
      "step = 3759800: loss = 2.9928743839263916\n",
      "step = 3760000: loss = 3.6508073806762695\n",
      "step = 3760000: Average Return = 3.7300000190734863\n",
      "step = 3760200: loss = 2.9531168937683105\n",
      "step = 3760400: loss = 5.107875347137451\n",
      "step = 3760600: loss = 4.8244452476501465\n",
      "step = 3760800: loss = 4.026353359222412\n",
      "step = 3761000: loss = 2.9203603267669678\n",
      "step = 3761200: loss = 5.036699295043945\n",
      "step = 3761400: loss = 3.244607448577881\n",
      "step = 3761600: loss = 4.622076034545898\n",
      "step = 3761800: loss = 4.7066969871521\n",
      "step = 3762000: loss = 3.801032543182373\n",
      "step = 3762200: loss = 4.2244367599487305\n",
      "step = 3762400: loss = 3.9948675632476807\n",
      "step = 3762600: loss = 4.139344215393066\n",
      "step = 3762800: loss = 4.93496561050415\n",
      "step = 3763000: loss = 4.553393840789795\n",
      "step = 3763200: loss = 4.306327819824219\n",
      "step = 3763400: loss = 3.5718181133270264\n",
      "step = 3763600: loss = 3.165903329849243\n",
      "step = 3763800: loss = 4.331836700439453\n",
      "step = 3764000: loss = 5.041034698486328\n",
      "step = 3764200: loss = 4.472532749176025\n",
      "step = 3764400: loss = 4.338699817657471\n",
      "step = 3764600: loss = 5.218127727508545\n",
      "step = 3764800: loss = 3.546999931335449\n",
      "step = 3765000: loss = 4.266200542449951\n",
      "step = 3765000: Average Return = 3.2839999198913574\n",
      "step = 3765200: loss = 5.961076736450195\n",
      "step = 3765400: loss = 4.2265543937683105\n",
      "step = 3765600: loss = 4.871456146240234\n",
      "step = 3765800: loss = 3.6318206787109375\n",
      "step = 3766000: loss = 4.557535171508789\n",
      "step = 3766200: loss = 3.936436176300049\n",
      "step = 3766400: loss = 5.309028625488281\n",
      "step = 3766600: loss = 4.272956371307373\n",
      "step = 3766800: loss = 4.027463436126709\n",
      "step = 3767000: loss = 3.733342409133911\n",
      "step = 3767200: loss = 4.449558734893799\n",
      "step = 3767400: loss = 3.5042812824249268\n",
      "step = 3767600: loss = 4.604641914367676\n",
      "step = 3767800: loss = 4.481177806854248\n",
      "step = 3768000: loss = 3.382814884185791\n",
      "step = 3768200: loss = 4.525794506072998\n",
      "step = 3768400: loss = 3.4624884128570557\n",
      "step = 3768600: loss = 4.740535259246826\n",
      "step = 3768800: loss = 3.1715774536132812\n",
      "step = 3769000: loss = 4.972329139709473\n",
      "step = 3769200: loss = 4.440968036651611\n",
      "step = 3769400: loss = 4.003682613372803\n",
      "step = 3769600: loss = 6.167642116546631\n",
      "step = 3769800: loss = 4.4980645179748535\n",
      "step = 3770000: loss = 3.5764715671539307\n",
      "step = 3770000: Average Return = 4.125999927520752\n",
      "step = 3770200: loss = 4.251311779022217\n",
      "step = 3770400: loss = 4.1614861488342285\n",
      "step = 3770600: loss = 4.789834022521973\n",
      "step = 3770800: loss = 3.1877191066741943\n",
      "step = 3771000: loss = 3.090458869934082\n",
      "step = 3771200: loss = 4.830406188964844\n",
      "step = 3771400: loss = 5.29080057144165\n",
      "step = 3771600: loss = 1.8311817646026611\n",
      "step = 3771800: loss = 4.0642313957214355\n",
      "step = 3772000: loss = 4.552729606628418\n",
      "step = 3772200: loss = 3.8259799480438232\n",
      "step = 3772400: loss = 5.875856399536133\n",
      "step = 3772600: loss = 3.7998037338256836\n",
      "step = 3772800: loss = 4.667062759399414\n",
      "step = 3773000: loss = 4.859687805175781\n",
      "step = 3773200: loss = 5.035994052886963\n",
      "step = 3773400: loss = 3.5120909214019775\n",
      "step = 3773600: loss = 3.777970552444458\n",
      "step = 3773800: loss = 2.841256618499756\n",
      "step = 3774000: loss = 4.351210594177246\n",
      "step = 3774200: loss = 3.5632164478302\n",
      "step = 3774400: loss = 5.421879768371582\n",
      "step = 3774600: loss = 5.55653190612793\n",
      "step = 3774800: loss = 3.410942554473877\n",
      "step = 3775000: loss = 5.786538124084473\n",
      "step = 3775000: Average Return = 3.6740000247955322\n",
      "step = 3775200: loss = 4.914874076843262\n",
      "step = 3775400: loss = 4.13538122177124\n",
      "step = 3775600: loss = 4.609925746917725\n",
      "step = 3775800: loss = 4.202632904052734\n",
      "step = 3776000: loss = 4.176978588104248\n",
      "step = 3776200: loss = 3.568939208984375\n",
      "step = 3776400: loss = 3.711371421813965\n",
      "step = 3776600: loss = 3.9765145778656006\n",
      "step = 3776800: loss = 4.731945037841797\n",
      "step = 3777000: loss = 4.336986064910889\n",
      "step = 3777200: loss = 4.402344703674316\n",
      "step = 3777400: loss = 5.672536373138428\n",
      "step = 3777600: loss = 5.416111469268799\n",
      "step = 3777800: loss = 3.9142937660217285\n",
      "step = 3778000: loss = 4.5297064781188965\n",
      "step = 3778200: loss = 6.037100791931152\n",
      "step = 3778400: loss = 3.6632080078125\n",
      "step = 3778600: loss = 4.044972896575928\n",
      "step = 3778800: loss = 4.622303485870361\n",
      "step = 3779000: loss = 4.8079376220703125\n",
      "step = 3779200: loss = 3.6417617797851562\n",
      "step = 3779400: loss = 6.265461444854736\n",
      "step = 3779600: loss = 5.368984222412109\n",
      "step = 3779800: loss = 4.468795299530029\n",
      "step = 3780000: loss = 5.064429759979248\n",
      "step = 3780000: Average Return = 3.562000036239624\n",
      "step = 3780200: loss = 4.3559699058532715\n",
      "step = 3780400: loss = 6.191690444946289\n",
      "step = 3780600: loss = 5.371243953704834\n",
      "step = 3780800: loss = 4.807289123535156\n",
      "step = 3781000: loss = 3.1640865802764893\n",
      "step = 3781200: loss = 5.080495357513428\n",
      "step = 3781400: loss = 5.02338171005249\n",
      "step = 3781600: loss = 3.686241865158081\n",
      "step = 3781800: loss = 4.09414529800415\n",
      "step = 3782000: loss = 4.993214130401611\n",
      "step = 3782200: loss = 4.051333904266357\n",
      "step = 3782400: loss = 4.233689308166504\n",
      "step = 3782600: loss = 3.590468406677246\n",
      "step = 3782800: loss = 4.448493003845215\n",
      "step = 3783000: loss = 4.079470634460449\n",
      "step = 3783200: loss = 2.725809335708618\n",
      "step = 3783400: loss = 5.096344947814941\n",
      "step = 3783600: loss = 3.9412856101989746\n",
      "step = 3783800: loss = 3.073560953140259\n",
      "step = 3784000: loss = 3.5870535373687744\n",
      "step = 3784200: loss = 4.271589279174805\n",
      "step = 3784400: loss = 4.201713562011719\n",
      "step = 3784600: loss = 4.901080131530762\n",
      "step = 3784800: loss = 3.240010976791382\n",
      "step = 3785000: loss = 3.067446231842041\n",
      "step = 3785000: Average Return = 4.070000171661377\n",
      "step = 3785200: loss = 3.5936293601989746\n",
      "step = 3785400: loss = 4.889443397521973\n",
      "step = 3785600: loss = 3.1190264225006104\n",
      "step = 3785800: loss = 3.9867467880249023\n",
      "step = 3786000: loss = 4.068185806274414\n",
      "step = 3786200: loss = 3.50087833404541\n",
      "step = 3786400: loss = 4.162204265594482\n",
      "step = 3786600: loss = 3.524756908416748\n",
      "step = 3786800: loss = 4.3197221755981445\n",
      "step = 3787000: loss = 4.04561710357666\n",
      "step = 3787200: loss = 4.928589344024658\n",
      "step = 3787400: loss = 3.240349054336548\n",
      "step = 3787600: loss = 4.7420878410339355\n",
      "step = 3787800: loss = 4.372984409332275\n",
      "step = 3788000: loss = 5.6314377784729\n",
      "step = 3788200: loss = 4.848437309265137\n",
      "step = 3788400: loss = 3.780705451965332\n",
      "step = 3788600: loss = 5.824822902679443\n",
      "step = 3788800: loss = 3.6126413345336914\n",
      "step = 3789000: loss = 3.738591194152832\n",
      "step = 3789200: loss = 4.586390972137451\n",
      "step = 3789400: loss = 4.582967281341553\n",
      "step = 3789600: loss = 2.7279129028320312\n",
      "step = 3789800: loss = 3.968201160430908\n",
      "step = 3790000: loss = 4.63946533203125\n",
      "step = 3790000: Average Return = 4.073999881744385\n",
      "step = 3790200: loss = 3.6136157512664795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3790400: loss = 4.935207843780518\n",
      "step = 3790600: loss = 4.791515350341797\n",
      "step = 3790800: loss = 5.392703533172607\n",
      "step = 3791000: loss = 5.27458381652832\n",
      "step = 3791200: loss = 4.341690540313721\n",
      "step = 3791400: loss = 3.068943977355957\n",
      "step = 3791600: loss = 4.667386531829834\n",
      "step = 3791800: loss = 4.598670959472656\n",
      "step = 3792000: loss = 3.712284564971924\n",
      "step = 3792200: loss = 4.595003128051758\n",
      "step = 3792400: loss = 4.517767429351807\n",
      "step = 3792600: loss = 4.012731075286865\n",
      "step = 3792800: loss = 4.421762466430664\n",
      "step = 3793000: loss = 4.940289497375488\n",
      "step = 3793200: loss = 4.480710983276367\n",
      "step = 3793400: loss = 2.6980960369110107\n",
      "step = 3793600: loss = 4.907787799835205\n",
      "step = 3793800: loss = 3.258685827255249\n",
      "step = 3794000: loss = 3.7272210121154785\n",
      "step = 3794200: loss = 4.570308208465576\n",
      "step = 3794400: loss = 3.6722400188446045\n",
      "step = 3794600: loss = 3.567366600036621\n",
      "step = 3794800: loss = 3.8829867839813232\n",
      "step = 3795000: loss = 4.363391399383545\n",
      "step = 3795000: Average Return = 3.364000082015991\n",
      "step = 3795200: loss = 3.8714702129364014\n",
      "step = 3795400: loss = 3.322197437286377\n",
      "step = 3795600: loss = 4.121020793914795\n",
      "step = 3795800: loss = 3.4421849250793457\n",
      "step = 3796000: loss = 3.201036214828491\n",
      "step = 3796200: loss = 5.283122539520264\n",
      "step = 3796400: loss = 3.001890182495117\n",
      "step = 3796600: loss = 3.295366048812866\n",
      "step = 3796800: loss = 4.35382604598999\n",
      "step = 3797000: loss = 4.563290596008301\n",
      "step = 3797200: loss = 4.615898132324219\n",
      "step = 3797400: loss = 3.835872173309326\n",
      "step = 3797600: loss = 3.5456619262695312\n",
      "step = 3797800: loss = 3.9646289348602295\n",
      "step = 3798000: loss = 4.630809307098389\n",
      "step = 3798200: loss = 3.244476318359375\n",
      "step = 3798400: loss = 4.930306434631348\n",
      "step = 3798600: loss = 2.558429718017578\n",
      "step = 3798800: loss = 4.01024055480957\n",
      "step = 3799000: loss = 5.163665771484375\n",
      "step = 3799200: loss = 3.968146562576294\n",
      "step = 3799400: loss = 5.925698280334473\n",
      "step = 3799600: loss = 4.2417073249816895\n",
      "step = 3799800: loss = 5.3050103187561035\n",
      "step = 3800000: loss = 5.9355149269104\n",
      "step = 3800000: Average Return = 3.885999917984009\n",
      "step = 3800200: loss = 3.7646377086639404\n",
      "step = 3800400: loss = 4.137292385101318\n",
      "step = 3800600: loss = 4.751484394073486\n",
      "step = 3800800: loss = 4.85284948348999\n",
      "step = 3801000: loss = 2.397099018096924\n",
      "step = 3801200: loss = 3.661400079727173\n",
      "step = 3801400: loss = 4.130843162536621\n",
      "step = 3801600: loss = 4.809062480926514\n",
      "step = 3801800: loss = 4.841336727142334\n",
      "step = 3802000: loss = 4.143274784088135\n",
      "step = 3802200: loss = 4.086233139038086\n",
      "step = 3802400: loss = 4.417956829071045\n",
      "step = 3802600: loss = 4.251527786254883\n",
      "step = 3802800: loss = 3.2841835021972656\n",
      "step = 3803000: loss = 4.248912334442139\n",
      "step = 3803200: loss = 4.286767959594727\n",
      "step = 3803400: loss = 3.1132824420928955\n",
      "step = 3803600: loss = 4.8432111740112305\n",
      "step = 3803800: loss = 3.33774471282959\n",
      "step = 3804000: loss = 3.675302028656006\n",
      "step = 3804200: loss = 4.434784412384033\n",
      "step = 3804400: loss = 3.9053969383239746\n",
      "step = 3804600: loss = 5.272686004638672\n",
      "step = 3804800: loss = 4.527090549468994\n",
      "step = 3805000: loss = 3.7296721935272217\n",
      "step = 3805000: Average Return = 4.044000148773193\n",
      "step = 3805200: loss = 3.948847532272339\n",
      "step = 3805400: loss = 5.722220420837402\n",
      "step = 3805600: loss = 4.365700721740723\n",
      "step = 3805800: loss = 4.845921516418457\n",
      "step = 3806000: loss = 3.7647128105163574\n",
      "step = 3806200: loss = 4.596705913543701\n",
      "step = 3806400: loss = 4.457701206207275\n",
      "step = 3806600: loss = 5.64224100112915\n",
      "step = 3806800: loss = 5.018807411193848\n",
      "step = 3807000: loss = 3.115429639816284\n",
      "step = 3807200: loss = 4.547056674957275\n",
      "step = 3807400: loss = 3.9027621746063232\n",
      "step = 3807600: loss = 3.0835161209106445\n",
      "step = 3807800: loss = 5.925063133239746\n",
      "step = 3808000: loss = 3.8792974948883057\n",
      "step = 3808200: loss = 4.1237897872924805\n",
      "step = 3808400: loss = 3.548091411590576\n",
      "step = 3808600: loss = 2.087822437286377\n",
      "step = 3808800: loss = 4.44516658782959\n",
      "step = 3809000: loss = 3.7517476081848145\n",
      "step = 3809200: loss = 4.240527153015137\n",
      "step = 3809400: loss = 5.704501628875732\n",
      "step = 3809600: loss = 5.017146110534668\n",
      "step = 3809800: loss = 5.20005989074707\n",
      "step = 3810000: loss = 3.921417713165283\n",
      "step = 3810000: Average Return = 3.630000114440918\n",
      "step = 3810200: loss = 3.992527961730957\n",
      "step = 3810400: loss = 3.978304147720337\n",
      "step = 3810600: loss = 3.0817112922668457\n",
      "step = 3810800: loss = 4.587665557861328\n",
      "step = 3811000: loss = 5.590683460235596\n",
      "step = 3811200: loss = 3.838082790374756\n",
      "step = 3811400: loss = 3.4135076999664307\n",
      "step = 3811600: loss = 3.2723569869995117\n",
      "step = 3811800: loss = 2.958285093307495\n",
      "step = 3812000: loss = 4.862159252166748\n",
      "step = 3812200: loss = 4.188579082489014\n",
      "step = 3812400: loss = 3.805044174194336\n",
      "step = 3812600: loss = 4.0827555656433105\n",
      "step = 3812800: loss = 3.4909815788269043\n",
      "step = 3813000: loss = 4.821502685546875\n",
      "step = 3813200: loss = 5.765669822692871\n",
      "step = 3813400: loss = 3.5070433616638184\n",
      "step = 3813600: loss = 3.767422914505005\n",
      "step = 3813800: loss = 4.104416370391846\n",
      "step = 3814000: loss = 3.345914125442505\n",
      "step = 3814200: loss = 4.512032985687256\n",
      "step = 3814400: loss = 4.543935298919678\n",
      "step = 3814600: loss = 3.4716784954071045\n",
      "step = 3814800: loss = 4.963631629943848\n",
      "step = 3815000: loss = 4.406094074249268\n",
      "step = 3815000: Average Return = 4.076000213623047\n",
      "step = 3815200: loss = 4.431873321533203\n",
      "step = 3815400: loss = 5.635696887969971\n",
      "step = 3815600: loss = 3.230865478515625\n",
      "step = 3815800: loss = 5.452709674835205\n",
      "step = 3816000: loss = 5.135490417480469\n",
      "step = 3816200: loss = 4.781770706176758\n",
      "step = 3816400: loss = 3.4119865894317627\n",
      "step = 3816600: loss = 3.096635580062866\n",
      "step = 3816800: loss = 3.8516318798065186\n",
      "step = 3817000: loss = 4.616571426391602\n",
      "step = 3817200: loss = 3.3868014812469482\n",
      "step = 3817400: loss = 6.299138069152832\n",
      "step = 3817600: loss = 4.443408489227295\n",
      "step = 3817800: loss = 3.273751735687256\n",
      "step = 3818000: loss = 4.586009979248047\n",
      "step = 3818200: loss = 5.294811248779297\n",
      "step = 3818400: loss = 3.918003559112549\n",
      "step = 3818600: loss = 3.0022404193878174\n",
      "step = 3818800: loss = 4.192166328430176\n",
      "step = 3819000: loss = 3.8850882053375244\n",
      "step = 3819200: loss = 3.0678343772888184\n",
      "step = 3819400: loss = 3.8458995819091797\n",
      "step = 3819600: loss = 4.032428741455078\n",
      "step = 3819800: loss = 4.027719020843506\n",
      "step = 3820000: loss = 4.019731044769287\n",
      "step = 3820000: Average Return = 3.9739999771118164\n",
      "step = 3820200: loss = 4.284484386444092\n",
      "step = 3820400: loss = 3.4646546840667725\n",
      "step = 3820600: loss = 4.610930919647217\n",
      "step = 3820800: loss = 4.771650791168213\n",
      "step = 3821000: loss = 4.548333168029785\n",
      "step = 3821200: loss = 3.45479679107666\n",
      "step = 3821400: loss = 4.2238078117370605\n",
      "step = 3821600: loss = 3.5736641883850098\n",
      "step = 3821800: loss = 4.641273498535156\n",
      "step = 3822000: loss = 3.5376341342926025\n",
      "step = 3822200: loss = 3.5071163177490234\n",
      "step = 3822400: loss = 4.997373580932617\n",
      "step = 3822600: loss = 4.2444281578063965\n",
      "step = 3822800: loss = 5.11134672164917\n",
      "step = 3823000: loss = 4.371668815612793\n",
      "step = 3823200: loss = 5.45860481262207\n",
      "step = 3823400: loss = 5.888258457183838\n",
      "step = 3823600: loss = 3.7433574199676514\n",
      "step = 3823800: loss = 4.793364524841309\n",
      "step = 3824000: loss = 3.21338152885437\n",
      "step = 3824200: loss = 3.3203697204589844\n",
      "step = 3824400: loss = 2.8712570667266846\n",
      "step = 3824600: loss = 4.089827060699463\n",
      "step = 3824800: loss = 3.6933562755584717\n",
      "step = 3825000: loss = 3.410053014755249\n",
      "step = 3825000: Average Return = 4.133999824523926\n",
      "step = 3825200: loss = 4.872521877288818\n",
      "step = 3825400: loss = 4.459038257598877\n",
      "step = 3825600: loss = 4.704353332519531\n",
      "step = 3825800: loss = 3.9661200046539307\n",
      "step = 3826000: loss = 4.090673923492432\n",
      "step = 3826200: loss = 3.6028857231140137\n",
      "step = 3826400: loss = 4.959829807281494\n",
      "step = 3826600: loss = 4.0560407638549805\n",
      "step = 3826800: loss = 4.8859052658081055\n",
      "step = 3827000: loss = 4.538972854614258\n",
      "step = 3827200: loss = 4.8352484703063965\n",
      "step = 3827400: loss = 3.177720785140991\n",
      "step = 3827600: loss = 2.8713977336883545\n",
      "step = 3827800: loss = 3.7666501998901367\n",
      "step = 3828000: loss = 2.901552677154541\n",
      "step = 3828200: loss = 4.06682014465332\n",
      "step = 3828400: loss = 5.476773738861084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3828600: loss = 3.9527742862701416\n",
      "step = 3828800: loss = 3.3281822204589844\n",
      "step = 3829000: loss = 4.680176258087158\n",
      "step = 3829200: loss = 3.789815664291382\n",
      "step = 3829400: loss = 4.379340648651123\n",
      "step = 3829600: loss = 4.198802471160889\n",
      "step = 3829800: loss = 3.857463836669922\n",
      "step = 3830000: loss = 4.506359577178955\n",
      "step = 3830000: Average Return = 3.9000000953674316\n",
      "step = 3830200: loss = 4.460474014282227\n",
      "step = 3830400: loss = 3.6853344440460205\n",
      "step = 3830600: loss = 3.552565336227417\n",
      "step = 3830800: loss = 3.562572479248047\n",
      "step = 3831000: loss = 4.2976908683776855\n",
      "step = 3831200: loss = 4.292316913604736\n",
      "step = 3831400: loss = 6.074583530426025\n",
      "step = 3831600: loss = 3.6276447772979736\n",
      "step = 3831800: loss = 4.772111892700195\n",
      "step = 3832000: loss = 4.900853157043457\n",
      "step = 3832200: loss = 4.544490814208984\n",
      "step = 3832400: loss = 3.2011897563934326\n",
      "step = 3832600: loss = 4.288830280303955\n",
      "step = 3832800: loss = 4.662025451660156\n",
      "step = 3833000: loss = 3.3969829082489014\n",
      "step = 3833200: loss = 4.806787014007568\n",
      "step = 3833400: loss = 4.658544063568115\n",
      "step = 3833600: loss = 5.305426120758057\n",
      "step = 3833800: loss = 2.6826939582824707\n",
      "step = 3834000: loss = 5.257344722747803\n",
      "step = 3834200: loss = 3.596195936203003\n",
      "step = 3834400: loss = 2.9638078212738037\n",
      "step = 3834600: loss = 3.7536120414733887\n",
      "step = 3834800: loss = 3.717191457748413\n",
      "step = 3835000: loss = 4.860444068908691\n",
      "step = 3835000: Average Return = 3.8380000591278076\n",
      "step = 3835200: loss = 4.774249076843262\n",
      "step = 3835400: loss = 3.4932377338409424\n",
      "step = 3835600: loss = 3.6894989013671875\n",
      "step = 3835800: loss = 5.063226699829102\n",
      "step = 3836000: loss = 2.8879637718200684\n",
      "step = 3836200: loss = 3.7669758796691895\n",
      "step = 3836400: loss = 5.178648471832275\n",
      "step = 3836600: loss = 4.951901912689209\n",
      "step = 3836800: loss = 3.815953254699707\n",
      "step = 3837000: loss = 4.150925636291504\n",
      "step = 3837200: loss = 3.3041820526123047\n",
      "step = 3837400: loss = 4.362900733947754\n",
      "step = 3837600: loss = 4.10056209564209\n",
      "step = 3837800: loss = 5.735781669616699\n",
      "step = 3838000: loss = 4.085176467895508\n",
      "step = 3838200: loss = 4.362712383270264\n",
      "step = 3838400: loss = 4.0100603103637695\n",
      "step = 3838600: loss = 3.948570966720581\n",
      "step = 3838800: loss = 4.69139289855957\n",
      "step = 3839000: loss = 4.413142681121826\n",
      "step = 3839200: loss = 2.972058057785034\n",
      "step = 3839400: loss = 5.19827127456665\n",
      "step = 3839600: loss = 4.233492374420166\n",
      "step = 3839800: loss = 4.847994804382324\n",
      "step = 3840000: loss = 2.9339146614074707\n",
      "step = 3840000: Average Return = 3.7679998874664307\n",
      "step = 3840200: loss = 4.638875961303711\n",
      "step = 3840400: loss = 5.20294189453125\n",
      "step = 3840600: loss = 5.947117805480957\n",
      "step = 3840800: loss = 3.82647967338562\n",
      "step = 3841000: loss = 3.795393228530884\n",
      "step = 3841200: loss = 3.8831193447113037\n",
      "step = 3841400: loss = 3.7707061767578125\n",
      "step = 3841600: loss = 3.234769821166992\n",
      "step = 3841800: loss = 3.853651762008667\n",
      "step = 3842000: loss = 4.435495376586914\n",
      "step = 3842200: loss = 4.788943290710449\n",
      "step = 3842400: loss = 4.67503023147583\n",
      "step = 3842600: loss = 3.3450396060943604\n",
      "step = 3842800: loss = 4.8373703956604\n",
      "step = 3843000: loss = 5.403909683227539\n",
      "step = 3843200: loss = 5.1618218421936035\n",
      "step = 3843400: loss = 3.828580856323242\n",
      "step = 3843600: loss = 2.8808979988098145\n",
      "step = 3843800: loss = 5.00844669342041\n",
      "step = 3844000: loss = 4.717641353607178\n",
      "step = 3844200: loss = 3.9138145446777344\n",
      "step = 3844400: loss = 3.5543837547302246\n",
      "step = 3844600: loss = 4.8708014488220215\n",
      "step = 3844800: loss = 4.018775939941406\n",
      "step = 3845000: loss = 5.230771541595459\n",
      "step = 3845000: Average Return = 3.9059998989105225\n",
      "step = 3845200: loss = 3.9706907272338867\n",
      "step = 3845400: loss = 5.037148952484131\n",
      "step = 3845600: loss = 5.0455756187438965\n",
      "step = 3845800: loss = 4.740303993225098\n",
      "step = 3846000: loss = 5.762443542480469\n",
      "step = 3846200: loss = 5.2682785987854\n",
      "step = 3846400: loss = 3.9955685138702393\n",
      "step = 3846600: loss = 2.0811779499053955\n",
      "step = 3846800: loss = 5.844241142272949\n",
      "step = 3847000: loss = 4.616169452667236\n",
      "step = 3847200: loss = 5.760223865509033\n",
      "step = 3847400: loss = 6.322503566741943\n",
      "step = 3847600: loss = 2.1693108081817627\n",
      "step = 3847800: loss = 4.134943962097168\n",
      "step = 3848000: loss = 3.9366455078125\n",
      "step = 3848200: loss = 5.173792839050293\n",
      "step = 3848400: loss = 5.302841663360596\n",
      "step = 3848600: loss = 3.99023175239563\n",
      "step = 3848800: loss = 4.98045539855957\n",
      "step = 3849000: loss = 2.1525251865386963\n",
      "step = 3849200: loss = 4.404213905334473\n",
      "step = 3849400: loss = 4.6546759605407715\n",
      "step = 3849600: loss = 4.158777236938477\n",
      "step = 3849800: loss = 3.3397793769836426\n",
      "step = 3850000: loss = 4.832950115203857\n",
      "step = 3850000: Average Return = 4.130000114440918\n",
      "step = 3850200: loss = 3.6358866691589355\n",
      "step = 3850400: loss = 3.691920042037964\n",
      "step = 3850600: loss = 5.187657833099365\n",
      "step = 3850800: loss = 2.6423559188842773\n",
      "step = 3851000: loss = 4.439454555511475\n",
      "step = 3851200: loss = 3.577129364013672\n",
      "step = 3851400: loss = 2.8374950885772705\n",
      "step = 3851600: loss = 3.3574960231781006\n",
      "step = 3851800: loss = 3.7268471717834473\n",
      "step = 3852000: loss = 4.198750972747803\n",
      "step = 3852200: loss = 3.7375974655151367\n",
      "step = 3852400: loss = 4.504695415496826\n",
      "step = 3852600: loss = 3.495342969894409\n",
      "step = 3852800: loss = 4.99849796295166\n",
      "step = 3853000: loss = 4.063925266265869\n",
      "step = 3853200: loss = 3.0133683681488037\n",
      "step = 3853400: loss = 3.6416397094726562\n",
      "step = 3853600: loss = 3.6799328327178955\n",
      "step = 3853800: loss = 4.194981098175049\n",
      "step = 3854000: loss = 5.002388954162598\n",
      "step = 3854200: loss = 4.894873142242432\n",
      "step = 3854400: loss = 3.9940803050994873\n",
      "step = 3854600: loss = 4.463603496551514\n",
      "step = 3854800: loss = 3.3282995223999023\n",
      "step = 3855000: loss = 3.2355473041534424\n",
      "step = 3855000: Average Return = 3.880000114440918\n",
      "step = 3855200: loss = 4.680000305175781\n",
      "step = 3855400: loss = 4.216041564941406\n",
      "step = 3855600: loss = 5.164002418518066\n",
      "step = 3855800: loss = 3.758373975753784\n",
      "step = 3856000: loss = 3.9152979850769043\n",
      "step = 3856200: loss = 4.365591049194336\n",
      "step = 3856400: loss = 3.661654233932495\n",
      "step = 3856600: loss = 4.816964149475098\n",
      "step = 3856800: loss = 3.7926933765411377\n",
      "step = 3857000: loss = 3.2535088062286377\n",
      "step = 3857200: loss = 4.235377311706543\n",
      "step = 3857400: loss = 3.2693724632263184\n",
      "step = 3857600: loss = 3.5131428241729736\n",
      "step = 3857800: loss = 3.8069701194763184\n",
      "step = 3858000: loss = 3.083815574645996\n",
      "step = 3858200: loss = 2.6954901218414307\n",
      "step = 3858400: loss = 4.3803606033325195\n",
      "step = 3858600: loss = 3.4755680561065674\n",
      "step = 3858800: loss = 3.602344036102295\n",
      "step = 3859000: loss = 3.795372486114502\n",
      "step = 3859200: loss = 4.625290870666504\n",
      "step = 3859400: loss = 4.276118755340576\n",
      "step = 3859600: loss = 3.1222035884857178\n",
      "step = 3859800: loss = 4.3486008644104\n",
      "step = 3860000: loss = 4.825473308563232\n",
      "step = 3860000: Average Return = 3.684000015258789\n",
      "step = 3860200: loss = 4.425693988800049\n",
      "step = 3860400: loss = 3.0643954277038574\n",
      "step = 3860600: loss = 4.398065567016602\n",
      "step = 3860800: loss = 3.595378875732422\n",
      "step = 3861000: loss = 4.79362154006958\n",
      "step = 3861200: loss = 4.912679672241211\n",
      "step = 3861400: loss = 2.462589740753174\n",
      "step = 3861600: loss = 5.566359519958496\n",
      "step = 3861800: loss = 4.858626365661621\n",
      "step = 3862000: loss = 3.717165946960449\n",
      "step = 3862200: loss = 4.479054927825928\n",
      "step = 3862400: loss = 5.154340744018555\n",
      "step = 3862600: loss = 4.707348346710205\n",
      "step = 3862800: loss = 3.234863519668579\n",
      "step = 3863000: loss = 4.267621994018555\n",
      "step = 3863200: loss = 4.826314926147461\n",
      "step = 3863400: loss = 3.3202829360961914\n",
      "step = 3863600: loss = 4.9376749992370605\n",
      "step = 3863800: loss = 3.76090145111084\n",
      "step = 3864000: loss = 4.192627906799316\n",
      "step = 3864200: loss = 4.3267951011657715\n",
      "step = 3864400: loss = 4.5196051597595215\n",
      "step = 3864600: loss = 4.355769157409668\n",
      "step = 3864800: loss = 4.102669715881348\n",
      "step = 3865000: loss = 3.8288328647613525\n",
      "step = 3865000: Average Return = 3.752000093460083\n",
      "step = 3865200: loss = 3.514415740966797\n",
      "step = 3865400: loss = 4.167405128479004\n",
      "step = 3865600: loss = 3.0975372791290283\n",
      "step = 3865800: loss = 4.030701637268066\n",
      "step = 3866000: loss = 4.015402317047119\n",
      "step = 3866200: loss = 3.7552921772003174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3866400: loss = 3.514282703399658\n",
      "step = 3866600: loss = 3.6731789112091064\n",
      "step = 3866800: loss = 3.5032079219818115\n",
      "step = 3867000: loss = 4.662186622619629\n",
      "step = 3867200: loss = 3.6677000522613525\n",
      "step = 3867400: loss = 5.224393844604492\n",
      "step = 3867600: loss = 2.841850519180298\n",
      "step = 3867800: loss = 3.284379482269287\n",
      "step = 3868000: loss = 4.238826751708984\n",
      "step = 3868200: loss = 5.158365249633789\n",
      "step = 3868400: loss = 5.748960018157959\n",
      "step = 3868600: loss = 5.197182655334473\n",
      "step = 3868800: loss = 4.258390426635742\n",
      "step = 3869000: loss = 3.4682021141052246\n",
      "step = 3869200: loss = 6.294928073883057\n",
      "step = 3869400: loss = 4.2209577560424805\n",
      "step = 3869600: loss = 4.892989635467529\n",
      "step = 3869800: loss = 3.1283161640167236\n",
      "step = 3870000: loss = 4.509505748748779\n",
      "step = 3870000: Average Return = 4.176000118255615\n",
      "step = 3870200: loss = 4.449050426483154\n",
      "step = 3870400: loss = 4.191192150115967\n",
      "step = 3870600: loss = 3.2071032524108887\n",
      "step = 3870800: loss = 2.7285923957824707\n",
      "step = 3871000: loss = 5.120009422302246\n",
      "step = 3871200: loss = 4.196752548217773\n",
      "step = 3871400: loss = 4.617303848266602\n",
      "step = 3871600: loss = 4.4170331954956055\n",
      "step = 3871800: loss = 4.487082481384277\n",
      "step = 3872000: loss = 4.188442707061768\n",
      "step = 3872200: loss = 3.968752384185791\n",
      "step = 3872400: loss = 4.077869892120361\n",
      "step = 3872600: loss = 4.338814735412598\n",
      "step = 3872800: loss = 5.80070686340332\n",
      "step = 3873000: loss = 5.654216766357422\n",
      "step = 3873200: loss = 3.747704267501831\n",
      "step = 3873400: loss = 4.887646198272705\n",
      "step = 3873600: loss = 2.615384101867676\n",
      "step = 3873800: loss = 3.4466609954833984\n",
      "step = 3874000: loss = 2.7078187465667725\n",
      "step = 3874200: loss = 3.689094066619873\n",
      "step = 3874400: loss = 3.192657947540283\n",
      "step = 3874600: loss = 2.4847471714019775\n",
      "step = 3874800: loss = 4.610443115234375\n",
      "step = 3875000: loss = 3.432925224304199\n",
      "step = 3875000: Average Return = 3.9240000247955322\n",
      "step = 3875200: loss = 3.948423385620117\n",
      "step = 3875400: loss = 4.603895664215088\n",
      "step = 3875600: loss = 4.622824668884277\n",
      "step = 3875800: loss = 4.815229892730713\n",
      "step = 3876000: loss = 4.703645706176758\n",
      "step = 3876200: loss = 4.029314041137695\n",
      "step = 3876400: loss = 3.5086193084716797\n",
      "step = 3876600: loss = 4.276378154754639\n",
      "step = 3876800: loss = 4.431695938110352\n",
      "step = 3877000: loss = 3.745015859603882\n",
      "step = 3877200: loss = 3.4643828868865967\n",
      "step = 3877400: loss = 5.138759136199951\n",
      "step = 3877600: loss = 4.0894951820373535\n",
      "step = 3877800: loss = 3.836468458175659\n",
      "step = 3878000: loss = 4.300727367401123\n",
      "step = 3878200: loss = 4.1654767990112305\n",
      "step = 3878400: loss = 4.119882583618164\n",
      "step = 3878600: loss = 3.412313222885132\n",
      "step = 3878800: loss = 3.7812392711639404\n",
      "step = 3879000: loss = 4.068907737731934\n",
      "step = 3879200: loss = 3.1921942234039307\n",
      "step = 3879400: loss = 5.353021144866943\n",
      "step = 3879600: loss = 4.714629173278809\n",
      "step = 3879800: loss = 3.9036107063293457\n",
      "step = 3880000: loss = 4.498119354248047\n",
      "step = 3880000: Average Return = 3.8559999465942383\n",
      "step = 3880200: loss = 5.297243595123291\n",
      "step = 3880400: loss = 3.7344701290130615\n",
      "step = 3880600: loss = 4.211795806884766\n",
      "step = 3880800: loss = 2.7744338512420654\n",
      "step = 3881000: loss = 3.5990617275238037\n",
      "step = 3881200: loss = 2.908621072769165\n",
      "step = 3881400: loss = 4.794198989868164\n",
      "step = 3881600: loss = 3.8700804710388184\n",
      "step = 3881800: loss = 3.2965922355651855\n",
      "step = 3882000: loss = 4.3024773597717285\n",
      "step = 3882200: loss = 5.508405685424805\n",
      "step = 3882400: loss = 3.830134868621826\n",
      "step = 3882600: loss = 4.472081661224365\n",
      "step = 3882800: loss = 3.392632484436035\n",
      "step = 3883000: loss = 3.4687840938568115\n",
      "step = 3883200: loss = 5.002200603485107\n",
      "step = 3883400: loss = 3.8950397968292236\n",
      "step = 3883600: loss = 4.041769981384277\n",
      "step = 3883800: loss = 3.2884814739227295\n",
      "step = 3884000: loss = 4.8353657722473145\n",
      "step = 3884200: loss = 3.2199175357818604\n",
      "step = 3884400: loss = 3.9145915508270264\n",
      "step = 3884600: loss = 4.789480209350586\n",
      "step = 3884800: loss = 3.8693652153015137\n",
      "step = 3885000: loss = 2.942049741744995\n",
      "step = 3885000: Average Return = 3.9000000953674316\n",
      "step = 3885200: loss = 4.722879409790039\n",
      "step = 3885400: loss = 5.29463529586792\n",
      "step = 3885600: loss = 5.155914306640625\n",
      "step = 3885800: loss = 4.023620128631592\n",
      "step = 3886000: loss = 3.628906488418579\n",
      "step = 3886200: loss = 4.221007823944092\n",
      "step = 3886400: loss = 3.331789493560791\n",
      "step = 3886600: loss = 3.432936906814575\n",
      "step = 3886800: loss = 3.9428930282592773\n",
      "step = 3887000: loss = 4.144465923309326\n",
      "step = 3887200: loss = 4.736616611480713\n",
      "step = 3887400: loss = 3.6683568954467773\n",
      "step = 3887600: loss = 3.735750675201416\n",
      "step = 3887800: loss = 3.4737627506256104\n",
      "step = 3888000: loss = 3.3489866256713867\n",
      "step = 3888200: loss = 4.659607887268066\n",
      "step = 3888400: loss = 3.973240375518799\n",
      "step = 3888600: loss = 4.3325018882751465\n",
      "step = 3888800: loss = 3.239973783493042\n",
      "step = 3889000: loss = 3.633147716522217\n",
      "step = 3889200: loss = 5.580780029296875\n",
      "step = 3889400: loss = 3.2318506240844727\n",
      "step = 3889600: loss = 3.2378413677215576\n",
      "step = 3889800: loss = 4.772694110870361\n",
      "step = 3890000: loss = 4.466285228729248\n",
      "step = 3890000: Average Return = 3.9600000381469727\n",
      "step = 3890200: loss = 4.423543453216553\n",
      "step = 3890400: loss = 4.166847229003906\n",
      "step = 3890600: loss = 3.043149948120117\n",
      "step = 3890800: loss = 4.151465892791748\n",
      "step = 3891000: loss = 4.18982458114624\n",
      "step = 3891200: loss = 3.6300010681152344\n",
      "step = 3891400: loss = 3.7653253078460693\n",
      "step = 3891600: loss = 3.6093499660491943\n",
      "step = 3891800: loss = 3.6858837604522705\n",
      "step = 3892000: loss = 2.523735523223877\n",
      "step = 3892200: loss = 4.204562664031982\n",
      "step = 3892400: loss = 4.559473991394043\n",
      "step = 3892600: loss = 5.27403450012207\n",
      "step = 3892800: loss = 3.931100606918335\n",
      "step = 3893000: loss = 5.94106912612915\n",
      "step = 3893200: loss = 3.701627731323242\n",
      "step = 3893400: loss = 3.4712822437286377\n",
      "step = 3893600: loss = 4.687888145446777\n",
      "step = 3893800: loss = 3.8539421558380127\n",
      "step = 3894000: loss = 4.7793097496032715\n",
      "step = 3894200: loss = 3.8915600776672363\n",
      "step = 3894400: loss = 4.210000038146973\n",
      "step = 3894600: loss = 4.845007419586182\n",
      "step = 3894800: loss = 3.513495445251465\n",
      "step = 3895000: loss = 4.525750637054443\n",
      "step = 3895000: Average Return = 3.8919999599456787\n",
      "step = 3895200: loss = 5.765163421630859\n",
      "step = 3895400: loss = 3.9838063716888428\n",
      "step = 3895600: loss = 4.589028358459473\n",
      "step = 3895800: loss = 5.549736022949219\n",
      "step = 3896000: loss = 4.280799865722656\n",
      "step = 3896200: loss = 4.231679916381836\n",
      "step = 3896400: loss = 4.013300895690918\n",
      "step = 3896600: loss = 4.990667819976807\n",
      "step = 3896800: loss = 3.6703405380249023\n",
      "step = 3897000: loss = 5.096930027008057\n",
      "step = 3897200: loss = 4.313814640045166\n",
      "step = 3897400: loss = 6.081208229064941\n",
      "step = 3897600: loss = 4.282174110412598\n",
      "step = 3897800: loss = 4.942716598510742\n",
      "step = 3898000: loss = 3.9965946674346924\n",
      "step = 3898200: loss = 3.3422112464904785\n",
      "step = 3898400: loss = 3.7857470512390137\n",
      "step = 3898600: loss = 4.330727577209473\n",
      "step = 3898800: loss = 4.381975173950195\n",
      "step = 3899000: loss = 4.795440673828125\n",
      "step = 3899200: loss = 3.5023510456085205\n",
      "step = 3899400: loss = 4.530428886413574\n",
      "step = 3899600: loss = 4.43215274810791\n",
      "step = 3899800: loss = 5.508116722106934\n",
      "step = 3900000: loss = 3.926886796951294\n",
      "step = 3900000: Average Return = 3.921999931335449\n",
      "step = 3900200: loss = 2.6174914836883545\n",
      "step = 3900400: loss = 2.8619279861450195\n",
      "step = 3900600: loss = 4.43550968170166\n",
      "step = 3900800: loss = 3.2842254638671875\n",
      "step = 3901000: loss = 4.1813249588012695\n",
      "step = 3901200: loss = 4.001110553741455\n",
      "step = 3901400: loss = 5.314908504486084\n",
      "step = 3901600: loss = 4.140279769897461\n",
      "step = 3901800: loss = 3.505247116088867\n",
      "step = 3902000: loss = 4.237140655517578\n",
      "step = 3902200: loss = 4.246325492858887\n",
      "step = 3902400: loss = 3.9592695236206055\n",
      "step = 3902600: loss = 4.367990016937256\n",
      "step = 3902800: loss = 4.205268859863281\n",
      "step = 3903000: loss = 3.73380184173584\n",
      "step = 3903200: loss = 5.304873466491699\n",
      "step = 3903400: loss = 5.863627910614014\n",
      "step = 3903600: loss = 4.900997638702393\n",
      "step = 3903800: loss = 3.6875133514404297\n",
      "step = 3904000: loss = 4.161765098571777\n",
      "step = 3904200: loss = 3.7858495712280273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3904400: loss = 3.2109880447387695\n",
      "step = 3904600: loss = 3.951733350753784\n",
      "step = 3904800: loss = 5.968691349029541\n",
      "step = 3905000: loss = 4.527663707733154\n",
      "step = 3905000: Average Return = 3.4159998893737793\n",
      "step = 3905200: loss = 5.257228851318359\n",
      "step = 3905400: loss = 4.100714683532715\n",
      "step = 3905600: loss = 3.5908432006835938\n",
      "step = 3905800: loss = 5.144362449645996\n",
      "step = 3906000: loss = 5.1807732582092285\n",
      "step = 3906200: loss = 3.590034008026123\n",
      "step = 3906400: loss = 2.613461971282959\n",
      "step = 3906600: loss = 4.6483869552612305\n",
      "step = 3906800: loss = 4.976380348205566\n",
      "step = 3907000: loss = 3.988351345062256\n",
      "step = 3907200: loss = 4.9432597160339355\n",
      "step = 3907400: loss = 5.863431930541992\n",
      "step = 3907600: loss = 3.6972200870513916\n",
      "step = 3907800: loss = 3.1776986122131348\n",
      "step = 3908000: loss = 2.665024757385254\n",
      "step = 3908200: loss = 4.483199596405029\n",
      "step = 3908400: loss = 5.066427230834961\n",
      "step = 3908600: loss = 4.908229351043701\n",
      "step = 3908800: loss = 4.487165927886963\n",
      "step = 3909000: loss = 2.8922319412231445\n",
      "step = 3909200: loss = 3.5412209033966064\n",
      "step = 3909400: loss = 5.383394241333008\n",
      "step = 3909600: loss = 3.8365821838378906\n",
      "step = 3909800: loss = 4.665404796600342\n",
      "step = 3910000: loss = 5.349547386169434\n",
      "step = 3910000: Average Return = 3.808000087738037\n",
      "step = 3910200: loss = 4.503410339355469\n",
      "step = 3910400: loss = 3.044874429702759\n",
      "step = 3910600: loss = 2.310633420944214\n",
      "step = 3910800: loss = 3.3446288108825684\n",
      "step = 3911000: loss = 4.443536281585693\n",
      "step = 3911200: loss = 3.3742897510528564\n",
      "step = 3911400: loss = 4.502081871032715\n",
      "step = 3911600: loss = 3.9988811016082764\n",
      "step = 3911800: loss = 4.276362895965576\n",
      "step = 3912000: loss = 4.20255708694458\n",
      "step = 3912200: loss = 4.245717525482178\n",
      "step = 3912400: loss = 6.224180221557617\n",
      "step = 3912600: loss = 5.434848308563232\n",
      "step = 3912800: loss = 4.119123458862305\n",
      "step = 3913000: loss = 3.739685535430908\n",
      "step = 3913200: loss = 5.1751227378845215\n",
      "step = 3913400: loss = 5.747775077819824\n",
      "step = 3913600: loss = 3.4949791431427\n",
      "step = 3913800: loss = 3.8486242294311523\n",
      "step = 3914000: loss = 4.631759166717529\n",
      "step = 3914200: loss = 4.18058443069458\n",
      "step = 3914400: loss = 3.385995388031006\n",
      "step = 3914600: loss = 5.041105270385742\n",
      "step = 3914800: loss = 4.816722393035889\n",
      "step = 3915000: loss = 4.3611321449279785\n",
      "step = 3915000: Average Return = 3.8540000915527344\n",
      "step = 3915200: loss = 3.1798698902130127\n",
      "step = 3915400: loss = 4.789592742919922\n",
      "step = 3915600: loss = 4.487545013427734\n",
      "step = 3915800: loss = 5.657244682312012\n",
      "step = 3916000: loss = 3.716963768005371\n",
      "step = 3916200: loss = 4.495113849639893\n",
      "step = 3916400: loss = 4.920625686645508\n",
      "step = 3916600: loss = 4.225964069366455\n",
      "step = 3916800: loss = 4.784198760986328\n",
      "step = 3917000: loss = 3.240950345993042\n",
      "step = 3917200: loss = 4.472949504852295\n",
      "step = 3917400: loss = 4.723848819732666\n",
      "step = 3917600: loss = 4.386425495147705\n",
      "step = 3917800: loss = 3.133167266845703\n",
      "step = 3918000: loss = 3.763979196548462\n",
      "step = 3918200: loss = 4.070556163787842\n",
      "step = 3918400: loss = 3.9640560150146484\n",
      "step = 3918600: loss = 5.973911762237549\n",
      "step = 3918800: loss = 2.8170886039733887\n",
      "step = 3919000: loss = 5.07522439956665\n",
      "step = 3919200: loss = 3.5207479000091553\n",
      "step = 3919400: loss = 4.635796546936035\n",
      "step = 3919600: loss = 4.259486675262451\n",
      "step = 3919800: loss = 4.288769245147705\n",
      "step = 3920000: loss = 5.585358619689941\n",
      "step = 3920000: Average Return = 3.572000026702881\n",
      "step = 3920200: loss = 4.826627731323242\n",
      "step = 3920400: loss = 4.948339939117432\n",
      "step = 3920600: loss = 4.566386699676514\n",
      "step = 3920800: loss = 4.3193793296813965\n",
      "step = 3921000: loss = 3.690581798553467\n",
      "step = 3921200: loss = 3.6330339908599854\n",
      "step = 3921400: loss = 4.808964252471924\n",
      "step = 3921600: loss = 3.937964916229248\n",
      "step = 3921800: loss = 4.960028171539307\n",
      "step = 3922000: loss = 3.219796657562256\n",
      "step = 3922200: loss = 2.1601462364196777\n",
      "step = 3922400: loss = 4.9418134689331055\n",
      "step = 3922600: loss = 4.565756320953369\n",
      "step = 3922800: loss = 2.7762956619262695\n",
      "step = 3923000: loss = 3.6345269680023193\n",
      "step = 3923200: loss = 3.4273135662078857\n",
      "step = 3923400: loss = 2.897272825241089\n",
      "step = 3923600: loss = 5.692812442779541\n",
      "step = 3923800: loss = 3.9680469036102295\n",
      "step = 3924000: loss = 3.294663667678833\n",
      "step = 3924200: loss = 4.433554649353027\n",
      "step = 3924400: loss = 3.5095207691192627\n",
      "step = 3924600: loss = 4.464609146118164\n",
      "step = 3924800: loss = 3.384129524230957\n",
      "step = 3925000: loss = 4.656662464141846\n",
      "step = 3925000: Average Return = 3.681999921798706\n",
      "step = 3925200: loss = 3.6186060905456543\n",
      "step = 3925400: loss = 4.072656154632568\n",
      "step = 3925600: loss = 3.381192445755005\n",
      "step = 3925800: loss = 4.642850399017334\n",
      "step = 3926000: loss = 4.433689594268799\n",
      "step = 3926200: loss = 2.629892587661743\n",
      "step = 3926400: loss = 3.2682716846466064\n",
      "step = 3926600: loss = 4.212937355041504\n",
      "step = 3926800: loss = 4.308718204498291\n",
      "step = 3927000: loss = 3.69258713722229\n",
      "step = 3927200: loss = 2.6051344871520996\n",
      "step = 3927400: loss = 4.57077693939209\n",
      "step = 3927600: loss = 2.8114776611328125\n",
      "step = 3927800: loss = 4.405107021331787\n",
      "step = 3928000: loss = 3.515937328338623\n",
      "step = 3928200: loss = 3.677175283432007\n",
      "step = 3928400: loss = 4.269783973693848\n",
      "step = 3928600: loss = 4.7178635597229\n",
      "step = 3928800: loss = 3.2656238079071045\n",
      "step = 3929000: loss = 4.645260810852051\n",
      "step = 3929200: loss = 4.513282775878906\n",
      "step = 3929400: loss = 4.337127208709717\n",
      "step = 3929600: loss = 4.146125316619873\n",
      "step = 3929800: loss = 3.6892902851104736\n",
      "step = 3930000: loss = 3.5479509830474854\n",
      "step = 3930000: Average Return = 4.414000034332275\n",
      "step = 3930200: loss = 5.545187950134277\n",
      "step = 3930400: loss = 2.7866880893707275\n",
      "step = 3930600: loss = 3.642913818359375\n",
      "step = 3930800: loss = 4.051634311676025\n",
      "step = 3931000: loss = 3.531956672668457\n",
      "step = 3931200: loss = 3.733954906463623\n",
      "step = 3931400: loss = 3.6712558269500732\n",
      "step = 3931600: loss = 4.089603900909424\n",
      "step = 3931800: loss = 4.345151424407959\n",
      "step = 3932000: loss = 4.860935688018799\n",
      "step = 3932200: loss = 4.781282901763916\n",
      "step = 3932400: loss = 4.164431571960449\n",
      "step = 3932600: loss = 3.080947160720825\n",
      "step = 3932800: loss = 5.099719524383545\n",
      "step = 3933000: loss = 3.155460834503174\n",
      "step = 3933200: loss = 3.924877882003784\n",
      "step = 3933400: loss = 4.28382682800293\n",
      "step = 3933600: loss = 4.262457847595215\n",
      "step = 3933800: loss = 3.7207469940185547\n",
      "step = 3934000: loss = 4.243617534637451\n",
      "step = 3934200: loss = 4.667108535766602\n",
      "step = 3934400: loss = 3.6977202892303467\n",
      "step = 3934600: loss = 4.095370769500732\n",
      "step = 3934800: loss = 3.665147542953491\n",
      "step = 3935000: loss = 4.670408725738525\n",
      "step = 3935000: Average Return = 3.384000062942505\n",
      "step = 3935200: loss = 4.375087261199951\n",
      "step = 3935400: loss = 4.141351699829102\n",
      "step = 3935600: loss = 3.9480462074279785\n",
      "step = 3935800: loss = 4.807497024536133\n",
      "step = 3936000: loss = 4.3600873947143555\n",
      "step = 3936200: loss = 4.777824878692627\n",
      "step = 3936400: loss = 2.3876466751098633\n",
      "step = 3936600: loss = 3.1676299571990967\n",
      "step = 3936800: loss = 4.00351619720459\n",
      "step = 3937000: loss = 2.768674373626709\n",
      "step = 3937200: loss = 3.57139253616333\n",
      "step = 3937400: loss = 3.0943095684051514\n",
      "step = 3937600: loss = 4.462618827819824\n",
      "step = 3937800: loss = 3.970546245574951\n",
      "step = 3938000: loss = 5.707112789154053\n",
      "step = 3938200: loss = 4.111183166503906\n",
      "step = 3938400: loss = 3.538137674331665\n",
      "step = 3938600: loss = 4.450256824493408\n",
      "step = 3938800: loss = 5.0283074378967285\n",
      "step = 3939000: loss = 4.522831439971924\n",
      "step = 3939200: loss = 3.9198055267333984\n",
      "step = 3939400: loss = 4.409426212310791\n",
      "step = 3939600: loss = 4.731769561767578\n",
      "step = 3939800: loss = 3.5911970138549805\n",
      "step = 3940000: loss = 4.447627544403076\n",
      "step = 3940000: Average Return = 4.291999816894531\n",
      "step = 3940200: loss = 4.782063961029053\n",
      "step = 3940400: loss = 2.6568126678466797\n",
      "step = 3940600: loss = 4.840163707733154\n",
      "step = 3940800: loss = 4.977414608001709\n",
      "step = 3941000: loss = 3.4245386123657227\n",
      "step = 3941200: loss = 4.192661762237549\n",
      "step = 3941400: loss = 3.093642473220825\n",
      "step = 3941600: loss = 5.341214656829834\n",
      "step = 3941800: loss = 6.34278678894043\n",
      "step = 3942000: loss = 4.441271781921387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3942200: loss = 4.839056968688965\n",
      "step = 3942400: loss = 3.574069023132324\n",
      "step = 3942600: loss = 5.500217914581299\n",
      "step = 3942800: loss = 6.385134220123291\n",
      "step = 3943000: loss = 4.23659610748291\n",
      "step = 3943200: loss = 5.12010383605957\n",
      "step = 3943400: loss = 5.040109634399414\n",
      "step = 3943600: loss = 4.114675998687744\n",
      "step = 3943800: loss = 3.3544349670410156\n",
      "step = 3944000: loss = 3.44779372215271\n",
      "step = 3944200: loss = 4.418452262878418\n",
      "step = 3944400: loss = 3.0448694229125977\n",
      "step = 3944600: loss = 5.002497673034668\n",
      "step = 3944800: loss = 4.403604030609131\n",
      "step = 3945000: loss = 3.901146650314331\n",
      "step = 3945000: Average Return = 3.624000072479248\n",
      "step = 3945200: loss = 3.780406951904297\n",
      "step = 3945400: loss = 6.094177722930908\n",
      "step = 3945600: loss = 4.552552700042725\n",
      "step = 3945800: loss = 4.800004959106445\n",
      "step = 3946000: loss = 3.245525360107422\n",
      "step = 3946200: loss = 4.630110740661621\n",
      "step = 3946400: loss = 4.857976913452148\n",
      "step = 3946600: loss = 4.067746639251709\n",
      "step = 3946800: loss = 4.082178115844727\n",
      "step = 3947000: loss = 4.610555171966553\n",
      "step = 3947200: loss = 4.201431751251221\n",
      "step = 3947400: loss = 3.871783971786499\n",
      "step = 3947600: loss = 4.3938889503479\n",
      "step = 3947800: loss = 6.252394676208496\n",
      "step = 3948000: loss = 4.814942359924316\n",
      "step = 3948200: loss = 3.982819080352783\n",
      "step = 3948400: loss = 3.811945676803589\n",
      "step = 3948600: loss = 4.301208019256592\n",
      "step = 3948800: loss = 3.5173864364624023\n",
      "step = 3949000: loss = 4.628168106079102\n",
      "step = 3949200: loss = 4.778596878051758\n",
      "step = 3949400: loss = 3.6785120964050293\n",
      "step = 3949600: loss = 4.565512180328369\n",
      "step = 3949800: loss = 4.9519453048706055\n",
      "step = 3950000: loss = 4.425273418426514\n",
      "step = 3950000: Average Return = 4.01800012588501\n",
      "step = 3950200: loss = 4.936685562133789\n",
      "step = 3950400: loss = 3.4026970863342285\n",
      "step = 3950600: loss = 3.9253411293029785\n",
      "step = 3950800: loss = 5.05510139465332\n",
      "step = 3951000: loss = 3.5361711978912354\n",
      "step = 3951200: loss = 5.889835834503174\n",
      "step = 3951400: loss = 4.545264720916748\n",
      "step = 3951600: loss = 4.527164936065674\n",
      "step = 3951800: loss = 3.7475039958953857\n",
      "step = 3952000: loss = 3.845569372177124\n",
      "step = 3952200: loss = 3.473226547241211\n",
      "step = 3952400: loss = 3.313791513442993\n",
      "step = 3952600: loss = 3.2286338806152344\n",
      "step = 3952800: loss = 3.070441484451294\n",
      "step = 3953000: loss = 5.164304256439209\n",
      "step = 3953200: loss = 3.4974205493927\n",
      "step = 3953400: loss = 4.078857898712158\n",
      "step = 3953600: loss = 5.308577537536621\n",
      "step = 3953800: loss = 3.8881783485412598\n",
      "step = 3954000: loss = 4.2663960456848145\n",
      "step = 3954200: loss = 3.617475748062134\n",
      "step = 3954400: loss = 5.295815467834473\n",
      "step = 3954600: loss = 3.5587339401245117\n",
      "step = 3954800: loss = 4.489885330200195\n",
      "step = 3955000: loss = 3.484644651412964\n",
      "step = 3955000: Average Return = 3.5940001010894775\n",
      "step = 3955200: loss = 3.9190831184387207\n",
      "step = 3955400: loss = 4.319138050079346\n",
      "step = 3955600: loss = 5.441578388214111\n",
      "step = 3955800: loss = 4.5514068603515625\n",
      "step = 3956000: loss = 4.675572872161865\n",
      "step = 3956200: loss = 4.050482749938965\n",
      "step = 3956400: loss = 4.871617794036865\n",
      "step = 3956600: loss = 3.504441976547241\n",
      "step = 3956800: loss = 2.9285454750061035\n",
      "step = 3957000: loss = 3.3103044033050537\n",
      "step = 3957200: loss = 3.808350086212158\n",
      "step = 3957400: loss = 2.0067899227142334\n",
      "step = 3957600: loss = 4.2253265380859375\n",
      "step = 3957800: loss = 4.408488750457764\n",
      "step = 3958000: loss = 4.704316139221191\n",
      "step = 3958200: loss = 4.159354209899902\n",
      "step = 3958400: loss = 7.27430534362793\n",
      "step = 3958600: loss = 3.805394411087036\n",
      "step = 3958800: loss = 3.550534248352051\n",
      "step = 3959000: loss = 3.7243926525115967\n",
      "step = 3959200: loss = 3.0322718620300293\n",
      "step = 3959400: loss = 3.230118989944458\n",
      "step = 3959600: loss = 5.761867046356201\n",
      "step = 3959800: loss = 4.485716819763184\n",
      "step = 3960000: loss = 4.935911655426025\n",
      "step = 3960000: Average Return = 4.035999774932861\n",
      "step = 3960200: loss = 3.5029876232147217\n",
      "step = 3960400: loss = 4.695832252502441\n",
      "step = 3960600: loss = 3.6018645763397217\n",
      "step = 3960800: loss = 4.430843830108643\n",
      "step = 3961000: loss = 4.2078776359558105\n",
      "step = 3961200: loss = 3.6656341552734375\n",
      "step = 3961400: loss = 5.028801441192627\n",
      "step = 3961600: loss = 2.9359166622161865\n",
      "step = 3961800: loss = 4.3179144859313965\n",
      "step = 3962000: loss = 3.96903920173645\n",
      "step = 3962200: loss = 3.363401412963867\n",
      "step = 3962400: loss = 4.452599048614502\n",
      "step = 3962600: loss = 4.992201328277588\n",
      "step = 3962800: loss = 3.8754899501800537\n",
      "step = 3963000: loss = 4.432926177978516\n",
      "step = 3963200: loss = 4.414090633392334\n",
      "step = 3963400: loss = 6.304313659667969\n",
      "step = 3963600: loss = 3.150171995162964\n",
      "step = 3963800: loss = 5.403364181518555\n",
      "step = 3964000: loss = 4.390625953674316\n",
      "step = 3964200: loss = 3.3687167167663574\n",
      "step = 3964400: loss = 4.488077163696289\n",
      "step = 3964600: loss = 3.9265215396881104\n",
      "step = 3964800: loss = 4.6764726638793945\n",
      "step = 3965000: loss = 4.841585636138916\n",
      "step = 3965000: Average Return = 4.050000190734863\n",
      "step = 3965200: loss = 2.935328245162964\n",
      "step = 3965400: loss = 4.946984767913818\n",
      "step = 3965600: loss = 4.86582612991333\n",
      "step = 3965800: loss = 3.9688780307769775\n",
      "step = 3966000: loss = 2.498810291290283\n",
      "step = 3966200: loss = 3.525362968444824\n",
      "step = 3966400: loss = 5.2685112953186035\n",
      "step = 3966600: loss = 3.8076171875\n",
      "step = 3966800: loss = 4.545377254486084\n",
      "step = 3967000: loss = 4.790377616882324\n",
      "step = 3967200: loss = 5.373941421508789\n",
      "step = 3967400: loss = 4.455748081207275\n",
      "step = 3967600: loss = 3.21872615814209\n",
      "step = 3967800: loss = 4.9317145347595215\n",
      "step = 3968000: loss = 3.3659307956695557\n",
      "step = 3968200: loss = 4.415576457977295\n",
      "step = 3968400: loss = 4.3871169090271\n",
      "step = 3968600: loss = 3.79280161857605\n",
      "step = 3968800: loss = 4.24542236328125\n",
      "step = 3969000: loss = 4.375736713409424\n",
      "step = 3969200: loss = 3.8137612342834473\n",
      "step = 3969400: loss = 3.878115177154541\n",
      "step = 3969600: loss = 4.091475963592529\n",
      "step = 3969800: loss = 3.3886091709136963\n",
      "step = 3970000: loss = 3.8522393703460693\n",
      "step = 3970000: Average Return = 3.75\n",
      "step = 3970200: loss = 3.933067560195923\n",
      "step = 3970400: loss = 3.2234420776367188\n",
      "step = 3970600: loss = 4.004797458648682\n",
      "step = 3970800: loss = 4.014126300811768\n",
      "step = 3971000: loss = 3.878431558609009\n",
      "step = 3971200: loss = 4.9350175857543945\n",
      "step = 3971400: loss = 3.2872509956359863\n",
      "step = 3971600: loss = 3.1704256534576416\n",
      "step = 3971800: loss = 3.6339666843414307\n",
      "step = 3972000: loss = 3.173264503479004\n",
      "step = 3972200: loss = 3.001560926437378\n",
      "step = 3972400: loss = 4.1724853515625\n",
      "step = 3972600: loss = 4.9228901863098145\n",
      "step = 3972800: loss = 3.2585506439208984\n",
      "step = 3973000: loss = 3.6575636863708496\n",
      "step = 3973200: loss = 3.3774425983428955\n",
      "step = 3973400: loss = 3.4181621074676514\n",
      "step = 3973600: loss = 4.241129398345947\n",
      "step = 3973800: loss = 3.3777782917022705\n",
      "step = 3974000: loss = 3.2400219440460205\n",
      "step = 3974200: loss = 4.392758846282959\n",
      "step = 3974400: loss = 3.5411086082458496\n",
      "step = 3974600: loss = 5.133471488952637\n",
      "step = 3974800: loss = 4.637651443481445\n",
      "step = 3975000: loss = 3.5457494258880615\n",
      "step = 3975000: Average Return = 3.875999927520752\n",
      "step = 3975200: loss = 3.7248122692108154\n",
      "step = 3975400: loss = 5.13359260559082\n",
      "step = 3975600: loss = 3.1422502994537354\n",
      "step = 3975800: loss = 5.879372596740723\n",
      "step = 3976000: loss = 4.851609706878662\n",
      "step = 3976200: loss = 3.5342156887054443\n",
      "step = 3976400: loss = 4.9505767822265625\n",
      "step = 3976600: loss = 4.159578323364258\n",
      "step = 3976800: loss = 3.7536516189575195\n",
      "step = 3977000: loss = 4.995182514190674\n",
      "step = 3977200: loss = 4.7330427169799805\n",
      "step = 3977400: loss = 4.333815574645996\n",
      "step = 3977600: loss = 4.387202739715576\n",
      "step = 3977800: loss = 5.471320629119873\n",
      "step = 3978000: loss = 3.881152868270874\n",
      "step = 3978200: loss = 3.900163412094116\n",
      "step = 3978400: loss = 3.8827409744262695\n",
      "step = 3978600: loss = 3.785714626312256\n",
      "step = 3978800: loss = 2.6941206455230713\n",
      "step = 3979000: loss = 4.927363395690918\n",
      "step = 3979200: loss = 3.5224251747131348\n",
      "step = 3979400: loss = 2.96891713142395\n",
      "step = 3979600: loss = 4.531070232391357\n",
      "step = 3979800: loss = 5.60125732421875\n",
      "step = 3980000: loss = 4.367778301239014\n",
      "step = 3980000: Average Return = 4.03000020980835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 3980200: loss = 3.544733762741089\n",
      "step = 3980400: loss = 4.970743179321289\n",
      "step = 3980600: loss = 3.505690813064575\n",
      "step = 3980800: loss = 3.2888243198394775\n",
      "step = 3981000: loss = 5.06947660446167\n",
      "step = 3981200: loss = 3.893873929977417\n",
      "step = 3981400: loss = 4.138236999511719\n",
      "step = 3981600: loss = 4.618637561798096\n",
      "step = 3981800: loss = 3.837792158126831\n",
      "step = 3982000: loss = 4.851649761199951\n",
      "step = 3982200: loss = 5.216436862945557\n",
      "step = 3982400: loss = 2.881943941116333\n",
      "step = 3982600: loss = 3.8555288314819336\n",
      "step = 3982800: loss = 4.794754981994629\n",
      "step = 3983000: loss = 5.8643012046813965\n",
      "step = 3983200: loss = 3.6659300327301025\n",
      "step = 3983400: loss = 4.48227071762085\n",
      "step = 3983600: loss = 4.088242530822754\n",
      "step = 3983800: loss = 3.6875576972961426\n",
      "step = 3984000: loss = 3.8256824016571045\n",
      "step = 3984200: loss = 3.579827308654785\n",
      "step = 3984400: loss = 4.529491901397705\n",
      "step = 3984600: loss = 3.9188456535339355\n",
      "step = 3984800: loss = 3.1455657482147217\n",
      "step = 3985000: loss = 4.120941162109375\n",
      "step = 3985000: Average Return = 4.0879998207092285\n",
      "step = 3985200: loss = 6.115040302276611\n",
      "step = 3985400: loss = 4.718416213989258\n",
      "step = 3985600: loss = 4.893028259277344\n",
      "step = 3985800: loss = 4.09222412109375\n",
      "step = 3986000: loss = 3.7460222244262695\n",
      "step = 3986200: loss = 4.174263000488281\n",
      "step = 3986400: loss = 4.900248050689697\n",
      "step = 3986600: loss = 4.234181880950928\n",
      "step = 3986800: loss = 3.7833197116851807\n",
      "step = 3987000: loss = 4.417069435119629\n",
      "step = 3987200: loss = 5.6854777336120605\n",
      "step = 3987400: loss = 3.7551536560058594\n",
      "step = 3987600: loss = 4.620124340057373\n",
      "step = 3987800: loss = 3.7167956829071045\n",
      "step = 3988000: loss = 4.587835311889648\n",
      "step = 3988200: loss = 3.7580511569976807\n",
      "step = 3988400: loss = 3.6100072860717773\n",
      "step = 3988600: loss = 3.778764009475708\n",
      "step = 3988800: loss = 3.904346227645874\n",
      "step = 3989000: loss = 3.9626781940460205\n",
      "step = 3989200: loss = 3.9288649559020996\n",
      "step = 3989400: loss = 4.128226280212402\n",
      "step = 3989600: loss = 3.5977630615234375\n",
      "step = 3989800: loss = 4.068087577819824\n",
      "step = 3990000: loss = 5.470956802368164\n",
      "step = 3990000: Average Return = 3.6040000915527344\n",
      "step = 3990200: loss = 4.185329437255859\n",
      "step = 3990400: loss = 4.1967597007751465\n",
      "step = 3990600: loss = 4.500508785247803\n",
      "step = 3990800: loss = 4.40484619140625\n",
      "step = 3991000: loss = 5.479709148406982\n",
      "step = 3991200: loss = 4.661221027374268\n",
      "step = 3991400: loss = 4.546252727508545\n",
      "step = 3991600: loss = 3.9904253482818604\n",
      "step = 3991800: loss = 3.9130773544311523\n",
      "step = 3992000: loss = 3.947512626647949\n",
      "step = 3992200: loss = 4.333837509155273\n",
      "step = 3992400: loss = 4.790587425231934\n",
      "step = 3992600: loss = 4.807610034942627\n",
      "step = 3992800: loss = 3.872959852218628\n",
      "step = 3993000: loss = 3.902263641357422\n",
      "step = 3993200: loss = 4.630240440368652\n",
      "step = 3993400: loss = 4.815138816833496\n",
      "step = 3993600: loss = 3.338082790374756\n",
      "step = 3993800: loss = 2.622161865234375\n",
      "step = 3994000: loss = 3.072904348373413\n",
      "step = 3994200: loss = 5.36043119430542\n",
      "step = 3994400: loss = 3.6939802169799805\n",
      "step = 3994600: loss = 4.906694412231445\n",
      "step = 3994800: loss = 4.149637222290039\n",
      "step = 3995000: loss = 4.0444865226745605\n",
      "step = 3995000: Average Return = 3.9739999771118164\n",
      "step = 3995200: loss = 2.6742682456970215\n",
      "step = 3995400: loss = 5.428703308105469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m collect_data(train_env, agent\u001b[38;5;241m.\u001b[39mcollect_policy, replay_buffer, collect_steps_per_iteration)\n\u001b[0;32m      4\u001b[0m experience, unused_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m----> 5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m      7\u001b[0m step \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_step_counter\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\opencv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(num_iterations):\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        train_checkpointer.save(train_step_counter)\n",
    "        with open(\"CheckpointsRC/returns.txt\", \"w\") as txt:\n",
    "            for item in returns:\n",
    "                txt.write(str(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eeacc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1109, 10.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, train_step_counter + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
